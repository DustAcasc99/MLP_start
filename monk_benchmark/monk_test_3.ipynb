{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KjmHOAvpT9v3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from train_val import search_space_dict, sigmoid, performing_tv \n",
        "\n",
        "from train_val import train_test\n",
        "\n",
        "#perform one-hot-encoding tranformation\n",
        "\n",
        "monks_3_train = pd.read_csv('monks-3.train', header=None, sep = '\\s+', index_col=None,\n",
        "                            names = ['class','a1','a2','a3','a4','a5','a6','Id'])\n",
        "\n",
        "monks_3_test = pd.read_csv('monks-3.test', header=None, sep = '\\s+', index_col=None,\n",
        "                            names = ['class','a1','a2','a3','a4','a5','a6','Id'])\n",
        "\n",
        "\n",
        "monks_3_train[['a1','a2','a3','a4','a5','a6']] = monks_3_train[['a1','a2','a3','a4','a5','a6']].astype(str)\n",
        "monks_3_test[['a1','a2','a3','a4','a5','a6']] = monks_3_test[['a1','a2','a3','a4','a5','a6']].astype(str)\n",
        "  \n",
        "one_hot_monks_3_train = pd.get_dummies(monks_3_train, columns = ['a1','a2','a3','a4','a5','a6'])\n",
        "one_hot_monks_3_test = pd.get_dummies(monks_3_test, columns = ['a1','a2','a3','a4','a5','a6'])\n",
        "\n",
        "train_columns = [x for x in one_hot_monks_3_train.columns[2:]] + [x for x in one_hot_monks_3_train.columns[:2]]\n",
        "test_columns = [x for x in one_hot_monks_3_test.columns[2:]] + [x for x in one_hot_monks_3_test.columns[:2]]\n",
        "\n",
        "one_hot_monks_3_train = one_hot_monks_3_train.reindex(columns = train_columns)\n",
        "one_hot_monks_3_test = one_hot_monks_3_test.reindex(columns = test_columns)\n",
        "\n",
        "one_hot_monks_3_train = one_hot_monks_3_train.drop(['Id'], axis = 1)\n",
        "one_hot_monks_3_test = one_hot_monks_3_test.drop(['Id'], axis = 1)\n",
        "\n",
        "X_train = np.array(one_hot_monks_3_train)\n",
        "X_test = np.array(one_hot_monks_3_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5cLHoPNUaaI",
        "outputId": "4ba47f70-dcd8-4d22-e562-15b9b7f34f24"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zypbPnwXjXio"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with no lambda value (slight overfitting noticeable looking at accuracy decrease in the test set."
      ],
      "metadata": {
        "id": "rXKM66nHjYH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_space_def = search_space_dict(layers_range=[1], units_range=[4], eta_0_range=[0.6],\n",
        "                        alpha_range=[0.4], lamb_range=[0.0], lamb0_range = [0.0], minibatch_size_range = [40], num_targets=1, configurations = 0)    \n",
        "\n",
        "search_space_def[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKnJ0SUtT_Mp",
        "outputId": "414f50bf-b6ba-4679-da45-27abb37de5a7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'units': array([4, 1]),\n",
              " 'layers': 2,\n",
              " 'eta_0': 0.6,\n",
              " 'alpha': 0.4,\n",
              " 'lamb': 0.0,\n",
              " 'lamb0': 0.0,\n",
              " 'minibatch_size': 40}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_test(hyperparams = search_space_def[0][0],\n",
        "           num_inputs = 17, seed = 0, activation_output = sigmoid, activation_hidden = sigmoid,\n",
        "           task = 'binary_classification', thr = 0.5, stop_class = 'GL', stop_param = 9, data_train = X_train, data_val = X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GyIQ06jWT_Og",
        "outputId": "2444854c-7f19-4399-fdd8-2cded25a4959"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error 0.12505322185931383, test error 0.24997605484727914\n",
            "training error 0.12500647441850912, test error 0.25003754993240124\n",
            "training error 0.12503814431134083, test error 0.2500643162270855\n",
            "training error 0.12502297427454043, test error 0.2501170761707759\n",
            "training error 0.1249987675209605, test error 0.2501711552970117\n",
            "training error 0.1250190693107748, test error 0.25020029303200975\n",
            "training error 0.12508462493308722, test error 0.25031728762662475\n",
            "training error 0.1251106528464938, test error 0.25026250257323845\n",
            "training error 0.12513395715201533, test error 0.2503654567150016\n",
            "training error 0.1250469679868615, test error 0.250379830697555\n",
            "training error 0.1250648426890075, test error 0.2504545477447767\n",
            "training error 0.12498192932107192, test error 0.2504077306341252\n",
            "training error 0.12499080157156231, test error 0.25042768890815587\n",
            "training error 0.1250204396964851, test error 0.2504541031399556\n",
            "training error 0.12522190897033103, test error 0.2503203501610822\n",
            "training error 0.12498407027845007, test error 0.2504188036438594\n",
            "training error 0.12497749668541712, test error 0.25048965650251326\n",
            "training error 0.12497992989273209, test error 0.2505486608628326\n",
            "training error 0.12498486198470403, test error 0.2506059831980654\n",
            "training error 0.12503011152836188, test error 0.2506123507946206\n",
            "training error 0.12497239790469625, test error 0.2507087958047726\n",
            "training error 0.12496901592255465, test error 0.2505968470998143\n",
            "training error 0.12504522539834792, test error 0.25069596086182583\n",
            "training error 0.1250717504638888, test error 0.25077555509731875\n",
            "training error 0.12502150005763, test error 0.25068209910389944\n",
            "training error 0.12503895467118603, test error 0.2508768870884264\n",
            "training error 0.12496680487933876, test error 0.2507732734227954\n",
            "training error 0.12496277726940912, test error 0.25075031363783634\n",
            "training error 0.12501571865116948, test error 0.2507396208321157\n",
            "training error 0.12512827710743332, test error 0.2506543073617167\n",
            "training error 0.12505873156465652, test error 0.2505843912111249\n",
            "training error 0.12495681334663619, test error 0.25058717274204545\n",
            "training error 0.12506858479400837, test error 0.2504442515729079\n",
            "training error 0.12498600107042618, test error 0.2505337899041193\n",
            "training error 0.12502966680984837, test error 0.25059169038326584\n",
            "training error 0.12510249381002927, test error 0.25059755085135954\n",
            "training error 0.12495410115328097, test error 0.2505360649555339\n",
            "training error 0.12498681944033546, test error 0.2505776920143905\n",
            "training error 0.1251841445297477, test error 0.25037163541919893\n",
            "training error 0.12497442864538978, test error 0.2504597034622971\n",
            "training error 0.125022455664839, test error 0.25059344043322207\n",
            "training error 0.1250122004988028, test error 0.2504784962837506\n",
            "training error 0.1250852053055658, test error 0.25057529495295233\n",
            "training error 0.1251277871571127, test error 0.2504696732425685\n",
            "training error 0.12521883009515322, test error 0.25066875280043666\n",
            "training error 0.12494057275978664, test error 0.2505594972443889\n",
            "training error 0.12501878391167687, test error 0.2505472551594782\n",
            "training error 0.12504979376375883, test error 0.2504337454203286\n",
            "training error 0.1252458539696801, test error 0.25071258691499465\n",
            "training error 0.12494574899360192, test error 0.2505317780690473\n",
            "Loss: 0.22231058175059548\n",
            "training error 0.12510246559094612, test error 0.25058774756797625\n",
            "Loss: 0.2447005258446877\n",
            "training error 0.12497023804421116, test error 0.25054129503850364\n",
            "Loss: 0.22611773418450554\n",
            "training error 0.1249782803003458, test error 0.25053923995814037\n",
            "Loss: 0.2252956232969172\n",
            "training error 0.12504021686085107, test error 0.2506114673049169\n",
            "Loss: 0.25418932946437156\n",
            "training error 0.125011013215478, test error 0.25054531161548665\n",
            "Loss: 0.22772451887653755\n",
            "training error 0.12495168383167954, test error 0.2505899039492487\n",
            "Loss: 0.24556316097739117\n",
            "training error 0.12503805388649145, test error 0.2505509768393909\n",
            "Loss: 0.22999082550647643\n",
            "training error 0.12494091586279238, test error 0.25064592898074217\n",
            "Loss: 0.26797532022508896\n",
            "training error 0.1250244382619319, test error 0.25074966477642235\n",
            "Loss: 0.30947361322901656\n",
            "training error 0.12494556316743886, test error 0.25062761805404243\n",
            "Loss: 0.26065024794528213\n",
            "training error 0.12495084386651024, test error 0.2506525028850423\n",
            "Loss: 0.27060513383030393\n",
            "training error 0.12494166257449166, test error 0.25061458170333056\n",
            "Loss: 0.25543520816084797\n",
            "training error 0.12494904044775529, test error 0.25055684704297543\n",
            "Loss: 0.23233913186249122\n",
            "training error 0.12498574330132024, test error 0.2506098035302786\n",
            "Loss: 0.25352375586000786\n",
            "training error 0.12497642953296975, test error 0.250578637099639\n",
            "Loss: 0.2410559894338693\n",
            "training error 0.12494897365728516, test error 0.2507020858865966\n",
            "Loss: 0.2904402342700507\n",
            "training error 0.12496667251016003, test error 0.25073409878165803\n",
            "Loss: 0.30324661889795657\n",
            "training error 0.12496027523828646, test error 0.2507735100582026\n",
            "Loss: 0.31901263959488446\n",
            "training error 0.12500924282984927, test error 0.25066911997153\n",
            "Loss: 0.2772526051242341\n",
            "training error 0.12493488459718148, test error 0.2508068391754585\n",
            "Loss: 0.33234556353283207\n",
            "training error 0.12499491546229934, test error 0.25093213161761385\n",
            "Loss: 0.38246734108946523\n",
            "training error 0.12493448812167104, test error 0.2508390639178871\n",
            "Loss: 0.34523669522474876\n",
            "training error 0.12516133099682383, test error 0.2509858616703897\n",
            "Loss: 0.40396142091589926\n",
            "training error 0.12495633799975382, test error 0.2508314815826805\n",
            "Loss: 0.3422034706179966\n",
            "training error 0.12498560426247612, test error 0.2508278335466684\n",
            "Loss: 0.3407441164353209\n",
            "training error 0.12498776570805842, test error 0.2507926965567111\n",
            "Loss: 0.3266879741465223\n",
            "training error 0.12493326975829772, test error 0.25080290866392946\n",
            "Loss: 0.33077320832008983\n",
            "training error 0.1250207464208221, test error 0.25094129503889856\n",
            "Loss: 0.38613306070820475\n",
            "training error 0.12502537365229574, test error 0.25071588306289366\n",
            "Loss: 0.2959596334402903\n",
            "training error 0.125047121141787, test error 0.2507104565879397\n",
            "Loss: 0.29378883553836665\n",
            "training error 0.12494456176234499, test error 0.25076776047431926\n",
            "Loss: 0.3167125857409747\n",
            "training error 0.12497731482250596, test error 0.25081657852725125\n",
            "Loss: 0.3362416774221222\n",
            "training error 0.12496147527169178, test error 0.2507642364886434\n",
            "Loss: 0.31530285644589817\n",
            "training error 0.12492142482382489, test error 0.25073388848453054\n",
            "Loss: 0.3031624919892506\n",
            "training error 0.12491452280511142, test error 0.2507556220417441\n",
            "Loss: 0.31185674761577964\n",
            "training error 0.1249267131167528, test error 0.25066531351143173\n",
            "Loss: 0.2757298752369186\n",
            "training error 0.1250057398216268, test error 0.25079351317900533\n",
            "Loss: 0.3270146543538299\n",
            "training error 0.12498036412404291, test error 0.2506266492670064\n",
            "Loss: 0.26026269601091556\n",
            "training error 0.12496162442227494, test error 0.2505799413887418\n",
            "Loss: 0.24157775505000423\n",
            "training error 0.12500915807177138, test error 0.2506462878467728\n",
            "Loss: 0.26811888038760223\n",
            "training error 0.1249540298484132, test error 0.250733796963035\n",
            "Loss: 0.30312587988430284\n",
            "training error 0.12490928461690913, test error 0.25087091697798525\n",
            "Loss: 0.35797913974313644\n",
            "training error 0.12504918202874407, test error 0.25089493331287\n",
            "Loss: 0.36758659390485615\n",
            "training error 0.124959409627409, test error 0.2507553368642173\n",
            "Loss: 0.3117426656782163\n",
            "training error 0.12499077213781899, test error 0.25066964384417584\n",
            "Loss: 0.27746217425521635\n",
            "training error 0.12491694785633901, test error 0.2506578130882082\n",
            "Loss: 0.27272941856195665\n",
            "training error 0.12490946725394021, test error 0.25060784276227155\n",
            "Loss: 0.25273937352856546\n",
            "training error 0.12496361461017824, test error 0.25067793546911316\n",
            "Loss: 0.2807791419313421\n",
            "training error 0.1248986767603601, test error 0.25061411042201054\n",
            "Loss: 0.25524667757526043\n",
            "training error 0.12503004071563253, test error 0.25065770752490885\n",
            "Loss: 0.2726871891974403\n",
            "training error 0.12489837592937748, test error 0.25068583378101095\n",
            "Loss: 0.2839387693215123\n",
            "training error 0.12496902856349802, test error 0.25069779211493975\n",
            "Loss: 0.2887225610875177\n",
            "training error 0.1249168035614294, test error 0.2507326220880615\n",
            "Loss: 0.3026558848784999\n",
            "training error 0.12506572193880383, test error 0.2509150705203298\n",
            "Loss: 0.37564224846429184\n",
            "training error 0.12497180860423919, test error 0.2509697794271121\n",
            "Loss: 0.3975279073990201\n",
            "training error 0.12495903525809143, test error 0.25094610051228167\n",
            "Loss: 0.38805543418756194\n",
            "training error 0.12513851947805707, test error 0.25088428353984704\n",
            "Loss: 0.3633262766398859\n",
            "training error 0.1249472005872453, test error 0.2507795758061379\n",
            "Loss: 0.3214391711836795\n",
            "training error 0.12491510878138704, test error 0.2507837758585892\n",
            "Loss: 0.32311935309303763\n",
            "training error 0.12497108715521996, test error 0.2508902665081363\n",
            "Loss: 0.3657196931984741\n",
            "training error 0.12495458223680822, test error 0.2509502642349062\n",
            "Loss: 0.38972108277419704\n",
            "training error 0.12489278246156077, test error 0.2508222992694461\n",
            "Loss: 0.33853019349552316\n",
            "training error 0.12489520608537569, test error 0.25086206355549745\n",
            "Loss: 0.35443743152103746\n",
            "training error 0.12491417241631644, test error 0.25094330311659546\n",
            "Loss: 0.38693636872830783\n",
            "training error 0.12495347320431575, test error 0.25080487024812015\n",
            "Loss: 0.3315579171562444\n",
            "training error 0.12490407739305052, test error 0.25091146082043236\n",
            "Loss: 0.37419823019637644\n",
            "training error 0.12495137115720986, test error 0.2508677541542439\n",
            "Loss: 0.3567138890601118\n",
            "training error 0.12488766738190213, test error 0.2509380817857933\n",
            "Loss: 0.38484763634736296\n",
            "training error 0.12485687459095428, test error 0.25092186187445703\n",
            "Loss: 0.37835905033212214\n",
            "training error 0.12494141669250769, test error 0.25086299623185493\n",
            "Loss: 0.3548105378003763\n",
            "training error 0.124871757312004, test error 0.2508269481911528\n",
            "Loss: 0.3403899403058963\n",
            "training error 0.12488618156080751, test error 0.25074825093701963\n",
            "Loss: 0.30890802329537337\n",
            "training error 0.12485374540332249, test error 0.25074966326528403\n",
            "Loss: 0.3094730087157771\n",
            "training error 0.12485265627745147, test error 0.25075838433749537\n",
            "Loss: 0.31296177175617057\n",
            "training error 0.12502326675952866, test error 0.2508047695661516\n",
            "Loss: 0.3315176405111142\n",
            "training error 0.12484472548148186, test error 0.2506892861659209\n",
            "Loss: 0.285319855566768\n",
            "training error 0.12483497275635275, test error 0.2506826701447382\n",
            "Loss: 0.2826731935947846\n",
            "training error 0.12486442330601905, test error 0.2507319429731956\n",
            "Loss: 0.302384212911222\n",
            "training error 0.12484451893286021, test error 0.2507037641769428\n",
            "Loss: 0.2911116147137571\n",
            "training error 0.12485954268696252, test error 0.2507080188879155\n",
            "Loss: 0.29281366212596804\n",
            "training error 0.12482647521388777, test error 0.25066385994726137\n",
            "Loss: 0.27514839387414725\n",
            "training error 0.12504663467067204, test error 0.25068793947660095\n",
            "Loss: 0.28478112823915325\n",
            "training error 0.12503643734623263, test error 0.2506640595455324\n",
            "Loss: 0.2752282408303497\n",
            "training error 0.12483356205038451, test error 0.2506972228558165\n",
            "Loss: 0.28849483562654044\n",
            "training error 0.12486062909945744, test error 0.2507273091923805\n",
            "Loss: 0.3005305230376232\n",
            "training error 0.1248567093302221, test error 0.2507401667418917\n",
            "Loss: 0.305674035490866\n",
            "training error 0.12485937122836385, test error 0.25075886823303334\n",
            "Loss: 0.3131553485122618\n",
            "training error 0.12488148497170407, test error 0.2507966080414138\n",
            "Loss: 0.328252717899713\n",
            "training error 0.12479976440820516, test error 0.25082400806227667\n",
            "Loss: 0.33921377610171\n",
            "training error 0.12478932281141455, test error 0.25076924595581207\n",
            "Loss: 0.3173068352556818\n",
            "training error 0.12480129212632901, test error 0.25087246048902717\n",
            "Loss: 0.35859660330093135\n",
            "training error 0.12478455697870988, test error 0.25092615999510104\n",
            "Loss: 0.3800784632761456\n",
            "training error 0.12475818696494827, test error 0.2508608751753101\n",
            "Loss: 0.35396203391222514\n",
            "training error 0.12486651065757064, test error 0.2507672333279191\n",
            "Loss: 0.3165017069828302\n",
            "training error 0.12489175942450026, test error 0.2508420540105928\n",
            "Loss: 0.3464328468751665\n",
            "training error 0.12475003098584969, test error 0.250868637386496\n",
            "Loss: 0.35706721580279766\n",
            "training error 0.12473192038872302, test error 0.2508673190550019\n",
            "Loss: 0.3565398326921043\n",
            "training error 0.12476692199487438, test error 0.25085436082019147\n",
            "Loss: 0.35135604226130823\n",
            "training error 0.1247137127428898, test error 0.25080809930164416\n",
            "Loss: 0.33284966228999124\n",
            "training error 0.12479955454838706, test error 0.25077762940624565\n",
            "Loss: 0.3206605366486981\n",
            "training error 0.12469766110711691, test error 0.25078303757255527\n",
            "Loss: 0.3228240103913649\n",
            "training error 0.12471911243179648, test error 0.2507014963334502\n",
            "Loss: 0.2902043904222218\n",
            "training error 0.12477735002890424, test error 0.2506612694849423\n",
            "Loss: 0.27411210969057187\n",
            "training error 0.12470280816553521, test error 0.2505155382890331\n",
            "Loss: 0.21581404750290467\n",
            "training error 0.12466246030505919, test error 0.2504956811884551\n",
            "Loss: 0.2078704464287373\n",
            "training error 0.12469003778262411, test error 0.25052501216481066\n",
            "Loss: 0.2196039608141298\n",
            "training error 0.12473874193850215, test error 0.2504561978484627\n",
            "Loss: 0.19207559759149007\n",
            "training error 0.1247127051599779, test error 0.25050035185577124\n",
            "Loss: 0.20973889231608034\n",
            "training error 0.12463584424312965, test error 0.2505322560664999\n",
            "Loss: 0.22250179904654832\n",
            "training error 0.1246485897851046, test error 0.25046699203568895\n",
            "Loss: 0.19639368607116126\n",
            "training error 0.1245884059539613, test error 0.25051017053774255\n",
            "Loss: 0.2136667413163762\n",
            "training error 0.12463921675155566, test error 0.25038187466576056\n",
            "Loss: 0.16234347674994876\n",
            "training error 0.1245665369987689, test error 0.25032486483530214\n",
            "Loss: 0.1395373601827954\n",
            "training error 0.12463815845426406, test error 0.2504567929254034\n",
            "Loss: 0.19231365116869625\n",
            "training error 0.12452580954931039, test error 0.2503849695424267\n",
            "Loss: 0.1635815459994472\n",
            "training error 0.12464024462022258, test error 0.2504577786681954\n",
            "Loss: 0.1927079860551295\n",
            "training error 0.12457711806849277, test error 0.25043107010497206\n",
            "Loss: 0.18202353740277566\n",
            "training error 0.12448338167069656, test error 0.25033349592867116\n",
            "Loss: 0.14299012823864032\n",
            "training error 0.12449749193259117, test error 0.25018415628603125\n",
            "Loss: 0.08324854909773638\n",
            "training error 0.12445963839898076, test error 0.2501741307075488\n",
            "Loss: 0.07923793356554576\n",
            "training error 0.12445160748658143, test error 0.2501007489924692\n",
            "Loss: 0.049882435846204665\n",
            "training error 0.12447568726369551, test error 0.25002441204846176\n",
            "Loss: 0.019344733323434227\n",
            "training error 0.1243955404495758, test error 0.2501231161643476\n",
            "Loss: 0.05883016161620791\n",
            "training error 0.12434663605358968, test error 0.2501053109460296\n",
            "Loss: 0.05170739206579178\n",
            "training error 0.12436489341196001, test error 0.2500552267735342\n",
            "Loss: 0.03167180404677339\n",
            "training error 0.1243360780293094, test error 0.25014683537664634\n",
            "Loss: 0.06831875535899812\n",
            "training error 0.12426157538038637, test error 0.2500846998193522\n",
            "Loss: 0.043462151660667026\n",
            "training error 0.12456847115549555, test error 0.2500933657121501\n",
            "Loss: 0.04692884082142523\n",
            "training error 0.1242322825288135, test error 0.24995348301831502\n",
            "Loss: 0.0\n",
            "training error 0.12430202424181788, test error 0.2499257473477676\n",
            "Loss: 0.0\n",
            "training error 0.12427234269510645, test error 0.2499879968845101\n",
            "Loss: 0.024907212403313395\n",
            "training error 0.12423616826489613, test error 0.24997096807911048\n",
            "Loss: 0.0180936665480802\n",
            "training error 0.12406708363065165, test error 0.24979451870125924\n",
            "Loss: 0.0\n",
            "training error 0.12403958665587801, test error 0.2496976021718806\n",
            "Loss: 0.0\n",
            "training error 0.12407763570736757, test error 0.24950564683423831\n",
            "Loss: 0.0\n",
            "training error 0.12416401248637296, test error 0.2495289722059918\n",
            "Loss: 0.009348634810257828\n",
            "training error 0.12390651028773587, test error 0.2492542719649985\n",
            "Loss: 0.0\n",
            "training error 0.12383387918470955, test error 0.2491795630089322\n",
            "Loss: 0.0\n",
            "training error 0.12383109953377067, test error 0.24909858856825093\n",
            "Loss: 0.0\n",
            "training error 0.12380663974608294, test error 0.24894250803291906\n",
            "Loss: 0.0\n",
            "training error 0.12370006939928259, test error 0.24897892621463957\n",
            "Loss: 0.01462915353760419\n",
            "training error 0.12364621006301202, test error 0.2488022328299372\n",
            "Loss: 0.0\n",
            "training error 0.12371061000132444, test error 0.24883310064971764\n",
            "Loss: 0.012406568634593107\n",
            "training error 0.12355682494987635, test error 0.24862712110312302\n",
            "Loss: 0.0\n",
            "training error 0.12354030375353997, test error 0.24842741261562387\n",
            "Loss: 0.0\n",
            "training error 0.12335006133585948, test error 0.24818064605457818\n",
            "Loss: 0.0\n",
            "training error 0.12332155058492567, test error 0.24800357246095373\n",
            "Loss: 0.0\n",
            "training error 0.12338731854093503, test error 0.24799820959338584\n",
            "Loss: 0.0\n",
            "training error 0.12311026140709529, test error 0.2476692359800033\n",
            "Loss: 0.0\n",
            "training error 0.1231382604162159, test error 0.24752929763253564\n",
            "Loss: 0.0\n",
            "training error 0.1229681847727121, test error 0.24740021741956605\n",
            "Loss: 0.0\n",
            "training error 0.12292672669059597, test error 0.2472150492255418\n",
            "Loss: 0.0\n",
            "training error 0.12276447477286817, test error 0.24691974573009054\n",
            "Loss: 0.0\n",
            "training error 0.1226445574361664, test error 0.24658162274297032\n",
            "Loss: 0.0\n",
            "training error 0.1226569960386987, test error 0.2462567670909574\n",
            "Loss: 0.0\n",
            "training error 0.12245100034439867, test error 0.24613929156027156\n",
            "Loss: 0.0\n",
            "training error 0.12238905443777717, test error 0.245875296128648\n",
            "Loss: 0.0\n",
            "training error 0.12222983613916182, test error 0.24558362092644898\n",
            "Loss: 0.0\n",
            "training error 0.12201356644716399, test error 0.24535173483728026\n",
            "Loss: 0.0\n",
            "training error 0.12190931811442621, test error 0.24503589111737198\n",
            "Loss: 0.0\n",
            "training error 0.12180450324432839, test error 0.24476070487432242\n",
            "Loss: 0.0\n",
            "training error 0.12158067869666014, test error 0.24447207867447193\n",
            "Loss: 0.0\n",
            "training error 0.12146411737070834, test error 0.24413019089197366\n",
            "Loss: 0.0\n",
            "training error 0.12135015989667933, test error 0.2438023707306981\n",
            "Loss: 0.0\n",
            "training error 0.1210810761752073, test error 0.24349255030832262\n",
            "Loss: 0.0\n",
            "training error 0.1209026751549504, test error 0.2431265566981205\n",
            "Loss: 0.0\n",
            "training error 0.12072559709155933, test error 0.24272271389014177\n",
            "Loss: 0.0\n",
            "training error 0.12046780656449017, test error 0.24229597872256917\n",
            "Loss: 0.0\n",
            "training error 0.12026640041214622, test error 0.24183565758462788\n",
            "Loss: 0.0\n",
            "training error 0.12005763373658855, test error 0.2413384649030138\n",
            "Loss: 0.0\n",
            "training error 0.11980425160010011, test error 0.2408830491447775\n",
            "Loss: 0.0\n",
            "training error 0.11957682813738363, test error 0.24038654059851275\n",
            "Loss: 0.0\n",
            "training error 0.11934110375771642, test error 0.23977453776596822\n",
            "Loss: 0.0\n",
            "training error 0.11909726778866388, test error 0.23914428411272914\n",
            "Loss: 0.0\n",
            "training error 0.11885481495874863, test error 0.23870437079276102\n",
            "Loss: 0.0\n",
            "training error 0.11861839782882673, test error 0.23795159786645484\n",
            "Loss: 0.0\n",
            "training error 0.11821133305756613, test error 0.2374517954303986\n",
            "Loss: 0.0\n",
            "training error 0.11797029904915493, test error 0.23682579710712906\n",
            "Loss: 0.0\n",
            "training error 0.11756426082660049, test error 0.23616786721449548\n",
            "Loss: 0.0\n",
            "training error 0.11726178716859384, test error 0.23544738850352329\n",
            "Loss: 0.0\n",
            "training error 0.11691490681732314, test error 0.23473846952644592\n",
            "Loss: 0.0\n",
            "training error 0.11650878454350115, test error 0.23386762465236813\n",
            "Loss: 0.0\n",
            "training error 0.11611546554739813, test error 0.23307061757615444\n",
            "Loss: 0.0\n",
            "training error 0.11573819041718261, test error 0.23232379216546228\n",
            "Loss: 0.0\n",
            "training error 0.11532885809270706, test error 0.2313957914095072\n",
            "Loss: 0.0\n",
            "training error 0.1148558518725833, test error 0.23061171700759042\n",
            "Loss: 0.0\n",
            "training error 0.11443319603651124, test error 0.22957027841598218\n",
            "Loss: 0.0\n",
            "training error 0.11409980475797371, test error 0.2286902551440738\n",
            "Loss: 0.0\n",
            "training error 0.11353436535629147, test error 0.22755087301858193\n",
            "Loss: 0.0\n",
            "training error 0.11304814937415995, test error 0.22649308188009504\n",
            "Loss: 0.0\n",
            "training error 0.11255247035842747, test error 0.22546993709138188\n",
            "Loss: 0.0\n",
            "training error 0.11206634897298617, test error 0.22448679626344453\n",
            "Loss: 0.0\n",
            "training error 0.11139885005901048, test error 0.22327692774520375\n",
            "Loss: 0.0\n",
            "training error 0.11088456617670658, test error 0.22212089087561543\n",
            "Loss: 0.0\n",
            "training error 0.11022838986988541, test error 0.22087235516555226\n",
            "Loss: 0.0\n",
            "training error 0.10969077579323325, test error 0.21963588925633323\n",
            "Loss: 0.0\n",
            "training error 0.10905491040465437, test error 0.21829314476184908\n",
            "Loss: 0.0\n",
            "training error 0.10848152520886879, test error 0.21707045893240584\n",
            "Loss: 0.0\n",
            "training error 0.10779948989401202, test error 0.21577850330927165\n",
            "Loss: 0.0\n",
            "training error 0.10714676067909674, test error 0.21430457426768978\n",
            "Loss: 0.0\n",
            "training error 0.1064003002339414, test error 0.2128641034731791\n",
            "Loss: 0.0\n",
            "training error 0.10580853246228404, test error 0.21131531160702746\n",
            "Loss: 0.0\n",
            "training error 0.10509852297412375, test error 0.2100254597157736\n",
            "Loss: 0.0\n",
            "training error 0.10430288552792115, test error 0.208308964932751\n",
            "Loss: 0.0\n",
            "training error 0.1036629374292512, test error 0.20686719153431557\n",
            "Loss: 0.0\n",
            "training error 0.10281425760515603, test error 0.20510326162281226\n",
            "Loss: 0.0\n",
            "training error 0.10203518551867354, test error 0.20349949109659143\n",
            "Loss: 0.0\n",
            "training error 0.10113110676766465, test error 0.2016445422182428\n",
            "Loss: 0.0\n",
            "training error 0.10037505611003561, test error 0.20011124619556206\n",
            "Loss: 0.0\n",
            "training error 0.09949131620210144, test error 0.19829565995329612\n",
            "Loss: 0.0\n",
            "training error 0.09872424092462487, test error 0.19650053888620794\n",
            "Loss: 0.0\n",
            "training error 0.0977717253937491, test error 0.19476887235864349\n",
            "Loss: 0.0\n",
            "training error 0.0969989334580843, test error 0.19297355945815362\n",
            "Loss: 0.0\n",
            "training error 0.09611992009278919, test error 0.1912360705666628\n",
            "Loss: 0.0\n",
            "training error 0.09523533379869092, test error 0.18920779260333562\n",
            "Loss: 0.0\n",
            "training error 0.09421365074263079, test error 0.1871674215625351\n",
            "Loss: 0.0\n",
            "training error 0.0935893787973651, test error 0.18511322054184912\n",
            "Loss: 0.0\n",
            "training error 0.09260038686062753, test error 0.18344505164995448\n",
            "Loss: 0.0\n",
            "training error 0.09152156384836325, test error 0.18137145076306224\n",
            "Loss: 0.0\n",
            "training error 0.0906333875647204, test error 0.1794021433644214\n",
            "Loss: 0.0\n",
            "training error 0.08972948980334174, test error 0.17727182886066506\n",
            "Loss: 0.0\n",
            "training error 0.08872692146522075, test error 0.1754712340015193\n",
            "Loss: 0.0\n",
            "training error 0.08785606705534808, test error 0.17341748272626653\n",
            "Loss: 0.0\n",
            "training error 0.08689088719924412, test error 0.17113935867544164\n",
            "Loss: 0.0\n",
            "training error 0.08585399445439859, test error 0.16920756110747423\n",
            "Loss: 0.0\n",
            "training error 0.08496542416454833, test error 0.16710710061165643\n",
            "Loss: 0.0\n",
            "training error 0.0841411753675483, test error 0.1651406955932697\n",
            "Loss: 0.0\n",
            "training error 0.08311274867219752, test error 0.1631267607453448\n",
            "Loss: 0.0\n",
            "training error 0.08213572180985594, test error 0.16093974285896992\n",
            "Loss: 0.0\n",
            "training error 0.08124180582471727, test error 0.15905332709375034\n",
            "Loss: 0.0\n",
            "training error 0.0802570384146824, test error 0.15694825090945358\n",
            "Loss: 0.0\n",
            "training error 0.07932577063828991, test error 0.15480323417427025\n",
            "Loss: 0.0\n",
            "training error 0.07848850312820803, test error 0.15270284987255367\n",
            "Loss: 0.0\n",
            "training error 0.07749984066335305, test error 0.15082602335673506\n",
            "Loss: 0.0\n",
            "training error 0.07657195546077086, test error 0.14894282873635387\n",
            "Loss: 0.0\n",
            "training error 0.07569548587928761, test error 0.14689520605049483\n",
            "Loss: 0.0\n",
            "training error 0.07486641313736947, test error 0.14480245687120777\n",
            "Loss: 0.0\n",
            "training error 0.07413119053898634, test error 0.14306056770822434\n",
            "Loss: 0.0\n",
            "training error 0.07304457498999202, test error 0.14097374024339984\n",
            "Loss: 0.0\n",
            "training error 0.07209054178576306, test error 0.13911861177369517\n",
            "Loss: 0.0\n",
            "training error 0.07138226876844338, test error 0.13724558835154005\n",
            "Loss: 0.0\n",
            "training error 0.07038636079093655, test error 0.13547923529879274\n",
            "Loss: 0.0\n",
            "training error 0.06952888133392081, test error 0.1336805381869746\n",
            "Loss: 0.0\n",
            "training error 0.06872977399751257, test error 0.1318778020692897\n",
            "Loss: 0.0\n",
            "training error 0.06789338565207464, test error 0.130062740605629\n",
            "Loss: 0.0\n",
            "training error 0.06707874114489866, test error 0.1283255141156746\n",
            "Loss: 0.0\n",
            "training error 0.06630169150682855, test error 0.12641355381042402\n",
            "Loss: 0.0\n",
            "training error 0.065476815800707, test error 0.12473566331380538\n",
            "Loss: 0.0\n",
            "training error 0.06478141115198424, test error 0.12294614384745237\n",
            "Loss: 0.0\n",
            "training error 0.0640051955203748, test error 0.12139879133696205\n",
            "Loss: 0.0\n",
            "training error 0.06319584612703502, test error 0.11981746815066738\n",
            "Loss: 0.0\n",
            "training error 0.062487007472405036, test error 0.11819431937925114\n",
            "Loss: 0.0\n",
            "training error 0.061860014062232516, test error 0.11673173439601035\n",
            "Loss: 0.0\n",
            "training error 0.061040336099322315, test error 0.1147420889810939\n",
            "Loss: 0.0\n",
            "training error 0.060392864089435, test error 0.1131714597168653\n",
            "Loss: 0.0\n",
            "training error 0.059695806328199014, test error 0.11175227849222746\n",
            "Loss: 0.0\n",
            "training error 0.05891287851220077, test error 0.11006802810525618\n",
            "Loss: 0.0\n",
            "training error 0.05836130589003988, test error 0.10869230581475156\n",
            "Loss: 0.0\n",
            "training error 0.05765844494750171, test error 0.10719218917773332\n",
            "Loss: 0.0\n",
            "training error 0.05701842783150708, test error 0.10560923154978896\n",
            "Loss: 0.0\n",
            "training error 0.05641453236505079, test error 0.10434675692492161\n",
            "Loss: 0.0\n",
            "training error 0.055836587548655545, test error 0.10292257254244269\n",
            "Loss: 0.0\n",
            "training error 0.0552053514167029, test error 0.10135165263880953\n",
            "Loss: 0.0\n",
            "training error 0.05458237929925644, test error 0.10010940998067215\n",
            "Loss: 0.0\n",
            "training error 0.05400195680481976, test error 0.09865449101736445\n",
            "Loss: 0.0\n",
            "training error 0.053530267533098815, test error 0.09744800401686612\n",
            "Loss: 0.0\n",
            "training error 0.052874360066611066, test error 0.09623553795987985\n",
            "Loss: 0.0\n",
            "training error 0.05240082341946918, test error 0.09518492620257415\n",
            "Loss: 0.0\n",
            "training error 0.05181900808675069, test error 0.09394793638768564\n",
            "Loss: 0.0\n",
            "training error 0.05129748546491284, test error 0.0926216648415912\n",
            "Loss: 0.0\n",
            "training error 0.05078930932694488, test error 0.09148315232581797\n",
            "Loss: 0.0\n",
            "training error 0.05028633234050781, test error 0.09027989608093032\n",
            "Loss: 0.0\n",
            "training error 0.04983957116057804, test error 0.08925232328266341\n",
            "Loss: 0.0\n",
            "training error 0.04934726163924617, test error 0.08809423068337724\n",
            "Loss: 0.0\n",
            "training error 0.048880335036767864, test error 0.08715453492491264\n",
            "Loss: 0.0\n",
            "training error 0.048421703579986416, test error 0.0861451117353027\n",
            "Loss: 0.0\n",
            "training error 0.04804551935595534, test error 0.08516945977581818\n",
            "Loss: 0.0\n",
            "training error 0.04760994705370966, test error 0.08409004530506349\n",
            "Loss: 0.0\n",
            "training error 0.04721604402205924, test error 0.08298815919308875\n",
            "Loss: 0.0\n",
            "training error 0.04676825387292615, test error 0.08220371039258241\n",
            "Loss: 0.0\n",
            "training error 0.046563964489909515, test error 0.08107950208009644\n",
            "Loss: 0.0\n",
            "training error 0.04599510610775426, test error 0.0803225104247812\n",
            "Loss: 0.0\n",
            "training error 0.04555113212237103, test error 0.07939191625653083\n",
            "Loss: 0.0\n",
            "training error 0.045251614274026686, test error 0.07867568862054038\n",
            "Loss: 0.0\n",
            "training error 0.044856938194649136, test error 0.07775909242306761\n",
            "Loss: 0.0\n",
            "training error 0.04456265138982571, test error 0.0770173468782292\n",
            "Loss: 0.0\n",
            "training error 0.04416921036540557, test error 0.07600261041444004\n",
            "Loss: 0.0\n",
            "training error 0.04380938510156256, test error 0.07520886950664377\n",
            "Loss: 0.0\n",
            "training error 0.043521258886821644, test error 0.07438164485388596\n",
            "Loss: 0.0\n",
            "training error 0.04319709791695057, test error 0.07358144496975193\n",
            "Loss: 0.0\n",
            "training error 0.042907659070373326, test error 0.07282195374091305\n",
            "Loss: 0.0\n",
            "training error 0.04260821065778073, test error 0.07234423564671977\n",
            "Loss: 0.0\n",
            "training error 0.04223758213273799, test error 0.07160261887289265\n",
            "Loss: 0.0\n",
            "training error 0.04197087956463951, test error 0.07093904956902354\n",
            "Loss: 0.0\n",
            "training error 0.041711785770830526, test error 0.07022241061972062\n",
            "Loss: 0.0\n",
            "training error 0.04150190085638236, test error 0.0697097920369929\n",
            "Loss: 0.0\n",
            "training error 0.04111225722445337, test error 0.06900532733486092\n",
            "Loss: 0.0\n",
            "training error 0.040896792716625545, test error 0.06833022563604728\n",
            "Loss: 0.0\n",
            "training error 0.040618029713642215, test error 0.06781166747810405\n",
            "Loss: 0.0\n",
            "training error 0.04036283055304487, test error 0.06723115632145686\n",
            "Loss: 0.0\n",
            "training error 0.04013866761018472, test error 0.06670860285738375\n",
            "Loss: 0.0\n",
            "training error 0.039899179895382414, test error 0.06604037653898875\n",
            "Loss: 0.0\n",
            "training error 0.03962466732159413, test error 0.06549368025736377\n",
            "Loss: 0.0\n",
            "training error 0.03940156893393766, test error 0.06496752358806554\n",
            "Loss: 0.0\n",
            "training error 0.03919772887255185, test error 0.0644533805118328\n",
            "Loss: 0.0\n",
            "training error 0.03902857170991793, test error 0.06397997611030719\n",
            "Loss: 0.0\n",
            "training error 0.03874819910566914, test error 0.06340061427338611\n",
            "Loss: 0.0\n",
            "training error 0.03856409718063011, test error 0.06296451685511228\n",
            "Loss: 0.0\n",
            "training error 0.03836663247901687, test error 0.06253447381286201\n",
            "Loss: 0.0\n",
            "training error 0.03814138602419645, test error 0.06202756532027061\n",
            "Loss: 0.0\n",
            "training error 0.037972153357839704, test error 0.061630516007952996\n",
            "Loss: 0.0\n",
            "training error 0.037746637874052184, test error 0.06118203826981484\n",
            "Loss: 0.0\n",
            "training error 0.037612754204942055, test error 0.060608711746354446\n",
            "Loss: 0.0\n",
            "training error 0.03740679927794476, test error 0.06040861341800273\n",
            "Loss: 0.0\n",
            "training error 0.03722911656964255, test error 0.05987027986866595\n",
            "Loss: 0.0\n",
            "training error 0.03702193042164428, test error 0.05951293064166565\n",
            "Loss: 0.0\n",
            "training error 0.03687910828856973, test error 0.059108038527874704\n",
            "Loss: 0.0\n",
            "training error 0.03672796484115172, test error 0.05855160678132253\n",
            "Loss: 0.0\n",
            "training error 0.03653152905229525, test error 0.05825628598705069\n",
            "Loss: 0.0\n",
            "training error 0.03640048685839852, test error 0.057835796226044296\n",
            "Loss: 0.0\n",
            "training error 0.03625322442342864, test error 0.05748770087783855\n",
            "Loss: 0.0\n",
            "training error 0.03607196687755061, test error 0.05715421233460108\n",
            "Loss: 0.0\n",
            "training error 0.03593064285486261, test error 0.0569030536176159\n",
            "Loss: 0.0\n",
            "training error 0.03581223754574394, test error 0.05646184664051194\n",
            "Loss: 0.0\n",
            "training error 0.035707251283947326, test error 0.056082785970336675\n",
            "Loss: 0.0\n",
            "training error 0.03549803887457298, test error 0.05589290272096972\n",
            "Loss: 0.0\n",
            "training error 0.03545985743378963, test error 0.055537807138182925\n",
            "Loss: 0.0\n",
            "training error 0.03523832777337829, test error 0.05532495788243054\n",
            "Loss: 0.0\n",
            "training error 0.035129269097124514, test error 0.05507098965826023\n",
            "Loss: 0.0\n",
            "training error 0.03500480173817117, test error 0.054858181585212856\n",
            "Loss: 0.0\n",
            "training error 0.03486732606882193, test error 0.054407154305493644\n",
            "Loss: 0.0\n",
            "training error 0.034709484969358606, test error 0.054288919235315404\n",
            "Loss: 0.0\n",
            "training error 0.034606003450691314, test error 0.05389252479081361\n",
            "Loss: 0.0\n",
            "training error 0.03456528023840784, test error 0.053714662546477894\n",
            "Loss: 0.0\n",
            "training error 0.03437026092675452, test error 0.05333831914702857\n",
            "Loss: 0.0\n",
            "training error 0.03424606523947629, test error 0.05293738226868269\n",
            "Loss: 0.0\n",
            "training error 0.03414618272240764, test error 0.052771831142453846\n",
            "Loss: 0.0\n",
            "training error 0.03402041860114594, test error 0.05251983730080673\n",
            "Loss: 0.0\n",
            "training error 0.03393023169100254, test error 0.05240224037350636\n",
            "Loss: 0.0\n",
            "training error 0.033806260962065, test error 0.052175335951774114\n",
            "Loss: 0.0\n",
            "training error 0.03370462135251889, test error 0.05194784766296837\n",
            "Loss: 0.0\n",
            "training error 0.03359765417326162, test error 0.051866819403932944\n",
            "Loss: 0.0\n",
            "training error 0.03349514582315099, test error 0.051590837335677604\n",
            "Loss: 0.0\n",
            "training error 0.03341137791168594, test error 0.05130858644924572\n",
            "Loss: 0.0\n",
            "training error 0.03328730883002417, test error 0.05119643659179036\n",
            "Loss: 0.0\n",
            "training error 0.033227331869659225, test error 0.05103793369200921\n",
            "Loss: 0.0\n",
            "training error 0.03311047877775602, test error 0.050773707010945024\n",
            "Loss: 0.0\n",
            "training error 0.03301491128455957, test error 0.05046779240975418\n",
            "Loss: 0.0\n",
            "training error 0.03293518851823833, test error 0.05027198910291321\n",
            "Loss: 0.0\n",
            "training error 0.032925083355698055, test error 0.049728341904151804\n",
            "Loss: 0.0\n",
            "training error 0.032749578209290905, test error 0.04970887680811595\n",
            "Loss: 0.0\n",
            "training error 0.032684094022307805, test error 0.04942068651118725\n",
            "Loss: 0.0\n",
            "training error 0.03257460525942353, test error 0.0492218463048924\n",
            "Loss: 0.0\n",
            "training error 0.032519557972950267, test error 0.04910212241518363\n",
            "Loss: 0.0\n",
            "training error 0.03243393697133398, test error 0.04896007019121418\n",
            "Loss: 0.0\n",
            "training error 0.03238187262872306, test error 0.04886824967864216\n",
            "Loss: 0.0\n",
            "training error 0.03227313503965381, test error 0.048893528218324024\n",
            "Loss: 0.05172794165557715\n",
            "training error 0.032196505603856014, test error 0.04881055422759821\n",
            "Loss: 0.0\n",
            "training error 0.0321080525430759, test error 0.04871617087505226\n",
            "Loss: 0.0\n",
            "training error 0.03200473888659517, test error 0.048498752927227225\n",
            "Loss: 0.0\n",
            "training error 0.031944955534232294, test error 0.04842042243301042\n",
            "Loss: 0.0\n",
            "training error 0.03185229707284003, test error 0.04831052488474089\n",
            "Loss: 0.0\n",
            "training error 0.03177735455814785, test error 0.048150242502761925\n",
            "Loss: 0.0\n",
            "training error 0.03176193509032671, test error 0.04822777687346293\n",
            "Loss: 0.16102591943654154\n",
            "training error 0.03164414132931383, test error 0.047908340983701676\n",
            "Loss: 0.0\n",
            "training error 0.03156406747723687, test error 0.04783545976658965\n",
            "Loss: 0.0\n",
            "training error 0.031504406285059676, test error 0.04763114690172281\n",
            "Loss: 0.0\n",
            "training error 0.0314142254315985, test error 0.04756636708274568\n",
            "Loss: 0.0\n",
            "training error 0.031398729981780796, test error 0.047470101191361656\n",
            "Loss: 0.0\n",
            "training error 0.03131615467569772, test error 0.04747746689217754\n",
            "Loss: 0.015516505402413117\n",
            "training error 0.03126356430497086, test error 0.04725775620681001\n",
            "Loss: 0.0\n",
            "training error 0.031159753146696355, test error 0.04721134882118432\n",
            "Loss: 0.0\n",
            "training error 0.03108573524331574, test error 0.047094940715878816\n",
            "Loss: 0.0\n",
            "training error 0.031032020014266557, test error 0.04699629235110005\n",
            "Loss: 0.0\n",
            "training error 0.03099505029778954, test error 0.04700754937394881\n",
            "Loss: 0.023953002004217794\n",
            "training error 0.03088633746448641, test error 0.04690417682887997\n",
            "Loss: 0.0\n",
            "training error 0.030850693430707614, test error 0.046744118550251415\n",
            "Loss: 0.0\n",
            "training error 0.030805696929310015, test error 0.046641317619991685\n",
            "Loss: 0.0\n",
            "training error 0.030728956974280377, test error 0.046542303951796746\n",
            "Loss: 0.0\n",
            "training error 0.030657908751277402, test error 0.04657836495117055\n",
            "Loss: 0.07748004785312901\n",
            "training error 0.030614446178703375, test error 0.046560558883714194\n",
            "Loss: 0.039222235187064136\n",
            "training error 0.030556161918931162, test error 0.046211887415473114\n",
            "Loss: 0.0\n",
            "training error 0.0305059276089161, test error 0.04628212315616388\n",
            "Loss: 0.15198630616253261\n",
            "training error 0.030449200981528465, test error 0.04618902983924887\n",
            "Loss: 0.0\n",
            "training error 0.030364267507569872, test error 0.046044960915533104\n",
            "Loss: 0.0\n",
            "training error 0.030339403058615513, test error 0.04605144406395563\n",
            "Loss: 0.0140800389306861\n",
            "training error 0.030266075333830876, test error 0.045905885231093\n",
            "Loss: 0.0\n",
            "training error 0.03022399188087538, test error 0.04582423857098082\n",
            "Loss: 0.0\n",
            "training error 0.030203718305129964, test error 0.045929964708102636\n",
            "Loss: 0.23072099050385209\n",
            "training error 0.03009062290565729, test error 0.04585744206059383\n",
            "Loss: 0.072458355334315\n",
            "training error 0.030038730813869208, test error 0.04568208399112137\n",
            "Loss: 0.0\n",
            "training error 0.02997838547532027, test error 0.045693703409567514\n",
            "Loss: 0.02543539486596913\n",
            "training error 0.029993645600194355, test error 0.04575135913890109\n",
            "Loss: 0.15164620728158607\n",
            "training error 0.02992746749179284, test error 0.04541578182707764\n",
            "Loss: 0.0\n",
            "training error 0.029830382371308126, test error 0.045460777591154404\n",
            "Loss: 0.09907517225640206\n",
            "training error 0.029799409831338344, test error 0.04522351747905667\n",
            "Loss: 0.0\n",
            "training error 0.029762775410039525, test error 0.045322889487336745\n",
            "Loss: 0.21973524798484245\n",
            "training error 0.029690059215056155, test error 0.045343954580841526\n",
            "Loss: 0.26631520168822753\n",
            "training error 0.029674633306309, test error 0.0453451348848567\n",
            "Loss: 0.26892513581313615\n",
            "training error 0.029738578272135413, test error 0.044968008818362916\n",
            "Loss: 0.0\n",
            "training error 0.029572086837745155, test error 0.045240456659551676\n",
            "Loss: 0.6058703695092316\n",
            "training error 0.029496619394452642, test error 0.045132673612340975\n",
            "Loss: 0.3661820887893352\n",
            "training error 0.029442573034004934, test error 0.04507992591515736\n",
            "Loss: 0.24888159323777082\n",
            "training error 0.029413355129503882, test error 0.04499149344442972\n",
            "Loss: 0.05222518560177303\n",
            "training error 0.029379451657713274, test error 0.04493219764132551\n",
            "Loss: 0.0\n",
            "training error 0.029361709058673257, test error 0.04473254987579054\n",
            "Loss: 0.0\n",
            "training error 0.0292930557456303, test error 0.04462695897384507\n",
            "Loss: 0.0\n",
            "training error 0.02922121456280412, test error 0.04461679734435297\n",
            "Loss: 0.0\n",
            "training error 0.02917318879337605, test error 0.04463985479260164\n",
            "Loss: 0.0516788510629862\n",
            "training error 0.02916348595729104, test error 0.04476105254903642\n",
            "Loss: 0.3233203933713291\n",
            "training error 0.02909261203414397, test error 0.0446598576975776\n",
            "Loss: 0.0965115288134255\n",
            "training error 0.0291285263936934, test error 0.04465718840158352\n",
            "Loss: 0.09052881343949437\n",
            "training error 0.029005156378966734, test error 0.04461086498923201\n",
            "Loss: 0.0\n",
            "training error 0.02899800694070719, test error 0.044648974885150725\n",
            "Loss: 0.08542738619372603\n",
            "training error 0.028927212085071114, test error 0.044517641019829614\n",
            "Loss: 0.0\n",
            "training error 0.028911375996697866, test error 0.04445025522849828\n",
            "Loss: 0.0\n",
            "training error 0.028861424077381372, test error 0.04447347946074522\n",
            "Loss: 0.0522476915544301\n",
            "training error 0.02879162146563911, test error 0.044657795298331165\n",
            "Loss: 0.46690411284708855\n",
            "training error 0.02877927072926104, test error 0.04471173216279369\n",
            "Loss: 0.5882461933036875\n",
            "training error 0.028698423484306748, test error 0.044675669289069145\n",
            "Loss: 0.5071153346861923\n",
            "training error 0.028663276125550533, test error 0.04459326315149573\n",
            "Loss: 0.3217257634682058\n",
            "training error 0.028655241560215268, test error 0.044533456469238286\n",
            "Loss: 0.1871783194771659\n",
            "training error 0.02860571328066817, test error 0.04445450108743061\n",
            "Loss: 0.009551933752716124\n",
            "training error 0.028551822361366802, test error 0.04463401026704277\n",
            "Loss: 0.4133947883986133\n",
            "training error 0.028555572274830267, test error 0.04448219522433131\n",
            "Loss: 0.07185559603390956\n",
            "training error 0.028466002042663696, test error 0.04424018018138738\n",
            "Loss: 0.0\n",
            "training error 0.028427889481130782, test error 0.04426505222645031\n",
            "Loss: 0.05622048771263266\n",
            "training error 0.028394710599106778, test error 0.04427931645741241\n",
            "Loss: 0.08846319310764628\n",
            "training error 0.028367803981063183, test error 0.04434960191444793\n",
            "Loss: 0.24733564061427682\n",
            "training error 0.028315412805098965, test error 0.04426990020733322\n",
            "Loss: 0.06717880854911762\n",
            "training error 0.028319639355447925, test error 0.04421905699752611\n",
            "Loss: 0.0\n",
            "training error 0.028269362099535313, test error 0.04425785594954215\n",
            "Loss: 0.08774260386921195\n",
            "training error 0.028210429196243312, test error 0.044222418315659866\n",
            "Loss: 0.007601514735933179\n",
            "training error 0.028176736585443226, test error 0.044288472600613714\n",
            "Loss: 0.15698119272757882\n",
            "training error 0.028163142097173143, test error 0.04417765572011042\n",
            "Loss: 0.0\n",
            "training error 0.028106965497591983, test error 0.04424924397253412\n",
            "Loss: 0.1620462907250042\n",
            "training error 0.028138856999817816, test error 0.044166312795410674\n",
            "Loss: 0.0\n",
            "training error 0.028056702473013718, test error 0.04414741821657574\n",
            "Loss: 0.0\n",
            "training error 0.02804553977151339, test error 0.04414986334138537\n",
            "Loss: 0.00553854541989196\n",
            "training error 0.027966750720440206, test error 0.04421436404087068\n",
            "Loss: 0.1516415387339931\n",
            "training error 0.027960732189822018, test error 0.044340039284133576\n",
            "Loss: 0.4363133232681671\n",
            "training error 0.027887756339730885, test error 0.044227318722441095\n",
            "Loss: 0.180985681820367\n",
            "training error 0.02785144982188506, test error 0.04414201774042078\n",
            "Loss: 0.0\n",
            "training error 0.02782808604421859, test error 0.044167871721605585\n",
            "Loss: 0.05857000315854233\n",
            "training error 0.027810250405805467, test error 0.04408759070215026\n",
            "Loss: 0.0\n",
            "training error 0.02776849243409042, test error 0.044234090373418265\n",
            "Loss: 0.3322923047842208\n",
            "training error 0.027737506491042216, test error 0.04408966439466089\n",
            "Loss: 0.004703574129605315\n",
            "training error 0.02768472983688721, test error 0.044035549024198574\n",
            "Loss: 0.0\n",
            "training error 0.027646530729028684, test error 0.044017659877320744\n",
            "Loss: 0.0\n",
            "training error 0.027669041463079278, test error 0.04401093714870665\n",
            "Loss: 0.0\n",
            "training error 0.02761683271776523, test error 0.04408990322688404\n",
            "Loss: 0.1794237598498949\n",
            "training error 0.02758157712302271, test error 0.043851305561238584\n",
            "Loss: 0.0\n",
            "training error 0.02756652286850139, test error 0.04395584226678783\n",
            "Loss: 0.23838903816275625\n",
            "training error 0.02752646194200231, test error 0.04397416891074845\n",
            "Loss: 0.2801817367519055\n",
            "training error 0.0274852179104825, test error 0.043981504683774986\n",
            "Loss: 0.29691048161515354\n",
            "training error 0.027486466315676413, test error 0.044035773754710746\n",
            "Loss: 0.4206675060439258\n",
            "training error 0.027456041668478395, test error 0.0440583659745863\n",
            "Loss: 0.4721875681866772\n",
            "training error 0.027449297095056335, test error 0.04389580565672184\n",
            "Loss: 0.10147952247649972\n",
            "training error 0.027397749150813583, test error 0.043958411679338\n",
            "Loss: 0.24424841342487458\n",
            "training error 0.027359016997237428, test error 0.04378898532052826\n",
            "Loss: 0.0\n",
            "training error 0.027314479829345525, test error 0.04382381334122833\n",
            "Loss: 0.07953603045409707\n",
            "training error 0.02726827580195344, test error 0.043773804970041734\n",
            "Loss: 0.0\n",
            "training error 0.02729195000216138, test error 0.043934177947548406\n",
            "Loss: 0.3663674602114808\n",
            "training error 0.027221952110809376, test error 0.04384854314286895\n",
            "Loss: 0.17073720888181043\n",
            "training error 0.02719838416874352, test error 0.04378849909860877\n",
            "Loss: 0.033568314605259\n",
            "training error 0.02717719592307373, test error 0.043612097652742376\n",
            "Loss: 0.0\n",
            "training error 0.027180361625692803, test error 0.043554393676118415\n",
            "Loss: 0.0\n",
            "training error 0.02710328115516723, test error 0.04382933353280195\n",
            "Loss: 0.6312563061446008\n",
            "training error 0.027066438905085047, test error 0.04366615393850637\n",
            "Loss: 0.25659928414807354\n",
            "training error 0.027138558266021323, test error 0.04351875534380363\n",
            "Loss: 0.0\n",
            "training error 0.02703798038130638, test error 0.04371989214147713\n",
            "Loss: 0.46218416883592184\n",
            "training error 0.02698933870245732, test error 0.043794588881686546\n",
            "Loss: 0.6338268080136933\n",
            "training error 0.026979634706570287, test error 0.043762543751677145\n",
            "Loss: 0.560191590838377\n",
            "training error 0.02693291340477269, test error 0.04377186195691994\n",
            "Loss: 0.5816035204057046\n",
            "training error 0.026961692834213152, test error 0.04365754565844321\n",
            "Loss: 0.31892068958112585\n",
            "training error 0.02692658138002007, test error 0.04348263381085324\n",
            "Loss: 0.0\n",
            "training error 0.026892886314879304, test error 0.04363038280874694\n",
            "Loss: 0.3397885200248085\n",
            "training error 0.026925278986581327, test error 0.04332424219520923\n",
            "Loss: 0.0\n",
            "training error 0.02682783611177079, test error 0.04366658714603871\n",
            "Loss: 0.7901925884518546\n",
            "training error 0.02678675987024707, test error 0.04357402800625094\n",
            "Loss: 0.5765497522524088\n",
            "training error 0.026756460348448155, test error 0.04347759732887764\n",
            "Loss: 0.35397072377498784\n",
            "training error 0.02675568374478713, test error 0.0435850345659768\n",
            "Loss: 0.6019548353379145\n",
            "training error 0.02671435184377926, test error 0.04356354102733893\n",
            "Loss: 0.5523439534186769\n",
            "training error 0.026674472694707935, test error 0.043510094874118374\n",
            "Loss: 0.42898079572109005\n",
            "training error 0.026692378358364757, test error 0.043730297422936784\n",
            "Loss: 0.9372471557562712\n",
            "training error 0.02661761523149902, test error 0.04357845964733752\n",
            "Loss: 0.5867787622985787\n",
            "training error 0.02663099150152425, test error 0.04323435087920604\n",
            "Loss: 0.0\n",
            "training error 0.02659402573766847, test error 0.04337307143113038\n",
            "Loss: 0.3208572561015588\n",
            "training error 0.026576584634405064, test error 0.0433015964887853\n",
            "Loss: 0.15553745623970716\n",
            "training error 0.026562196851281025, test error 0.04340939172051957\n",
            "Loss: 0.40486520036484563\n",
            "training error 0.026525177041545844, test error 0.0433455781417813\n",
            "Loss: 0.25726594782473633\n",
            "training error 0.026500271538289922, test error 0.0431956431836537\n",
            "Loss: 0.0\n",
            "training error 0.02653006123238545, test error 0.04319476881545299\n",
            "Loss: 0.0\n",
            "training error 0.02647426561264945, test error 0.04334828520015589\n",
            "Loss: 0.35540503841746673\n",
            "training error 0.026461013184304234, test error 0.043225067735571325\n",
            "Loss: 0.070144883163481\n",
            "training error 0.026397684357771123, test error 0.043345117015766294\n",
            "Loss: 0.34807039008741825\n",
            "training error 0.026390685588719764, test error 0.0433571406087237\n",
            "Loss: 0.3759061518871176\n",
            "training error 0.026356342305525374, test error 0.043426854682992615\n",
            "Loss: 0.5373008674527169\n",
            "training error 0.026346602117755222, test error 0.0433744805302897\n",
            "Loss: 0.4160497202902347\n",
            "training error 0.026311548451667156, test error 0.04343833053296039\n",
            "Loss: 0.5638685521110132\n",
            "training error 0.026281398838381948, test error 0.04361250167795921\n",
            "Loss: 0.9670913260144021\n",
            "training error 0.026270117306173256, test error 0.04347626438923785\n",
            "Loss: 0.6516890389841601\n",
            "training error 0.02623232663309036, test error 0.04349269795575786\n",
            "Loss: 0.6897343092117314\n",
            "training error 0.0262230607051044, test error 0.043350847632314775\n",
            "Loss: 0.36133731269316627\n",
            "training error 0.026191251864596504, test error 0.04343432212154103\n",
            "Loss: 0.554588698255376\n",
            "training error 0.026191200373398574, test error 0.043523292934619706\n",
            "Loss: 0.7605645965378605\n",
            "training error 0.026162188939810007, test error 0.04344384597947679\n",
            "Loss: 0.5766373356180354\n",
            "training error 0.02614323337096726, test error 0.043440597357510755\n",
            "Loss: 0.5691164666445037\n",
            "training error 0.026139664956873584, test error 0.043590150765308096\n",
            "Loss: 0.9153468364290873\n",
            "training error 0.02611949631022253, test error 0.04365469331508902\n",
            "Loss: 1.0647689807092853\n",
            "training error 0.0261709485987016, test error 0.04312606843154374\n",
            "Loss: 0.0\n",
            "training error 0.026073489601179816, test error 0.043341243650964784\n",
            "Loss: 0.49894466907549617\n",
            "training error 0.026050643751308125, test error 0.04348507484182682\n",
            "Loss: 0.8324580082066824\n",
            "training error 0.025992801617991605, test error 0.043605980188421246\n",
            "Loss: 1.1128112863784345\n",
            "training error 0.026027289357538348, test error 0.043561146872666316\n",
            "Loss: 1.0088525500839385\n",
            "training error 0.02595325232221451, test error 0.04354426127922358\n",
            "Loss: 0.969698520846296\n",
            "training error 0.026056412667170285, test error 0.04345479307493638\n",
            "Loss: 0.7622411579540245\n",
            "training error 0.025988817619955498, test error 0.043285284045829195\n",
            "Loss: 0.36918648065076276\n",
            "training error 0.025918049238913856, test error 0.043285313486972295\n",
            "Loss: 0.3692547482767372\n",
            "training error 0.0258957007991709, test error 0.043340556245489374\n",
            "Loss: 0.4973507248547282\n",
            "training error 0.025898596825583316, test error 0.043184967533809356\n",
            "Loss: 0.1365742447844598\n",
            "training error 0.025853594879244705, test error 0.04320448689552763\n",
            "Loss: 0.18183541147129834\n",
            "training error 0.025852398163016325, test error 0.04337421652994858\n",
            "Loss: 0.5754016246547922\n",
            "training error 0.025816910649186876, test error 0.043316537345235814\n",
            "Loss: 0.4416561041134859\n",
            "training error 0.02588010685343521, test error 0.04315243382011985\n",
            "Loss: 0.06113561828144931\n",
            "training error 0.02576343274995948, test error 0.043359074469864055\n",
            "Loss: 0.54029047115709\n",
            "training error 0.025771833557597067, test error 0.04326708943542578\n",
            "Loss: 0.32699712496604416\n",
            "training error 0.025744736686585622, test error 0.043445229992093236\n",
            "Loss: 0.7400664427737347\n",
            "training error 0.02570705582380349, test error 0.04324647286491975\n",
            "Loss: 0.27919176905062404\n",
            "training error 0.025701842761324326, test error 0.04336345123232474\n",
            "Loss: 0.550439234120792\n",
            "training error 0.02568918708825711, test error 0.043368643820085005\n",
            "Loss: 0.5624797190272979\n",
            "training error 0.02565184825598498, test error 0.04336891393832395\n",
            "Loss: 0.5631060646432262\n",
            "training error 0.025648044868442695, test error 0.04340187316206921\n",
            "Loss: 0.6395313566857297\n",
            "training error 0.025685620386214837, test error 0.04357691095571571\n",
            "Loss: 1.0454060399399001\n",
            "training error 0.025597954626583215, test error 0.04334019157204934\n",
            "Loss: 0.4965051262335374\n",
            "training error 0.025619007274827185, test error 0.04340424162400585\n",
            "Loss: 0.6450233062716304\n",
            "training error 0.02557292637912029, test error 0.04333280311157957\n",
            "Loss: 0.47937288872967887\n",
            "training error 0.025560395255091888, test error 0.043303940949945\n",
            "Loss: 0.4124477951974681\n",
            "training error 0.02551020401984989, test error 0.04324494808014199\n",
            "Loss: 0.2756561238290267\n",
            "training error 0.025516387374198302, test error 0.04330997842622844\n",
            "Loss: 0.42644739335937487\n",
            "training error 0.025568429236476176, test error 0.0430271682362623\n",
            "Loss: 0.0\n",
            "training error 0.025483109972940707, test error 0.04324649934898239\n",
            "Loss: 0.5097502850193392\n",
            "training error 0.025490833936265483, test error 0.043198317394420045\n",
            "Loss: 0.3977699792325762\n",
            "training error 0.025457774629595992, test error 0.04330659900727985\n",
            "Loss: 0.6494286806958849\n",
            "training error 0.02543096532415199, test error 0.04341587099787872\n",
            "Loss: 0.9033891319132348\n",
            "training error 0.02544149559405257, test error 0.043496913862406325\n",
            "Loss: 1.0917419049393473\n",
            "training error 0.025383495976914375, test error 0.04351322727278097\n",
            "Loss: 1.1296561136668881\n",
            "training error 0.025395015802245218, test error 0.043700032205665575\n",
            "Loss: 1.5638118820847646\n",
            "training error 0.025343089667715705, test error 0.04353192804849165\n",
            "Loss: 1.1731188291493222\n",
            "training error 0.02533537884494345, test error 0.043577621943196\n",
            "Loss: 1.2793166027361025\n",
            "training error 0.025378159882806173, test error 0.04311811706181538\n",
            "Loss: 0.21137534558091087\n",
            "training error 0.02536203946381145, test error 0.04300959976464786\n",
            "Loss: 0.0\n",
            "training error 0.02529732236187212, test error 0.04326566781218534\n",
            "Loss: 0.5953741698102366\n",
            "training error 0.025259637879861508, test error 0.04322397484269127\n",
            "Loss: 0.4984354172475225\n",
            "training error 0.025259566351485692, test error 0.04311705497233567\n",
            "Loss: 0.24984005495474193\n",
            "training error 0.025320336580352102, test error 0.04338729170959733\n",
            "Loss: 0.8781573114286845\n",
            "training error 0.025314813464304933, test error 0.042965573739997424\n",
            "Loss: 0.0\n",
            "training error 0.02526431036695486, test error 0.04309999905692202\n",
            "Loss: 0.3128675011721205\n",
            "training error 0.02520822794970084, test error 0.043185945343524844\n",
            "Loss: 0.5129027366444161\n",
            "training error 0.025182526208388945, test error 0.042995958176803246\n",
            "Loss: 0.0707180986100342\n",
            "training error 0.025219337988827056, test error 0.04281822888353425\n",
            "Loss: 0.0\n",
            "training error 0.02516131970289758, test error 0.04308708743687541\n",
            "Loss: 0.6279067592273702\n",
            "training error 0.025197883860251447, test error 0.04291997202229677\n",
            "Loss: 0.2376164110833745\n",
            "training error 0.025193382498821124, test error 0.04290921205096566\n",
            "Loss: 0.21248699398306314\n",
            "training error 0.025127366425119372, test error 0.04330430804515057\n",
            "Loss: 1.1352154778247803\n",
            "training error 0.02508404367356092, test error 0.043151455366372826\n",
            "Loss: 0.7782350917525216\n",
            "training error 0.025143309933675924, test error 0.04330758905876042\n",
            "Loss: 1.1428781338836513\n",
            "training error 0.02507120191573424, test error 0.04283288885528425\n",
            "Loss: 0.03423768832166996\n",
            "training error 0.025089464097660826, test error 0.042641210316331626\n",
            "Loss: 0.0\n",
            "training error 0.025038037087665273, test error 0.04286533063309876\n",
            "Loss: 0.5255955802016654\n",
            "training error 0.02511522488635591, test error 0.04289528289339041\n",
            "Loss: 0.5958380992799217\n",
            "training error 0.025050546099108475, test error 0.04271324556842668\n",
            "Loss: 0.16893341338264367\n",
            "training error 0.025011025019426897, test error 0.04284304970999279\n",
            "Loss: 0.47334349134047216\n",
            "training error 0.024992216809950982, test error 0.0425265464387659\n",
            "Loss: 0.0\n",
            "training error 0.024991785582758753, test error 0.04260403877018815\n",
            "Loss: 0.18222107815371036\n",
            "training error 0.024966269922641126, test error 0.042484555217112176\n",
            "Loss: 0.0\n",
            "training error 0.025037387392774293, test error 0.04264881635771879\n",
            "Loss: 0.3866373079985941\n",
            "training error 0.024945409649642626, test error 0.04267847840847908\n",
            "Loss: 0.4564557410943415\n",
            "training error 0.024917746986091418, test error 0.04295670233093986\n",
            "Loss: 1.111338253195382\n",
            "training error 0.024904693644428745, test error 0.04286638822760003\n",
            "Loss: 0.8987572272712852\n",
            "training error 0.024874663263541485, test error 0.042852244886318495\n",
            "Loss: 0.8654666791903187\n",
            "training error 0.024910357718795198, test error 0.043055392950089526\n",
            "Loss: 1.3436358932326087\n",
            "training error 0.02487405909233317, test error 0.04315385860567312\n",
            "Loss: 1.5754040148014958\n",
            "training error 0.024826712065850035, test error 0.04304295583403316\n",
            "Loss: 1.3143614522203473\n",
            "training error 0.024818474305844232, test error 0.042908790677527814\n",
            "Loss: 0.9985639681235448\n",
            "training error 0.024847921895222093, test error 0.042994066792059436\n",
            "Loss: 1.1992865933124763\n",
            "training error 0.024779234666054317, test error 0.04304482457072546\n",
            "Loss: 1.3187600782216036\n",
            "training error 0.024769907241898634, test error 0.043085394570676346\n",
            "Loss: 1.4142536046185539\n",
            "training error 0.024759998474419374, test error 0.043023322148227516\n",
            "Loss: 1.2681477500753768\n",
            "training error 0.024827479784046307, test error 0.04343544256012041\n",
            "Loss: 2.2381953586399606\n",
            "training error 0.024832052298916173, test error 0.0428141431475191\n",
            "Loss: 0.7757829374053893\n",
            "training error 0.02469980099679227, test error 0.042901087728557816\n",
            "Loss: 0.9804327933221835\n",
            "training error 0.024844184352837334, test error 0.04273122677244467\n",
            "Loss: 0.5806146588375638\n",
            "training error 0.024714183081395863, test error 0.0428399379884262\n",
            "Loss: 0.8364987452448958\n",
            "training error 0.024750735369283877, test error 0.04311329667582918\n",
            "Loss: 1.4799294837945176\n",
            "training error 0.024716890195907303, test error 0.042746590172826046\n",
            "Loss: 0.6167769778329335\n",
            "training error 0.024657149526067634, test error 0.042737634507249614\n",
            "Loss: 0.5956971629904162\n",
            "training error 0.02464855494914364, test error 0.04285001045350599\n",
            "Loss: 0.8602072789186677\n",
            "training error 0.02464020615183428, test error 0.043026270475299506\n",
            "Loss: 1.2750875122005123\n",
            "training error 0.024658970019043203, test error 0.04278929346116825\n",
            "Loss: 0.7172918311107335\n",
            "training error 0.024651791526066766, test error 0.042738579532893804\n",
            "Loss: 0.5979215611025435\n",
            "training error 0.024772503420179833, test error 0.04324273721093993\n",
            "Loss: 1.7846061702968319\n",
            "training error 0.024551476593952234, test error 0.042894838317854664\n",
            "Loss: 0.9657229518957866\n",
            "training error 0.02466766751206703, test error 0.042891913013163835\n",
            "Loss: 0.9588373797722705\n",
            "training error 0.02467172557425349, test error 0.0430254968965413\n",
            "Loss: 1.2732666651791735\n",
            "training error 0.02452106285233278, test error 0.04306175825271033\n",
            "Loss: 1.3586185206563295\n",
            "training error 0.024600925298193806, test error 0.043158519567725526\n",
            "Loss: 1.5863749712551778\n",
            "training error 0.02449270819695676, test error 0.04299942909178669\n",
            "Loss: 1.2119083559738675\n",
            "training error 0.02453027501320582, test error 0.04305649945399676\n",
            "Loss: 1.346240378325092\n",
            "training error 0.024496598274937515, test error 0.04270211126444473\n",
            "Loss: 0.5120826762120112\n",
            "training error 0.024485683844882707, test error 0.04245608233243566\n",
            "Loss: 0.0\n",
            "training error 0.024473917530927602, test error 0.04244117863470944\n",
            "Loss: 0.0\n",
            "training error 0.024473143602150513, test error 0.0426626405556926\n",
            "Loss: 0.5218090734220082\n",
            "training error 0.024429863192602732, test error 0.04287398601809492\n",
            "Loss: 1.0197817245148677\n",
            "training error 0.024436647876809114, test error 0.04280918788897169\n",
            "Loss: 0.8671042277824226\n",
            "training error 0.024465728031542067, test error 0.04304302071777676\n",
            "Loss: 1.4180616618764796\n",
            "training error 0.02439850715748312, test error 0.04289360737203479\n",
            "Loss: 1.0660136025424594\n",
            "training error 0.024370608158261725, test error 0.042917450023187184\n",
            "Loss: 1.1221917105012746\n",
            "training error 0.02437889210479343, test error 0.04297170350138061\n",
            "Loss: 1.2500238771344874\n",
            "training error 0.024340834650575478, test error 0.0429593801833389\n",
            "Loss: 1.220987647608962\n",
            "training error 0.024395440595462448, test error 0.0432141061866205\n",
            "Loss: 1.8211736261229516\n",
            "training error 0.024344221145902654, test error 0.04301050658577635\n",
            "Loss: 1.3414517913536317\n",
            "training error 0.024404760433102926, test error 0.04339322969597565\n",
            "Loss: 2.243224839395941\n",
            "training error 0.02434549802286677, test error 0.043292645406825626\n",
            "Loss: 2.0062279123884474\n",
            "training error 0.02431159970801249, test error 0.04318836591122316\n",
            "Loss: 1.7605243316750174\n",
            "training error 0.02438034060022186, test error 0.04300113144308375\n",
            "Loss: 1.3193620591779753\n",
            "training error 0.024459195184461383, test error 0.04332848959530455\n",
            "Loss: 2.0906840694321582\n",
            "training error 0.02432000358560733, test error 0.04274252368249814\n",
            "Loss: 0.7100298754244561\n",
            "training error 0.02426336855351744, test error 0.04259688592093301\n",
            "Loss: 0.36687785597036005\n",
            "training error 0.02428713725485362, test error 0.04254157732107378\n",
            "Loss: 0.23655960930886177\n",
            "training error 0.02425978266295931, test error 0.04265277424703335\n",
            "Loss: 0.49856205489746674\n",
            "training error 0.024269176652468014, test error 0.04271546932391122\n",
            "Loss: 0.6462843352268655\n",
            "training error 0.02420835874051565, test error 0.042834416953959155\n",
            "Loss: 0.9265490071195037\n",
            "training error 0.02424157795142009, test error 0.04271653505022193\n",
            "Loss: 0.6487954019431807\n",
            "training error 0.02421794817440655, test error 0.04270725910254201\n",
            "Loss: 0.626939393278203\n",
            "training error 0.024205757168509656, test error 0.04233728114925135\n",
            "Loss: 0.0\n",
            "training error 0.024204216668867205, test error 0.042588576588860144\n",
            "Loss: 0.5935559223156028\n",
            "training error 0.024168207954468376, test error 0.0426846456242954\n",
            "Loss: 0.8204694907532728\n",
            "training error 0.024303337957073862, test error 0.04245409484774648\n",
            "Loss: 0.2759121401379794\n",
            "training error 0.02413371538631471, test error 0.042827129106666514\n",
            "Loss: 1.1570132614050044\n",
            "training error 0.024153412408932415, test error 0.0429505945263121\n",
            "Loss: 1.4486366635085401\n",
            "training error 0.024114561143361322, test error 0.042521186368313\n",
            "Loss: 0.43438126887112016\n",
            "training error 0.024138922774264625, test error 0.04271360876063602\n",
            "Loss: 0.8888799685978999\n",
            "training error 0.024103674118511408, test error 0.042595714223765756\n",
            "Loss: 0.6104149050179863\n",
            "training error 0.024115424289726173, test error 0.04279402984558648\n",
            "Loss: 1.0788333212162549\n",
            "training error 0.02407244998158243, test error 0.04287174899716111\n",
            "Loss: 1.26240474919872\n",
            "training error 0.02418352156374547, test error 0.04270667085503678\n",
            "Loss: 0.8724927434126561\n",
            "training error 0.024204373900465124, test error 0.04305122487511909\n",
            "Loss: 1.6863239832309418\n",
            "training error 0.02404824738614382, test error 0.04282820581883267\n",
            "Loss: 1.1595564388054758\n",
            "training error 0.02402199904730288, test error 0.04279218382463972\n",
            "Loss: 1.0744730484338438\n",
            "training error 0.024099789586477925, test error 0.042961648916813755\n",
            "Loss: 1.4747469620482345\n",
            "training error 0.02409320523073115, test error 0.04278906209199365\n",
            "Loss: 1.0670995644468606\n",
            "training error 0.024008720526589394, test error 0.04282834311007919\n",
            "Loss: 1.1598807185957405\n",
            "training error 0.023974357387675845, test error 0.043002208959379236\n",
            "Loss: 1.5705491521380788\n",
            "training error 0.023959629505897344, test error 0.04306606869450638\n",
            "Loss: 1.7213848538970566\n",
            "training error 0.023995393836606572, test error 0.04277950640957044\n",
            "Loss: 1.044529191093102\n",
            "training error 0.023967150823775388, test error 0.04264438388567454\n",
            "Loss: 0.7253718899439043\n",
            "training error 0.023949935040155152, test error 0.04279222500706916\n",
            "Loss: 1.0745703206920698\n",
            "training error 0.02403986516359919, test error 0.04275994215357921\n",
            "Loss: 0.998318722541125\n",
            "training error 0.024010671927810603, test error 0.04317877148843767\n",
            "Loss: 1.9875871013535829\n",
            "training error 0.02393397717968907, test error 0.04308930681108488\n",
            "Loss: 1.77627292405107\n",
            "training error 0.02390624860663264, test error 0.04305634177605266\n",
            "Loss: 1.6984100237008803\n",
            "training error 0.023901272059923345, test error 0.04320177245871753\n",
            "Loss: 2.0419150356363103\n",
            "training error 0.023880047362785903, test error 0.04310545345344513\n",
            "Loss: 1.814411042328734\n",
            "training error 0.023876462608868696, test error 0.04304581389202928\n",
            "Loss: 1.67354332527907\n",
            "training error 0.023843354477507624, test error 0.043185388072303335\n",
            "Loss: 2.003215369598621\n",
            "training error 0.0238662194635367, test error 0.04325897096752341\n",
            "Loss: 2.177017024364014\n",
            "training error 0.023867925143602916, test error 0.04297317929030804\n",
            "Loss: 1.5019815250180146\n",
            "training error 0.023893192976182344, test error 0.042962359812308566\n",
            "Loss: 1.4764260861570788\n",
            "training error 0.023846119009521532, test error 0.04318771461370029\n",
            "Loss: 2.0087106242153485\n",
            "training error 0.02380069993315665, test error 0.04336271050523197\n",
            "Loss: 2.4220481999438714\n",
            "training error 0.02381494419194691, test error 0.04290405889836643\n",
            "Loss: 1.3387202336329063\n",
            "training error 0.023790813630623964, test error 0.04301036453864927\n",
            "Loss: 1.5898125036067023\n",
            "training error 0.023928954083181034, test error 0.04327448623443986\n",
            "Loss: 2.2136638436572875\n",
            "training error 0.023773838566019643, test error 0.0429737124246583\n",
            "Loss: 1.5032407800664949\n",
            "training error 0.023805991001314183, test error 0.04311435315541398\n",
            "Loss: 1.8354320000455893\n",
            "training error 0.02374690111785861, test error 0.04295262875165441\n",
            "Loss: 1.4534414721478761\n",
            "training error 0.023792478784374755, test error 0.04281288988130521\n",
            "Loss: 1.1233804324306984\n",
            "training error 0.023848859813672428, test error 0.043256942579459584\n",
            "Loss: 2.1722260032857488\n",
            "training error 0.02372134912654438, test error 0.04311489721492015\n",
            "Loss: 1.836717060142501\n",
            "training error 0.023701185618158132, test error 0.04290948995830841\n",
            "Loss: 1.3515483127975347\n",
            "training error 0.023722892299570152, test error 0.04298564272444234\n",
            "Loss: 1.5314199627163516\n",
            "training error 0.02369763736413982, test error 0.04292476220122354\n",
            "Loss: 1.3876211131771843\n",
            "training error 0.023673857162231807, test error 0.043088747270701014\n",
            "Loss: 1.7749512983616489\n",
            "training error 0.02378801732225755, test error 0.042798585191313944\n",
            "Loss: 1.0895929770179613\n",
            "training error 0.023709600508078833, test error 0.043105022625376105\n",
            "Loss: 1.8133934331263202\n",
            "training error 0.02365802540491707, test error 0.043239635645909734\n",
            "Loss: 2.131347295253372\n",
            "training error 0.023703203294705315, test error 0.04269398108285329\n",
            "Loss: 0.8425196987602135\n",
            "training error 0.023637592910459724, test error 0.04287231757884981\n",
            "Loss: 1.2637477303095945\n",
            "training error 0.023618212713280143, test error 0.04294579937318525\n",
            "Loss: 1.4373105863569657\n",
            "training error 0.023640684659292532, test error 0.0431868608610508\n",
            "Loss: 2.006694073727666\n",
            "training error 0.023621208692629934, test error 0.043274223319584464\n",
            "Loss: 2.213042842855484\n",
            "training error 0.023637521071122738, test error 0.043060541745499774\n",
            "Loss: 1.7083302862522354\n",
            "training error 0.023585040019810037, test error 0.04329343407131618\n",
            "Loss: 2.258418339841217\n",
            "training error 0.023621892798563755, test error 0.04337024829114661\n",
            "Loss: 2.439852333109771\n",
            "training error 0.023605748341478185, test error 0.0431361033957708\n",
            "Loss: 1.8868057296909768\n",
            "training error 0.02357848542509599, test error 0.04302424640866129\n",
            "Loss: 1.6226012648005828\n",
            "training error 0.023695357700019334, test error 0.04323704826871633\n",
            "Loss: 2.12523595053975\n",
            "training error 0.023576971920558076, test error 0.043044050962318996\n",
            "Loss: 1.6693793127056766\n",
            "training error 0.023545923994119037, test error 0.04329210997643096\n",
            "Loss: 2.255290848303537\n",
            "training error 0.023512767299259125, test error 0.04337785191038756\n",
            "Loss: 2.4578119635691564\n",
            "training error 0.023555181019100657, test error 0.043185254417844335\n",
            "Loss: 2.002899679843928\n",
            "training error 0.023517173058825412, test error 0.043420343236260775\n",
            "Loss: 2.5581758148127465\n",
            "training error 0.023541141456986752, test error 0.04331303142757426\n",
            "Loss: 2.3047069907089712\n",
            "training error 0.02355846321651546, test error 0.04347448632353971\n",
            "Loss: 2.686060945386104\n",
            "training error 0.023480237808684693, test error 0.043349147737551594\n",
            "Loss: 2.3900131534972946\n",
            "training error 0.023528880721433457, test error 0.04328995796549099\n",
            "Loss: 2.2502078319133645\n",
            "training error 0.023461999341305367, test error 0.04343946718987701\n",
            "Loss: 2.603346295998854\n",
            "training error 0.02347639691410364, test error 0.04339916486575697\n",
            "Loss: 2.508152832871269\n",
            "training error 0.023453164949816887, test error 0.043364876959972175\n",
            "Loss: 2.4271653323656883\n",
            "training error 0.023428475474793438, test error 0.04351436625024976\n",
            "Loss: 2.7802567123969\n",
            "training error 0.023483918756828197, test error 0.04345162493991286\n",
            "Loss: 2.6320627126080964\n",
            "training error 0.023411291677287442, test error 0.04335280148660511\n",
            "Loss: 2.3986432519692125\n",
            "training error 0.023450064699211386, test error 0.043266421624043705\n",
            "Loss: 2.1946153592547812\n",
            "training error 0.0234136863439315, test error 0.043072352586759434\n",
            "Loss: 1.7362273097243586\n",
            "training error 0.02339951896272263, test error 0.043132641686647336\n",
            "Loss: 1.8786292265488225\n",
            "training error 0.023411220519317783, test error 0.04285223406417968\n",
            "Loss: 1.2163107808292217\n",
            "training error 0.023510160360865395, test error 0.04283987863873579\n",
            "Loss: 1.187127457978776\n",
            "training error 0.02338285497879683, test error 0.04317186571405847\n",
            "Loss: 1.9712757696106253\n",
            "training error 0.02338967651647244, test error 0.04323127196141361\n",
            "Loss: 2.1115924024754484\n",
            "training error 0.02339135245550072, test error 0.0433329046364688\n",
            "Loss: 2.3516472012163003\n",
            "training error 0.023354570059495424, test error 0.04342450265573685\n",
            "Loss: 2.5680002989628026\n",
            "training error 0.02337005128668081, test error 0.043224811863875036\n",
            "Loss: 2.0963337525026216\n",
            "training error 0.023419901973688494, test error 0.04317533374202001\n",
            "Loss: 1.9794671977500844\n",
            "training error 0.023345712027982884, test error 0.04328660654434078\n",
            "Loss: 2.242291827249776\n",
            "training error 0.023351850668776798, test error 0.0435553144685057\n",
            "Loss: 2.8769757674339536\n",
            "training error 0.02328557412273281, test error 0.04338702302592696\n",
            "Loss: 2.4794739959209\n",
            "training error 0.02333441570936218, test error 0.043222534278993016\n",
            "Loss: 2.090954132413203\n",
            "training error 0.02331889563595162, test error 0.043336427376572624\n",
            "Loss: 2.35996785858541\n",
            "training error 0.02328097920595077, test error 0.04339930786384429\n",
            "Loss: 2.5084905921308076\n",
            "training error 0.023256328842435153, test error 0.043286882967645456\n",
            "Loss: 2.242944734798824\n",
            "training error 0.02327291879819467, test error 0.04331980405601634\n",
            "Loss: 2.3207038338180164\n",
            "training error 0.02327341389809289, test error 0.043307079774621574\n",
            "Loss: 2.290649278944956\n",
            "training error 0.02325618381021177, test error 0.04308823643299706\n",
            "Loss: 1.7737447076451751\n",
            "training error 0.023257039038006584, test error 0.043143506633659165\n",
            "Loss: 1.9042920625101933\n",
            "training error 0.02329804750002928, test error 0.04319765262138447\n",
            "Loss: 2.032184043892782\n",
            "training error 0.02326357560500592, test error 0.04316474643603683\n",
            "Loss: 1.954460145582848\n",
            "training error 0.023223385083673796, test error 0.042825428516251075\n",
            "Loss: 1.1529964932770698\n",
            "training error 0.023227289047992, test error 0.04249023626630232\n",
            "Loss: 0.36127760899846706\n",
            "training error 0.02329208406110042, test error 0.04256319998620837\n",
            "Loss: 0.533616781296331\n",
            "training error 0.02321226518985948, test error 0.0426688937676975\n",
            "Loss: 0.7832638503098899\n",
            "training error 0.02326676695529826, test error 0.042682904134311396\n",
            "Loss: 0.8163561184801216\n",
            "training error 0.023199147354041525, test error 0.04306024501336488\n",
            "Loss: 1.7076294095619282\n",
            "training error 0.023172626950145398, test error 0.04306983638685446\n",
            "Loss: 1.7302840846596501\n",
            "training error 0.02318015523826825, test error 0.04315662432365276\n",
            "Loss: 1.935275842378692\n",
            "training error 0.023184445433241732, test error 0.042914321413428554\n",
            "Loss: 1.3629601346930365\n",
            "training error 0.02317670796972765, test error 0.043209671208836814\n",
            "Loss: 2.060571760642893\n",
            "training error 0.023225961754834636, test error 0.04291911419425663\n",
            "Loss: 1.374280608511791\n",
            "training error 0.02315971346221605, test error 0.04321617840852685\n",
            "Loss: 2.0759416651653373\n",
            "training error 0.023145338088899365, test error 0.042985470550464124\n",
            "Loss: 1.531013290456018\n",
            "training error 0.023104363374102398, test error 0.043159515514525744\n",
            "Loss: 1.9421047902811184\n",
            "training error 0.023174852467607757, test error 0.04271731983915178\n",
            "Loss: 0.897645478368525\n",
            "training error 0.02309755404363659, test error 0.04297884058261792\n",
            "Loss: 1.5153534094569876\n",
            "training error 0.02310571643769345, test error 0.04301496462981037\n",
            "Loss: 1.6006778474271677\n",
            "training error 0.023130499607838858, test error 0.04319516078238119\n",
            "Loss: 2.026298358899248\n",
            "training error 0.02307006655216466, test error 0.0431188537432156\n",
            "Loss: 1.8460623184775926\n",
            "training error 0.023127142357357744, test error 0.043163605456217106\n",
            "Loss: 1.9517651689836102\n",
            "training error 0.023060337560077897, test error 0.043055341485072014\n",
            "Loss: 1.6960473519527453\n",
            "training error 0.023047460949526997, test error 0.043135345270354605\n",
            "Loss: 1.8850150492419226\n",
            "training error 0.023142132002444502, test error 0.043020914506198715\n",
            "Loss: 1.6147313629738091\n",
            "training error 0.02303934110209038, test error 0.04321151575732536\n",
            "Loss: 2.064928555501888\n",
            "training error 0.023014219330188157, test error 0.04331291393214612\n",
            "Loss: 2.3044294683340016\n",
            "training error 0.023017686003788698, test error 0.04329691352339378\n",
            "Loss: 2.266636751565221\n",
            "training error 0.02302444692866792, test error 0.04332701333841907\n",
            "Loss: 2.337732046794927\n",
            "training error 0.022992441634031222, test error 0.0434294760566695\n",
            "Loss: 2.5797473946610694\n",
            "training error 0.02299654544676273, test error 0.04350895853152142\n",
            "Loss: 2.767483764816081\n",
            "training error 0.022981940746656188, test error 0.04340676510545884\n",
            "Loss: 2.5261044809118616\n",
            "training error 0.022983865487755883, test error 0.04347967962206267\n",
            "Loss: 2.6983274357746945\n",
            "training error 0.023029152936573302, test error 0.04380432090135241\n",
            "Loss: 3.465125091356991\n",
            "training error 0.023017859421314023, test error 0.04378461210446166\n",
            "Loss: 3.418573219447052\n",
            "training error 0.022985027078513885, test error 0.04352862488413382\n",
            "Loss: 2.8139353840002768\n",
            "training error 0.023065012705729082, test error 0.04346743254589096\n",
            "Loss: 2.669400032220981\n",
            "training error 0.02305794106292925, test error 0.04325731761156251\n",
            "Loss: 2.1731118232835023\n",
            "training error 0.0229692671464048, test error 0.04321544021602301\n",
            "Loss: 2.074198065945465\n",
            "training error 0.02293222282308332, test error 0.043458936464260606\n",
            "Loss: 2.649332419469941\n",
            "training error 0.022934887774955316, test error 0.043439559967031506\n",
            "Loss: 2.603565434195687\n",
            "training error 0.022922477015115985, test error 0.0434273839908258\n",
            "Loss: 2.574805967656535\n",
            "training error 0.022893853753337752, test error 0.043562301882510375\n",
            "Loss: 2.893479930703302\n",
            "training error 0.02295419862843723, test error 0.04373604958750436\n",
            "Loss: 3.303869309231122\n",
            "training error 0.023019872444550254, test error 0.04353189737805366\n",
            "Loss: 2.821664963772541\n",
            "training error 0.022886014029309388, test error 0.04357636240599538\n",
            "Loss: 2.9266906686234773\n",
            "training error 0.022879805069214776, test error 0.043525045047842374\n",
            "Loss: 2.8054798663235125\n",
            "training error 0.022893035991851923, test error 0.04346913794412155\n",
            "Loss: 2.6734281563335793\n",
            "training error 0.02287081442719199, test error 0.04358526361351799\n",
            "Loss: 2.947715182434929\n",
            "training error 0.02301656538193724, test error 0.04351005771708363\n",
            "Loss: 2.7700800240286982\n",
            "training error 0.022897084737523186, test error 0.04369916158199659\n",
            "Loss: 3.216740413594832\n",
            "training error 0.023006386371506506, test error 0.04338789766789567\n",
            "Loss: 2.4815398866558924\n",
            "training error 0.022843899848778795, test error 0.04369672586759015\n",
            "Loss: 3.210987294026646\n",
            "training error 0.022823507477489475, test error 0.04364863835883783\n",
            "Loss: 3.0974053458075357\n",
            "training error 0.022810422150064086, test error 0.04366847578644309\n",
            "Loss: 3.144261041465768\n",
            "training error 0.022838186528233776, test error 0.043572540041643605\n",
            "Loss: 2.9176623034379734\n",
            "training error 0.022883835991723257, test error 0.04346434967213239\n",
            "Loss: 2.662118332322261\n",
            "training error 0.022798019688364243, test error 0.04363758988836925\n",
            "Loss: 3.071309030293956\n",
            "training error 0.022786848914738617, test error 0.04366774421907513\n",
            "Loss: 3.1425330907138393\n",
            "training error 0.02281986695604298, test error 0.04384341157027789\n",
            "Loss: 3.5574566437485355\n",
            "training error 0.022804941802042893, test error 0.043765386879164356\n",
            "Loss: 3.3731635361243795\n",
            "training error 0.02281341498964084, test error 0.04353093629510237\n",
            "Loss: 2.8193949007803143\n",
            "training error 0.022886672229881562, test error 0.04384995770743877\n",
            "Loss: 3.572918517971879\n",
            "training error 0.022767661502277547, test error 0.0435505939958365\n",
            "Loss: 2.865826084362544\n",
            "training error 0.022752677744816104, test error 0.043491858620690195\n",
            "Loss: 2.727094041227196\n",
            "training error 0.022770455842699225, test error 0.043653932506649756\n",
            "Loss: 3.1099100406490887\n",
            "training error 0.022763596994356095, test error 0.04364864645781352\n",
            "Loss: 3.0974244754622404\n",
            "training error 0.02279797735364736, test error 0.04341985687287958\n",
            "Loss: 2.557027032066217\n",
            "training error 0.022776734001546662, test error 0.04348640033453779\n",
            "Loss: 2.714201654176729\n",
            "training error 0.022709690577894712, test error 0.04375347568279764\n",
            "Loss: 3.3450294754492793\n",
            "training error 0.022715486918001838, test error 0.043980033239541784\n",
            "Loss: 3.880154902954791\n",
            "training error 0.022700450172736652, test error 0.044062237277491874\n",
            "Loss: 4.074319562844719\n",
            "training error 0.022738033057609495, test error 0.043841032527362954\n",
            "Loss: 3.5518373813623993\n",
            "training error 0.022773851703793844, test error 0.0442169158514869\n",
            "Loss: 4.439667950356285\n",
            "training error 0.02267527033540165, test error 0.04415837933988679\n",
            "Loss: 4.301405619826015\n",
            "training error 0.022653378753075645, test error 0.044080716679814794\n",
            "Loss: 4.117967623894692\n",
            "training error 0.022705896176315157, test error 0.0440371397544025\n",
            "Loss: 4.015039603413939\n",
            "training error 0.022702950478021976, test error 0.04364343371266828\n",
            "Loss: 3.0851120524540976\n",
            "training error 0.022659269465671313, test error 0.043469061711394995\n",
            "Loss: 2.6732480958183924\n",
            "training error 0.022648492561726363, test error 0.043627530745913946\n",
            "Loss: 3.0475494921700097\n",
            "training error 0.022764275160891176, test error 0.0435528824212407\n",
            "Loss: 2.8712313095968556\n",
            "training error 0.02264659228334477, test error 0.043645898811316086\n",
            "Loss: 3.090934577143667\n",
            "training error 0.022760765686186662, test error 0.04343765738910107\n",
            "Loss: 2.599071574696943\n",
            "training error 0.022627935689473105, test error 0.04369067762946759\n",
            "Loss: 3.196701449592676\n",
            "training error 0.022649546832846465, test error 0.043838158827029176\n",
            "Loss: 3.5450497458417107\n",
            "training error 0.022666482412513893, test error 0.044022382343640545\n",
            "Loss: 3.980182828577772\n",
            "training error 0.02263275816836166, test error 0.04361604472862917\n",
            "Loss: 3.02041969787763\n",
            "training error 0.022599727284940696, test error 0.04374159390689592\n",
            "Loss: 3.316964905455211\n",
            "training error 0.02268250805119809, test error 0.04405358977976235\n",
            "Loss: 4.053894307620998\n",
            "training error 0.02258573532279719, test error 0.04389206008605773\n",
            "Loss: 3.6723636818465577\n",
            "training error 0.02257337568037594, test error 0.043585769172176395\n",
            "Loss: 2.948909304127856\n",
            "training error 0.022564303786889617, test error 0.04352037747399521\n",
            "Loss: 2.794455129447493\n",
            "training error 0.022601275294424693, test error 0.043522262982899594\n",
            "Loss: 2.798908672172007\n",
            "training error 0.02261641472772775, test error 0.04344410250253572\n",
            "Loss: 2.6142948324491933\n",
            "training error 0.022597059857059884, test error 0.043744465801435845\n",
            "Loss: 3.323748275718863\n",
            "training error 0.022562562815603284, test error 0.043501970079576846\n",
            "Loss: 2.75097714994883\n",
            "training error 0.02256752260728525, test error 0.043798947579019\n",
            "Loss: 3.4524333875263213\n",
            "training error 0.022591138677803622, test error 0.043655437048827045\n",
            "Loss: 3.113463745885814\n",
            "training error 0.022564412782938654, test error 0.04359826113776394\n",
            "Loss: 2.978415132675294\n",
            "training error 0.02253245531256134, test error 0.04361188771588165\n",
            "Loss: 3.010600898382987\n",
            "training error 0.022515620171887542, test error 0.04388916169154188\n",
            "Loss: 3.6655177190516586\n",
            "training error 0.02260531725580519, test error 0.04363476864193251\n",
            "Loss: 3.064645290062762\n",
            "training error 0.022526373133288733, test error 0.043780797938852374\n",
            "Loss: 3.409564219563843\n",
            "training error 0.022493000662754573, test error 0.043781770274752316\n",
            "Loss: 3.4118608618458834\n",
            "training error 0.022535171527240747, test error 0.04367007656010445\n",
            "Loss: 3.148042043972077\n",
            "training error 0.022495455823821908, test error 0.043832353882655065\n",
            "Loss: 3.5313385574598843\n",
            "training error 0.02257168782360732, test error 0.043712697282090276\n",
            "Loss: 3.2487115268223787\n",
            "training error 0.02247471376346573, test error 0.04404491230038994\n",
            "Loss: 4.033398236222796\n",
            "training error 0.022491212117531703, test error 0.04398022846698425\n",
            "Loss: 3.880616027139361\n",
            "training error 0.022487676900481895, test error 0.04391449386023339\n",
            "Loss: 3.725351907747454\n",
            "training error 0.022518666555017334, test error 0.043889740492812614\n",
            "Loss: 3.66688483865647\n",
            "training error 0.02261137324016252, test error 0.04407813796124847\n",
            "Loss: 4.1118767307236626\n",
            "training error 0.022462015603164694, test error 0.04394773502303052\n",
            "Loss: 3.8038670175863265\n",
            "training error 0.02251555694063355, test error 0.044201299431658944\n",
            "Loss: 4.4027822094583335\n",
            "training error 0.022446826718213218, test error 0.04388050338914604\n",
            "Loss: 3.6450669433740535\n",
            "training error 0.0224413356367452, test error 0.04384562291161845\n",
            "Loss: 3.562679797622681\n",
            "training error 0.022442361936143584, test error 0.04370913350654702\n",
            "Loss: 3.2402939443831658\n",
            "training error 0.022454190042672212, test error 0.04354113888950307\n",
            "Loss: 2.843493270169528\n",
            "training error 0.02242577146827575, test error 0.04368768554832122\n",
            "Loss: 3.1896342004327005\n",
            "training error 0.022405163322901923, test error 0.04375541430176991\n",
            "Loss: 3.3496084633286127\n",
            "training error 0.022463970161968664, test error 0.04416154003422882\n",
            "Loss: 4.308871130733261\n",
            "training error 0.022452807482403383, test error 0.04378154873562495\n",
            "Loss: 3.411337589870578\n",
            "training error 0.022458586638440137, test error 0.04419546393429884\n",
            "Loss: 4.388998855398496\n",
            "training error 0.022354297634431156, test error 0.043909137531928294\n",
            "Loss: 3.712700343547537\n",
            "training error 0.02237392068377165, test error 0.04393584783888114\n",
            "Loss: 3.7757896733953356\n",
            "training error 0.02236714098083043, test error 0.043895257906949245\n",
            "Loss: 3.6799168850866204\n",
            "training error 0.022477000165917672, test error 0.043713135665818935\n",
            "Loss: 3.24974698237539\n",
            "training error 0.022365716699272134, test error 0.044133677700545805\n",
            "Loss: 4.243060731655457\n",
            "training error 0.02237441959883957, test error 0.04419960755783415\n",
            "Loss: 4.398786029782009\n",
            "training error 0.022373295638188816, test error 0.044054012900615086\n",
            "Loss: 4.054893712498342\n",
            "training error 0.022323609374162966, test error 0.044085974357922324\n",
            "Loss: 4.130386177861345\n",
            "training error 0.022355425499440147, test error 0.044145835394893085\n",
            "Loss: 4.2717770167291835\n",
            "training error 0.022323174684189116, test error 0.044075318196802965\n",
            "Loss: 4.105216490932717\n",
            "training error 0.022495712928801224, test error 0.044233586718589776\n",
            "Loss: 4.479044279327704\n",
            "training error 0.022313843023777223, test error 0.044129376003331146\n",
            "Loss: 4.232900189698374\n",
            "training error 0.02233822009489748, test error 0.04392152379616217\n",
            "Loss: 3.741956507140598\n",
            "training error 0.022296609964563407, test error 0.04421127477105857\n",
            "Loss: 4.42634380606739\n",
            "training error 0.022310517214659497, test error 0.044026798107419045\n",
            "Loss: 3.990612793985626\n",
            "training error 0.022300844026378722, test error 0.04404890360854664\n",
            "Loss: 4.042825644049541\n",
            "training error 0.022304807688702227, test error 0.044341039252366626\n",
            "Loss: 4.732845493907467\n",
            "training error 0.02231393829299728, test error 0.044170723261428006\n",
            "Loss: 4.330561770637176\n",
            "training error 0.022264861540773433, test error 0.044149589348961626\n",
            "Loss: 4.280643797889039\n",
            "training error 0.02228919089247639, test error 0.04426152323398796\n",
            "Loss: 4.545029894463681\n",
            "training error 0.022291188220548746, test error 0.04454001147075174\n",
            "Loss: 5.202814780984921\n",
            "training error 0.022387097340812057, test error 0.04465268174862742\n",
            "Loss: 5.468940226023489\n",
            "training error 0.022350377540044666, test error 0.044043782514367594\n",
            "Loss: 4.030729699198976\n",
            "training error 0.02225206742734533, test error 0.04428893880626523\n",
            "Loss: 4.60978505004539\n",
            "training error 0.022292421026422145, test error 0.04424181709909915\n",
            "Loss: 4.498484310160933\n",
            "training error 0.022246295497721422, test error 0.04416557146240207\n",
            "Loss: 4.318393301415502\n",
            "training error 0.022227332521245848, test error 0.044257914078242214\n",
            "Loss: 4.53650512469157\n",
            "training error 0.02227183111751778, test error 0.044196123073923954\n",
            "Loss: 4.390555732947599\n",
            "training error 0.022279052876637433, test error 0.044530597408685015\n",
            "Loss: 5.180578912711886\n",
            "training error 0.0222441256254337, test error 0.044392197032160054\n",
            "Loss: 4.853679374602549\n",
            "training error 0.02222626814427505, test error 0.04450082636033036\n",
            "Loss: 5.1102601592480035\n",
            "training error 0.022257977024377276, test error 0.04418359707097197\n",
            "Loss: 4.360969508674417\n",
            "training error 0.022189629803131165, test error 0.04436333111419653\n",
            "Loss: 4.7854985250535\n",
            "training error 0.022188453389805178, test error 0.04438957793594687\n",
            "Loss: 4.847493110057233\n",
            "training error 0.022161512592782963, test error 0.044322466586998045\n",
            "Loss: 4.688977146993278\n",
            "training error 0.02219249787490956, test error 0.04438747816398863\n",
            "Loss: 4.842533481329925\n",
            "training error 0.022221551878695518, test error 0.04450549221584873\n",
            "Loss: 5.121280837458109\n",
            "training error 0.022291700307585538, test error 0.04391462969180204\n",
            "Loss: 3.725672739801289\n",
            "training error 0.02221763954877128, test error 0.04351763227986153\n",
            "Loss: 2.7879710235739763\n",
            "training error 0.02218486474115353, test error 0.04382312900202796\n",
            "Loss: 3.5095495328067106\n",
            "training error 0.022190585556698193, test error 0.04365232852622688\n",
            "Loss: 3.1061214638219203\n",
            "training error 0.02215206332327935, test error 0.043958553297970486\n",
            "Loss: 3.829419614839402\n",
            "training error 0.022134735522803782, test error 0.043920530957551676\n",
            "Loss: 3.7396114377749035\n",
            "training error 0.02218310698896461, test error 0.04397465311154449\n",
            "Loss: 3.867447124251844\n",
            "training error 0.022152067905437587, test error 0.044225851394860495\n",
            "Loss: 4.460773564914056\n",
            "training error 0.02222110020311815, test error 0.04406258951366494\n",
            "Loss: 4.0751515392104976\n",
            "training error 0.02211327637541177, test error 0.04439780112899624\n",
            "Loss: 4.866916164221657\n",
            "training error 0.02213946703464719, test error 0.04412446431782468\n",
            "Loss: 4.221298864877454\n",
            "training error 0.022123181727508778, test error 0.04409110489084254\n",
            "Loss: 4.142504416871828\n",
            "training error 0.022117815838404983, test error 0.044071768819218315\n",
            "Loss: 4.096832916247939\n",
            "training error 0.022290436015989505, test error 0.04461088410881408\n",
            "Loss: 5.3702148504709335\n",
            "training error 0.02210951344960749, test error 0.04413944916448128\n",
            "Loss: 4.256692839761622\n",
            "training error 0.022102935237664975, test error 0.04437476617440453\n",
            "Loss: 4.812507959522594\n",
            "training error 0.0221189424195648, test error 0.04426931632857582\n",
            "Loss: 4.563437062747311\n",
            "training error 0.022086354010073587, test error 0.04439351992083971\n",
            "Loss: 4.8568040171014015\n",
            "training error 0.022199462497756964, test error 0.04395017354962456\n",
            "Loss: 3.8096267794979433\n",
            "training error 0.022093505110007468, test error 0.0437693720849929\n",
            "Loss: 3.3825765303468813\n",
            "training error 0.022078731927593357, test error 0.04389960817527633\n",
            "Loss: 3.6901921512562774\n",
            "training error 0.02213130927967054, test error 0.04408916866230592\n",
            "Loss: 4.137931075164358\n",
            "training error 0.022051380339743746, test error 0.04412687467871944\n",
            "Loss: 4.2269920998451616\n",
            "training error 0.022100356773057158, test error 0.044125242534594475\n",
            "Loss: 4.2231370007914215\n",
            "training error 0.02206269379654167, test error 0.044206352168988675\n",
            "Loss: 4.414716696493337\n",
            "training error 0.02206839870987702, test error 0.044284217052562976\n",
            "Loss: 4.59863234119382\n",
            "training error 0.022017184253640385, test error 0.04449552971842836\n",
            "Loss: 5.097749573404453\n",
            "training error 0.022031112336818183, test error 0.0443676974027555\n",
            "Loss: 4.795811630761859\n",
            "training error 0.022079722458682366, test error 0.044544753095526454\n",
            "Loss: 5.214014425000779\n",
            "training error 0.022025516664868166, test error 0.04425304862526914\n",
            "Loss: 4.525013000395894\n",
            "training error 0.02200555014665555, test error 0.04438008252152028\n",
            "Loss: 4.825065088774716\n",
            "training error 0.02200403783199756, test error 0.044506554897068694\n",
            "Loss: 5.1237908739817595\n",
            "training error 0.02201345929461538, test error 0.04450180634019761\n",
            "Loss: 5.1125748564620155\n",
            "training error 0.02198027540947652, test error 0.044474603434863126\n",
            "Loss: 5.048322016893536\n",
            "training error 0.02197831534471269, test error 0.04437223733719595\n",
            "Loss: 4.806534885343194\n",
            "training error 0.02201342887161465, test error 0.044384290473623765\n",
            "Loss: 4.8350042062363485\n",
            "training error 0.022056768920891184, test error 0.04459029946176576\n",
            "Loss: 5.3215942341027\n",
            "training error 0.02204794871748161, test error 0.0441611481716765\n",
            "Loss: 4.307945557475645\n",
            "training error 0.02195912423573771, test error 0.044488135985903454\n",
            "Loss: 5.08028569210599\n",
            "training error 0.02197088140189477, test error 0.044369955438818706\n",
            "Loss: 4.801145076845104\n",
            "training error 0.021943423796178682, test error 0.044368951419611065\n",
            "Loss: 4.798773599082762\n",
            "training error 0.021982518991052385, test error 0.04409884859743924\n",
            "Loss: 4.160794931488021\n",
            "training error 0.021976707181663627, test error 0.044343577800035595\n",
            "Loss: 4.738841504043356\n",
            "training error 0.022005055248444004, test error 0.04444104811262294\n",
            "Loss: 4.96906486733335\n",
            "training error 0.02199299813084784, test error 0.043992241170174834\n",
            "Loss: 3.908989845354638\n",
            "training error 0.021948571535049748, test error 0.04428206726542219\n",
            "Loss: 4.593554577382752\n",
            "training error 0.02193184120672937, test error 0.04420461788872957\n",
            "Loss: 4.410620353478323\n",
            "training error 0.02192827321536913, test error 0.044267176810665824\n",
            "Loss: 4.558383554699752\n",
            "training error 0.02191772174295554, test error 0.044343107351948664\n",
            "Loss: 4.737730312974486\n",
            "training error 0.021927543684824528, test error 0.04438994733098662\n",
            "Loss: 4.848365615399386\n",
            "training error 0.02199666263349234, test error 0.04422168600095205\n",
            "Loss: 4.4509349692475775\n",
            "training error 0.0219737325046565, test error 0.04444943267747982\n",
            "Loss: 4.988869079198821\n",
            "training error 0.02188525489169818, test error 0.04432796771480513\n",
            "Loss: 4.701970725366178\n",
            "training error 0.021936590310223304, test error 0.04458361229463118\n",
            "Loss: 5.305799249273591\n",
            "training error 0.021870314633047474, test error 0.04454979789212388\n",
            "Loss: 5.225930156149494\n",
            "training error 0.02192996392131699, test error 0.04410764848015304\n",
            "Loss: 4.18158011767602\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9537037037037036 0.04410764848015304\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<topologyNN.HiddenLayer at 0x7f9b7d58a520>,\n",
              "  <topologyNN.OutputLayer at 0x7f9b7d58a880>],\n",
              " 0.9537037037037036,\n",
              " 0.04410764848015304,\n",
              " 0.02192996392131699,\n",
              " 0.9508196721311475)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with lambda values seemingly tackles the overfitting problem after after about 300 epochs "
      ],
      "metadata": {
        "id": "OnU8tSy0o6RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_space_def = search_space_dict(layers_range=[1], units_range=[4], eta_0_range=[0.6],\n",
        "                        alpha_range=[0.4], lamb_range=[0.001], lamb0_range = [0.0], minibatch_size_range = [40], num_targets=1, configurations = 0)    \n",
        "\n",
        "search_space_def[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqDRQUjmoB-T",
        "outputId": "bb8c9471-02a8-4f71-a1ef-43bd088fb5ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'units': array([4, 1]),\n",
              " 'layers': 2,\n",
              " 'eta_0': 0.6,\n",
              " 'alpha': 0.4,\n",
              " 'lamb': 0.001,\n",
              " 'lamb0': 0.0,\n",
              " 'minibatch_size': 40}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_test(hyperparams = search_space_def[0][0],\n",
        "           num_inputs = 17, seed = 0, activation_output = sigmoid, activation_hidden = sigmoid,\n",
        "           task = 'binary_classification', thr = 0.5, stop_class = 'GL', stop_param = 3, data_train = X_train, data_val = X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_c20MpxncR3",
        "outputId": "45be3fcd-f6aa-453b-d394-7596253297ac"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error 0.1250104793423171, test error 0.24997005005498446\n",
            "training error 0.12502432306325656, test error 0.25005583052253794\n",
            "training error 0.1250308213290121, test error 0.250066129306235\n",
            "training error 0.12499501087166902, test error 0.25006607006360104\n",
            "training error 0.12504080138193663, test error 0.2502035047196502\n",
            "training error 0.12502329555558014, test error 0.2502490760237485\n",
            "training error 0.1250535394612472, test error 0.2501775615194901\n",
            "training error 0.12500664539923736, test error 0.2502826821760011\n",
            "training error 0.12520010782604893, test error 0.2502733523940853\n",
            "training error 0.12498160023992583, test error 0.2504057214319338\n",
            "training error 0.1250455690091534, test error 0.2504606404612979\n",
            "training error 0.12514867934042706, test error 0.2506699788073581\n",
            "training error 0.12498684733689831, test error 0.2506068434170616\n",
            "training error 0.12505741091217049, test error 0.2506307114956747\n",
            "training error 0.12496820703241676, test error 0.2504649118577696\n",
            "training error 0.12512527884506647, test error 0.2504497331656798\n",
            "training error 0.1249979007623721, test error 0.2504469986679023\n",
            "training error 0.12519423521166664, test error 0.25050898767413776\n",
            "training error 0.125052634056904, test error 0.2505722226395062\n",
            "training error 0.12497167908717671, test error 0.25055888089493805\n",
            "training error 0.1250129968131523, test error 0.25060887392046216\n",
            "training error 0.125114915685558, test error 0.2504935785278313\n",
            "training error 0.12504679584225903, test error 0.25053966013954754\n",
            "training error 0.12503509087891085, test error 0.2506208334258439\n",
            "training error 0.12507913441560986, test error 0.2506912307298787\n",
            "training error 0.12499034642481513, test error 0.25055218040661703\n",
            "training error 0.12502897481164488, test error 0.250610854173806\n",
            "training error 0.1250084508864322, test error 0.25063727645768896\n",
            "training error 0.12504291091471434, test error 0.25058577606669646\n",
            "training error 0.12503631372061363, test error 0.25038039409519963\n",
            "training error 0.12503354527266441, test error 0.2503868785451872\n",
            "training error 0.12496608763007544, test error 0.250471611376115\n",
            "training error 0.12498743226241493, test error 0.25057773504415315\n",
            "training error 0.12503955619242263, test error 0.2506792896322371\n",
            "training error 0.12500718890791745, test error 0.25067993547229345\n",
            "training error 0.12502854188738743, test error 0.2504837322929925\n",
            "training error 0.12502920210056717, test error 0.25060632326537796\n",
            "training error 0.1250784035472146, test error 0.2505216261020449\n",
            "training error 0.12500418235875393, test error 0.25049856443040036\n",
            "training error 0.1249669094648118, test error 0.25042525242465163\n",
            "training error 0.12495630441183801, test error 0.2504464191377516\n",
            "training error 0.1250856072926839, test error 0.25050683562060083\n",
            "training error 0.1249572238078712, test error 0.25046138067693896\n",
            "training error 0.1250304785318903, test error 0.25043831103449243\n",
            "training error 0.12510831056294527, test error 0.2506087416474773\n",
            "training error 0.1249541660631965, test error 0.25054988851215443\n",
            "training error 0.12503824755393383, test error 0.2505771266064274\n",
            "training error 0.12499662039095838, test error 0.25060936852722626\n",
            "training error 0.12498177270064648, test error 0.2505286493435409\n",
            "training error 0.1251380755359051, test error 0.25064070236005687\n",
            "Loss: 0.26829306347895265\n",
            "training error 0.1250450722451289, test error 0.25063307781373345\n",
            "Loss: 0.2652428795382189\n",
            "training error 0.12499872370317812, test error 0.2505759526057771\n",
            "Loss: 0.2423900585927674\n",
            "training error 0.12503486184824547, test error 0.2506284034923797\n",
            "Loss: 0.2633729269768237\n",
            "training error 0.12501205544907057, test error 0.25071945389386646\n",
            "Loss: 0.29979745122152845\n",
            "training error 0.12499913846675703, test error 0.25065744501243414\n",
            "Loss: 0.2749909268324302\n",
            "training error 0.1250611737489601, test error 0.2507523854667101\n",
            "Loss: 0.31297165862611553\n",
            "training error 0.12496154221121338, test error 0.25068026894586715\n",
            "Loss: 0.28412159405755144\n",
            "training error 0.12495613115349853, test error 0.2506079926689889\n",
            "Loss: 0.2552076194184627\n",
            "training error 0.12505097969070972, test error 0.2506788442047016\n",
            "Loss: 0.28355162930968625\n",
            "training error 0.12499270248782993, test error 0.2505620307210181\n",
            "Loss: 0.2368206374737447\n",
            "training error 0.1250201834444403, test error 0.2505531145375621\n",
            "Loss: 0.2332537367774279\n",
            "training error 0.12506705212835711, test error 0.2504735759720022\n",
            "Loss: 0.20143449861573792\n",
            "training error 0.1249417097506573, test error 0.25050786590322816\n",
            "Loss: 0.21515211447347138\n",
            "training error 0.12507603646000942, test error 0.2504549130730033\n",
            "Loss: 0.19396844458454687\n",
            "training error 0.12500647728026168, test error 0.25051676318982824\n",
            "Loss: 0.21871145552176952\n",
            "training error 0.12494855432354958, test error 0.2506461539630071\n",
            "Loss: 0.2704739659306954\n",
            "training error 0.12498462862087005, test error 0.2506767473833105\n",
            "Loss: 0.28271280026170853\n",
            "training error 0.1251234969257222, test error 0.2507315536412068\n",
            "Loss: 0.30463793004595896\n",
            "training error 0.12494373879969534, test error 0.25068866103535786\n",
            "Loss: 0.2874788320502031\n",
            "training error 0.12495215619114385, test error 0.2506700433610123\n",
            "Loss: 0.28003087004777605\n",
            "training error 0.12496925157616702, test error 0.2507618067461644\n",
            "Loss: 0.3167406219288127\n",
            "training error 0.12495436250734517, test error 0.25069532404735567\n",
            "Loss: 0.2901443561785344\n",
            "training error 0.12494955592268134, test error 0.25072448053932433\n",
            "Loss: 0.3018083503099289\n",
            "training error 0.12494600975825049, test error 0.25071327781510166\n",
            "Loss: 0.2973267237229793\n",
            "training error 0.12493967060624925, test error 0.25065788229600827\n",
            "Loss: 0.2751658612191843\n",
            "training error 0.1250039548422507, test error 0.2506312540581344\n",
            "Loss: 0.26451328989394174\n",
            "training error 0.1250560048237033, test error 0.25064126883842613\n",
            "Loss: 0.2685196819755209\n",
            "training error 0.12494480439272004, test error 0.2506642207144255\n",
            "Loss: 0.2777015323589094\n",
            "training error 0.12493066539012193, test error 0.25066817116229034\n",
            "Loss: 0.2792819008326397\n",
            "training error 0.1249350529658215, test error 0.25073939225972175\n",
            "Loss: 0.30777375312285304\n",
            "training error 0.12493832842685566, test error 0.2507707055545167\n",
            "Loss: 0.3203005717509422\n",
            "training error 0.1251121611900602, test error 0.25063240199527426\n",
            "Loss: 0.26497251976551617\n",
            "training error 0.12529609012224505, test error 0.25080831419001187\n",
            "Loss: 0.3353458283674504\n",
            "training error 0.1249193670543643, test error 0.25063400529237617\n",
            "Loss: 0.26561391544532853\n",
            "training error 0.12500355227981463, test error 0.2505505549362404\n",
            "Loss: 0.2322297735781742\n",
            "training error 0.1249587713094674, test error 0.25064053887403265\n",
            "Loss: 0.26822766123408837\n",
            "training error 0.12493295745250782, test error 0.25062369722534283\n",
            "Loss: 0.261490194611147\n",
            "training error 0.12494879628772977, test error 0.250675561729942\n",
            "Loss: 0.2822384820910795\n",
            "training error 0.12493263792031319, test error 0.250663663989326\n",
            "Loss: 0.2774788156376884\n",
            "training error 0.12493846585426689, test error 0.25068412766231446\n",
            "Loss: 0.28566526556799143\n",
            "training error 0.12503885776104895, test error 0.2505978309527165\n",
            "Loss: 0.2511424459025857\n",
            "training error 0.12491903401404246, test error 0.250622076698298\n",
            "Loss: 0.2608419061284062\n",
            "training error 0.12493328231120857, test error 0.2506062016557654\n",
            "Loss: 0.254491128293588\n",
            "training error 0.1249430515772051, test error 0.2505703495811271\n",
            "Loss: 0.24014858020415808\n",
            "training error 0.12494632390083364, test error 0.25058487908461685\n",
            "Loss: 0.24596107793599664\n",
            "training error 0.1251057133930215, test error 0.2505423489025099\n",
            "Loss: 0.22894696680646476\n",
            "training error 0.1249727466050548, test error 0.25054568163153756\n",
            "Loss: 0.23028021814073263\n",
            "training error 0.12494905554668032, test error 0.25056418893617494\n",
            "Loss: 0.23768402697035285\n",
            "training error 0.12492266284730714, test error 0.25063111412413713\n",
            "Loss: 0.2644573095885949\n",
            "training error 0.1250239191127102, test error 0.2507080516735899\n",
            "Loss: 0.29523601665204513\n",
            "training error 0.12503359438264794, test error 0.2507268755138693\n",
            "Loss: 0.30276645490863174\n",
            "training error 0.12499137888744408, test error 0.25066962845634927\n",
            "Loss: 0.27986488829798706\n",
            "training error 0.12491649108378333, test error 0.25065008543746253\n",
            "Loss: 0.272046744131349\n",
            "training error 0.1249208885733768, test error 0.25062639081030386\n",
            "Loss: 0.26256775768738194\n",
            "training error 0.1249911772657334, test error 0.25062985686610806\n",
            "Loss: 0.2639543461220528\n",
            "training error 0.1249141548606548, test error 0.2506906206524498\n",
            "Loss: 0.28826277280291634\n",
            "training error 0.12497218804708851, test error 0.2508258623524332\n",
            "Loss: 0.3423659343431362\n",
            "training error 0.12492522672060002, test error 0.2507788745721316\n",
            "Loss: 0.3235685703024238\n",
            "training error 0.12495261806410955, test error 0.2508048964177541\n",
            "Loss: 0.3339785556653707\n",
            "training error 0.12512826798893484, test error 0.25059130534044816\n",
            "Loss: 0.24853188825102457\n",
            "training error 0.1250098829467978, test error 0.25082594905092637\n",
            "Loss: 0.3424006178954775\n",
            "training error 0.1249310661376766, test error 0.2506982581302587\n",
            "Loss: 0.2913181299575962\n",
            "training error 0.12494485387411593, test error 0.250632453209901\n",
            "Loss: 0.2649930080707019\n",
            "training error 0.12494330441339403, test error 0.2507435347712249\n",
            "Loss: 0.3094309562566844\n",
            "training error 0.12495898259279682, test error 0.2506990738217683\n",
            "Loss: 0.2916444456539935\n",
            "training error 0.12493573001755309, test error 0.25086183468663215\n",
            "Loss: 0.35675659202032417\n",
            "training error 0.12506851775320463, test error 0.25093691129954787\n",
            "Loss: 0.386790835282369\n",
            "training error 0.1252193792033682, test error 0.2507769555039333\n",
            "Loss: 0.3228008510504976\n",
            "training error 0.12494175213099039, test error 0.2507426595949105\n",
            "Loss: 0.30908084378751255\n",
            "training error 0.1250127441550906, test error 0.2508281410703255\n",
            "Loss: 0.3432775307091074\n",
            "training error 0.12495008617179547, test error 0.2507088875440384\n",
            "Loss: 0.2955704048910768\n",
            "training error 0.12497651506090729, test error 0.2507336335512959\n",
            "Loss: 0.3054699937626415\n",
            "training error 0.12491805510120435, test error 0.2508083173809381\n",
            "Loss: 0.3353471048908707\n",
            "training error 0.12488873749801392, test error 0.2507590238379538\n",
            "Loss: 0.3156273252718833\n",
            "training error 0.12489970410707729, test error 0.2507105662731538\n",
            "Loss: 0.2962419769914293\n",
            "training error 0.12492547030275739, test error 0.2507026151558122\n",
            "Loss: 0.29306114899230185\n",
            "training error 0.12505909918023134, test error 0.2508269542213684\n",
            "Loss: 0.3428027342457396\n",
            "training error 0.1249121594035077, test error 0.25080375360993784\n",
            "Loss: 0.33352137776905266\n",
            "training error 0.12492844910721153, test error 0.25088066629913053\n",
            "Loss: 0.3642901395370268\n",
            "training error 0.12488996061274447, test error 0.2507355960165761\n",
            "Loss: 0.3062550739271419\n",
            "training error 0.12488612453703529, test error 0.25072093786110344\n",
            "Loss: 0.30039110923640866\n",
            "training error 0.12489232115359472, test error 0.2508294015256513\n",
            "Loss: 0.34378177324756454\n",
            "training error 0.12494247628234063, test error 0.2507086486173198\n",
            "Loss: 0.2954748227529169\n",
            "training error 0.1248882817575, test error 0.2508178705138536\n",
            "Loss: 0.33916881589721726\n",
            "training error 0.12496148735124735, test error 0.25085889321790883\n",
            "Loss: 0.3555798635591856\n",
            "training error 0.12506443226327066, test error 0.2507623146654241\n",
            "Loss: 0.316943813975068\n",
            "training error 0.12494475797059057, test error 0.25077566879657515\n",
            "Loss: 0.3222861064409388\n",
            "training error 0.12491870138401899, test error 0.2508144584545227\n",
            "Loss: 0.3378038286396823\n",
            "training error 0.12520079649067534, test error 0.2509666796565587\n",
            "Loss: 0.3986996047546709\n",
            "training error 0.12483045083388981, test error 0.250841628969975\n",
            "Loss: 0.34867333698529457\n",
            "training error 0.12496899122923974, test error 0.2507840691232995\n",
            "Loss: 0.3256466397218327\n",
            "training error 0.12484707718761832, test error 0.25082118843620865\n",
            "Loss: 0.3404961438528309\n",
            "training error 0.12485718487157084, test error 0.25085252003828584\n",
            "Loss: 0.35303028627120536\n",
            "training error 0.12494343437372724, test error 0.2508574294159938\n",
            "Loss: 0.35499427263951944\n",
            "training error 0.12497905011885993, test error 0.2508405463172144\n",
            "Loss: 0.34824022399422283\n",
            "training error 0.12488576562222829, test error 0.2507953260579243\n",
            "Loss: 0.33014995306770434\n",
            "training error 0.1253089321463036, test error 0.250875312719653\n",
            "Loss: 0.3621484511722173\n",
            "training error 0.12493199736124412, test error 0.2507391897274759\n",
            "Loss: 0.3076927305180055\n",
            "training error 0.1248372148978651, test error 0.2508276374532243\n",
            "Loss: 0.34307605973242783\n",
            "training error 0.12493129946335574, test error 0.2506756977280331\n",
            "Loss: 0.2822928878453368\n",
            "training error 0.12483299226099316, test error 0.2507374108998453\n",
            "Loss: 0.30698111421429974\n",
            "training error 0.12486717915689845, test error 0.2507803520526863\n",
            "Loss: 0.3241596333334984\n",
            "training error 0.12495683523304156, test error 0.2507506347833717\n",
            "Loss: 0.31227130138811177\n",
            "training error 0.1248460399010925, test error 0.25058505673015996\n",
            "Loss: 0.24603214466702994\n",
            "training error 0.12481645526286081, test error 0.25068224788742965\n",
            "Loss: 0.28491326552462226\n",
            "training error 0.12484604404267174, test error 0.25066349142630906\n",
            "Loss: 0.2774097821607313\n",
            "training error 0.12482337826467906, test error 0.250664779870193\n",
            "Loss: 0.27792522146381504\n",
            "training error 0.12480459807477359, test error 0.2506195018579181\n",
            "Loss: 0.2598118465755306\n",
            "training error 0.12479654731389118, test error 0.2505955961642513\n",
            "Loss: 0.2502484234128355\n",
            "training error 0.12486302323782304, test error 0.2504951657094572\n",
            "Loss: 0.21007142829998493\n",
            "training error 0.12478715939586503, test error 0.2504885623777676\n",
            "Loss: 0.20742977915517713\n",
            "training error 0.1247959276297965, test error 0.25047389338632264\n",
            "Loss: 0.2015614795561893\n",
            "training error 0.12483280382800264, test error 0.25053805050359135\n",
            "Loss: 0.22722740123544138\n",
            "training error 0.12480322073071183, test error 0.25045067449858427\n",
            "Loss: 0.19227281168048282\n",
            "training error 0.12483102656894753, test error 0.25053408430365226\n",
            "Loss: 0.22564073117707917\n",
            "training error 0.1247841042276354, test error 0.2504756496556581\n",
            "Loss: 0.20226407146073377\n",
            "training error 0.12486560561703011, test error 0.2504682328879303\n",
            "Loss: 0.19929700891616609\n",
            "training error 0.12476075948412331, test error 0.25051381885134955\n",
            "Loss: 0.2175335790209676\n",
            "training error 0.12475265818351688, test error 0.2504420269393598\n",
            "Loss: 0.18881337355076688\n",
            "training error 0.12488080209457675, test error 0.2504361773204912\n",
            "Loss: 0.18647324565652657\n",
            "training error 0.1247816053787503, test error 0.2503240240402582\n",
            "Loss: 0.14160655854407178\n",
            "training error 0.1249492400106196, test error 0.2503552570329354\n",
            "Loss: 0.15410125247652395\n",
            "training error 0.12471479993495181, test error 0.250434365333001\n",
            "Loss: 0.1857483638197488\n",
            "training error 0.1247170238162208, test error 0.2504508673166169\n",
            "Loss: 0.1923499481344626\n",
            "training error 0.124763704576247, test error 0.2505491182970729\n",
            "Loss: 0.23165504905928724\n",
            "training error 0.12470723765120825, test error 0.250495250399942\n",
            "Loss: 0.2101053085527882\n",
            "training error 0.12470847947271888, test error 0.25052074664861607\n",
            "Loss: 0.2203050299467657\n",
            "training error 0.12476937648014234, test error 0.25056457074252947\n",
            "Loss: 0.23783676781048424\n",
            "training error 0.12470309948427054, test error 0.2504256882292528\n",
            "Loss: 0.18227710646461226\n",
            "training error 0.12490703615239714, test error 0.2504920174759668\n",
            "Loss: 0.2088119840226943\n",
            "training error 0.12480161415030633, test error 0.2506049876901335\n",
            "Loss: 0.25400548386071975\n",
            "training error 0.12464773485867657, test error 0.25042514841735236\n",
            "Loss: 0.1820611558335905\n",
            "training error 0.12461406531323017, test error 0.25039028334478897\n",
            "Loss: 0.16811345587683935\n",
            "training error 0.1246157354531301, test error 0.2503899897401336\n",
            "Loss: 0.16799599994350434\n",
            "training error 0.12470540499810288, test error 0.25032849600514845\n",
            "Loss: 0.1433955588220126\n",
            "training error 0.12461161249868416, test error 0.25032190224699213\n",
            "Loss: 0.14075773954931314\n",
            "training error 0.12470373471942921, test error 0.25020708616760734\n",
            "Loss: 0.09482580515975947\n",
            "training error 0.12461623018852322, test error 0.2503484025090812\n",
            "Loss: 0.15135911442731853\n",
            "training error 0.12469380142490635, test error 0.25024525942518555\n",
            "Loss: 0.11009693766934614\n",
            "training error 0.12457918247169616, test error 0.250384017981895\n",
            "Loss: 0.1656070104476548\n",
            "training error 0.12453327038051488, test error 0.25035110597082644\n",
            "Loss: 0.1524406286905755\n",
            "training error 0.12473127592153475, test error 0.25023435262348265\n",
            "Loss: 0.10573369427260282\n",
            "training error 0.12450252235073732, test error 0.2500969296234249\n",
            "Loss: 0.05075790816240211\n",
            "training error 0.12477656795411127, test error 0.2499383370909583\n",
            "Loss: 0.0\n",
            "training error 0.12449411149298266, test error 0.25002882155457556\n",
            "Loss: 0.03620271490576421\n",
            "training error 0.12446161392163684, test error 0.24994343948462006\n",
            "Loss: 0.002041460994384181\n",
            "training error 0.12456521558840562, test error 0.2500480979113266\n",
            "Loss: 0.04391515989334849\n",
            "training error 0.12451534722367787, test error 0.24992099421294658\n",
            "Loss: 0.0\n",
            "training error 0.12434614008707118, test error 0.24989121864265285\n",
            "Loss: 0.0\n",
            "training error 0.12435450802331369, test error 0.24991437207949366\n",
            "Loss: 0.009265406350245087\n",
            "training error 0.12442918736855013, test error 0.24989994978752086\n",
            "Loss: 0.003493978266000397\n",
            "training error 0.12442013213343237, test error 0.2497224347567295\n",
            "Loss: 0.0\n",
            "training error 0.12425764376368828, test error 0.24973054242680187\n",
            "Loss: 0.0032466726829216697\n",
            "training error 0.1243323343498154, test error 0.24968040788611615\n",
            "Loss: 0.0\n",
            "training error 0.1241874346511479, test error 0.24956051502365983\n",
            "Loss: 0.0\n",
            "training error 0.12418693661104259, test error 0.24945679417472602\n",
            "Loss: 0.0\n",
            "training error 0.12415957825138597, test error 0.2493430354950051\n",
            "Loss: 0.0\n",
            "training error 0.12416211807517011, test error 0.2492856743310288\n",
            "Loss: 0.0\n",
            "training error 0.12411878391308814, test error 0.24935768276350911\n",
            "Loss: 0.028885908776565827\n",
            "training error 0.12404398751154869, test error 0.24917500354861677\n",
            "Loss: 0.0\n",
            "training error 0.1240433702527097, test error 0.24916183378315188\n",
            "Loss: 0.0\n",
            "training error 0.12398957280480687, test error 0.249217050955412\n",
            "Loss: 0.022161167872991605\n",
            "training error 0.12394510379577771, test error 0.24919620294223774\n",
            "Loss: 0.013793909991766107\n",
            "training error 0.12397946809417099, test error 0.2491437114649059\n",
            "Loss: 0.0\n",
            "training error 0.1238614939477454, test error 0.2489729840473283\n",
            "Loss: 0.0\n",
            "training error 0.12373814058634972, test error 0.24879858683801467\n",
            "Loss: 0.0\n",
            "training error 0.1236997064216311, test error 0.24868935590106445\n",
            "Loss: 0.0\n",
            "training error 0.12366850587447527, test error 0.24867409274529603\n",
            "Loss: 0.0\n",
            "training error 0.12361660200207113, test error 0.2485006985375466\n",
            "Loss: 0.0\n",
            "training error 0.12363651667560038, test error 0.24820243913388831\n",
            "Loss: 0.0\n",
            "training error 0.1234877055666871, test error 0.24811910791091146\n",
            "Loss: 0.0\n",
            "training error 0.12353099983115703, test error 0.24811646901635484\n",
            "Loss: 0.0\n",
            "training error 0.12338901323726258, test error 0.24803072228949208\n",
            "Loss: 0.0\n",
            "training error 0.12342999992133838, test error 0.24766296918940003\n",
            "Loss: 0.0\n",
            "training error 0.12330470509319677, test error 0.24761634745787478\n",
            "Loss: 0.0\n",
            "training error 0.12316555578380518, test error 0.24746733314639477\n",
            "Loss: 0.0\n",
            "training error 0.12305773052199351, test error 0.2473802959073851\n",
            "Loss: 0.0\n",
            "training error 0.12300914418043854, test error 0.2472423988445133\n",
            "Loss: 0.0\n",
            "training error 0.12286932941834959, test error 0.24704264676140045\n",
            "Loss: 0.0\n",
            "training error 0.1228839021138279, test error 0.24693515398939953\n",
            "Loss: 0.0\n",
            "training error 0.12270167333895389, test error 0.24665499642497882\n",
            "Loss: 0.0\n",
            "training error 0.12259179675433304, test error 0.24641377085141292\n",
            "Loss: 0.0\n",
            "training error 0.12250160299478868, test error 0.2461406373582165\n",
            "Loss: 0.0\n",
            "training error 0.1223912484965835, test error 0.24599439922085117\n",
            "Loss: 0.0\n",
            "training error 0.12233070594325839, test error 0.24562932102939244\n",
            "Loss: 0.0\n",
            "training error 0.12214899839040344, test error 0.24548429931687138\n",
            "Loss: 0.0\n",
            "training error 0.1221237644852236, test error 0.24517014804452456\n",
            "Loss: 0.0\n",
            "training error 0.1219963126552999, test error 0.24495854199747943\n",
            "Loss: 0.0\n",
            "training error 0.12178366335904253, test error 0.24458571245961244\n",
            "Loss: 0.0\n",
            "training error 0.12164428198447928, test error 0.24436434352136244\n",
            "Loss: 0.0\n",
            "training error 0.12156624744022142, test error 0.24415471486189388\n",
            "Loss: 0.0\n",
            "training error 0.12142477219028981, test error 0.2438581533157639\n",
            "Loss: 0.0\n",
            "training error 0.12117983887848033, test error 0.24353832084622598\n",
            "Loss: 0.0\n",
            "training error 0.12102112967899613, test error 0.24308667675577136\n",
            "Loss: 0.0\n",
            "training error 0.12090305656927068, test error 0.24273847733848916\n",
            "Loss: 0.0\n",
            "training error 0.12068497623974399, test error 0.24242864423369262\n",
            "Loss: 0.0\n",
            "training error 0.12061325756601388, test error 0.24192097045302643\n",
            "Loss: 0.0\n",
            "training error 0.12031183291908841, test error 0.24167972234864032\n",
            "Loss: 0.0\n",
            "training error 0.12007648330130068, test error 0.2412335470641465\n",
            "Loss: 0.0\n",
            "training error 0.11988726599004386, test error 0.24078376443580124\n",
            "Loss: 0.0\n",
            "training error 0.11986175967794674, test error 0.24025375041435243\n",
            "Loss: 0.0\n",
            "training error 0.11946654213123514, test error 0.23985839503141299\n",
            "Loss: 0.0\n",
            "training error 0.11925320950996357, test error 0.23944299117981954\n",
            "Loss: 0.0\n",
            "training error 0.11925451395596784, test error 0.23885975655220074\n",
            "Loss: 0.0\n",
            "training error 0.11872438324018336, test error 0.23851230875395174\n",
            "Loss: 0.0\n",
            "training error 0.11852358938776522, test error 0.23796039422659895\n",
            "Loss: 0.0\n",
            "training error 0.11826008701185944, test error 0.23732307614262255\n",
            "Loss: 0.0\n",
            "training error 0.1179685157626727, test error 0.23677109167183283\n",
            "Loss: 0.0\n",
            "training error 0.11761379160033333, test error 0.23604909047681571\n",
            "Loss: 0.0\n",
            "training error 0.11746101143209689, test error 0.235358110632394\n",
            "Loss: 0.0\n",
            "training error 0.11718383772408815, test error 0.23474808122654892\n",
            "Loss: 0.0\n",
            "training error 0.11670511944067949, test error 0.23402777416086212\n",
            "Loss: 0.0\n",
            "training error 0.11633471607184319, test error 0.23339702856880878\n",
            "Loss: 0.0\n",
            "training error 0.11600631641158389, test error 0.23275897923975844\n",
            "Loss: 0.0\n",
            "training error 0.1158349056487845, test error 0.23194430142298555\n",
            "Loss: 0.0\n",
            "training error 0.1152982709430549, test error 0.23117773823427196\n",
            "Loss: 0.0\n",
            "training error 0.11493084155433733, test error 0.2304784169730615\n",
            "Loss: 0.0\n",
            "training error 0.11455325635386995, test error 0.22960002829608553\n",
            "Loss: 0.0\n",
            "training error 0.1140834401249534, test error 0.22869495385795335\n",
            "Loss: 0.0\n",
            "training error 0.11365289424247464, test error 0.22786746340905453\n",
            "Loss: 0.0\n",
            "training error 0.113217684862632, test error 0.2270488466021422\n",
            "Loss: 0.0\n",
            "training error 0.11280892173832811, test error 0.22616758490699734\n",
            "Loss: 0.0\n",
            "training error 0.11233622997935337, test error 0.22519961072064937\n",
            "Loss: 0.0\n",
            "training error 0.11189447916742562, test error 0.22433564673298034\n",
            "Loss: 0.0\n",
            "training error 0.11145240702971311, test error 0.22325030704617121\n",
            "Loss: 0.0\n",
            "training error 0.11090221629204292, test error 0.22222525004964278\n",
            "Loss: 0.0\n",
            "training error 0.11070034264949123, test error 0.22126154290289898\n",
            "Loss: 0.0\n",
            "training error 0.10996349819557406, test error 0.22018220674664685\n",
            "Loss: 0.0\n",
            "training error 0.10946448323889303, test error 0.2189263040229847\n",
            "Loss: 0.0\n",
            "training error 0.1088311025717943, test error 0.2178293115464635\n",
            "Loss: 0.0\n",
            "training error 0.10822737256734077, test error 0.21656721718666808\n",
            "Loss: 0.0\n",
            "training error 0.10776885696921618, test error 0.2152393105385763\n",
            "Loss: 0.0\n",
            "training error 0.10708969090846966, test error 0.21406676848553746\n",
            "Loss: 0.0\n",
            "training error 0.10653969289885018, test error 0.2129017291180059\n",
            "Loss: 0.0\n",
            "training error 0.10589075208484745, test error 0.21142046672694506\n",
            "Loss: 0.0\n",
            "training error 0.10528662910979257, test error 0.21013786372236812\n",
            "Loss: 0.0\n",
            "training error 0.10469107003827666, test error 0.20864619999562645\n",
            "Loss: 0.0\n",
            "training error 0.10408807098760162, test error 0.20734389986910096\n",
            "Loss: 0.0\n",
            "training error 0.10328660099144507, test error 0.20592631956758203\n",
            "Loss: 0.0\n",
            "training error 0.10260709278451238, test error 0.20449324220093162\n",
            "Loss: 0.0\n",
            "training error 0.10199060553264504, test error 0.20302030724540399\n",
            "Loss: 0.0\n",
            "training error 0.10125419061292713, test error 0.20151222235245222\n",
            "Loss: 0.0\n",
            "training error 0.10058355624618755, test error 0.20010159336813121\n",
            "Loss: 0.0\n",
            "training error 0.09985214585077617, test error 0.19867600466256133\n",
            "Loss: 0.0\n",
            "training error 0.09912320058733157, test error 0.1973220227309042\n",
            "Loss: 0.0\n",
            "training error 0.09840352640177355, test error 0.1957916555499624\n",
            "Loss: 0.0\n",
            "training error 0.09776465217629463, test error 0.19415231495646612\n",
            "Loss: 0.0\n",
            "training error 0.09707754846956279, test error 0.1928145570119208\n",
            "Loss: 0.0\n",
            "training error 0.09613207487160796, test error 0.19119910631366574\n",
            "Loss: 0.0\n",
            "training error 0.0953974985837682, test error 0.189489889152464\n",
            "Loss: 0.0\n",
            "training error 0.09464760048996884, test error 0.18791628968691154\n",
            "Loss: 0.0\n",
            "training error 0.0938560764340359, test error 0.18625753059011874\n",
            "Loss: 0.0\n",
            "training error 0.09304703041062126, test error 0.18468239933386765\n",
            "Loss: 0.0\n",
            "training error 0.09238050387763774, test error 0.182926433970943\n",
            "Loss: 0.0\n",
            "training error 0.09153699389054583, test error 0.18122991741652358\n",
            "Loss: 0.0\n",
            "training error 0.09084250932405794, test error 0.17947931573612733\n",
            "Loss: 0.0\n",
            "training error 0.08995181561687515, test error 0.17791059784105248\n",
            "Loss: 0.0\n",
            "training error 0.08921381405216017, test error 0.17634263945111148\n",
            "Loss: 0.0\n",
            "training error 0.0883952113220696, test error 0.17468259858554286\n",
            "Loss: 0.0\n",
            "training error 0.08764177383313407, test error 0.17304148666041375\n",
            "Loss: 0.0\n",
            "training error 0.08682492136880074, test error 0.17119415835839358\n",
            "Loss: 0.0\n",
            "training error 0.08620453271762131, test error 0.1695364522687146\n",
            "Loss: 0.0\n",
            "training error 0.0852745030428327, test error 0.16797249266151315\n",
            "Loss: 0.0\n",
            "training error 0.08454099477455598, test error 0.16636434597525934\n",
            "Loss: 0.0\n",
            "training error 0.08380046697695845, test error 0.1645746890786106\n",
            "Loss: 0.0\n",
            "training error 0.08313857728454739, test error 0.16271644251761497\n",
            "Loss: 0.0\n",
            "training error 0.08236545383563455, test error 0.1612115697243943\n",
            "Loss: 0.0\n",
            "training error 0.0814149423649412, test error 0.15941554432799174\n",
            "Loss: 0.0\n",
            "training error 0.08065758950602023, test error 0.15778458134344098\n",
            "Loss: 0.0\n",
            "training error 0.07990408205135197, test error 0.15616212397383952\n",
            "Loss: 0.0\n",
            "training error 0.07916531130707972, test error 0.15447177689957747\n",
            "Loss: 0.0\n",
            "training error 0.07839027320725306, test error 0.15287783255045093\n",
            "Loss: 0.0\n",
            "training error 0.07763422641671429, test error 0.15123524218274748\n",
            "Loss: 0.0\n",
            "training error 0.07696260108312587, test error 0.1496999932226535\n",
            "Loss: 0.0\n",
            "training error 0.07625344937988354, test error 0.14822148765037074\n",
            "Loss: 0.0\n",
            "training error 0.0755139645503255, test error 0.1465758558557425\n",
            "Loss: 0.0\n",
            "training error 0.07475433907649523, test error 0.14515635886044806\n",
            "Loss: 0.0\n",
            "training error 0.07407232757178454, test error 0.1436729120503521\n",
            "Loss: 0.0\n",
            "training error 0.07336506831264651, test error 0.1421862773350847\n",
            "Loss: 0.0\n",
            "training error 0.07268769411845659, test error 0.14073136430992206\n",
            "Loss: 0.0\n",
            "training error 0.07198785269667247, test error 0.1391360154881865\n",
            "Loss: 0.0\n",
            "training error 0.07134205978883772, test error 0.1376020224566851\n",
            "Loss: 0.0\n",
            "training error 0.070695751495956, test error 0.13615121063336522\n",
            "Loss: 0.0\n",
            "training error 0.07009682779411257, test error 0.13476770967199522\n",
            "Loss: 0.0\n",
            "training error 0.06939968466054464, test error 0.13341825682437267\n",
            "Loss: 0.0\n",
            "training error 0.06882411674304693, test error 0.13191402238693117\n",
            "Loss: 0.0\n",
            "training error 0.0682066746963309, test error 0.1306847606702352\n",
            "Loss: 0.0\n",
            "training error 0.06753890891623753, test error 0.1292758243295164\n",
            "Loss: 0.0\n",
            "training error 0.06705212464394758, test error 0.12785168276785602\n",
            "Loss: 0.0\n",
            "training error 0.06631567784955618, test error 0.12673233007634088\n",
            "Loss: 0.0\n",
            "training error 0.06573430673032404, test error 0.1252882977668941\n",
            "Loss: 0.0\n",
            "training error 0.06516776914623013, test error 0.12407741590698372\n",
            "Loss: 0.0\n",
            "training error 0.06455739667976251, test error 0.12292770579080325\n",
            "Loss: 0.0\n",
            "training error 0.06399287460913136, test error 0.12166527291035557\n",
            "Loss: 0.0\n",
            "training error 0.0635128647086581, test error 0.12040822304386893\n",
            "Loss: 0.0\n",
            "training error 0.06291923405230926, test error 0.11895570947200071\n",
            "Loss: 0.0\n",
            "training error 0.06241436865029047, test error 0.1179085071089511\n",
            "Loss: 0.0\n",
            "training error 0.061782617202502436, test error 0.11654243077479844\n",
            "Loss: 0.0\n",
            "training error 0.061265148004352274, test error 0.1154849680711796\n",
            "Loss: 0.0\n",
            "training error 0.06074206432708816, test error 0.11431916998953386\n",
            "Loss: 0.0\n",
            "training error 0.06031693330639467, test error 0.11333028216009164\n",
            "Loss: 0.0\n",
            "training error 0.05976866237386289, test error 0.11222361814127613\n",
            "Loss: 0.0\n",
            "training error 0.05930867175130828, test error 0.1111279608482452\n",
            "Loss: 0.0\n",
            "training error 0.05881215301628435, test error 0.10999114813720652\n",
            "Loss: 0.0\n",
            "training error 0.05841079508405219, test error 0.10875629812680866\n",
            "Loss: 0.0\n",
            "training error 0.05795636860367746, test error 0.1076610167631695\n",
            "Loss: 0.0\n",
            "training error 0.05747095748633052, test error 0.10667502139729099\n",
            "Loss: 0.0\n",
            "training error 0.05706171664256711, test error 0.10559921679307685\n",
            "Loss: 0.0\n",
            "training error 0.0566653836755087, test error 0.10489666047051732\n",
            "Loss: 0.0\n",
            "training error 0.05615354819247414, test error 0.10381306360737316\n",
            "Loss: 0.0\n",
            "training error 0.055774017725770925, test error 0.10289084680865326\n",
            "Loss: 0.0\n",
            "training error 0.055422693944670226, test error 0.10176422109444687\n",
            "Loss: 0.0\n",
            "training error 0.054937151182327996, test error 0.1009978186040193\n",
            "Loss: 0.0\n",
            "training error 0.05459247250482455, test error 0.10012170968756565\n",
            "Loss: 0.0\n",
            "training error 0.0541985601795757, test error 0.09905556801308293\n",
            "Loss: 0.0\n",
            "training error 0.05383877081386228, test error 0.09850553995094494\n",
            "Loss: 0.0\n",
            "training error 0.05352129347934639, test error 0.09740933523924518\n",
            "Loss: 0.0\n",
            "training error 0.053102047111042106, test error 0.09672542484482903\n",
            "Loss: 0.0\n",
            "training error 0.05277742748632056, test error 0.09571320251036913\n",
            "Loss: 0.0\n",
            "training error 0.052408008766954776, test error 0.09517146839099061\n",
            "Loss: 0.0\n",
            "training error 0.052013970557389745, test error 0.09423633876145754\n",
            "Loss: 0.0\n",
            "training error 0.051696094759166804, test error 0.09350731167811528\n",
            "Loss: 0.0\n",
            "training error 0.051438883711019075, test error 0.09263115653015305\n",
            "Loss: 0.0\n",
            "training error 0.05108574451431532, test error 0.09213163783652069\n",
            "Loss: 0.0\n",
            "training error 0.05076596796461793, test error 0.09140463029935904\n",
            "Loss: 0.0\n",
            "training error 0.0504933969210286, test error 0.09078685113160499\n",
            "Loss: 0.0\n",
            "training error 0.050149025385869114, test error 0.09001243953161672\n",
            "Loss: 0.0\n",
            "training error 0.04988340465584906, test error 0.08932007164471054\n",
            "Loss: 0.0\n",
            "training error 0.04960486845683249, test error 0.0885743851891963\n",
            "Loss: 0.0\n",
            "training error 0.04931996866910628, test error 0.08804471338320803\n",
            "Loss: 0.0\n",
            "training error 0.0490956143219587, test error 0.08736517896049305\n",
            "Loss: 0.0\n",
            "training error 0.04879122913453834, test error 0.08682142774080473\n",
            "Loss: 0.0\n",
            "training error 0.048546685421866964, test error 0.08612304078527898\n",
            "Loss: 0.0\n",
            "training error 0.04835372992695243, test error 0.08542410746295709\n",
            "Loss: 0.0\n",
            "training error 0.048066684753759144, test error 0.08504754019390691\n",
            "Loss: 0.0\n",
            "training error 0.04784200921514667, test error 0.0844575885940265\n",
            "Loss: 0.0\n",
            "training error 0.04758757269914006, test error 0.08398112872046065\n",
            "Loss: 0.0\n",
            "training error 0.04733755990371638, test error 0.08339134971052856\n",
            "Loss: 0.0\n",
            "training error 0.047235320850555346, test error 0.08270662589353844\n",
            "Loss: 0.0\n",
            "training error 0.046972559092649795, test error 0.08238966212603924\n",
            "Loss: 0.0\n",
            "training error 0.046688550423546034, test error 0.08170361117696519\n",
            "Loss: 0.0\n",
            "training error 0.04644683211367115, test error 0.0812557872800597\n",
            "Loss: 0.0\n",
            "training error 0.04626058080296907, test error 0.08081941452111953\n",
            "Loss: 0.0\n",
            "training error 0.046048591445587844, test error 0.08031225420272903\n",
            "Loss: 0.0\n",
            "training error 0.045861277387147126, test error 0.07977687547068495\n",
            "Loss: 0.0\n",
            "training error 0.045672031749897035, test error 0.07942249231319845\n",
            "Loss: 0.0\n",
            "training error 0.04548375408651504, test error 0.07920376471794675\n",
            "Loss: 0.0\n",
            "training error 0.04535004083091079, test error 0.07871031344698375\n",
            "Loss: 0.0\n",
            "training error 0.04511176246105377, test error 0.07803048191036573\n",
            "Loss: 0.0\n",
            "training error 0.04494733815658673, test error 0.0776462735799679\n",
            "Loss: 0.0\n",
            "training error 0.04470284937487417, test error 0.0771389996187316\n",
            "Loss: 0.0\n",
            "training error 0.04458016823549207, test error 0.07689118956095942\n",
            "Loss: 0.0\n",
            "training error 0.04440964515019858, test error 0.07646846524907332\n",
            "Loss: 0.0\n",
            "training error 0.044240716098028494, test error 0.07603186679071951\n",
            "Loss: 0.0\n",
            "training error 0.04408761981882856, test error 0.07561466626132407\n",
            "Loss: 0.0\n",
            "training error 0.043892430009319645, test error 0.07537784370598453\n",
            "Loss: 0.0\n",
            "training error 0.04373415810131176, test error 0.07503221073999589\n",
            "Loss: 0.0\n",
            "training error 0.043600320891437075, test error 0.0746373638777285\n",
            "Loss: 0.0\n",
            "training error 0.0434605153037605, test error 0.07428648704914735\n",
            "Loss: 0.0\n",
            "training error 0.04328455390286185, test error 0.07401664454499202\n",
            "Loss: 0.0\n",
            "training error 0.04318741739434141, test error 0.07349676872838395\n",
            "Loss: 0.0\n",
            "training error 0.04299411786238599, test error 0.0732474131485469\n",
            "Loss: 0.0\n",
            "training error 0.04287333533507813, test error 0.07288755682817576\n",
            "Loss: 0.0\n",
            "training error 0.04277773214068884, test error 0.07268245727920551\n",
            "Loss: 0.0\n",
            "training error 0.042621259757157984, test error 0.0722018767510352\n",
            "Loss: 0.0\n",
            "training error 0.04244537502478129, test error 0.07202192082700276\n",
            "Loss: 0.0\n",
            "training error 0.04241636681840956, test error 0.07176072670616446\n",
            "Loss: 0.0\n",
            "training error 0.04220934366099191, test error 0.0714741614124821\n",
            "Loss: 0.0\n",
            "training error 0.042099021103206596, test error 0.07109205807925643\n",
            "Loss: 0.0\n",
            "training error 0.04197962922529354, test error 0.07085850705709569\n",
            "Loss: 0.0\n",
            "training error 0.04183495482919728, test error 0.07045744062000699\n",
            "Loss: 0.0\n",
            "training error 0.041909243737892274, test error 0.0702100789779955\n",
            "Loss: 0.0\n",
            "training error 0.04162129441262902, test error 0.06982142905881387\n",
            "Loss: 0.0\n",
            "training error 0.041526761771532764, test error 0.0695463371254129\n",
            "Loss: 0.0\n",
            "training error 0.04145027980494895, test error 0.06925728595011045\n",
            "Loss: 0.0\n",
            "training error 0.04132538565041807, test error 0.06888990517537004\n",
            "Loss: 0.0\n",
            "training error 0.04118496200182422, test error 0.06880682049176345\n",
            "Loss: 0.0\n",
            "training error 0.04111155667707974, test error 0.06856383568620716\n",
            "Loss: 0.0\n",
            "training error 0.0409952350428123, test error 0.06842544101988307\n",
            "Loss: 0.0\n",
            "training error 0.04090187758887694, test error 0.06812700887622128\n",
            "Loss: 0.0\n",
            "training error 0.04080463445910002, test error 0.06808579402892001\n",
            "Loss: 0.0\n",
            "training error 0.04069947638763809, test error 0.06780151948217263\n",
            "Loss: 0.0\n",
            "training error 0.04066087549343281, test error 0.06770483947074137\n",
            "Loss: 0.0\n",
            "training error 0.04056615758180703, test error 0.06733552497643244\n",
            "Loss: 0.0\n",
            "training error 0.04043029887317409, test error 0.06707657591232886\n",
            "Loss: 0.0\n",
            "training error 0.04036048225781457, test error 0.06686942386499083\n",
            "Loss: 0.0\n",
            "training error 0.040264588126447724, test error 0.06669344396826206\n",
            "Loss: 0.0\n",
            "training error 0.04021526912852592, test error 0.0666027392314781\n",
            "Loss: 0.0\n",
            "training error 0.0401251488723708, test error 0.06640031728446133\n",
            "Loss: 0.0\n",
            "training error 0.040054599836123035, test error 0.06612864369986293\n",
            "Loss: 0.0\n",
            "training error 0.039962929555294, test error 0.0661768935355098\n",
            "Loss: 0.07296359481658854\n",
            "training error 0.039866335752222565, test error 0.06597470662259268\n",
            "Loss: 0.0\n",
            "training error 0.0399156236473393, test error 0.06572737776616865\n",
            "Loss: 0.0\n",
            "training error 0.03970979929735185, test error 0.06543810069450605\n",
            "Loss: 0.0\n",
            "training error 0.039609024361525726, test error 0.0651520200215726\n",
            "Loss: 0.0\n",
            "training error 0.039551588322105084, test error 0.06500807452033709\n",
            "Loss: 0.0\n",
            "training error 0.03946622137349727, test error 0.06473490231247905\n",
            "Loss: 0.0\n",
            "training error 0.03940755023904986, test error 0.06459486916505795\n",
            "Loss: 0.0\n",
            "training error 0.03934597754825828, test error 0.06455055972086605\n",
            "Loss: 0.0\n",
            "training error 0.039301196197456846, test error 0.06434955487144947\n",
            "Loss: 0.0\n",
            "training error 0.03929212885637169, test error 0.0642550880160718\n",
            "Loss: 0.0\n",
            "training error 0.03927082065505891, test error 0.0641299604094497\n",
            "Loss: 0.0\n",
            "training error 0.03908472364202761, test error 0.06392432688543341\n",
            "Loss: 0.0\n",
            "training error 0.038999286799155826, test error 0.06379703530071824\n",
            "Loss: 0.0\n",
            "training error 0.038975418697360285, test error 0.06355461252397133\n",
            "Loss: 0.0\n",
            "training error 0.039008999669020425, test error 0.06361099192454975\n",
            "Loss: 0.0887101633373133\n",
            "training error 0.038823169988025416, test error 0.06339620896114397\n",
            "Loss: 0.0\n",
            "training error 0.03883983139262136, test error 0.06330788573491536\n",
            "Loss: 0.0\n",
            "training error 0.038775351055539774, test error 0.06330154586483182\n",
            "Loss: 0.0\n",
            "training error 0.03866428175375949, test error 0.06294388918261262\n",
            "Loss: 0.0\n",
            "training error 0.038606552807330184, test error 0.06292851441083479\n",
            "Loss: 0.0\n",
            "training error 0.038601567351877794, test error 0.06259571290591646\n",
            "Loss: 0.0\n",
            "training error 0.03852040350158163, test error 0.06252100196028385\n",
            "Loss: 0.0\n",
            "training error 0.03848689689410884, test error 0.06250090695656242\n",
            "Loss: 0.0\n",
            "training error 0.038432479252708165, test error 0.062315487899995\n",
            "Loss: 0.0\n",
            "training error 0.038348035997906024, test error 0.0621046399127135\n",
            "Loss: 0.0\n",
            "training error 0.03830557346024892, test error 0.062042260824549514\n",
            "Loss: 0.0\n",
            "training error 0.038289572699003724, test error 0.06191233425533451\n",
            "Loss: 0.0\n",
            "training error 0.03822041103182822, test error 0.06187631720866857\n",
            "Loss: 0.0\n",
            "training error 0.03814856752799593, test error 0.061821868073179506\n",
            "Loss: 0.0\n",
            "training error 0.038100696907528396, test error 0.06186572641456261\n",
            "Loss: 0.07094308656474535\n",
            "training error 0.038090642723122764, test error 0.06183517100039237\n",
            "Loss: 0.021518157939048876\n",
            "training error 0.03803020508614292, test error 0.061549004654516036\n",
            "Loss: 0.0\n",
            "training error 0.03799208300749653, test error 0.061492667870025434\n",
            "Loss: 0.0\n",
            "training error 0.03793000240196585, test error 0.06133987028246788\n",
            "Loss: 0.0\n",
            "training error 0.03787543104268088, test error 0.06119996104238807\n",
            "Loss: 0.0\n",
            "training error 0.03784966696514942, test error 0.061113434091689074\n",
            "Loss: 0.0\n",
            "training error 0.03777950518749625, test error 0.061054451128561164\n",
            "Loss: 0.0\n",
            "training error 0.03782272509345089, test error 0.061187654254359576\n",
            "Loss: 0.21817103149111006\n",
            "training error 0.037773747549988766, test error 0.06096792862976174\n",
            "Loss: 0.0\n",
            "training error 0.037710791278443144, test error 0.06098419504487043\n",
            "Loss: 0.026680281705937503\n",
            "training error 0.0376775554658372, test error 0.060909763535290734\n",
            "Loss: 0.0\n",
            "training error 0.037584315892475816, test error 0.06077572934400771\n",
            "Loss: 0.0\n",
            "training error 0.037600306820361966, test error 0.060769787538198894\n",
            "Loss: 0.0\n",
            "training error 0.037528494125962625, test error 0.06053606579481961\n",
            "Loss: 0.0\n",
            "training error 0.03746787299869846, test error 0.060602879971534455\n",
            "Loss: 0.11037086047398503\n",
            "training error 0.03745983491740378, test error 0.06047881965817913\n",
            "Loss: 0.0\n",
            "training error 0.03739979209179865, test error 0.06055030465728657\n",
            "Loss: 0.1181984031954686\n",
            "training error 0.03737725738700902, test error 0.06050511260016321\n",
            "Loss: 0.04347462819658254\n",
            "training error 0.037336145350030055, test error 0.06045034440423405\n",
            "Loss: 0.0\n",
            "training error 0.037330761843434444, test error 0.060403966234130115\n",
            "Loss: 0.0\n",
            "training error 0.037287355926371474, test error 0.06035598787299826\n",
            "Loss: 0.0\n",
            "training error 0.037243397004548595, test error 0.06024854509621752\n",
            "Loss: 0.0\n",
            "training error 0.03721303934784186, test error 0.06024393238238136\n",
            "Loss: 0.0\n",
            "training error 0.03717535113494302, test error 0.06015053624724351\n",
            "Loss: 0.0\n",
            "training error 0.037138960124623796, test error 0.059906607467675774\n",
            "Loss: 0.0\n",
            "training error 0.03710663126087048, test error 0.059775158406617075\n",
            "Loss: 0.0\n",
            "training error 0.03707232519762501, test error 0.059590770139559365\n",
            "Loss: 0.0\n",
            "training error 0.03702792128498774, test error 0.059526121884790874\n",
            "Loss: 0.0\n",
            "training error 0.03706303481879163, test error 0.05938074636384517\n",
            "Loss: 0.0\n",
            "training error 0.036986838161527195, test error 0.059399071059613694\n",
            "Loss: 0.03085965888041109\n",
            "training error 0.03697190393529261, test error 0.05949571369730794\n",
            "Loss: 0.19361045541315125\n",
            "training error 0.03693648411286245, test error 0.059158398639885666\n",
            "Loss: 0.0\n",
            "training error 0.0368903977305529, test error 0.05912624997515569\n",
            "Loss: 0.0\n",
            "training error 0.036877471284632826, test error 0.0591306914545485\n",
            "Loss: 0.007511857076480943\n",
            "training error 0.036850289098134925, test error 0.05906444713050808\n",
            "Loss: 0.0\n",
            "training error 0.03684110239931129, test error 0.05879916513455612\n",
            "Loss: 0.0\n",
            "training error 0.03682081551089188, test error 0.05885188090573408\n",
            "Loss: 0.08965394501321278\n",
            "training error 0.036770398733738184, test error 0.05885877105613448\n",
            "Loss: 0.10137205424933793\n",
            "training error 0.03677259694903842, test error 0.05883114332437873\n",
            "Loss: 0.05438544875497886\n",
            "training error 0.03674149889622265, test error 0.05885549297227044\n",
            "Loss: 0.09579700253468104\n",
            "training error 0.03678962596778808, test error 0.05909349030509059\n",
            "Loss: 0.5005601182617836\n",
            "training error 0.03663659089243201, test error 0.05900290103552402\n",
            "Loss: 0.34649454716180195\n",
            "training error 0.03663850309771113, test error 0.058919213507497036\n",
            "Loss: 0.2041667983996076\n",
            "training error 0.03660275520367576, test error 0.059010617689205735\n",
            "Loss: 0.3596182941810966\n",
            "training error 0.03660661135025371, test error 0.05892672673191178\n",
            "Loss: 0.21694457236551834\n",
            "training error 0.03656298634886187, test error 0.058805250657044034\n",
            "Loss: 0.010349674989407198\n",
            "training error 0.03651467138956774, test error 0.05876767511241404\n",
            "Loss: 0.0\n",
            "training error 0.036503211155896005, test error 0.05876550007316641\n",
            "Loss: 0.0\n",
            "training error 0.03651967985945368, test error 0.058608895227667504\n",
            "Loss: 0.0\n",
            "training error 0.03644066986711963, test error 0.05868138497429417\n",
            "Loss: 0.12368386461658609\n",
            "training error 0.03645765850584765, test error 0.05865679218084588\n",
            "Loss: 0.08172300977917768\n",
            "training error 0.0364128186579759, test error 0.05862238651321679\n",
            "Loss: 0.02301917737379533\n",
            "training error 0.03637964223117831, test error 0.05846022672796009\n",
            "Loss: 0.0\n",
            "training error 0.036357709832512214, test error 0.05840486296470044\n",
            "Loss: 0.0\n",
            "training error 0.036348062787913575, test error 0.05840383453442371\n",
            "Loss: 0.0\n",
            "training error 0.036335151022529925, test error 0.058399956076685945\n",
            "Loss: 0.0\n",
            "training error 0.03634479684964152, test error 0.058109223364082786\n",
            "Loss: 0.0\n",
            "training error 0.03633653241521889, test error 0.05818980801314084\n",
            "Loss: 0.1386778972300995\n",
            "training error 0.036276334599024185, test error 0.05824603371500162\n",
            "Loss: 0.23543655034186095\n",
            "training error 0.036233373358264986, test error 0.058151173998276635\n",
            "Loss: 0.07219272908021157\n",
            "training error 0.03629714977981322, test error 0.058331913164254366\n",
            "Loss: 0.38322625442146485\n",
            "training error 0.036203836393500874, test error 0.05820920498361161\n",
            "Loss: 0.17205808947469148\n",
            "training error 0.03617960888186843, test error 0.05815093010819533\n",
            "Loss: 0.07177301932126401\n",
            "training error 0.03616741622435644, test error 0.058169914154551174\n",
            "Loss: 0.10444261161111079\n",
            "training error 0.03619313086311353, test error 0.05785812335708055\n",
            "Loss: 0.0\n",
            "training error 0.036138192709092326, test error 0.05784543994514163\n",
            "Loss: 0.0\n",
            "training error 0.0361790286318592, test error 0.05796797772030735\n",
            "Loss: 0.21183653418823667\n",
            "training error 0.03610650530351685, test error 0.05781736655495065\n",
            "Loss: 0.0\n",
            "training error 0.03612139207725531, test error 0.057807961268968984\n",
            "Loss: 0.0\n",
            "training error 0.03610015573529153, test error 0.05798018224797507\n",
            "Loss: 0.2979191364399947\n",
            "training error 0.036099601427507275, test error 0.057802772663860996\n",
            "Loss: 0.0\n",
            "training error 0.03604684959596484, test error 0.05772346287124614\n",
            "Loss: 0.0\n",
            "training error 0.03602710738765412, test error 0.057762887033694484\n",
            "Loss: 0.06829833223325199\n",
            "training error 0.03604166690847154, test error 0.05736573335014471\n",
            "Loss: 0.0\n",
            "training error 0.03598346740710156, test error 0.05745951846115507\n",
            "Loss: 0.163486293181192\n",
            "training error 0.036073624347274985, test error 0.057547581265573654\n",
            "Loss: 0.3169974561625377\n",
            "training error 0.03594174788318875, test error 0.05741114442951263\n",
            "Loss: 0.07916063600328371\n",
            "training error 0.03592064574533717, test error 0.05748308049269257\n",
            "Loss: 0.2045596485825696\n",
            "training error 0.03592474130571706, test error 0.05748484693050232\n",
            "Loss: 0.20763890462374146\n",
            "training error 0.03590075095555606, test error 0.05755705573869431\n",
            "Loss: 0.33351336656297637\n",
            "training error 0.03597144418099552, test error 0.057474885870884264\n",
            "Loss: 0.19027477618618338\n",
            "training error 0.03586356951085725, test error 0.057442049522058614\n",
            "Loss: 0.1330344222187474\n",
            "training error 0.03586083368534254, test error 0.05747675148506198\n",
            "Loss: 0.19352691656471244\n",
            "training error 0.035863200336249484, test error 0.057613450831664255\n",
            "Loss: 0.4318213453448738\n",
            "training error 0.03584212472638194, test error 0.05748711056925768\n",
            "Loss: 0.2115848818180721\n",
            "training error 0.0358354678395489, test error 0.05731155616630328\n",
            "Loss: 0.0\n",
            "training error 0.03580376931674828, test error 0.05725695922292899\n",
            "Loss: 0.0\n",
            "training error 0.03581473921177889, test error 0.057234340175980024\n",
            "Loss: 0.0\n",
            "training error 0.035772845329950106, test error 0.057291858292452126\n",
            "Loss: 0.10049581474207425\n",
            "training error 0.035758654097920554, test error 0.05728697237531504\n",
            "Loss: 0.09195912658936933\n",
            "training error 0.035753260941929724, test error 0.05725291904889277\n",
            "Loss: 0.032461058964994294\n",
            "training error 0.0358140200147177, test error 0.057175143916838096\n",
            "Loss: 0.0\n",
            "training error 0.0357423350612566, test error 0.057292915042220796\n",
            "Loss: 0.20598308515671349\n",
            "training error 0.03572986486775382, test error 0.05724727050315548\n",
            "Loss: 0.12615024882540737\n",
            "training error 0.035710403464453776, test error 0.05706882423090986\n",
            "Loss: 0.0\n",
            "training error 0.03568598008979347, test error 0.05711380706905871\n",
            "Loss: 0.07882208676115443\n",
            "training error 0.03571513137198146, test error 0.057148501379325714\n",
            "Loss: 0.13961589272186714\n",
            "training error 0.03566847888217037, test error 0.056984307267299555\n",
            "Loss: 0.0\n",
            "training error 0.035708685614626125, test error 0.05710906243619456\n",
            "Loss: 0.21892899094098084\n",
            "training error 0.03564533609093093, test error 0.05699440215561286\n",
            "Loss: 0.017715207567503555\n",
            "training error 0.03562302187283122, test error 0.05712185220960086\n",
            "Loss: 0.24137336908582885\n",
            "training error 0.035621516216571544, test error 0.05687159696146876\n",
            "Loss: 0.0\n",
            "training error 0.03564963996180732, test error 0.05685186739810406\n",
            "Loss: 0.0\n",
            "training error 0.03558538837645454, test error 0.056936333037445566\n",
            "Loss: 0.14857144225366437\n",
            "training error 0.03560133277057766, test error 0.05693926799876386\n",
            "Loss: 0.15373391351911625\n",
            "training error 0.03559145770791012, test error 0.05695726843451311\n",
            "Loss: 0.18539590911057502\n",
            "training error 0.03562282846773931, test error 0.057054129140529014\n",
            "Loss: 0.3557697428100637\n",
            "training error 0.035578514653462526, test error 0.057051094722050284\n",
            "Loss: 0.35043233065881196\n",
            "training error 0.03556504885870439, test error 0.05702495439760863\n",
            "Loss: 0.3044526194584396\n",
            "training error 0.03553255873320205, test error 0.056934892456135235\n",
            "Loss: 0.14603752142352544\n",
            "training error 0.03557771882707907, test error 0.05694815831982218\n",
            "Loss: 0.1693716075214402\n",
            "training error 0.035524681131052095, test error 0.056739260580082294\n",
            "Loss: 0.0\n",
            "training error 0.03549149446372047, test error 0.056788030801418546\n",
            "Loss: 0.0859549822074479\n",
            "training error 0.03552823199861841, test error 0.056906339301425926\n",
            "Loss: 0.2944675690791154\n",
            "training error 0.035486676632312183, test error 0.05684232557779923\n",
            "Loss: 0.1816467057611293\n",
            "training error 0.035474882942136514, test error 0.05680914855332511\n",
            "Loss: 0.12317392318530462\n",
            "training error 0.035515829778030174, test error 0.05686359936091587\n",
            "Loss: 0.21914064364318975\n",
            "training error 0.035478350259469424, test error 0.05682382319375535\n",
            "Loss: 0.14903721481125398\n",
            "training error 0.035489634856010505, test error 0.056902912500561884\n",
            "Loss: 0.2884280105282855\n",
            "training error 0.03545503715908382, test error 0.05672808288379832\n",
            "Loss: 0.0\n",
            "training error 0.03542111386126906, test error 0.05658291966377292\n",
            "Loss: 0.0\n",
            "training error 0.03540856479141372, test error 0.05665931118377576\n",
            "Loss: 0.13500809158801808\n",
            "training error 0.03541700342344208, test error 0.056665073632947274\n",
            "Loss: 0.1451921704686221\n",
            "training error 0.0354296689704506, test error 0.056531800261492834\n",
            "Loss: 0.0\n",
            "training error 0.03543493202118472, test error 0.05655699793082962\n",
            "Loss: 0.04457255778205216\n",
            "training error 0.03540680346901752, test error 0.05666016624681235\n",
            "Loss: 0.22706863168295843\n",
            "training error 0.035386284733574684, test error 0.05657313365253255\n",
            "Loss: 0.07311529236382341\n",
            "training error 0.035396836550879066, test error 0.05684775070639182\n",
            "Loss: 0.5588897637038404\n",
            "training error 0.03537252246429619, test error 0.056793543865644266\n",
            "Loss: 0.46300242154100335\n",
            "training error 0.035332689347451395, test error 0.05670349329951113\n",
            "Loss: 0.3037105438427945\n",
            "training error 0.035341190022281734, test error 0.05668407864551164\n",
            "Loss: 0.2693676538062295\n",
            "training error 0.035345550744669295, test error 0.05670958429673447\n",
            "Loss: 0.3144850056415738\n",
            "training error 0.035344488990505206, test error 0.0567896452374232\n",
            "Loss: 0.45610607611588083\n",
            "training error 0.03528949059658603, test error 0.05658190666562475\n",
            "Loss: 0.0886340146610376\n",
            "training error 0.03530578858783248, test error 0.056660430040561684\n",
            "Loss: 0.22753526063890028\n",
            "training error 0.035301770108546224, test error 0.05661518258811352\n",
            "Loss: 0.1474963228395243\n",
            "training error 0.03530435702096004, test error 0.05649156490193002\n",
            "Loss: 0.0\n",
            "training error 0.035321486751264695, test error 0.05650780385573519\n",
            "Loss: 0.028745802728891157\n",
            "training error 0.03529097345927772, test error 0.05673077550742451\n",
            "Loss: 0.4234448203192187\n",
            "training error 0.0353297369435869, test error 0.05669765019362327\n",
            "Loss: 0.364807192102079\n",
            "training error 0.035257785139028254, test error 0.05668135243377411\n",
            "Loss: 0.33595729233837357\n",
            "training error 0.03527993853093662, test error 0.05655539987501799\n",
            "Loss: 0.11299912331830253\n",
            "training error 0.03523542262882969, test error 0.05656982538707714\n",
            "Loss: 0.13853481538876888\n",
            "training error 0.03530163407641797, test error 0.05653440403635997\n",
            "Loss: 0.07583279823160094\n",
            "training error 0.03524475358462248, test error 0.05662709768466131\n",
            "Loss: 0.23991684947404224\n",
            "training error 0.035288683365607, test error 0.05680422589013863\n",
            "Loss: 0.5534649088786869\n",
            "training error 0.03521509233734754, test error 0.05661284737578688\n",
            "Loss: 0.21469129783784346\n",
            "training error 0.035249610215753915, test error 0.05657406258612004\n",
            "Loss: 0.14603540251227898\n",
            "training error 0.03523807172513894, test error 0.05641226294974919\n",
            "Loss: 0.0\n",
            "training error 0.03520925510997941, test error 0.05650745125906143\n",
            "Loss: 0.16873690991094925\n",
            "training error 0.03517903366486294, test error 0.05649046973345163\n",
            "Loss: 0.1386343670916057\n",
            "training error 0.03518224911966456, test error 0.05648528472460552\n",
            "Loss: 0.12944308743894073\n",
            "training error 0.03519653348305246, test error 0.05659461648106353\n",
            "Loss: 0.3232515800275282\n",
            "training error 0.03522140865776387, test error 0.056212212074885616\n",
            "Loss: 0.0\n",
            "training error 0.035170702277994334, test error 0.05628850581365975\n",
            "Loss: 0.13572449109902074\n",
            "training error 0.0352016842096569, test error 0.05623477710619688\n",
            "Loss: 0.04014257841551405\n",
            "training error 0.035150170581195975, test error 0.05618379464795652\n",
            "Loss: 0.0\n",
            "training error 0.03516326351165824, test error 0.056199390177603094\n",
            "Loss: 0.02775805682813104\n",
            "training error 0.03516530183822945, test error 0.05624392225826668\n",
            "Loss: 0.10701948967120067\n",
            "training error 0.03514648084030856, test error 0.05632491739143665\n",
            "Loss: 0.25118051275174746\n",
            "training error 0.035131014313511316, test error 0.056355956893302986\n",
            "Loss: 0.30642687348767517\n",
            "training error 0.035132324207141484, test error 0.05641368260770936\n",
            "Loss: 0.4091712943087966\n",
            "training error 0.0351707847911106, test error 0.05630586894259737\n",
            "Loss: 0.21727669945712513\n",
            "training error 0.03517031605654269, test error 0.056538044791186896\n",
            "Loss: 0.6305201445542918\n",
            "training error 0.035216670185065496, test error 0.0562438802767384\n",
            "Loss: 0.10694476789681051\n",
            "training error 0.035105502278823524, test error 0.05643958263332413\n",
            "Loss: 0.4552700417804756\n",
            "training error 0.03511169458801173, test error 0.05643520051924667\n",
            "Loss: 0.44747043674326914\n",
            "training error 0.03510880577040558, test error 0.0563227972356535\n",
            "Loss: 0.2474069054394734\n",
            "training error 0.03512659965639169, test error 0.056307073695133984\n",
            "Loss: 0.21942100555849553\n",
            "training error 0.035134502828554307, test error 0.05631513176391263\n",
            "Loss: 0.2337633418658669\n",
            "training error 0.03509145865297134, test error 0.056251805968936724\n",
            "Loss: 0.12105149074810306\n",
            "training error 0.035091079821132715, test error 0.05623533838993927\n",
            "Loss: 0.09174129712263834\n",
            "training error 0.03511997330251148, test error 0.056376115189402255\n",
            "Loss: 0.3423060735765704\n",
            "training error 0.03509675183312408, test error 0.05602201178852614\n",
            "Loss: 0.0\n",
            "training error 0.03508703795216069, test error 0.05602351129636884\n",
            "Loss: 0.00267664047546301\n",
            "training error 0.035067557937499105, test error 0.05594389031535519\n",
            "Loss: 0.0\n",
            "training error 0.03508933503430498, test error 0.05600042130676426\n",
            "Loss: 0.10104944631201818\n",
            "training error 0.03508427506455621, test error 0.05573487084337187\n",
            "Loss: 0.0\n",
            "training error 0.03504743189368722, test error 0.055746197197624296\n",
            "Loss: 0.02032184533853343\n",
            "training error 0.03505224261648346, test error 0.05572903123942942\n",
            "Loss: 0.0\n",
            "training error 0.035023440486892106, test error 0.05583491542781593\n",
            "Loss: 0.18999825769732137\n",
            "training error 0.03504598573781566, test error 0.05595041693427834\n",
            "Loss: 0.3972538009817228\n",
            "training error 0.03504362731954341, test error 0.055975574802110004\n",
            "Loss: 0.4423970006249611\n",
            "training error 0.03512857894879218, test error 0.05598650338158147\n",
            "Loss: 0.46200720957425556\n",
            "training error 0.03504145632765408, test error 0.05599371188507242\n",
            "Loss: 0.4749421257761455\n",
            "training error 0.03501239678441149, test error 0.05594600389881242\n",
            "Loss: 0.38933506389302597\n",
            "training error 0.03501182742619462, test error 0.05597620704482437\n",
            "Loss: 0.44353149498150835\n",
            "training error 0.03502089600378835, test error 0.05601667774706749\n",
            "Loss: 0.516151996976677\n",
            "training error 0.0350173631090381, test error 0.05600917963411041\n",
            "Loss: 0.5026974064511913\n",
            "training error 0.034998423295160536, test error 0.05602816755638151\n",
            "Loss: 0.53676927500661\n",
            "training error 0.03514048909174871, test error 0.05593413127673208\n",
            "Loss: 0.36803086782808414\n",
            "training error 0.03502862468216445, test error 0.05588985939801348\n",
            "Loss: 0.2885895466100852\n",
            "training error 0.03501107347481371, test error 0.05601471825090936\n",
            "Loss: 0.512635883176471\n",
            "training error 0.03498905062945456, test error 0.055857439413253315\n",
            "Loss: 0.23041522697966332\n",
            "training error 0.03500199739580085, test error 0.05603570464515684\n",
            "Loss: 0.5502938036189065\n",
            "training error 0.03499556905766955, test error 0.055959552102649024\n",
            "Loss: 0.41364591864017086\n",
            "training error 0.03500854630795899, test error 0.05586989122783613\n",
            "Loss: 0.25275872426622215\n",
            "training error 0.03498138357840871, test error 0.055851020279152286\n",
            "Loss: 0.2188967527513075\n",
            "training error 0.03501515299677952, test error 0.05581282908032398\n",
            "Loss: 0.1503665845805413\n",
            "training error 0.03501725962278364, test error 0.055829748711470215\n",
            "Loss: 0.1807271179864678\n",
            "training error 0.03499850517463575, test error 0.05587910015961064\n",
            "Loss: 0.26928320274666184\n",
            "training error 0.03500513555190508, test error 0.05603338183162452\n",
            "Loss: 0.5461257542545672\n",
            "training error 0.03498695898532646, test error 0.055970911038555636\n",
            "Loss: 0.4340283578356585\n",
            "training error 0.03500785844159132, test error 0.05599587859882338\n",
            "Loss: 0.47883007017921475\n",
            "training error 0.03496376631357611, test error 0.055824306536986064\n",
            "Loss: 0.17096169705035358\n",
            "training error 0.034969296547245295, test error 0.05585822164707445\n",
            "Loss: 0.2318188649107933\n",
            "training error 0.0350382328244685, test error 0.05560939666553518\n",
            "Loss: 0.0\n",
            "training error 0.034933320901468404, test error 0.05581925760661648\n",
            "Loss: 0.3773839560668568\n",
            "training error 0.03496617594377752, test error 0.055931229183138934\n",
            "Loss: 0.5787376538886502\n",
            "training error 0.03498145525013631, test error 0.05588265810517058\n",
            "Loss: 0.49139436142229265\n",
            "training error 0.034950917169609, test error 0.05595511706603487\n",
            "Loss: 0.6216942121832991\n",
            "training error 0.03499109489886925, test error 0.05597098811550424\n",
            "Loss: 0.6502344417506745\n",
            "training error 0.03494101331074976, test error 0.05575501209210073\n",
            "Loss: 0.26185399464295767\n",
            "training error 0.03494205077133801, test error 0.05581635057444427\n",
            "Loss: 0.37215636442491373\n",
            "training error 0.03493789361866356, test error 0.05568188398197471\n",
            "Loss: 0.13035084137940078\n",
            "training error 0.034917266296769396, test error 0.05560668015995295\n",
            "Loss: 0.0\n",
            "training error 0.03491337973678854, test error 0.05562574434654775\n",
            "Loss: 0.034283986276384404\n",
            "training error 0.03491685917215972, test error 0.05566841915324201\n",
            "Loss: 0.11102801517994809\n",
            "training error 0.034955210342377675, test error 0.05563713817491075\n",
            "Loss: 0.054774021520764826\n",
            "training error 0.034915899684401036, test error 0.05564412180428361\n",
            "Loss: 0.0673329970840797\n",
            "training error 0.0349108314069701, test error 0.055695424267078346\n",
            "Loss: 0.15959252893738007\n",
            "training error 0.03489735478257219, test error 0.05574847050956365\n",
            "Loss: 0.25498797842784526\n",
            "training error 0.034997718693892774, test error 0.05578901276462896\n",
            "Loss: 0.3278969435893897\n",
            "training error 0.03496357963069516, test error 0.05567035428719172\n",
            "Loss: 0.11450805380865958\n",
            "training error 0.034893509179744826, test error 0.055604485980638475\n",
            "Loss: 0.0\n",
            "training error 0.035002638017694404, test error 0.05578648630319587\n",
            "Loss: 0.32731230106284315\n",
            "training error 0.034930406033984805, test error 0.05554112003615755\n",
            "Loss: 0.0\n",
            "training error 0.034984255184365015, test error 0.05571460030695281\n",
            "Loss: 0.3123456471211483\n",
            "training error 0.03490908835313035, test error 0.05568824971696527\n",
            "Loss: 0.2649022574840787\n",
            "training error 0.03487013259605026, test error 0.055797180930901254\n",
            "Loss: 0.4610294041189844\n",
            "training error 0.0349515252434151, test error 0.05582991034670528\n",
            "Loss: 0.519957664447035\n",
            "training error 0.03489033552255108, test error 0.05582636301179408\n",
            "Loss: 0.513570802048724\n",
            "training error 0.034985664855584964, test error 0.055595549226271745\n",
            "Loss: 0.09799800594363361\n",
            "training error 0.03487660631805464, test error 0.05560689051424551\n",
            "Loss: 0.11841763011828377\n",
            "training error 0.0348441308061344, test error 0.05573143914102827\n",
            "Loss: 0.3426634262089445\n",
            "training error 0.03487687874909508, test error 0.055787933265279296\n",
            "Loss: 0.4443792796419599\n",
            "training error 0.034848207243151276, test error 0.05562747191062454\n",
            "Loss: 0.15547377224438552\n",
            "training error 0.03488577965968106, test error 0.055531719484445526\n",
            "Loss: 0.0\n",
            "training error 0.034928561184286125, test error 0.05571221137764757\n",
            "Loss: 0.32502485944561066\n",
            "training error 0.03487965880787167, test error 0.055571028769161854\n",
            "Loss: 0.07078708363665864\n",
            "training error 0.03489619670229017, test error 0.05554516254367015\n",
            "Loss: 0.02420789298338555\n",
            "training error 0.034855451258539956, test error 0.05566596733685201\n",
            "Loss: 0.2417498569337262\n",
            "training error 0.03483164034813929, test error 0.05576084964190236\n",
            "Loss: 0.41261131400949225\n",
            "training error 0.034879202367719124, test error 0.055705449123178014\n",
            "Loss: 0.3128475767460337\n",
            "training error 0.034837412908233605, test error 0.05577776491842053\n",
            "Loss: 0.4430718808264622\n",
            "training error 0.034890148363080305, test error 0.05580695459840622\n",
            "Loss: 0.49563585733696236\n",
            "training error 0.03481652484630376, test error 0.055727655818329856\n",
            "Loss: 0.3528367853605108\n",
            "training error 0.03485036670477784, test error 0.05575616358345982\n",
            "Loss: 0.40417278826951897\n",
            "training error 0.034815173761151265, test error 0.05564137312535454\n",
            "Loss: 0.19746127425377402\n",
            "training error 0.03486111488809446, test error 0.05583016336126041\n",
            "Loss: 0.5374295620334202\n",
            "training error 0.03481966893857555, test error 0.05574009259545002\n",
            "Loss: 0.37523259308198664\n",
            "training error 0.03481425734317333, test error 0.05567910844052776\n",
            "Loss: 0.26541399663218\n",
            "training error 0.03484129041333703, test error 0.05573219716009085\n",
            "Loss: 0.36101470926264145\n",
            "training error 0.034866787817016257, test error 0.05556019064117173\n",
            "Loss: 0.051270079497856\n",
            "training error 0.03482515131172711, test error 0.05558380308884633\n",
            "Loss: 0.09379072876609662\n",
            "training error 0.03486512286407789, test error 0.05557588603224881\n",
            "Loss: 0.07953391001274124\n",
            "training error 0.03483876203110995, test error 0.055835403550550625\n",
            "Loss: 0.5468659514318963\n",
            "training error 0.03481565328521764, test error 0.055693651342533367\n",
            "Loss: 0.2916024563820674\n",
            "training error 0.03481928495058626, test error 0.05586107823045177\n",
            "Loss: 0.593100212030162\n",
            "training error 0.03481670144953941, test error 0.05577702437746118\n",
            "Loss: 0.44173833494272063\n",
            "training error 0.034815192111766, test error 0.055799827099494145\n",
            "Loss: 0.48280085244563065\n",
            "training error 0.034870135331930695, test error 0.05567155705855937\n",
            "Loss: 0.25181567473884225\n",
            "training error 0.03482715419193229, test error 0.05551037308226967\n",
            "Loss: 0.0\n",
            "training error 0.03483887296078234, test error 0.05556419829748154\n",
            "Loss: 0.09696424690228689\n",
            "training error 0.034944131281849755, test error 0.05570749404280498\n",
            "Loss: 0.35510653160835126\n",
            "training error 0.03481054319997543, test error 0.0555737855612269\n",
            "Loss: 0.11423536797932066\n",
            "training error 0.03479424017078092, test error 0.05561594926097383\n",
            "Loss: 0.1901918017154891\n",
            "training error 0.03480889661394604, test error 0.0557024436699956\n",
            "Loss: 0.3460084612316727\n",
            "training error 0.03486917727137762, test error 0.05588163707874641\n",
            "Loss: 0.6688191339058402\n",
            "training error 0.03484171302024028, test error 0.055779963937484855\n",
            "Loss: 0.48565851794155623\n",
            "training error 0.03489757755240879, test error 0.05539561269647224\n",
            "Loss: 0.0\n",
            "training error 0.03479359611617599, test error 0.05552219178055165\n",
            "Loss: 0.22850019688918088\n",
            "training error 0.03482811558679082, test error 0.05560817906183021\n",
            "Loss: 0.38372418863328317\n",
            "training error 0.034804679407544274, test error 0.05555436978603341\n",
            "Loss: 0.2865878394215926\n",
            "training error 0.03482902452401715, test error 0.05528836079919141\n",
            "Loss: 0.0\n",
            "training error 0.034804309520884105, test error 0.055452953165724274\n",
            "Loss: 0.2976980401547147\n",
            "training error 0.03479539619430696, test error 0.05548913296153787\n",
            "Loss: 0.3631363987723679\n",
            "training error 0.03479153353334718, test error 0.055490462037313\n",
            "Loss: 0.36554029672832833\n",
            "training error 0.034795036571334474, test error 0.05561839870248209\n",
            "Loss: 0.5969392083975622\n",
            "training error 0.03481038415794761, test error 0.055689295987976124\n",
            "Loss: 0.7251710540685519\n",
            "training error 0.03481187752326653, test error 0.055588195585283544\n",
            "Loss: 0.5423108621019468\n",
            "training error 0.03481832301625228, test error 0.055668641565233844\n",
            "Loss: 0.6878134213883058\n",
            "training error 0.03478821734098223, test error 0.055594885756643085\n",
            "Loss: 0.554411368000185\n",
            "training error 0.03480442772169553, test error 0.0553872419243078\n",
            "Loss: 0.17884618694978816\n",
            "training error 0.03482003031084851, test error 0.055461509238054235\n",
            "Loss: 0.31317339917473763\n",
            "training error 0.03477803007603202, test error 0.055448424914093415\n",
            "Loss: 0.28950779619487665\n",
            "training error 0.03477468653866405, test error 0.05547735992370681\n",
            "Loss: 0.34184251763558215\n",
            "training error 0.034825910343254954, test error 0.05550115112636844\n",
            "Loss: 0.38487364085524334\n",
            "training error 0.03478316502288982, test error 0.05566438793505518\n",
            "Loss: 0.6801198849600842\n",
            "training error 0.034784911795508244, test error 0.05564818650047445\n",
            "Loss: 0.6508163672819478\n",
            "training error 0.034817053543970175, test error 0.05567962314392686\n",
            "Loss: 0.7076757912149567\n",
            "training error 0.03480138064625093, test error 0.05537524071928506\n",
            "Loss: 0.15713962005348137\n",
            "training error 0.03475909416048394, test error 0.05534467022092046\n",
            "Loss: 0.10184679182942968\n",
            "training error 0.034785411956186486, test error 0.05535711540290837\n",
            "Loss: 0.12435637939545163\n",
            "training error 0.03479500024254606, test error 0.055427057377637215\n",
            "Loss: 0.2508603554906452\n",
            "training error 0.03478990584872915, test error 0.05549017296616021\n",
            "Loss: 0.3650174540384521\n",
            "training error 0.03481598927683959, test error 0.05570587741131941\n",
            "Loss: 0.7551618570216467\n",
            "training error 0.034829472466399, test error 0.05569196879181438\n",
            "Loss: 0.730005351558316\n",
            "training error 0.03480802279364915, test error 0.05544421850384618\n",
            "Loss: 0.281899666406904\n",
            "training error 0.03475715075831894, test error 0.05564205900279091\n",
            "Loss: 0.6397335686694205\n",
            "training error 0.034750912695516153, test error 0.05552386984854168\n",
            "Loss: 0.42596496974407483\n",
            "training error 0.03475581363382585, test error 0.05564017819551442\n",
            "Loss: 0.6363317545275526\n",
            "training error 0.034750353210892225, test error 0.05548484054052156\n",
            "Loss: 0.35537270139689436\n",
            "training error 0.03477027708466359, test error 0.05547550739922998\n",
            "Loss: 0.33849185856367203\n",
            "training error 0.03473971446970542, test error 0.05549852647218709\n",
            "Loss: 0.380126431599237\n",
            "training error 0.03475071313854899, test error 0.05551324851253567\n",
            "Loss: 0.40675417048636753\n",
            "training error 0.034787052812379586, test error 0.05540031324306706\n",
            "Loss: 0.2024882674352746\n",
            "training error 0.03472777096826187, test error 0.0554124595954947\n",
            "Loss: 0.22445736229008695\n",
            "training error 0.03475981691215899, test error 0.05541736404828936\n",
            "Loss: 0.23332804089906123\n",
            "training error 0.03472420960968416, test error 0.05547208082400317\n",
            "Loss: 0.33229421555658334\n",
            "training error 0.03471860594200687, test error 0.05552413457614503\n",
            "Loss: 0.42644378228169355\n",
            "training error 0.034759655687094985, test error 0.055526707287690005\n",
            "Loss: 0.4310970429459493\n",
            "training error 0.03478736584804293, test error 0.05565563966329728\n",
            "Loss: 0.6642968950369754\n",
            "training error 0.03474980246924187, test error 0.055762679520139816\n",
            "Loss: 0.8578997714747683\n",
            "training error 0.034740560891666945, test error 0.0555987704284684\n",
            "Loss: 0.5614375698429708\n",
            "training error 0.03473446407817871, test error 0.05570066909688972\n",
            "Loss: 0.7457415841931381\n",
            "training error 0.03474285832740496, test error 0.05551590365782387\n",
            "Loss: 0.4115565289752432\n",
            "training error 0.034761455529989424, test error 0.05565494015713503\n",
            "Loss: 0.6630316989773677\n",
            "training error 0.03473019842491321, test error 0.055669284868929046\n",
            "Loss: 0.6889769641049037\n",
            "training error 0.034743850030995685, test error 0.05549597672951086\n",
            "Loss: 0.37551471470371744\n",
            "training error 0.034721867217546554, test error 0.05533714219187783\n",
            "Loss: 0.08823085362144045\n",
            "training error 0.03469878023357363, test error 0.055460242993909385\n",
            "Loss: 0.31088314472236256\n",
            "training error 0.034773889491833715, test error 0.05525399969382547\n",
            "Loss: 0.0\n",
            "training error 0.03470849742858553, test error 0.05552138774672461\n",
            "Loss: 0.48392524411047244\n",
            "training error 0.03471682332232048, test error 0.05554537053209726\n",
            "Loss: 0.5273298582660901\n",
            "training error 0.03474193859048855, test error 0.055617023046944686\n",
            "Loss: 0.6570082801802712\n",
            "training error 0.03470574750211906, test error 0.055627066935967515\n",
            "Loss: 0.6751859489074086\n",
            "training error 0.03471710988122913, test error 0.05565078502344199\n",
            "Loss: 0.7181115065247656\n",
            "training error 0.03471528673778069, test error 0.05555624167938249\n",
            "Loss: 0.5470047186299842\n",
            "training error 0.03475978081429496, test error 0.05562072296522863\n",
            "Loss: 0.6637044801014458\n",
            "training error 0.03475773530169063, test error 0.05555634049255823\n",
            "Loss: 0.547183553060604\n",
            "training error 0.03471653619376707, test error 0.055594925855678504\n",
            "Loss: 0.6170162589897332\n",
            "training error 0.03474658943602614, test error 0.05576215028505186\n",
            "Loss: 0.9196630000401163\n",
            "training error 0.03473715703039628, test error 0.055620771306941284\n",
            "Loss: 0.6637919700803074\n",
            "training error 0.03476057783932929, test error 0.05588466709303415\n",
            "Loss: 1.1413968268421337\n",
            "training error 0.03476575239589502, test error 0.0558468514197229\n",
            "Loss: 1.0729571237965585\n",
            "training error 0.03471373716298289, test error 0.05576488207399998\n",
            "Loss: 0.9246070565125164\n",
            "training error 0.034738041148530054, test error 0.05564092768443219\n",
            "Loss: 0.7002714604386506\n",
            "training error 0.03474570422827726, test error 0.05542324893487111\n",
            "Loss: 0.30631129327016726\n",
            "training error 0.03476921196783784, test error 0.05535921358937161\n",
            "Loss: 0.19041860522162057\n",
            "training error 0.03471939495990484, test error 0.05526848753469538\n",
            "Loss: 0.02622043824918041\n",
            "training error 0.034700153870222766, test error 0.05534923190259739\n",
            "Loss: 0.17235351160029833\n",
            "training error 0.03473903531355585, test error 0.05554868162030798\n",
            "Loss: 0.5333223442925528\n",
            "training error 0.03470819296519212, test error 0.05544169550932811\n",
            "Loss: 0.33969634151862493\n",
            "training error 0.03480136214846965, test error 0.05549666660848635\n",
            "Loss: 0.4391843414152108\n",
            "training error 0.034757587213275626, test error 0.05559466784866693\n",
            "Loss: 0.6165493117768372\n",
            "training error 0.034809107959460676, test error 0.05564291405537747\n",
            "Loss: 0.7038664417183593\n",
            "training error 0.03468678279280846, test error 0.05553846279330982\n",
            "Loss: 0.5148280686658291\n",
            "training error 0.03474935698940607, test error 0.055736094050344374\n",
            "Loss: 0.8725058080687242\n",
            "training error 0.03481555112184504, test error 0.055276798427177545\n",
            "Loss: 0.04126168870743907\n",
            "training error 0.03471777631598533, test error 0.05544960877853431\n",
            "Loss: 0.3540179639351937\n",
            "training error 0.03471030293458178, test error 0.05518155563682398\n",
            "Loss: 0.0\n",
            "training error 0.03469645380842433, test error 0.05516249936220111\n",
            "Loss: 0.0\n",
            "training error 0.03468293086983256, test error 0.055127183354351315\n",
            "Loss: 0.0\n",
            "training error 0.03469589136699745, test error 0.05517751377529959\n",
            "Loss: 0.09129873482698336\n",
            "training error 0.03476467743708525, test error 0.05512393786539332\n",
            "Loss: 0.0\n",
            "training error 0.034748084801735656, test error 0.055428548565642784\n",
            "Loss: 0.5525924163714446\n",
            "training error 0.03468437221772623, test error 0.055367655177806965\n",
            "Loss: 0.44212609231362165\n",
            "training error 0.034680715524169514, test error 0.05533192496539345\n",
            "Loss: 0.377308131556231\n",
            "training error 0.03468897242623948, test error 0.05537512430495614\n",
            "Loss: 0.45567579039109685\n",
            "training error 0.034796061780153806, test error 0.05509122449731458\n",
            "Loss: 0.0\n",
            "training error 0.034691945573561395, test error 0.05523456779984109\n",
            "Loss: 0.2601926238424701\n",
            "training error 0.034681289999289995, test error 0.055294833407522954\n",
            "Loss: 0.3695850147936053\n",
            "training error 0.03471285835291478, test error 0.055335144189445296\n",
            "Loss: 0.4427559822029581\n",
            "training error 0.03479943430343107, test error 0.05515192282043179\n",
            "Loss: 0.11017784351512461\n",
            "training error 0.034759131840863614, test error 0.05540610087979518\n",
            "Loss: 0.5715545177180648\n",
            "training error 0.03468154791283109, test error 0.05520067671914159\n",
            "Loss: 0.19867451272996828\n",
            "training error 0.034693205769008265, test error 0.055107484221800715\n",
            "Loss: 0.029514182402912148\n",
            "training error 0.034731035645105636, test error 0.05519082824278192\n",
            "Loss: 0.18079784280742306\n",
            "training error 0.034691414573332933, test error 0.055242224828370816\n",
            "Loss: 0.2740914409401052\n",
            "training error 0.034685238896722724, test error 0.05530583094092095\n",
            "Loss: 0.38954741987415886\n",
            "training error 0.03472058901937579, test error 0.05530944215111317\n",
            "Loss: 0.3961023843447764\n",
            "training error 0.034721337855695034, test error 0.0553583211902852\n",
            "Loss: 0.4848262049133467\n",
            "training error 0.0346813899392668, test error 0.05549018775047915\n",
            "Loss: 0.7241865774539447\n",
            "training error 0.034689806403217734, test error 0.05552061683603565\n",
            "Loss: 0.7794205749447558\n",
            "training error 0.03478532525750648, test error 0.055392323831581027\n",
            "Loss: 0.5465468175990917\n",
            "training error 0.034726632197665176, test error 0.05573224509829597\n",
            "Loss: 1.1635620860317442\n",
            "training error 0.034688536674598355, test error 0.05572539384850417\n",
            "Loss: 1.151125895233096\n",
            "training error 0.03466779788504096, test error 0.055700688341948044\n",
            "Loss: 1.106281173080803\n",
            "training error 0.0346820950846339, test error 0.05555929970219386\n",
            "Loss: 0.8496365966635944\n",
            "training error 0.03470449300911267, test error 0.05574108577382361\n",
            "Loss: 1.1796094249832967\n",
            "training error 0.034717418113865355, test error 0.05580917561750339\n",
            "Loss: 1.3032041432003538\n",
            "training error 0.03473873002539436, test error 0.05582985999346834\n",
            "Loss: 1.3407498252099304\n",
            "training error 0.034683501025380405, test error 0.05573277708796545\n",
            "Loss: 1.1645277383190988\n",
            "training error 0.03470062176860272, test error 0.05569714736891648\n",
            "Loss: 1.0998537010761922\n",
            "training error 0.03467915720294232, test error 0.05572541091688746\n",
            "Loss: 1.1511568772696101\n",
            "training error 0.034677298313597914, test error 0.05559758192355027\n",
            "Loss: 0.9191253795064469\n",
            "training error 0.03477338529854488, test error 0.05566501592112919\n",
            "Loss: 1.0415296248181605\n",
            "training error 0.03478968876691699, test error 0.055416222329850634\n",
            "Loss: 0.5899266816113302\n",
            "training error 0.034695208868923785, test error 0.05576246480384397\n",
            "Loss: 1.218416022976787\n",
            "training error 0.03472187381467032, test error 0.055641754326274266\n",
            "Loss: 0.9993058494942808\n",
            "training error 0.03466827079395509, test error 0.055426099852286866\n",
            "Loss: 0.6078560751333617\n",
            "training error 0.034685434845848656, test error 0.05532965846526276\n",
            "Loss: 0.4327984540619667\n",
            "training error 0.03473491305568564, test error 0.054966048564705966\n",
            "Loss: 0.0\n",
            "training error 0.034687645248242485, test error 0.054865409439913936\n",
            "Loss: 0.0\n",
            "training error 0.03468798013799645, test error 0.055114591070290854\n",
            "Loss: 0.454168907004715\n",
            "training error 0.03472083398131697, test error 0.054957996688639106\n",
            "Loss: 0.16875340887880785\n",
            "training error 0.03466114184657342, test error 0.055074957464748366\n",
            "Loss: 0.38193103263708394\n",
            "training error 0.03469825095899056, test error 0.055291076775746364\n",
            "Loss: 0.7758391674787424\n",
            "training error 0.03467177104896962, test error 0.05531119521776542\n",
            "Loss: 0.812507885026692\n",
            "training error 0.03469108340695283, test error 0.05535496948950387\n",
            "Loss: 0.8922927115418311\n",
            "training error 0.03469295425599721, test error 0.0552034079560614\n",
            "Loss: 0.6160502939791757\n",
            "training error 0.03468068315888342, test error 0.05518341393378705\n",
            "Loss: 0.5796083490844595\n",
            "training error 0.03466714241785577, test error 0.05499277471332792\n",
            "Loss: 0.23214129761206515\n",
            "training error 0.03468314646540528, test error 0.05497474408566371\n",
            "Loss: 0.19927791821094942\n",
            "training error 0.034685273505575064, test error 0.054772491519896674\n",
            "Loss: 0.0\n",
            "training error 0.03466202140059082, test error 0.05485536570454045\n",
            "Loss: 0.1513062165771073\n",
            "training error 0.03467282186875757, test error 0.05474162722243739\n",
            "Loss: 0.0\n",
            "training error 0.03470354794075293, test error 0.05487300845764209\n",
            "Loss: 0.2400024293593761\n",
            "training error 0.03467971139762828, test error 0.05488711910669706\n",
            "Loss: 0.26577924632835703\n",
            "training error 0.034647721443161486, test error 0.05492196393127784\n",
            "Loss: 0.3294324958731387\n",
            "training error 0.034688481420602796, test error 0.055013879338891125\n",
            "Loss: 0.49734019660663265\n",
            "training error 0.03464848555266561, test error 0.055062394441183306\n",
            "Loss: 0.5859658088761321\n",
            "training error 0.03469108185851159, test error 0.05520302689967483\n",
            "Loss: 0.842868034160893\n",
            "training error 0.034715353185310015, test error 0.05484606488070914\n",
            "Loss: 0.1907828896780428\n",
            "training error 0.03469999541024433, test error 0.05510211835977727\n",
            "Loss: 0.6585320086943236\n",
            "training error 0.03468776186991823, test error 0.054803827290296564\n",
            "Loss: 0.11362480623096083\n",
            "training error 0.0346588668942227, test error 0.05485765572024171\n",
            "Loss: 0.2119566108856219\n",
            "training error 0.034726071867986846, test error 0.05506393287363031\n",
            "Loss: 0.588776160933735\n",
            "training error 0.03466594742463601, test error 0.05507184065774127\n",
            "Loss: 0.6032218113686927\n",
            "training error 0.0346460906207181, test error 0.054991206150164354\n",
            "Loss: 0.45592164572094696\n",
            "training error 0.03464581654366814, test error 0.05510597993510152\n",
            "Loss: 0.6655861930878082\n",
            "training error 0.034633645909565164, test error 0.055175240879395085\n",
            "Loss: 0.792109549823472\n",
            "training error 0.03466606553133914, test error 0.05492572418979427\n",
            "Loss: 0.3363015984322537\n",
            "training error 0.03464829539356611, test error 0.054932827942874414\n",
            "Loss: 0.34927847442332016\n",
            "training error 0.034705332361922706, test error 0.05519995366176523\n",
            "Loss: 0.837253955688011\n",
            "training error 0.034649409510193234, test error 0.05509391375912625\n",
            "Loss: 0.6435441446732693\n",
            "training error 0.034647527208431866, test error 0.05511362462390835\n",
            "Loss: 0.6795512306555729\n",
            "training error 0.034646305027845124, test error 0.05509386562418956\n",
            "Loss: 0.6434562135335886\n",
            "training error 0.03476018283497433, test error 0.0551676129911799\n",
            "Loss: 0.7781752029612843\n",
            "training error 0.03469134915521933, test error 0.05525242567332894\n",
            "Loss: 0.9331079049147872\n",
            "training error 0.034664801540760175, test error 0.055195606161257016\n",
            "Loss: 0.8293121009628202\n",
            "training error 0.03467738933879975, test error 0.05510476672804773\n",
            "Loss: 0.6633699508689528\n",
            "training error 0.03466009285073821, test error 0.05531871058063121\n",
            "Loss: 1.054194746255721\n",
            "training error 0.03465810766070944, test error 0.055202161525868515\n",
            "Loss: 0.8412872009810446\n",
            "training error 0.0347039213655844, test error 0.0552307402855553\n",
            "Loss: 0.8934938326375486\n",
            "training error 0.034693734004812564, test error 0.05514839671661694\n",
            "Loss: 0.7430716162796669\n",
            "training error 0.03464195175141951, test error 0.055230797110469784\n",
            "Loss: 0.8935976383104283\n",
            "training error 0.03473889945816273, test error 0.0553448543706638\n",
            "Loss: 1.101953264515232\n",
            "training error 0.03465238358521503, test error 0.05523126825771773\n",
            "Loss: 0.8944583128497996\n",
            "training error 0.03466249795712945, test error 0.055247634868959894\n",
            "Loss: 0.9243562389301907\n",
            "training error 0.03465578919744541, test error 0.05520930453823125\n",
            "Loss: 0.8543357943918917\n",
            "training error 0.03468322483982967, test error 0.055435919903713486\n",
            "Loss: 1.2683084455179072\n",
            "training error 0.03465043815747209, test error 0.05518954749216663\n",
            "Loss: 0.8182443461338229\n",
            "training error 0.03463400386856442, test error 0.055253174562221063\n",
            "Loss: 0.9344759477190046\n",
            "training error 0.034638531614173115, test error 0.05533970597542491\n",
            "Loss: 1.0925483646244016\n",
            "training error 0.03465393309347158, test error 0.05550528151606474\n",
            "Loss: 1.3950156989749551\n",
            "training error 0.03470355487640246, test error 0.05534203807854875\n",
            "Loss: 1.0968085652106163\n",
            "training error 0.034733189058225464, test error 0.05563740944301027\n",
            "Loss: 1.6363821574630233\n",
            "training error 0.03480953850081587, test error 0.05549418275810945\n",
            "Loss: 1.3747408943729944\n",
            "training error 0.034644681952895946, test error 0.05528670397112872\n",
            "Loss: 0.9957262440819736\n",
            "training error 0.034665846529715195, test error 0.055347230481215204\n",
            "Loss: 1.1062938562586\n",
            "training error 0.03465026500657416, test error 0.055400220845704166\n",
            "Loss: 1.2030947136274284\n",
            "training error 0.03475408501932897, test error 0.055057465868520415\n",
            "Loss: 0.5769624728173328\n",
            "training error 0.03464649263572211, test error 0.055218035276242784\n",
            "Loss: 0.8702847868762742\n",
            "training error 0.034640403660610365, test error 0.055080087798310785\n",
            "Loss: 0.6182873857550675\n",
            "training error 0.034643741691476916, test error 0.05510513513005249\n",
            "Loss: 0.6640429341605358\n",
            "training error 0.034641359100089614, test error 0.05522178446477941\n",
            "Loss: 0.8771336672016572\n",
            "training error 0.03463158572824098, test error 0.05518897335515411\n",
            "Loss: 0.817195533663928\n",
            "training error 0.03464527274126416, test error 0.05525372957212293\n",
            "Loss: 0.9354898194835481\n",
            "training error 0.03466736108045388, test error 0.05512645994244398\n",
            "Loss: 0.7029983205337764\n",
            "training error 0.03466489404222702, test error 0.05540559426982731\n",
            "Loss: 1.2129106880435891\n",
            "training error 0.03465640221297365, test error 0.05519796109295777\n",
            "Loss: 0.8336140039573836\n",
            "training error 0.034639368810031025, test error 0.05522897989533119\n",
            "Loss: 0.8902780162407087\n",
            "training error 0.0346597063729125, test error 0.055278788848454205\n",
            "Loss: 0.98126718782785\n",
            "training error 0.03466285586933113, test error 0.05550453031661471\n",
            "Loss: 1.3936434353281735\n",
            "training error 0.03464826140370553, test error 0.055548931304358576\n",
            "Loss: 1.474753533797557\n",
            "training error 0.03473427072173622, test error 0.05552056535941955\n",
            "Loss: 1.422935664329117\n",
            "training error 0.034619823571653316, test error 0.055408627123960594\n",
            "Loss: 1.2184509949127342\n",
            "training error 0.034634378585422815, test error 0.055477700601258235\n",
            "Loss: 1.3446318938052038\n",
            "training error 0.034651413982827874, test error 0.05543927351339572\n",
            "Loss: 1.2744346968779574\n",
            "training error 0.03463899891454852, test error 0.05547129742805775\n",
            "Loss: 1.3329348114834438\n",
            "training error 0.03463906893598588, test error 0.055525973608023076\n",
            "Loss: 1.4328152548307882\n",
            "training error 0.03464021675885912, test error 0.05550204584244106\n",
            "Loss: 1.3891048888879043\n",
            "training error 0.0346717196900827, test error 0.05503884127562353\n",
            "Loss: 0.5429397485362308\n",
            "training error 0.034620415097420726, test error 0.05509670117776884\n",
            "Loss: 0.6486360989757189\n",
            "training error 0.034695420867246, test error 0.05528776261823846\n",
            "Loss: 0.9976601418549391\n",
            "training error 0.03471557084934944, test error 0.05521429069189409\n",
            "Loss: 0.8634443173128226\n",
            "training error 0.034684569676255925, test error 0.055077176152343296\n",
            "Loss: 0.6129684975246352\n",
            "training error 0.03464844351470206, test error 0.055245533311930065\n",
            "Loss: 0.9205171915060095\n",
            "training error 0.03465277965340769, test error 0.05532333670454866\n",
            "Loss: 1.0626455800218704\n",
            "training error 0.03470868535586499, test error 0.055444513033420816\n",
            "Loss: 1.2840060601913006\n",
            "training error 0.0346255055201502, test error 0.05506720472725603\n",
            "Loss: 0.5947530633236164\n",
            "training error 0.03461882834739778, test error 0.05512522621002873\n",
            "Loss: 0.7007445833362214\n",
            "training error 0.03473721324176624, test error 0.05534412553536324\n",
            "Loss: 1.100621854877737\n",
            "training error 0.03463439855879748, test error 0.055202908503445995\n",
            "Loss: 0.8426517522656551\n",
            "training error 0.0346773133562545, test error 0.05515582510758091\n",
            "Loss: 0.7566415288688066\n",
            "training error 0.03462441988834824, test error 0.05528927878411921\n",
            "Loss: 1.0004298181646831\n",
            "training error 0.034642952240404395, test error 0.055269274533415486\n",
            "Loss: 0.9638867855243927\n",
            "training error 0.03465306867869468, test error 0.0550927963862162\n",
            "Loss: 0.6415029687588003\n",
            "training error 0.034698422029146074, test error 0.055278715366483155\n",
            "Loss: 0.981132953654007\n",
            "training error 0.03460825035267761, test error 0.055233471096980104\n",
            "Loss: 0.8984823789474738\n",
            "training error 0.03462402062017585, test error 0.05513414048018309\n",
            "Loss: 0.7170288456182838\n",
            "training error 0.034605479094861155, test error 0.055079861102873745\n",
            "Loss: 0.6178732668321585\n",
            "training error 0.03469097113915045, test error 0.055051255539866686\n",
            "Loss: 0.5656176718517258\n",
            "training error 0.034618278491638364, test error 0.05529532178136943\n",
            "Loss: 1.0114689442499714\n",
            "training error 0.03464059535563786, test error 0.055305734413051984\n",
            "Loss: 1.0304903585024894\n",
            "training error 0.03462582227769296, test error 0.055364588516166026\n",
            "Loss: 1.1380028788645413\n",
            "training error 0.034650888528645694, test error 0.05538482792751033\n",
            "Loss: 1.1749754943515933\n",
            "training error 0.03468414948568973, test error 0.0554975561094187\n",
            "Loss: 1.3809032090143392\n",
            "training error 0.03465333446028273, test error 0.05559386995398361\n",
            "Loss: 1.5568458133025764\n",
            "training error 0.0346381957038764, test error 0.05553276950200162\n",
            "Loss: 1.445229745088672\n",
            "training error 0.03462517894711909, test error 0.05550479734196078\n",
            "Loss: 1.3941312274520445\n",
            "training error 0.034634350673843295, test error 0.05539150180454518\n",
            "Loss: 1.1871670885249541\n",
            "training error 0.03462425073429104, test error 0.055334602356816635\n",
            "Loss: 1.083225261042653\n",
            "training error 0.03466375037105439, test error 0.055366014199424815\n",
            "Loss: 1.140607264833915\n",
            "training error 0.034643103244185694, test error 0.05519156205448863\n",
            "Loss: 0.8219244748114196\n",
            "training error 0.034644991805192746, test error 0.055258040303010215\n",
            "Loss: 0.9433645048117167\n",
            "training error 0.03465490626373672, test error 0.055095864112334\n",
            "Loss: 0.6471069784922623\n",
            "training error 0.0346358385109886, test error 0.0552560954944015\n",
            "Loss: 0.9398117996632038\n",
            "training error 0.03461387237886495, test error 0.05519408060360195\n",
            "Loss: 0.8265252681036728\n",
            "training error 0.034632251617609064, test error 0.055101480150214724\n",
            "Loss: 0.6573661508363804\n",
            "training error 0.034675858471750656, test error 0.05524579810200313\n",
            "Loss: 0.9210009003150166\n",
            "training error 0.034603506405849464, test error 0.05524889371628632\n",
            "Loss: 0.926655855127767\n",
            "training error 0.034623418344998935, test error 0.05523717315692266\n",
            "Loss: 0.9052451664830263\n",
            "training error 0.034634480681694994, test error 0.055187763357355445\n",
            "Loss: 0.8149851539948294\n",
            "training error 0.03461823798349245, test error 0.05527936058929268\n",
            "Loss: 0.9823116230547102\n",
            "training error 0.03463087375058837, test error 0.05538469188437491\n",
            "Loss: 1.174726975733642\n",
            "training error 0.034700955902013435, test error 0.05540686144197468\n",
            "Loss: 1.21522551171922\n",
            "training error 0.03467964241113128, test error 0.0554461785401008\n",
            "Loss: 1.2870485468043036\n",
            "training error 0.03460032500734274, test error 0.05549532942868197\n",
            "Loss: 1.3768355901843776\n",
            "training error 0.03460932163121576, test error 0.0554904242544323\n",
            "Loss: 1.3678749974900262\n",
            "training error 0.034620501641869784, test error 0.05543703437626011\n",
            "Loss: 1.2703443231546707\n",
            "training error 0.03463077920430057, test error 0.05544898363500972\n",
            "Loss: 1.292172791462809\n",
            "training error 0.03462051070876163, test error 0.055599260405321545\n",
            "Loss: 1.5666928924111811\n",
            "training error 0.03459214233302749, test error 0.05552709943399071\n",
            "Loss: 1.4348718724813025\n",
            "training error 0.03460582391378229, test error 0.055495779091575284\n",
            "Loss: 1.377657017891476\n",
            "training error 0.0346145672931041, test error 0.055463433527092064\n",
            "Loss: 1.3185693251712927\n",
            "training error 0.0346111662175764, test error 0.055336919999506406\n",
            "Loss: 1.0874590458374689\n",
            "training error 0.03460435677125245, test error 0.05527534588923148\n",
            "Loss: 0.974977715999148\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xV1X338c/vzHBRQeQWiTICVqpihKFM0AMR8YZYDTZqqlQDiWkHYxJN8hguzas1ai5CnzTGJ1ahjbUW0mCkRqM2GIwIymgEBRRQIWaUsWLGQS5eYJg5v+ePvWc458yZ+7nP9+3rvObstdfeZ+3ZOL+zLnstc3dERESSRXJdABERyU8KECIikpIChIiIpKQAISIiKSlAiIhISgoQIiKSkgKESBeZ2Vlm9lquyyGSKabnIKQQmVk18LfuvirXZREpVqpBiLTCzEpyXYbuKoZrkNxRgJCiYmYRM5tvZn8wszoze8DMBsXt/6WZ7TKzvWa2xsxOi9t3n5ndbWaPm9mHwDlmVm1mN5nZ5vCY5WbWN8w/1cxq4o5vNW+4f66ZvWNm/2tmf2tmbmYntXIdg8zs38O875vZr8L0L5rZM0l5m8+T4hpuCq+3JC7/58xsc0d+X9KzKUBIsfk68FfA2cBxwPvAXXH7/wcYDXwCeBFYlnT83wDfB/oDTX+I/xqYDowCxgJfbOPzU+Y1s+nAt4DzgZOAqe1cx38CRwKnhWX9cTv5W7uGnwAfAucm7f95+L6935f0YAoQUmyuA77j7jXufhD4LnCFmZUCuPu97r4/bt84MxsQd/zD7v6su8fc/UCYdqe7/6+77wZ+DZS38fmt5f1r4N/dfYu7fxR+dkpm9kngIuA6d3/f3Q+5+9Od+B0kX8N/ATPDc/cH/jJMg3Z+X9KzKUBIsRkBPGRme8xsD7ANaASONbMSM7s9bE7ZB1SHxwyJO35ninPuinv/EdCvjc9vLe9xSedO9TlNyoDd7v5+G3naknzunwOXmVkf4DLgRXd/M9zX6u+ri58tRUQBQorNTuAidz8m7tXX3d8maFq5lKCZZwAwMjzG4o7P1LC+d4DhcdtlbeTdCQwys2NS7PuQoOkJADMbliJPwjW4+1bgTYJaSXzzUtNntfb7kh5OAUIKWS8z6xv3KgXuAb5vZiMAzGyomV0a5u8PHATqCP7I/iCLZX0A+JKZnWpmRwL/0FpGd3+HoK/kX8xsoJn1MrMp4e5NwGlmVh52gH+3g5//c+BGYArwy7j0tn5f0sMpQEghexz4OO71XYJO2UeAJ8xsP/AccEaY/36Cb9JvA1vDfVnh7v8D3Ak8BeyI++yDrRzyBeAQ8CrwJ+Ab4XleB24FVgHbOdyR3p7/IuiI/p27vxeX3tbvS3o4PSgnkgNmdirwCtDH3RtyXR6RVFSDEMmS8PmDPmY2EFgI/FrBQfKZAoRI9swhaC76A8FIoa/ktjgibVMTk4iIpKQahIiIpFQ0T0sOGTLER44cmetiiIgUlA0bNrzn7kNT7SuaADFy5EjWr1+f62KIiBQUM3uztX1qYhIRkZQUIEREJCUFCBERSalo+iBEJD8cOnSImpoaDhw40H5myZq+ffsyfPhwevXq1eFjFCBEJK1qamro378/I0eOxMzaP0Ayzt2pq6ujpqaGUaNGdfg4NTGJSFodOHCAwYMHKzjkETNj8ODBna7VZbQGES6z+BOgBPg3d789af+3gL8FGoBa4NqmhUzMrBF4Ocz6lrvPyGRZW1O1s4rV1auZOnIq0bJoyjxLNixhxdYVXD7mcionVLJkwxLueO4O3j/wPoOOGMT4YePZXred444+jrmT5jafp+k4gOfffp76xnpKI6XUN9bTEGvAzIhYhJjHcPfm/+HcnZJICf1692NYv2HceMaNVE6ozM4vRKQDFBzyT1fuScam2ggXSX8duACoAV4AZoaLlzTlOQd43t0/MrOvAFPd/cpw3wfu3tbKXQkqKiq8q89BzFs1j7t+fxcHGg5gGGaGu9PojXjc2isllBCJRHB3Yh4jYpEWeToiQgQP/0uXCBEiFmkOJM1lcycSCSqK8ftiHqM0Usrlp17O0suWpq0cItu2bePUU0/NdTEkhVT3xsw2uHtFqvyZbGKaCOxw9zfcvR74BcFqXs3c/alwfV4I5qEfTpbd+PiNLHp2ER8e+pBGb6TBGzgUO0SDN7T4A95IY/O+GLGUeToiRiytwaHpnA3eQCPBNdTH6mn0xiA91kBDrCG4vlhDcw3lQMMBlr28DLvFKL21lD7f68Oon4xiyYYlaS2bSDbV1dVRXl5OeXk5w4YN4/jjj2/erq+vb/PY9evXc8MNN7T7GZMmTUpLWVevXs2AAQOay1deXs6qVavScu50yGQT0/Ekro1bQ9sLkXyZYBWtJn3NbD1B89Pt7v6r5APMrBKoBDjhhBO6VMhHdzzapeOKTaM30tjYSPWeauY8Oodv/OYbfP2Mr7Pw/IW5LppIpwwePJiNGzcC8N3vfpd+/fpx0003Ne9vaGigtDT1n76KigoqKlJ+mU6wbt269BQWOOuss3j00db/Drl7i5aA+O3WtHWdHZUXndRmdg1QAfxTXPKIsNrzN8AdZvZnyce5+xJ3r3D3iqFDU04l0q4rxlzRpeMyLUKE0kgpvUt6UxoppcRKKI2UUhopJZKF2/Zxw8csenYRA344QDUKybiqKvjhD4OfmfDFL36R6667jjPOOIO5c+fy+9//nmg0yvjx45k0aRKvvfYaEHyjv+SSS4AguFx77bVMnTqVE088kTvvvLP5fP369WvOP3XqVK644gpOOeUUrr76apqa7R9//HFOOeUUJkyYwA033NB83o6orq7m5JNPZtasWXzqU59i7dq1Cds7d+7k29/+Np/61Kc4/fTTWb58eXN5zjrrLGbMmMGYMWO6/XvLZA3ibRIXZh8epiUws/OB7wBnu3vz8otNi6a7+xtmthoYTzCPflo1fUNu7oNI6hguiZRQakHHMUbKTuOIRTi6z9EcOHSAA40HiFiEE44+geP6H8frda9zsPEgQ44cQt1Hdew7uA/CvqKm4/qW9OXskWfTv3d/AGaNm9Vqh3iTqp1V3L/pfp5840mq91Y3N1kl9zMkd2437TvUeKhDzVz76vcx59E5rNi6gpVfWNml37H0XN/4BoRf5lu1dy9s3gyxGEQiMHYsDBjQev7ycrjjjs6XpaamhnXr1lFSUsK+fftYu3YtpaWlrFq1ir//+79nxYoVLY559dVXeeqpp9i/fz8nn3wyX/nKV1o8R/DSSy+xZcsWjjvuOCZPnsyzzz5LRUUFc+bMYc2aNYwaNYqZM2e2Wq61a9dSXl7evL1ixQpKSkrYvn07//Ef/8GZZ55JdXV1wvaKFSvYuHEjmzZt4r333uPTn/40U6YEy5a/+OKLvPLKK50aztqaTAaIF4DRZjaKIDBcRVAbaGZm44HFwHR3/1Nc+kDgI3c/aGZDgMnAokwVdOH5CwuuKSVaFm03iLRn3qp5LF6/mA8PfUgsFiNGrNW8T7zxBGX/XMYDn3+g258rEm/v3iA4QPBz7962A0RXff7zn6ekpCT8zL3Mnj2b7du3Y2YcOnQo5TEXX3wxffr0oU+fPnziE5/g3XffZfjwxK7SiRMnNqeVl5dTXV1Nv379OPHEE5v/SM+cOZMlS1LXxFM1MVVXVzNixAjOPPPM5rT47WeeeYaZM2dSUlLCsccey9lnn80LL7zA0UcfzcSJE9MSHCCDAcLdG8zsa8BKgmGu97r7FjO7FVjv7o8QNCn1A34ZfsttGs56KrDYzGIEzWC3x49+kvRIDoxVO6u4/rHr2fhu6q98NftrmHzvZO655B4Nq5UO6cg3/aoqOO88qK+H3r1h2TKIZuA7yFFHHdX8/h/+4R8455xzeOihh6iurmbq1Kkpj+nTp0/z+5KSEhoaWq4Q25E83S1vqu2OHtcdGW3MdvfH3f3P3f3P3P37Ydo/hsEBdz/f3Y919/LwNSNMX+fup7v7uPDnzzJZTglEy6K8dN1LrLt2HeXHlqfM4zhzHp1D1c4MNRZLjxONwpNPwm23BT8zERyS7d27l+OPPx6A++67L+3nP/nkk3njjTeorq4GaO4jSJezzjqL5cuX09jYSG1tLWvWrGHixIlp/QzIk05qyS9NgWLxJYtbzTP7odlZLJEUu2gUFizITnAAmDt3LgsWLGD8+PFp+8Yf74gjjuBf/uVfmD59OhMmTKB///4MaKXdrKkPoun14IMPtnv+z33uc4wdO5Zx48Zx7rnnsmjRIoYNG5buyyieNam786CctK5qZxWzH5rN9ve3t9g37cRp6riWFvSgXOCDDz6gX79+uDtf/epXGT16NN/85jdzWqZ8elBOikC0LMrrN7zO1adf3WLfE288wbxV83JQKpH896//+q+Ul5dz2mmnsXfvXubMmZPrInWaahDSYaPvHM2O93e0SF937TqNbJJmqkHkL9UgJGPu/9z9GC0n/Lr+setzUBoRyTQFCOmwaFmUey65p0X6xnc36mlrkSKkACGdUjmhkrmT57ZIv+O5LjzaKiJ5TQFCOm3h+Qs5aeBJCWnb3tumZyNEiowChHTJ+See3yJt/qr5OSiJSKLuTPcNwYR3rc3Wet999zF06NCE5xa2bi3eSR60JrV0yaxxs1i8YXHChH9r3lpD1c4qjWiSnGpvuu/2rF69mn79+rW65sOVV17JT3/601aPT55mu6PTbqdjeu50Uw1CuiRaFuXbk7/dIl21COmKqp1V/HDtDzPWTLlhwwbOPvtsJkyYwIUXXsg777wDwJ133smYMWMYO3YsV111FdXV1dxzzz38+Mc/pry8nLVr13bo/MnTbCdvHzhwgC996UucfvrpjB8/nqeeegoIaiQzZszg3HPP5bzzzsvItXdHfoUrKSgLz1/IA1seoHpPdXPa2rfWqhYhzb7xm2+wcVfb833vPbiXze9ubl7Gd+yxYxnQp/XpXMuHlXPH9I4PinB3vv71r/Pwww8zdOhQli9fzne+8x3uvfdebr/9dv74xz/Sp08f9uzZwzHHHMN1113XZq1j+fLlPPPMM83bVeEiFvHTbK9evTph+0c/+hFmxssvv8yrr77KtGnTeP3115uP27x5M4MGDerwNWWLahDSLQs+syBh23EWPZuxmdmlCO09sJeYB/N9xzzG3gN703r+gwcP8sorr3DBBRdQXl7O9773PWpqagAYO3YsV199NUuXLu1w886VV17Jxo0bm19HHHEEQItptuO3n3nmGa655hoATjnlFEaMGNEcIC644IK8DA6gGoR0U+WESu5ef3fCt8SHX3tYtQgB6NA3/aqdVZx3/3nUN9bTu6Q3yy5bltZ/O+7Oaaed1vxNP95jjz3GmjVr+PWvf833v/99Xn755S5/Tj5Mz51uqkFIt515/JkJ245z/6b7c1QaKTTRsihPznqS2865jSdnPZn2LxZ9+vShtra2OUAcOnSILVu2EIvF2LlzJ+eccw4LFy5k7969fPDBB/Tv35/9+/entQxnnXUWy5YtA+D111/nrbfe4uSTT07rZ2SCAoR026xxs1pMwbHrg105Ko0UomhZlAVnLchIrTMSifDggw8yb948xo0bR3l5OevWraOxsZFrrrmmueP4hhtu4JhjjuGzn/0sDz30UKud1MuXL08Y5trakNh4119/PbFYjNNPP50rr7yS++67L2GhoXylyfokLc6+72zWvLmmeXvKCVN4+ktP57BEkiuarC9/abI+yYkxQ8YkbD/z1jN6slqkwClASFrMGjeLSNw/pxgxjWYSKXAKEJIW0bIonxnxmYS0ptFM0vMUS9N1MenKPVGAkLRJbmbSaKaeqW/fvtTV1SlI5BF3p66ujr59+3bqOD0HIWkza9wslry4pPmhJ+mZhg8fTk1NDbW1tbkuisTp27cvw4cP79QxChCSNtGyKDdNuimh7+HovkfnsESSC7169Up4olgKl5qYJK2O6XNMwvaP1v1I/RAiBUoBQtJq6siplFhJ83ajN6ofQqRAKUBIWkXLokw+YXJCmp6qFilMChCSdoP6Js5Mufvj3TkqiYh0hwKEpN2wfsMStvVUtUhhUoCQtEv1VLX6IUQKjwKEpF20LMqMU2bkuhgi0k0KEJIRF510UcK2nocQKTwKEJIRdR/VJawRoechRAqPAoRkxNSRU4nY4X9eeh5CpPAoQEhG6HkIkcKnACEZkzy7q4gUFgUIyZhZ42YlTLvx2PbH1A8hUkAUICRjomVRLhl9SfP2odgh9UOIFJCMBggzm25mr5nZDjObn2L/t8xsq5ltNrMnzWxE3L7ZZrY9fM3OZDklc8wsYVv9ECKFI2MBwsxKgLuAi4AxwEwzS26UfgmocPexwIPAovDYQcDNwBnAROBmMxuYqbJK5iRPuyEihSOTNYiJwA53f8Pd64FfAJfGZ3D3p9z9o3DzOaBpuaMLgd+6+253fx/4LTA9g2WVDFE/hEjhymSAOB7YGbddE6a15svA/3TmWDOrNLP1ZrZeyxvmp2hZNOGpavVDiBSOvOikNrNrgArgnzpznLsvcfcKd68YOnRoZgon3VYaSVzZVv0QIoUhkwHibaAsbnt4mJbAzM4HvgPMcPeDnTlWCkNyP4T6JUQKQyYDxAvAaDMbZWa9gauAR+IzmNl4YDFBcPhT3K6VwDQzGxh2Tk8L06QAjf/k+IRtTdwnUhgyFiDcvQH4GsEf9m3AA+6+xcxuNbOmuaD/CegH/NLMNprZI+Gxu4HbCILMC8CtYZoUIE3cJ1KYStvP0nXu/jjweFLaP8a9P7+NY+8F7s1c6SRbmibua/RG4PDEfdGyaI5LJiJtyYtOailu0bIonz35s7kuhoh0kgKEZIUWEBIpPAoQkhV1H9UlbP+46sfqhxDJcwoQkhVTR05NeB6iIdbA6urVuSuQiLRLAUKyIloW5VvRbzVvO87gIwfnsEQi0h4FCMmafQf2JWy/9M5LOSqJiHSEAoTkjKbcEMlvChCSNZrZVaSwKEBI1kTLolz4Zxc2b2tmV5H8pgAhWXXCgBNyXQQR6SAFCMmq5In7krdFJH8oQEhWxT8wZ1iLB+hEJH8oQEhWxT/74Dh7Du7JYWlEpC0KEJJVmvpbpHAoQEhWNU393aRp6m8RyT8KEJJVmvpbpHAoQEjWaepvkcKgACFZp6m/RQqDAoRknab+FikMChCSdZr6W6QwKEBIThzT55jm93pgTiQ/KUBITuiBOZH8pwAhOaGOapH8pwAhOaGOapH8pwAhOaGOapH8pwAhOaM1qkXymwKE5A2tUS2SXxQgJGe0RrVIflOAkJzRGtUi+U0BQnJKa1SL5C8FCMkprVEtkr8UICSnkkcuaSSTSP5QgJC8opFMIvlDAUJyata4WQlPVGskk0j+UICQnIqWRblk9CXN2xrJJJI/FCAk54b1G5brIohICu0GCDOLmNmkrpzczKab2WtmtsPM5qfYP8XMXjSzBjO7Imlfo5ltDF+PdOXzpTAkj1zSGtUi+aHdAOHuMeCuzp7YzErC4y4CxgAzzWxMUra3gC8CP09xio/dvTx8zejs50vh0NTfIvmpo01MT5rZ5WZmnTj3RGCHu7/h7vXAL4BL4zO4e7W7bwZinTivFBlN/S2SnzoaIOYAvwTqzWyfme03s33tHHM8sDNuuyZM66i+ZrbezJ4zs79KlcHMKsM862traztxasknmvpbJD+Vtp8F3L1/pguSwgh3f9vMTgR+Z2Yvu/sfksq1BFgCUFFR4Tkoo6SJpv4WyT8dChAAZjYDmBJurnb3R9s55G2gLG57eJjWIe7+dvjzDTNbDYwH/tDmQVI09MCcSO51qInJzG4HbgS2hq8bzeyH7Rz2AjDazEaZWW/gKqBDo5HMbKCZ9QnfDwEmh58rRUpTf4vkn472QfwlcIG73+vu9wLTgYvbOsDdG4CvASuBbcAD7r7FzG4NayOY2afNrAb4PLDYzLaEh58KrDezTcBTwO3urgBRxPTAnEj+6XATE3AMsDt8P6AjB7j748DjSWn/GPf+BYKmp+Tj1gGnd6JsUgSSB8mpmUkktzoaIH4AvGRmTwFG0BfR4sE3ke5IfqJaT1iL5Fa7AcLMIgTPKZwJfDpMnufu+nonaaUnqkXyS0efpJ7r7u+4+yPhS8FB0q7uozqMw81MP1r3I3VUi+RQRzupV5nZTWZWZmaDml4ZLZn0OFNHTiVih/9JNnqjOqpFcqijAeJK4KvAGmBD+FqfqUJJzxQti/LZkz+b62KISKhDs7kC8919VNLrxCyUT3qYi066KGFb/RAiudPRPohvZ6EsIuqHEMkj6oOQvKJ+CJH8oT4IySvqhxDJHx0KECn6H9QHIRmjfgiR/NBmgDCzuXHvP5+07weZKpT0bOqHEMkP7dUgrop7vyBp3/Q0l0UEUD+ESL5oL0BYK+9TbYukRbQsyuQTJiekaeI+kexrL0B4K+9TbYukzZghY3JdBJEer70AMa5pDWpgbPi+aVvTcUvGzBo3i9LI4bkktYCQSPa1OZuru5e0tb9YzJsHd90FBw6AGUQiEIuBe7ANh9/n2z6A3r3hpJPg6KOhthaGDoUxY2D8eKirg6lTIRrNyq8ybaJlUS4efTEPv/YwcHgBoWhZgV2ISAHrzIJBRemb34Q77sh1Kbqnvh42bjy8vW0brFmTmKekJAgskUjqoAMwZAjccgtUVman3O0xtICQSC519EG5ovVIh1bJLnyNjUFQaGgIAkpDQ5DW0HD4tWsXzJkTBJPSUjjySDj7bKjKk5ad3R/vbj+TiKRNjw8QV1yR6xLkn1gsCB4ffxzURCZNgj59sh8skleUe+atZ9QPIZJFPT5ALFwIc+fCUUcd/ubcu3fws2k7/n0+7SvNYgNhff3hYFFenp1AMWvcLCJx/0RjxPQ8hEgW9fgAAUGQ+OCDoJnl0CE4eDD42bQd/z6f9h06BOvWwV/9FYwYAcOGHX4NGhQ0EfXu3TLIJG+XdHIowqZNQaCYNy8z96NJtCzKjFNmZPZDRKRVChAFLhqFhx6C6mp4553Dr7o6+PDDIMAkB5nk7YaGINBMmQL9+gVBwzrwGOSiRTB6dGZrE5qXSSR3FCAECALN00/D/v1B0IjFYPHioDYSaeNfyY4dQW1iyZLMlEvzMonkjgKEtKqyMqiNNDYGwaJ//9bzzpmTmZqE5mUSyR0FCOmQykrYtw+uvrr1PLNnp/9zU83LtLV2a/o/SERaUICQTlm6NOivGD685b7t2+GMM9L/mcnzMmm4q0h2KEBIp0WjsHMnTJvWct/vf5/+IKHhriK5oQAhXbZyZetB4ppr0vc50bIonxnxmYQ0TbshknkKENItK1fCxIkt05ctS+/IpkF9ByVsa9oNkcxTgJBue/751H0S112XvpFNmnZDJPsUICQtHnigZZp78DBdOqgfQiT7FCAkLaLRYE6rZL/6VXpqEan6ITTcVSSzFCAkbRYuDKbrSDZ/fnrOr+GuItmlACFpdfvtLdPWrElPLULNTCLZpQAhaRWNpq5FXH99Gs6tZiaRrFKAkLRLVYvYuDE9w17VzCSSPQoQknatdVj/4AfdP7eamUSyJ6MBwsymm9lrZrbDzFp0VZrZFDN70cwazOyKpH2zzWx7+MrANHCSSQsXwrhxiWlvvtn9WkSqZqbnap7r3klFJKWMBQgzKwHuAi4CxgAzzWxMUra3gC8CP086dhBwM3AGMBG42cwGZqqskhl3390yLR21iORmpo3vbmTJhgwtSCHSg2WyBjER2OHub7h7PfAL4NL4DO5e7e6bgVjSsRcCv3X33e7+PvBbYHoGyyoZkKrDOh21iFnjZrVI+9mLP+veSUWkhUwGiOOBnXHbNWFa2o41s0ozW29m62tra7tcUMmcVB3Wd9zRvXNGy6KUDytPSKuP1XfvpCLSQkF3Urv7EnevcPeKoUOH5ro4kkI0CuWJf8t59dXuPxdx5vFnJmxv2rVJo5lE0iyTAeJtoCxue3iYluljJc+cmfi3PC1zNM0aNythrWrHmb8qTY9siwiQ2QDxAjDazEaZWW/gKuCRDh67EphmZgPDzulpYZoUoFmzwCwx7eGHu1eLiJZFOXXoqQlpa95ao1qESBplLEC4ewPwNYI/7NuAB9x9i5ndamYzAMzs02ZWA3weWGxmW8JjdwO3EQSZF4BbwzQpQNEoXHppYpo73N/NxxduPOPGFmmqRYikj7l7rsuQFhUVFb5+/fpcF0NaUVUFkycHgaFJeTm89FL3zjvqJ6Oo3lOdkLbu2nVEy6LdO7FID2FmG9y9ItW+gu6klsKRqhaRjuk3FnxmQYs01SJE0kMBQrIm1fQb3R3yWjmhkpHHjExIU1+ESHooQEjWZGrIa6paxKJn07SUnUgPpgAhWZWJIa+VEypbrFmt+ZlEuk8BQrIqE0NeAc4cnhh5dn24i3mr5nXvpCI9nAKEZFWmhrzOndSyg2PRs4vUFyHSDQoQknVz57asRTzXzRahaFmUKSNaLmV3/WNpWMpOpIdSgJCsy9SQ19vPazkzoKYCF+k6BQjJiUysOBctizJ3cssT3/zUzd07sUgPpQAhOZGptSIWnr+wxYgmdViLdI0ChORMqrUifpaGdX9umXpLizR1WIt0ngKE5EyqB+fef7/7562cUMm4Y8e1SJ/9kJY2F+kMBQjJqeQH57Zv734zE8DdF7dcEHv7+9u58D8v7P7JRXoIBQjJqVktl5fudmc1tN5h/cQbT6g/QqSDFCAkpzLVWQ1Bh/W0E6e1SF/07CINfRXpAAUIyblUndXpqEUArPzCSgb1HdQifc6jcxQkRNqhACE5l8laBMAPz/9hyvQ5j87RyCaRNihASF7IZC2ickIlV59+dcp9f/3Lv07Ph4gUIQUIyQvRKIwcmZj25pvdn+W1ydLLlqYMEjX7ayj75zLVJERSUICQvLGg5bo/XJ/GufaWXrY0Zad1zf4aJt07SaObRJIoQEjeqKxsWYtIxyR+8VZ+YSUTj5uYct+iZxcpSIjEUYCQvJKqFpGuvogmz//d820GifJ7ytXkJIIChOSZykoYlzRLxptvwrw0f7F//u+eT9ncBLDp3U1qchJBAWUY5cwAAA3vSURBVELy0N0tZ8lg0aL0dVg3WfmFla2OboKgNnHk949UoJAeSwFC8k6q5yIgCBLp1tropiYfN3zMomcXMeauMen/cJE8pwAheSnVcxFr1mTms5ZetpTFlyymf+/+rebZ9t42IrdEGLhwoGoU0mMoQEheSlWL2L0bLszQZKyVEyrZt2Bfm7UJx9lzYA+Lnl1E79t6c/Z9Z6szW4qaAoTkrVS1iCeeSO+w12RLL1vKumvXMeWEKZRQ0mq+Q7FDrHlzDZPunUSv23ox6iejNLeTFB1z91yXIS0qKip8/fr1uS6GpNk118CyZYlpw4bBO+9k5/PH3DWGbe9t63D+CBFKS0qJeYyIRTh1yKncffHdRMuiGSylSNeZ2QZ3r0i1TzUIyWtLl8JJJyWm7doVBI5s2PrVrcydPJejeh3VofwxYtQ31tMQa6C+sb55yGyv23rp+QopOKpBSN6rqoJJk1qmr1sX9FVkrRw7q5i/aj7P1TxHfay+y+cpoQTHiUSC72fujpkRsQhH9jqSygmVLDx/YbqKLdKmtmoQChBSEMrLYdOmxLTRo+H113NTniUblnDzUzdT+2EtjTSm/fyGURIpSQgeMY81bwOURkoZ+4mx7K/fz8cNH1M+rJy5k+aqOUs6RQFCCl5rtYhp02DlyuyXJ17Vziquf+x6ttRuCWoGFqGhsYEYsZyUp4QSsCDIpAos7o5F2thnRt/Svnyy3yfZf3A/+w7uoz5Wj7tTEimhX+9+zXmbmt7+ZuzfdLrWU7WzitXVq5k6cipA8/t0BLglG5awYusKLh9zOZUTKrt9vmKmACFFYd681A/LXX110FeRb5ZsWMIP1v6Amn01NHr6axn5xjB6RXrR6I0tgk7EIpjZ4YCEtVrzKrESDGtRe4p5rEWzXFO+pqCHQ4M3tChXiQUj0pzWa2SxWKxFOZM/r1ekF4ZR31gPwWFt1vLSsi8Way53qn0YDDlyCLdMvaVLwVABQorGhRcGQ12TzZ0LC/O42b4pWNR+WNv8bTz5f/jGWCNOcfz/KLmx+JLFnQ4SChBSVEaPhh07WqYvXhxM9lfIrvnva3hw64PNNY7WvmUqmEgq006cxsovdK7Nta0AUZqWUolk0f33p+6PmDMn+FnIQWLpZUtZelnH2svmrZrHss3LGHrUUHDYsXsH9bH6hEDSlaYNd2/RfxIJR8Tnql9FOubyMZen9XwZrUGY2XTgJ0AJ8G/ufnvS/j7A/cAEoA640t2rzWwksA14Lcz6nLtf19ZnqQbRs7TWHwH539xUCJZsWMIdz92BmXHjGTc2N1tU7azi/k33s+uDXez+eDcHGg4AwRTpjd7YqYAUscOPYTX1O6Srbb93SW8GHzGYuo/qqI/Vp6VPAEjoSyqxEsxa9pWkvQ+inX1QgH0QZlYCvA5cANQALwAz3X1rXJ7rgbHufp2ZXQV8zt2vDAPEo+7+qY5+ngJEz5PqKesm+dpxLYUtfuRVsQwnzlUT00Rgh7u/ERbiF8ClwNa4PJcC3w3fPwj81JrCpEg7mgJAqiCxbBnU1uZ+CKwUl2hZtGgCQ0dkcqqN44Gdcds1YVrKPO7eAOwFBof7RpnZS2b2tJmdleoDzKzSzNab2fra2tr0ll4KwtKlQW0hlSeegE98Iv0LDYn0FPk6F9M7wAnuPh74FvBzMzs6OZO7L3H3CnevGDp0aNYLKfmhrSBRWxt0aGdr7iaRYpLJAPE2UBa3PTxMS5nHzEqBAUCdux909zoAd98A/AH48wyWVQrc0qVB53Rrli2Dvn3Tv7a1SDHLZIB4ARhtZqPMrDdwFfBIUp5HgNnh+yuA37m7m9nQsJMbMzsRGA28kcGyShFYuDCYwK+1yuTBg8HIpwEDMrumhEixyFiACPsUvgasJBiy+oC7bzGzW81sRpjtZ8BgM9tB0JQ0P0yfAmw2s40EndfXufvuTJVVikc0Cn/6E0yc2HqeffuCZyZUoxBpm56klqK1ZAncdBPs3992vkgEjj46eMBOz09IT6MFg6RHqqwMagutdWA3icVgz56g+SkSgSOOUKe2CChASA+wdGnQN1Fe3n5edzhwIOjUjkSgVy8FDOm5FCCkR4hG4aWXDgeKjjyO6Q4NDYcDhhmUlgZBo08fGDhQfRhS3BQgpEdpChSxWDAs9qiOLTXdrLExCBr19YnNUr16JQYP1TykGChASI+1cCF88EFQq5gyBfr161jNIllTTSM+eLRV84gPJsmB5cgjYdQoDcOV/KBRTCJJ5s0L1pb4+GM4dCgIALlgFrwikeAViwVlsebZTg/vT/e+Xr2C50X+/M9hzBgYPx7q6mDq1KAWJsVDCwaJdENTwPjww8N/QBsbcxc4cq0kWL2zU0EnEgm2m/ZFIsHP+H3uwf5wlc/m99kOjoW2D2DIELjllq6thaIAIZIB11wDDz4YBAtI/B83lzUP6bm6sqqinoMQyYClS4N+hkOHgldDQ/Dz4MHDneADBgR9DCUlwc/evRO34983fTMX6aoVK9J7PgUIkQxZuDAY6ZQcPOK34983NATfAEeMCDqrOxpY0r1PgapwXZ7eFUe1JrVIPqmszI81tauqYP582Lw5aEJrGqHVJN/a4XvyPuheH0RbFCBEpIVoFJ5+OtelkFxTE5OIiKSkACEiIikpQIiISEoKECIikpIChIiIpKQAISIiKRXNVBtmVgu82Y1TDAHeS1NxCkVPu+aedr2ga+4punPNI9x9aKodRRMgusvM1rc2H0mx6mnX3NOuF3TNPUWmrllNTCIikpIChIiIpKQAcVhPXMOrp11zT7te0DX3FBm5ZvVBiIhISqpBiIhISgoQIiKSUo8PEGY23cxeM7MdZjY/1+VJFzMrM7OnzGyrmW0xsxvD9EFm9lsz2x7+HBimm5ndGf4eNpvZX+T2CrrGzErM7CUzezTcHmVmz4fXtdzMeofpfcLtHeH+kbksd3eY2TFm9qCZvWpm28wsWsz32cy+Gf6bfsXM/svM+hbjfTaze83sT2b2Slxap++rmc0O8283s9mdKUOPDhBmVgLcBVwEjAFmmtmY3JYqbRqA/+PuY4Azga+G1zYfeNLdRwNPhtsQ/A5Gh69K4O7sFzktbgS2xW0vBH7s7icB7wNfDtO/DLwfpv84zFeofgL8xt1PAcYRXH9R3mczOx64Aahw908BJcBVFOd9vg+YnpTWqftqZoOAm4EzgInAzU1BpUPcvce+gCiwMm57AbAg1+XK0LU+DFwAvAZ8Mkz7JPBa+H4xMDMuf3O+QnkBw8P/ac4FHgWM4OnS0uT7DawEouH70jCf5foaunDNA4A/Jpe9WO8zcDywExgU3rdHgQuL9T4DI4FXunpfgZnA4rj0hHztvXp0DYLD/9ia1IRpRSWsVo8HngeOdfd3wl27gGPD98Xwu7gDmAuECzEyGNjj7k2LZcZfU/P1hvv3hvkLzSigFvj3sGnt38zsKIr0Prv728D/Bd4C3iG4bxso/vvcpLP3tVv3u6cHiKJnZv2AFcA33H1f/D4PvlIUxThnM7sE+JO7b8h1WbKsFPgL4G53Hw98yOFmB6Do7vNA4FKCwHgccBQtm2F6hGzc154eIN4GyuK2h4dpRcHMehEEh2Xu/t9h8rtm9slw/yeBP4Xphf67mAzMMLNq4BcEzUw/AY4xs6a11+Ovqfl6w/0DgLpsFjhNaoAad38+3H6QIGAU630+H/iju9e6+yHgvwnufbHf5yadva/dut89PUC8AIwOR0D0JujseiTHZUoLMzPgZ8A2d//nuF2PAE0jGWYT9E00pc8KR0OcCeyNq8rmPXdf4O7D3X0kwX38nbtfDTwFXBFmS77ept/DFWH+gvuW7e67gJ1mdnKYdB6wlSK9zwRNS2ea2ZHhv/Gm6y3q+xyns/d1JTDNzAaGta9pYVrH5LoTJtcv4C+B14E/AN/JdXnSeF2fIah+bgY2hq+/JGh/fRLYDqwCBoX5jWBE1x+AlwlGieT8Orp47VOBR8P3JwK/B3YAvwT6hOl9w+0d4f4Tc13ublxvObA+vNe/AgYW830GbgFeBV4B/hPoU4z3Gfgvgn6WQwQ1xS935b4C14bXvwP4UmfKoKk2REQkpZ7exCQiIq1QgBARkZQUIEREJCUFCBERSUkBQkREUlKAEGmHmTWa2ca4V9pm/TWzkfGzdYrkk9L2s4j0eB+7e3muCyGSbapBiHSRmVWb2SIze9nMfm9mJ4XpI83sd+G8/E+a2Qlh+rFm9pCZbQpfk8JTlZjZv4ZrHDxhZkeE+W+wYD2PzWb2ixxdpvRgChAi7TsiqYnpyrh9e939dOCnBLPJAvw/4D/cfSywDLgzTL8TeNrdxxHMl7QlTB8N3OXupwF7gMvD9PnA+PA812Xq4kRaoyepRdphZh+4e78U6dXAue7+Rjgx4i53H2xm7xHM2X8oTH/H3YeYWS0w3N0Pxp1jJPBbDxaAwczmAb3c/Xtm9hvgA4LpM37l7h9k+FJFEqgGIdI93sr7zjgY976Rw32DFxPMr/MXwAtxs5WKZIUChEj3XBn3syp8v45gRlmAq4G14fsnga9A89rZA1o7qZlFgDJ3fwqYRzBNdYtajEgm6RuJSPuOMLONcdu/cfemoa4DzWwzQS1gZpj2dYIV3r5NsNrbl8L0G4ElZvZlgprCVwhm60ylBFgaBhED7nT3PWm7IpEOUB+ESBeFfRAV7v5erssikglqYhIRkZRUgxARkZRUgxARkZQUIEREJCUFCBERSUkBQkREUlKAEBGRlP4/2vtIwpLGLxQAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.05527534588923148\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<topologyNN.HiddenLayer at 0x7f9b7d7d3400>,\n",
              "  <topologyNN.OutputLayer at 0x7f9b7d823f10>],\n",
              " 0.9722222222222222,\n",
              " 0.05527534588923148,\n",
              " 0.03460435677125245,\n",
              " 0.9344262295081968)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EnjWGFSqncWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple training trials with no lambda"
      ],
      "metadata": {
        "id": "j3pV4HgfpHvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_space_def = search_space_dict(layers_range=[1], units_range=[4], eta_0_range=[0.6],\n",
        "                        alpha_range=[0.4], lamb_range=[0.0], lamb0_range = [0.0], minibatch_size_range = [40], num_targets=1, configurations = 0)    \n",
        "\n",
        "search_space_def[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KkxQJ0xnccP",
        "outputId": "ffd8975c-0628-4ed7-8111-873ca4b3bdb8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'units': array([4, 1]),\n",
              " 'layers': 2,\n",
              " 'eta_0': 0.6,\n",
              " 'alpha': 0.4,\n",
              " 'lamb': 0.0,\n",
              " 'lamb0': 0.0,\n",
              " 'minibatch_size': 40}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "error_vals = []\n",
        "acc_vals = []\n",
        "\n",
        "error_trains = []\n",
        "acc_trains = []\n",
        "\n",
        "models = []\n",
        "\n",
        "for i in range(10):\n",
        "  model, acc_val, error_val, error_train, acc_train = train_test(hyperparams = search_space_def[0][0],\n",
        "           num_inputs = 17, seed = i, activation_output = sigmoid, activation_hidden = sigmoid,\n",
        "           task = 'binary_classification', thr = 0.5, stop_class = 'GL', stop_param = 3, data_train = X_train, data_val = X_test)\n",
        "  \n",
        "  models += [model]\n",
        "\n",
        "  acc_vals += [acc_val]\n",
        "  error_vals += [error_val]\n",
        "  \n",
        "\n",
        "  error_trains += [error_train]\n",
        "  acc_trains += [acc_train]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rZgtj3KAT_Qh",
        "outputId": "b2b15137-a5f9-4a5a-bbe5-0c198f210d1e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error 0.12507337778440739, test error 0.2500475257473136\n",
            "training error 0.12500840940681623, test error 0.2500989273097838\n",
            "training error 0.12508773259888215, test error 0.25021244022177846\n",
            "training error 0.12499000278952807, test error 0.25015325078366984\n",
            "training error 0.12500161967797813, test error 0.250271541416718\n",
            "training error 0.1251157116445189, test error 0.25042595878472906\n",
            "training error 0.12515185921499167, test error 0.2504229510988416\n",
            "training error 0.12510046288684717, test error 0.25023720304338765\n",
            "training error 0.1250237716157477, test error 0.25035706618413917\n",
            "training error 0.12513628079504593, test error 0.2505481746443148\n",
            "training error 0.12499630025682516, test error 0.2504316272910247\n",
            "training error 0.12497167880930386, test error 0.2503190639652848\n",
            "training error 0.12497596759070778, test error 0.25029773532908567\n",
            "training error 0.12503725294933066, test error 0.25019424013704794\n",
            "training error 0.12500928808304185, test error 0.2503194885741708\n",
            "training error 0.12500605931948944, test error 0.25028549862926064\n",
            "training error 0.12518263954762435, test error 0.250344637740906\n",
            "training error 0.12503157827415962, test error 0.25026562595411783\n",
            "training error 0.12499291648208016, test error 0.2502854115750195\n",
            "training error 0.12498627695813998, test error 0.2502983906988961\n",
            "training error 0.12507473121747897, test error 0.2503357505112673\n",
            "training error 0.12510742770911218, test error 0.25038528834987095\n",
            "training error 0.12501229731777203, test error 0.2504295731917679\n",
            "training error 0.12500853439927573, test error 0.2504596836730458\n",
            "training error 0.12508097858960948, test error 0.25038400613590106\n",
            "training error 0.12498923670684425, test error 0.25052541136446344\n",
            "training error 0.12529034842838402, test error 0.2506367880165876\n",
            "training error 0.12496680730197796, test error 0.2505354587937612\n",
            "training error 0.12497331399867488, test error 0.25045393525921994\n",
            "training error 0.1251172215394416, test error 0.250387997744413\n",
            "training error 0.12512810071745692, test error 0.2505808164206069\n",
            "training error 0.12498219890766676, test error 0.250552254991908\n",
            "training error 0.12506258698070297, test error 0.2506054929740343\n",
            "training error 0.12498104525389532, test error 0.25046594692026536\n",
            "training error 0.12499725984223282, test error 0.25043269273153074\n",
            "training error 0.1249656570229919, test error 0.25047990168085377\n",
            "training error 0.1250307767471862, test error 0.2504836724766756\n",
            "training error 0.12498740596532186, test error 0.2504834483454252\n",
            "training error 0.12512742829629503, test error 0.2504123404333131\n",
            "training error 0.12499447337844213, test error 0.25054377791308596\n",
            "training error 0.12498637127324376, test error 0.2505136579974306\n",
            "training error 0.12500326600242023, test error 0.2504548091440913\n",
            "training error 0.12496677024374492, test error 0.25047736162242357\n",
            "training error 0.1249590403189008, test error 0.2504818897691864\n",
            "training error 0.12499938953327926, test error 0.25037363983935906\n",
            "training error 0.12518543191954576, test error 0.25054349053171115\n",
            "training error 0.12521734714222837, test error 0.2504518679181029\n",
            "training error 0.12496253828180101, test error 0.25048861648759835\n",
            "training error 0.1250503922048949, test error 0.25046206992574227\n",
            "training error 0.12494375645479133, test error 0.25051042708135535\n",
            "Loss: 0.18512534073604758\n",
            "training error 0.1253289069383119, test error 0.25039280328691654\n",
            "Loss: 0.13808476551449012\n",
            "training error 0.12527149482600494, test error 0.2505371520140595\n",
            "Loss: 0.1958132820081282\n",
            "training error 0.12497417879529632, test error 0.250553396538825\n",
            "Loss: 0.20230985689602132\n",
            "training error 0.12500607303741784, test error 0.2505249986418698\n",
            "Loss: 0.1909528571135244\n",
            "training error 0.12498226593751473, test error 0.25045624643716713\n",
            "Loss: 0.16345720223864824\n",
            "training error 0.12498559755522237, test error 0.25056526993232686\n",
            "Loss: 0.20705831160132693\n",
            "training error 0.12498406009196857, test error 0.25051296492427255\n",
            "Loss: 0.18614028495900214\n",
            "training error 0.12501919696421113, test error 0.25049008777740117\n",
            "Loss: 0.17699116548539617\n",
            "training error 0.12507108370352446, test error 0.2505118960057049\n",
            "Loss: 0.18571279879833202\n",
            "training error 0.12509032270440432, test error 0.2506951056908217\n",
            "Loss: 0.25898274400943944\n",
            "training error 0.12498462957914278, test error 0.2505020211739864\n",
            "Loss: 0.1817636168622183\n",
            "training error 0.12507725207291798, test error 0.25053324524151444\n",
            "Loss: 0.19425087000928265\n",
            "training error 0.12499287504298669, test error 0.25068042615610153\n",
            "Loss: 0.25311204615858784\n",
            "training error 0.12507424489476435, test error 0.25052133061479626\n",
            "Loss: 0.18948592515226625\n",
            "training error 0.12500523244065298, test error 0.2506591950133413\n",
            "Loss: 0.2446212031891104\n",
            "training error 0.12498897374667145, test error 0.25072369530456345\n",
            "Loss: 0.2704164159309297\n",
            "training error 0.12501753116318676, test error 0.25059979029857016\n",
            "Loss: 0.22086383362762696\n",
            "training error 0.12504567234431221, test error 0.25082026576821986\n",
            "Loss: 0.30903725945570226\n",
            "training error 0.12495359804021933, test error 0.2507607781501161\n",
            "Loss: 0.28524673486403085\n",
            "training error 0.125136174454691, test error 0.2506657747379978\n",
            "Loss: 0.24725259281668244\n",
            "training error 0.12512472974039907, test error 0.25068214254905835\n",
            "Loss: 0.2537984728495424\n",
            "training error 0.12493820232772446, test error 0.25064207219860113\n",
            "Loss: 0.23777337908488239\n",
            "training error 0.12495999213648389, test error 0.25064226874440987\n",
            "Loss: 0.2378519824656422\n",
            "training error 0.12504967338630887, test error 0.25071118282949384\n",
            "Loss: 0.265412377185803\n",
            "training error 0.12495169908682738, test error 0.25066348691438745\n",
            "Loss: 0.2463376373083248\n",
            "training error 0.12493576000168413, test error 0.2506159509741007\n",
            "Loss: 0.227326875196332\n",
            "training error 0.1249449654077522, test error 0.2506280226555694\n",
            "Loss: 0.2321546300131816\n",
            "training error 0.12495888934861586, test error 0.25065003538588526\n",
            "Loss: 0.24095804858335068\n",
            "training error 0.1249604247834005, test error 0.2507730882913283\n",
            "Loss: 0.2901698554489851\n",
            "training error 0.12498657060514017, test error 0.25078272570146637\n",
            "Loss: 0.2940240868033017\n",
            "training error 0.12502825222037578, test error 0.2509937294398395\n",
            "Loss: 0.37840954022561046\n",
            "training error 0.124976300278808, test error 0.2507806304220632\n",
            "Loss: 0.2931861343393072\n",
            "training error 0.12494116479143821, test error 0.2508447193925833\n",
            "Loss: 0.31881685007164595\n",
            "training error 0.12495182777543459, test error 0.25087542799982754\n",
            "Loss: 0.3310979582939799\n",
            "training error 0.12493581809496242, test error 0.2507859981216674\n",
            "Loss: 0.2953328060922633\n",
            "training error 0.12498612676142823, test error 0.2507667022090321\n",
            "Loss: 0.2876159080435148\n",
            "training error 0.12493812673202732, test error 0.25077602289532325\n",
            "Loss: 0.29134347393857674\n",
            "training error 0.12501857905993263, test error 0.25081796141949914\n",
            "Loss: 0.30811569515953874\n",
            "training error 0.12492333841782935, test error 0.2507257061953405\n",
            "Loss: 0.2712206193602906\n",
            "training error 0.1249201128480984, test error 0.2507438486134121\n",
            "Loss: 0.27847620727996514\n",
            "training error 0.12512533824886954, test error 0.2508037708467369\n",
            "Loss: 0.30244054491765926\n",
            "training error 0.12505237016100362, test error 0.2506478901583707\n",
            "Loss: 0.2401001206721709\n",
            "training error 0.12497297682479183, test error 0.2508003676860988\n",
            "Loss: 0.30107953939364496\n",
            "training error 0.12492337222328642, test error 0.25071833440097735\n",
            "Loss: 0.2682724620685306\n",
            "training error 0.12508636743681642, test error 0.2506831297956991\n",
            "Loss: 0.2541932964487126\n",
            "training error 0.12491947728146133, test error 0.2506679290936065\n",
            "Loss: 0.24811417127152957\n",
            "training error 0.12510208233692813, test error 0.25052937222104293\n",
            "Loss: 0.19270195627381081\n",
            "training error 0.1250198870406126, test error 0.2507702207600127\n",
            "Loss: 0.2890230609317923\n",
            "training error 0.12490555378550366, test error 0.2507577814442542\n",
            "Loss: 0.2840482803490474\n",
            "training error 0.1250216700852709, test error 0.25082131147857173\n",
            "Loss: 0.309455464094488\n",
            "training error 0.1249314742211119, test error 0.25087227569898474\n",
            "Loss: 0.32983727761601855\n",
            "training error 0.12491206067266157, test error 0.2509729629156426\n",
            "Loss: 0.37010450935803796\n",
            "training error 0.12494013266312885, test error 0.2510045435715578\n",
            "Loss: 0.3827343707497155\n",
            "training error 0.1249344511786176, test error 0.25088458787126666\n",
            "Loss: 0.3347612104744213\n",
            "training error 0.12499254287121075, test error 0.25094035276476057\n",
            "Loss: 0.3570629282487747\n",
            "training error 0.12494448885761344, test error 0.2508858018798515\n",
            "Loss: 0.3352467216112487\n",
            "training error 0.12493345093627556, test error 0.2507913586572543\n",
            "Loss: 0.29747661278296533\n",
            "training error 0.12491966476580779, test error 0.25086336331541753\n",
            "Loss: 0.32627300176861773\n",
            "training error 0.12490285579624753, test error 0.2509051441366459\n",
            "Loss: 0.3429821538002198\n",
            "training error 0.12491250177267002, test error 0.25098090955317365\n",
            "Loss: 0.3732825602134948\n",
            "training error 0.12494457989180263, test error 0.25097940687149606\n",
            "Loss: 0.37268160178645093\n",
            "training error 0.12498200345878163, test error 0.250860708003583\n",
            "Loss: 0.3252110789095264\n",
            "training error 0.12490908663910755, test error 0.2508656541680288\n",
            "Loss: 0.3271891686470685\n",
            "training error 0.12488139622392282, test error 0.25080204785078214\n",
            "Loss: 0.3017514775295327\n",
            "training error 0.12488388892190698, test error 0.2508160092380503\n",
            "Loss: 0.30733497099799667\n",
            "training error 0.12501489369681465, test error 0.25070924824257695\n",
            "Loss: 0.26463868949939684\n",
            "training error 0.12487981098555724, test error 0.2507329094760212\n",
            "Loss: 0.27410138399059925\n",
            "training error 0.12498130114531117, test error 0.25058039188368353\n",
            "Loss: 0.21310594247128112\n",
            "training error 0.1248712448929123, test error 0.25062961837287595\n",
            "Loss: 0.23279279561860822\n",
            "training error 0.12490295610951363, test error 0.2505894176159703\n",
            "Loss: 0.21671554918896163\n",
            "training error 0.12492782373958686, test error 0.2506763607990037\n",
            "Loss: 0.2514862123953021\n",
            "training error 0.1249108846753903, test error 0.2507479251101141\n",
            "Loss: 0.28010649603800175\n",
            "training error 0.12492483952657338, test error 0.2506836907911503\n",
            "Loss: 0.2544176519785335\n",
            "training error 0.1249365371931165, test error 0.25070249142973167\n",
            "Loss: 0.2619364780597655\n",
            "training error 0.12485188245440736, test error 0.2507105419508903\n",
            "Loss: 0.265156074468309\n",
            "training error 0.12484835387711073, test error 0.25071578900977937\n",
            "Loss: 0.26725449910713905\n",
            "training error 0.12487214635066696, test error 0.2506490956021842\n",
            "Loss: 0.24058220655160945\n",
            "training error 0.12494555131861476, test error 0.2505994640862163\n",
            "Loss: 0.22073337348695876\n",
            "training error 0.12497460524456808, test error 0.2506146200694873\n",
            "Loss: 0.22679461453531946\n",
            "training error 0.1249045482988188, test error 0.25076244536152065\n",
            "Loss: 0.28591349267319544\n",
            "training error 0.12486427727241767, test error 0.25073855090151664\n",
            "Loss: 0.27635752528956115\n",
            "training error 0.12482920268662764, test error 0.2507656286541717\n",
            "Loss: 0.287186567718245\n",
            "training error 0.12483864933762932, test error 0.2507775634417716\n",
            "Loss: 0.29195957539518425\n",
            "training error 0.12483129544789859, test error 0.25068553295228985\n",
            "Loss: 0.2551543763808395\n",
            "training error 0.12486324731932238, test error 0.25072918074074163\n",
            "Loss: 0.27261017336235316\n",
            "training error 0.1248178866845932, test error 0.25075534869325206\n",
            "Loss: 0.2830753649023299\n",
            "training error 0.12487432678660701, test error 0.2508445683937938\n",
            "Loss: 0.31875646203580743\n",
            "training error 0.12486457772856585, test error 0.2508268922054904\n",
            "Loss: 0.31168733057747033\n",
            "training error 0.12485793931963086, test error 0.25071339362516887\n",
            "Loss: 0.26629652737621967\n",
            "training error 0.12496032774496384, test error 0.25073926695003423\n",
            "Loss: 0.27664389025776615\n",
            "training error 0.12491013938670101, test error 0.2509387555337803\n",
            "Loss: 0.35642415728893706\n",
            "training error 0.12487329642769004, test error 0.25100544489591964\n",
            "Loss: 0.3830948319696992\n",
            "training error 0.12496144090729275, test error 0.2510953510863213\n",
            "Loss: 0.4190504728555533\n",
            "training error 0.12485427643940562, test error 0.2510060202566734\n",
            "Loss: 0.38332493252839495\n",
            "training error 0.1250600735562635, test error 0.2508770251570992\n",
            "Loss: 0.3317366997759752\n",
            "training error 0.12476675322517675, test error 0.2508326552242911\n",
            "Loss: 0.31399209995419675\n",
            "training error 0.12480482647500411, test error 0.25082979393819\n",
            "Loss: 0.31284780304801085\n",
            "training error 0.12475609103376299, test error 0.2507306475197946\n",
            "Loss: 0.27319677346913096\n",
            "training error 0.12474061209014976, test error 0.25075415649008254\n",
            "Loss: 0.28259857427384016\n",
            "training error 0.12484265566786255, test error 0.25067085726163685\n",
            "Loss: 0.24928521586458086\n",
            "training error 0.12472920424914272, test error 0.2505579007108513\n",
            "Loss: 0.2041111832690179\n",
            "training error 0.12475950703645013, test error 0.2505107155702755\n",
            "Loss: 0.18524071437122647\n",
            "training error 0.12471295117240452, test error 0.25050286828279267\n",
            "Loss: 0.182102395981798\n",
            "training error 0.12471653115871753, test error 0.250447119974532\n",
            "Loss: 0.15980731103983903\n",
            "training error 0.12472249519560175, test error 0.250362595412724\n",
            "Loss: 0.12600391244375064\n",
            "training error 0.12470803500377384, test error 0.25044631130599987\n",
            "Loss: 0.15948390510740484\n",
            "training error 0.1246961258093359, test error 0.250433744218576\n",
            "Loss: 0.15445802557259025\n",
            "training error 0.12466607385104585, test error 0.25041961901212445\n",
            "Loss: 0.14880901688540948\n",
            "training error 0.1247430428184283, test error 0.2503576961935737\n",
            "Loss: 0.1240445972553017\n",
            "training error 0.1246426442691775, test error 0.25038535212206714\n",
            "Loss: 0.1351048660625187\n",
            "training error 0.12466872228306589, test error 0.2504131976429796\n",
            "Loss: 0.14624095742323817\n",
            "training error 0.12467316092474108, test error 0.2504647614666246\n",
            "Loss: 0.1668625666516821\n",
            "training error 0.12463014401955828, test error 0.2503391886561382\n",
            "Loss: 0.11664298934888073\n",
            "training error 0.12463649420202805, test error 0.25041103098504425\n",
            "Loss: 0.14537445897304657\n",
            "training error 0.12461046543482042, test error 0.2504007410726116\n",
            "Loss: 0.1412592763085252\n",
            "training error 0.1250231225175048, test error 0.2504818450814374\n",
            "Loss: 0.17369471376522672\n",
            "training error 0.12455095413537019, test error 0.25016399861983946\n",
            "Loss: 0.0465802939572324\n",
            "training error 0.12462191185127391, test error 0.2502367334213064\n",
            "Loss: 0.07566868475397115\n",
            "training error 0.12450083565717732, test error 0.2501713229611758\n",
            "Loss: 0.04950947364594782\n",
            "training error 0.12469040180867383, test error 0.250039887015524\n",
            "Loss: 0.0\n",
            "training error 0.12476521292851465, test error 0.25031234206553815\n",
            "Loss: 0.1089646349093254\n",
            "training error 0.1244278679624439, test error 0.2501785136742003\n",
            "Loss: 0.055441817835921725\n",
            "training error 0.12489547928858369, test error 0.25004187526082927\n",
            "Loss: 0.0007951712540865685\n",
            "training error 0.12437819928120263, test error 0.2501880506804807\n",
            "Loss: 0.05925601180083451\n",
            "training error 0.12436716750291231, test error 0.2500815122658354\n",
            "Loss: 0.016647444057138472\n",
            "training error 0.12440613529785882, test error 0.24991890365808783\n",
            "Loss: 0.0\n",
            "training error 0.12439247975045181, test error 0.2498181584395529\n",
            "Loss: 0.0\n",
            "training error 0.12426189050629334, test error 0.24983682612069322\n",
            "Loss: 0.007472507705963416\n",
            "training error 0.12430337579471117, test error 0.24985025798359434\n",
            "Loss: 0.012849163664463426\n",
            "training error 0.12436540228997049, test error 0.24972653303801448\n",
            "Loss: 0.0\n",
            "training error 0.12418719672068233, test error 0.24982014283596465\n",
            "Loss: 0.037484922731834125\n",
            "training error 0.12434211922797203, test error 0.2496040421744146\n",
            "Loss: 0.0\n",
            "training error 0.1241641296137648, test error 0.24951998295513247\n",
            "Loss: 0.0\n",
            "training error 0.12408358570245516, test error 0.24944378527054564\n",
            "Loss: 0.0\n",
            "training error 0.12409146748557186, test error 0.24928803989246065\n",
            "Loss: 0.0\n",
            "training error 0.1240036131751926, test error 0.24923387601374702\n",
            "Loss: 0.0\n",
            "training error 0.1239378935048486, test error 0.24917102272708852\n",
            "Loss: 0.0\n",
            "training error 0.12396850008125745, test error 0.24913930501510173\n",
            "Loss: 0.0\n",
            "training error 0.12385015719001304, test error 0.24893058380316888\n",
            "Loss: 0.0\n",
            "training error 0.12389687117113729, test error 0.24889090648719017\n",
            "Loss: 0.0\n",
            "training error 0.12373650217183466, test error 0.24884222936217754\n",
            "Loss: 0.0\n",
            "training error 0.1236746423127997, test error 0.24882304981811296\n",
            "Loss: 0.0\n",
            "training error 0.12371622988359848, test error 0.24873960224010658\n",
            "Loss: 0.0\n",
            "training error 0.12367456138192001, test error 0.248593939525056\n",
            "Loss: 0.0\n",
            "training error 0.12355195822990538, test error 0.24861163995899646\n",
            "Loss: 0.007120219412537487\n",
            "training error 0.12341258294571117, test error 0.24836115798172612\n",
            "Loss: 0.0\n",
            "training error 0.12338718167183482, test error 0.24819643880427983\n",
            "Loss: 0.0\n",
            "training error 0.12331479670810452, test error 0.2481591534553259\n",
            "Loss: 0.0\n",
            "training error 0.12330390265781163, test error 0.24792702456587754\n",
            "Loss: 0.0\n",
            "training error 0.12319941846756272, test error 0.24766918262964835\n",
            "Loss: 0.0\n",
            "training error 0.1231219363780139, test error 0.24742896332552558\n",
            "Loss: 0.0\n",
            "training error 0.12290527578693924, test error 0.24740129119961987\n",
            "Loss: 0.0\n",
            "training error 0.1228166604941055, test error 0.24716986129229607\n",
            "Loss: 0.0\n",
            "training error 0.1228861215558192, test error 0.24694767126604353\n",
            "Loss: 0.0\n",
            "training error 0.12276646726827117, test error 0.24663044045968308\n",
            "Loss: 0.0\n",
            "training error 0.1225083103562161, test error 0.2464541049797153\n",
            "Loss: 0.0\n",
            "training error 0.12239879955001845, test error 0.24628791659716764\n",
            "Loss: 0.0\n",
            "training error 0.12233593433198826, test error 0.24611741346992652\n",
            "Loss: 0.0\n",
            "training error 0.12215405321913356, test error 0.2457097707905765\n",
            "Loss: 0.0\n",
            "training error 0.12198107780059034, test error 0.24540839495379418\n",
            "Loss: 0.0\n",
            "training error 0.1218453748530547, test error 0.2450467839349071\n",
            "Loss: 0.0\n",
            "training error 0.12180969627154944, test error 0.24476150275008682\n",
            "Loss: 0.0\n",
            "training error 0.12162313345983713, test error 0.24433696417249967\n",
            "Loss: 0.0\n",
            "training error 0.12148322227585985, test error 0.2439491569162861\n",
            "Loss: 0.0\n",
            "training error 0.12122429688593665, test error 0.24364577313512564\n",
            "Loss: 0.0\n",
            "training error 0.12108227522597065, test error 0.24331300072546155\n",
            "Loss: 0.0\n",
            "training error 0.120867558311298, test error 0.24309677026664703\n",
            "Loss: 0.0\n",
            "training error 0.12066548516107357, test error 0.24273695068889123\n",
            "Loss: 0.0\n",
            "training error 0.12043077726547087, test error 0.24225897708010782\n",
            "Loss: 0.0\n",
            "training error 0.12047236571636358, test error 0.2418501447169631\n",
            "Loss: 0.0\n",
            "training error 0.11999182104778462, test error 0.2413445154099018\n",
            "Loss: 0.0\n",
            "training error 0.11984688193464453, test error 0.2407350700633\n",
            "Loss: 0.0\n",
            "training error 0.11952892684116742, test error 0.24021633370233875\n",
            "Loss: 0.0\n",
            "training error 0.11921670174464498, test error 0.239796418492241\n",
            "Loss: 0.0\n",
            "training error 0.11894574815099072, test error 0.23919701682910388\n",
            "Loss: 0.0\n",
            "training error 0.11862954110246854, test error 0.23855036962878565\n",
            "Loss: 0.0\n",
            "training error 0.11840425932941846, test error 0.23792899960945768\n",
            "Loss: 0.0\n",
            "training error 0.11802844716829129, test error 0.2373588892374953\n",
            "Loss: 0.0\n",
            "training error 0.11774235267302707, test error 0.2366090332789933\n",
            "Loss: 0.0\n",
            "training error 0.117390520750708, test error 0.2358471607560094\n",
            "Loss: 0.0\n",
            "training error 0.11703605918653331, test error 0.2350671217049368\n",
            "Loss: 0.0\n",
            "training error 0.11679800499211798, test error 0.2342374923302846\n",
            "Loss: 0.0\n",
            "training error 0.11626798806198907, test error 0.23358673440594682\n",
            "Loss: 0.0\n",
            "training error 0.11588035326744563, test error 0.23273725822873992\n",
            "Loss: 0.0\n",
            "training error 0.1155260045253323, test error 0.23185860697014454\n",
            "Loss: 0.0\n",
            "training error 0.1150544389343512, test error 0.2310427301675485\n",
            "Loss: 0.0\n",
            "training error 0.11476986346506894, test error 0.2301585974321099\n",
            "Loss: 0.0\n",
            "training error 0.11416500621026152, test error 0.22912979816006251\n",
            "Loss: 0.0\n",
            "training error 0.11371074387936433, test error 0.22818784569660683\n",
            "Loss: 0.0\n",
            "training error 0.1132405497611408, test error 0.2272379526626098\n",
            "Loss: 0.0\n",
            "training error 0.11275580595533548, test error 0.22623996633545668\n",
            "Loss: 0.0\n",
            "training error 0.1122097751283783, test error 0.2251842577560387\n",
            "Loss: 0.0\n",
            "training error 0.11175850230510025, test error 0.22406603034445927\n",
            "Loss: 0.0\n",
            "training error 0.11116588487314821, test error 0.2228785173513802\n",
            "Loss: 0.0\n",
            "training error 0.11056154463469638, test error 0.22172412164619937\n",
            "Loss: 0.0\n",
            "training error 0.11002611610254646, test error 0.2202901560616326\n",
            "Loss: 0.0\n",
            "training error 0.10939722507101339, test error 0.21899132484555067\n",
            "Loss: 0.0\n",
            "training error 0.1087068842080084, test error 0.2177776091563214\n",
            "Loss: 0.0\n",
            "training error 0.10807636573436771, test error 0.21645614038551403\n",
            "Loss: 0.0\n",
            "training error 0.10742171423347505, test error 0.21509063242642648\n",
            "Loss: 0.0\n",
            "training error 0.10674016157903461, test error 0.21366593213775098\n",
            "Loss: 0.0\n",
            "training error 0.10610909989160479, test error 0.2122234780429756\n",
            "Loss: 0.0\n",
            "training error 0.10543668907679778, test error 0.21067233906650085\n",
            "Loss: 0.0\n",
            "training error 0.1046066692537862, test error 0.20898204967483916\n",
            "Loss: 0.0\n",
            "training error 0.10385994334749696, test error 0.20735440193272686\n",
            "Loss: 0.0\n",
            "training error 0.10319551803035348, test error 0.20562079997837518\n",
            "Loss: 0.0\n",
            "training error 0.10233475623833078, test error 0.20412530104430268\n",
            "Loss: 0.0\n",
            "training error 0.10159046495743197, test error 0.20249196578565065\n",
            "Loss: 0.0\n",
            "training error 0.10074154393576361, test error 0.20079383596190356\n",
            "Loss: 0.0\n",
            "training error 0.0999724406301108, test error 0.199301380923065\n",
            "Loss: 0.0\n",
            "training error 0.09910795229384398, test error 0.19753203468301617\n",
            "Loss: 0.0\n",
            "training error 0.0982244018068899, test error 0.1957485029115665\n",
            "Loss: 0.0\n",
            "training error 0.09740138257815958, test error 0.19387894548714643\n",
            "Loss: 0.0\n",
            "training error 0.09652014681278766, test error 0.19204324138298254\n",
            "Loss: 0.0\n",
            "training error 0.09566981472191634, test error 0.18999620710650986\n",
            "Loss: 0.0\n",
            "training error 0.09470886605334429, test error 0.1882076758165855\n",
            "Loss: 0.0\n",
            "training error 0.0938531582228833, test error 0.18628909291934748\n",
            "Loss: 0.0\n",
            "training error 0.09295117027910892, test error 0.18432068542477759\n",
            "Loss: 0.0\n",
            "training error 0.09205556705978267, test error 0.18234098748406746\n",
            "Loss: 0.0\n",
            "training error 0.09117793458907072, test error 0.18045882974510782\n",
            "Loss: 0.0\n",
            "training error 0.09018582680152788, test error 0.1783994484156403\n",
            "Loss: 0.0\n",
            "training error 0.08924755218492184, test error 0.17641659150844424\n",
            "Loss: 0.0\n",
            "training error 0.08828232663649715, test error 0.17447390929909223\n",
            "Loss: 0.0\n",
            "training error 0.08738361893235946, test error 0.17239690108593003\n",
            "Loss: 0.0\n",
            "training error 0.08643245695412337, test error 0.17031678364584582\n",
            "Loss: 0.0\n",
            "training error 0.08548352979441352, test error 0.16826015283470874\n",
            "Loss: 0.0\n",
            "training error 0.08459385614190981, test error 0.16632517980151101\n",
            "Loss: 0.0\n",
            "training error 0.08372571411032471, test error 0.16411746634584579\n",
            "Loss: 0.0\n",
            "training error 0.08278243972991031, test error 0.16234158127704065\n",
            "Loss: 0.0\n",
            "training error 0.08175653071930473, test error 0.1601469742828087\n",
            "Loss: 0.0\n",
            "training error 0.08104101946401969, test error 0.15800117578261427\n",
            "Loss: 0.0\n",
            "training error 0.07989840994010065, test error 0.15608158745119047\n",
            "Loss: 0.0\n",
            "training error 0.07891060295586134, test error 0.15400058703131556\n",
            "Loss: 0.0\n",
            "training error 0.07802306079820402, test error 0.1520809929919922\n",
            "Loss: 0.0\n",
            "training error 0.0771196012621408, test error 0.1500229865028485\n",
            "Loss: 0.0\n",
            "training error 0.07627172422219149, test error 0.14818180183264523\n",
            "Loss: 0.0\n",
            "training error 0.0753595970112742, test error 0.1462458184732761\n",
            "Loss: 0.0\n",
            "training error 0.07441280166062456, test error 0.14409859786446455\n",
            "Loss: 0.0\n",
            "training error 0.0736158253362903, test error 0.14205800156580456\n",
            "Loss: 0.0\n",
            "training error 0.07258399921084815, test error 0.14029123521474166\n",
            "Loss: 0.0\n",
            "training error 0.07176516518419267, test error 0.13846942409902233\n",
            "Loss: 0.0\n",
            "training error 0.07090939474203152, test error 0.13649954060164068\n",
            "Loss: 0.0\n",
            "training error 0.07001538788235762, test error 0.13476259517665987\n",
            "Loss: 0.0\n",
            "training error 0.06919523164572627, test error 0.13295029729321056\n",
            "Loss: 0.0\n",
            "training error 0.06837594435921156, test error 0.13113376282240183\n",
            "Loss: 0.0\n",
            "training error 0.06756007681499993, test error 0.12940440297849037\n",
            "Loss: 0.0\n",
            "training error 0.06676244091072389, test error 0.1275304433813716\n",
            "Loss: 0.0\n",
            "training error 0.06599926460693421, test error 0.12570200841620513\n",
            "Loss: 0.0\n",
            "training error 0.06520796148199338, test error 0.1242052061958957\n",
            "Loss: 0.0\n",
            "training error 0.06438301365907424, test error 0.12244728978545011\n",
            "Loss: 0.0\n",
            "training error 0.06364533368702587, test error 0.12065324974489679\n",
            "Loss: 0.0\n",
            "training error 0.06291186629144362, test error 0.11897487714701507\n",
            "Loss: 0.0\n",
            "training error 0.062173361756475184, test error 0.11742934862813666\n",
            "Loss: 0.0\n",
            "training error 0.06144074716138012, test error 0.11576854740454563\n",
            "Loss: 0.0\n",
            "training error 0.0607379313165831, test error 0.11406983929670443\n",
            "Loss: 0.0\n",
            "training error 0.06014648913406495, test error 0.11249726691004337\n",
            "Loss: 0.0\n",
            "training error 0.059409964533914734, test error 0.11117183762718759\n",
            "Loss: 0.0\n",
            "training error 0.0587020479862613, test error 0.1094977665966216\n",
            "Loss: 0.0\n",
            "training error 0.05805247448043404, test error 0.10795060848304479\n",
            "Loss: 0.0\n",
            "training error 0.0573570061899231, test error 0.10651464640004983\n",
            "Loss: 0.0\n",
            "training error 0.056823958413520136, test error 0.10506984621181725\n",
            "Loss: 0.0\n",
            "training error 0.05618814888231834, test error 0.10354264508556155\n",
            "Loss: 0.0\n",
            "training error 0.05552868030912956, test error 0.10208223738152403\n",
            "Loss: 0.0\n",
            "training error 0.054964612638125, test error 0.1007929283733137\n",
            "Loss: 0.0\n",
            "training error 0.054461369196569714, test error 0.09947523693886705\n",
            "Loss: 0.0\n",
            "training error 0.05374379152291561, test error 0.09809225275758858\n",
            "Loss: 0.0\n",
            "training error 0.05323817939150184, test error 0.09670049652768764\n",
            "Loss: 0.0\n",
            "training error 0.05265235730552517, test error 0.09548612419437268\n",
            "Loss: 0.0\n",
            "training error 0.05214948880747174, test error 0.09423315348967087\n",
            "Loss: 0.0\n",
            "training error 0.05155953751928587, test error 0.09314599397937942\n",
            "Loss: 0.0\n",
            "training error 0.05110045803209208, test error 0.09200132916921008\n",
            "Loss: 0.0\n",
            "training error 0.050780672288079774, test error 0.09051137893711504\n",
            "Loss: 0.0\n",
            "training error 0.050057488883450144, test error 0.08954796300372093\n",
            "Loss: 0.0\n",
            "training error 0.049623936224780965, test error 0.08844365659756091\n",
            "Loss: 0.0\n",
            "training error 0.04921073428431792, test error 0.0875692353513087\n",
            "Loss: 0.0\n",
            "training error 0.048722579091666524, test error 0.08644438793911369\n",
            "Loss: 0.0\n",
            "training error 0.04824031411463338, test error 0.08549904370065649\n",
            "Loss: 0.0\n",
            "training error 0.04784684824953614, test error 0.08444435393379959\n",
            "Loss: 0.0\n",
            "training error 0.04737735242644401, test error 0.08336958478726637\n",
            "Loss: 0.0\n",
            "training error 0.0469806056740928, test error 0.08245085333399445\n",
            "Loss: 0.0\n",
            "training error 0.04657527818153876, test error 0.08151076445242636\n",
            "Loss: 0.0\n",
            "training error 0.04622793509149487, test error 0.08050032526063042\n",
            "Loss: 0.0\n",
            "training error 0.045793932373519704, test error 0.07975799251909009\n",
            "Loss: 0.0\n",
            "training error 0.04540468005207785, test error 0.07883902732899296\n",
            "Loss: 0.0\n",
            "training error 0.04504999783310542, test error 0.07811763501379874\n",
            "Loss: 0.0\n",
            "training error 0.044785560735543205, test error 0.0771200579508917\n",
            "Loss: 0.0\n",
            "training error 0.04440623953013369, test error 0.07636531299717828\n",
            "Loss: 0.0\n",
            "training error 0.044022778978557435, test error 0.0756105348459577\n",
            "Loss: 0.0\n",
            "training error 0.043675513347855535, test error 0.07479982481786698\n",
            "Loss: 0.0\n",
            "training error 0.043415416488340415, test error 0.0740661616374455\n",
            "Loss: 0.0\n",
            "training error 0.043071057093931324, test error 0.07338955886488556\n",
            "Loss: 0.0\n",
            "training error 0.04275513145368416, test error 0.07264855561326256\n",
            "Loss: 0.0\n",
            "training error 0.042545900789061716, test error 0.07204806417021671\n",
            "Loss: 0.0\n",
            "training error 0.04212217301001877, test error 0.07123958699122393\n",
            "Loss: 0.0\n",
            "training error 0.0418722599619817, test error 0.07062021003516358\n",
            "Loss: 0.0\n",
            "training error 0.041570528948410505, test error 0.06994533937743556\n",
            "Loss: 0.0\n",
            "training error 0.041339940249675, test error 0.06939332824478903\n",
            "Loss: 0.0\n",
            "training error 0.04106117339593594, test error 0.06851520264017455\n",
            "Loss: 0.0\n",
            "training error 0.04075862817941899, test error 0.06800663316074007\n",
            "Loss: 0.0\n",
            "training error 0.040516477534250854, test error 0.06740888873291408\n",
            "Loss: 0.0\n",
            "training error 0.04029597559717877, test error 0.06668551825615852\n",
            "Loss: 0.0\n",
            "training error 0.040045732986488676, test error 0.06595763847477538\n",
            "Loss: 0.0\n",
            "training error 0.03977166870533661, test error 0.0654357471783341\n",
            "Loss: 0.0\n",
            "training error 0.03957926169533095, test error 0.06487795956831366\n",
            "Loss: 0.0\n",
            "training error 0.03943021482472363, test error 0.0645753442311209\n",
            "Loss: 0.0\n",
            "training error 0.03910640340613674, test error 0.06388281375405302\n",
            "Loss: 0.0\n",
            "training error 0.03886098349519978, test error 0.06343560810489106\n",
            "Loss: 0.0\n",
            "training error 0.03866686530045117, test error 0.06301649286712709\n",
            "Loss: 0.0\n",
            "training error 0.03847676781335987, test error 0.06254016254655279\n",
            "Loss: 0.0\n",
            "training error 0.03827003938237983, test error 0.06207828149344505\n",
            "Loss: 0.0\n",
            "training error 0.038080761446969244, test error 0.061539835313258844\n",
            "Loss: 0.0\n",
            "training error 0.03788029526797655, test error 0.06118610108651671\n",
            "Loss: 0.0\n",
            "training error 0.03769847635802037, test error 0.06078402138455829\n",
            "Loss: 0.0\n",
            "training error 0.03752780579398393, test error 0.0604736998743008\n",
            "Loss: 0.0\n",
            "training error 0.037362422010342115, test error 0.05992555541988518\n",
            "Loss: 0.0\n",
            "training error 0.03717912418566607, test error 0.05946267590093104\n",
            "Loss: 0.0\n",
            "training error 0.03701996457438252, test error 0.0591410213898249\n",
            "Loss: 0.0\n",
            "training error 0.03680333358120149, test error 0.058777976194641805\n",
            "Loss: 0.0\n",
            "training error 0.03664717425741592, test error 0.058411737797890606\n",
            "Loss: 0.0\n",
            "training error 0.03651799693322643, test error 0.05813616611786125\n",
            "Loss: 0.0\n",
            "training error 0.036339738128986596, test error 0.05772711509249052\n",
            "Loss: 0.0\n",
            "training error 0.036191074984260115, test error 0.05734598453812883\n",
            "Loss: 0.0\n",
            "training error 0.03609943103683024, test error 0.056807710038896245\n",
            "Loss: 0.0\n",
            "training error 0.03587578059002265, test error 0.05663767876740854\n",
            "Loss: 0.0\n",
            "training error 0.035762536797002586, test error 0.056183664816182616\n",
            "Loss: 0.0\n",
            "training error 0.03557989332214573, test error 0.055987051117853584\n",
            "Loss: 0.0\n",
            "training error 0.03545004177758486, test error 0.05570426060167403\n",
            "Loss: 0.0\n",
            "training error 0.035304863150515516, test error 0.05541652115996761\n",
            "Loss: 0.0\n",
            "training error 0.035190985174776014, test error 0.05496138388204729\n",
            "Loss: 0.0\n",
            "training error 0.03504587355657497, test error 0.05461867208089127\n",
            "Loss: 0.0\n",
            "training error 0.034926876749198386, test error 0.05435663536638652\n",
            "Loss: 0.0\n",
            "training error 0.03481869727085114, test error 0.05403054245381978\n",
            "Loss: 0.0\n",
            "training error 0.03468100097747969, test error 0.053869575787154\n",
            "Loss: 0.0\n",
            "training error 0.03457042993443505, test error 0.053723039067738086\n",
            "Loss: 0.0\n",
            "training error 0.034471835429717906, test error 0.05305353568338442\n",
            "Loss: 0.0\n",
            "training error 0.034321478139185575, test error 0.05274131461240464\n",
            "Loss: 0.0\n",
            "training error 0.034260945019446314, test error 0.05262999409622416\n",
            "Loss: 0.0\n",
            "training error 0.03410106418280586, test error 0.052399101797281036\n",
            "Loss: 0.0\n",
            "training error 0.034026554107258036, test error 0.0523444612337837\n",
            "Loss: 0.0\n",
            "training error 0.03399546589247712, test error 0.05191903930416299\n",
            "Loss: 0.0\n",
            "training error 0.033764371667535854, test error 0.05188527929674824\n",
            "Loss: 0.0\n",
            "training error 0.0336767362157161, test error 0.05169717565848577\n",
            "Loss: 0.0\n",
            "training error 0.03358019558863701, test error 0.05149591755520422\n",
            "Loss: 0.0\n",
            "training error 0.03350901183991507, test error 0.0514183483083499\n",
            "Loss: 0.0\n",
            "training error 0.033355264731306566, test error 0.05109747629530211\n",
            "Loss: 0.0\n",
            "training error 0.03331024409493015, test error 0.05105075771209137\n",
            "Loss: 0.0\n",
            "training error 0.03320161815512919, test error 0.0508261687103482\n",
            "Loss: 0.0\n",
            "training error 0.03309066166196586, test error 0.050548353200142636\n",
            "Loss: 0.0\n",
            "training error 0.0330137744371633, test error 0.05047415926141594\n",
            "Loss: 0.0\n",
            "training error 0.032909287728108694, test error 0.05018999811650735\n",
            "Loss: 0.0\n",
            "training error 0.032804863002479326, test error 0.05010389464554194\n",
            "Loss: 0.0\n",
            "training error 0.03270464722715843, test error 0.04990491119657227\n",
            "Loss: 0.0\n",
            "training error 0.03264085424919246, test error 0.04976052315571599\n",
            "Loss: 0.0\n",
            "training error 0.03254243069042498, test error 0.04953676463062565\n",
            "Loss: 0.0\n",
            "training error 0.032457547271569034, test error 0.04937280081280259\n",
            "Loss: 0.0\n",
            "training error 0.03244617410974211, test error 0.0492556862524182\n",
            "Loss: 0.0\n",
            "training error 0.032319896178716535, test error 0.04899172018628106\n",
            "Loss: 0.0\n",
            "training error 0.03220981727789631, test error 0.048887456210337146\n",
            "Loss: 0.0\n",
            "training error 0.032158167358105685, test error 0.048652611746085185\n",
            "Loss: 0.0\n",
            "training error 0.03211243228275482, test error 0.04865722881596625\n",
            "Loss: 0.009489870564727276\n",
            "training error 0.03201020851473886, test error 0.04852451311646707\n",
            "Loss: 0.0\n",
            "training error 0.03190725089057783, test error 0.048499814641174116\n",
            "Loss: 0.0\n",
            "training error 0.031832395572618744, test error 0.04838069252271675\n",
            "Loss: 0.0\n",
            "training error 0.03177813030293006, test error 0.048391401161303194\n",
            "Loss: 0.022134115962524348\n",
            "training error 0.031705526017817294, test error 0.048102511600244875\n",
            "Loss: 0.0\n",
            "training error 0.031677498179745085, test error 0.04795154706986008\n",
            "Loss: 0.0\n",
            "training error 0.03163573584632213, test error 0.047819651792806055\n",
            "Loss: 0.0\n",
            "training error 0.03151327485606457, test error 0.047496871842768316\n",
            "Loss: 0.0\n",
            "training error 0.03146272690941071, test error 0.0474363875020908\n",
            "Loss: 0.0\n",
            "training error 0.031444171109296114, test error 0.04750572701376005\n",
            "Loss: 0.1461736766236621\n",
            "training error 0.031261759148214094, test error 0.047339899665021716\n",
            "Loss: 0.0\n",
            "training error 0.031188508369937366, test error 0.04723463225322813\n",
            "Loss: 0.0\n",
            "training error 0.031162998178178945, test error 0.047008555348839326\n",
            "Loss: 0.0\n",
            "training error 0.031131890436781946, test error 0.04724182564938105\n",
            "Loss: 0.4962294603837014\n",
            "training error 0.031037220023581256, test error 0.04707679374199573\n",
            "Loss: 0.1451616469598349\n",
            "training error 0.030939885455701557, test error 0.046887473636801576\n",
            "Loss: 0.0\n",
            "training error 0.030881254968765016, test error 0.046912642586203486\n",
            "Loss: 0.053679474387702264\n",
            "training error 0.030798684012708872, test error 0.04677913981117568\n",
            "Loss: 0.0\n",
            "training error 0.030766932944981125, test error 0.04661420182905893\n",
            "Loss: 0.0\n",
            "training error 0.0306975749224491, test error 0.04662888383705918\n",
            "Loss: 0.031496855945523095\n",
            "training error 0.030628994852836128, test error 0.046612693529729084\n",
            "Loss: 0.0\n",
            "training error 0.030600878151170564, test error 0.04621311210391041\n",
            "Loss: 0.0\n",
            "training error 0.0305134295399274, test error 0.04622329177781018\n",
            "Loss: 0.022027674476632342\n",
            "training error 0.03050604954861312, test error 0.04624177466205387\n",
            "Loss: 0.062022566407149426\n",
            "training error 0.030408938267753233, test error 0.04615650096053384\n",
            "Loss: 0.0\n",
            "training error 0.03035208908603943, test error 0.04593867189028877\n",
            "Loss: 0.0\n",
            "training error 0.030331417537024036, test error 0.046027569639878266\n",
            "Loss: 0.19351397402564974\n",
            "training error 0.03023603446185812, test error 0.04592192614730326\n",
            "Loss: 0.0\n",
            "training error 0.030198290964703816, test error 0.04588545077884452\n",
            "Loss: 0.0\n",
            "training error 0.030121650303612822, test error 0.04587614363678514\n",
            "Loss: 0.0\n",
            "training error 0.030086204447341218, test error 0.045729187298503136\n",
            "Loss: 0.0\n",
            "training error 0.03001005028706719, test error 0.04566889886420515\n",
            "Loss: 0.0\n",
            "training error 0.029959766423906613, test error 0.045636756555730955\n",
            "Loss: 0.0\n",
            "training error 0.029891852127631294, test error 0.04561659692408005\n",
            "Loss: 0.0\n",
            "training error 0.029847754303862828, test error 0.045613125216544405\n",
            "Loss: 0.0\n",
            "training error 0.029802282543408498, test error 0.045534957059065385\n",
            "Loss: 0.0\n",
            "training error 0.029746290319332545, test error 0.04546561090146516\n",
            "Loss: 0.0\n",
            "training error 0.029751200948428078, test error 0.04554045838155283\n",
            "Loss: 0.16462438006139557\n",
            "training error 0.029697297936526548, test error 0.04540849991105189\n",
            "Loss: 0.0\n",
            "training error 0.02960092954676162, test error 0.045493366979026596\n",
            "Loss: 0.18689687644593\n",
            "training error 0.02965561670756595, test error 0.04544265819643272\n",
            "Loss: 0.07522443033296522\n",
            "training error 0.02952668027058266, test error 0.04522688797828514\n",
            "Loss: 0.0\n",
            "training error 0.02947521044855043, test error 0.044976601319914815\n",
            "Loss: 0.0\n",
            "training error 0.029424345088639885, test error 0.04507170144014319\n",
            "Loss: 0.21144354494893047\n",
            "training error 0.029373000152577846, test error 0.04495124309610721\n",
            "Loss: 0.0\n",
            "training error 0.02934235345432971, test error 0.04486888536147387\n",
            "Loss: 0.0\n",
            "training error 0.029296198967163675, test error 0.044867098072008074\n",
            "Loss: 0.0\n",
            "training error 0.029245408180322653, test error 0.04486711952182215\n",
            "Loss: 4.780744689014682e-05\n",
            "training error 0.02919621243279949, test error 0.04474947842183888\n",
            "Loss: 0.0\n",
            "training error 0.02919059709990418, test error 0.04454857579852752\n",
            "Loss: 0.0\n",
            "training error 0.02910727924210098, test error 0.04464134077657864\n",
            "Loss: 0.20823331922137367\n",
            "training error 0.02906570394086344, test error 0.0447037565598591\n",
            "Loss: 0.3483405665613004\n",
            "training error 0.029051099746012825, test error 0.044797653165757\n",
            "Loss: 0.5591140968365327\n",
            "training error 0.029002372575682698, test error 0.044638257088560236\n",
            "Loss: 0.20131123930493544\n",
            "training error 0.02895773685019826, test error 0.04472000563501978\n",
            "Loss: 0.38481552646612016\n",
            "training error 0.028885408240113257, test error 0.0445672132765067\n",
            "Loss: 0.041836304853970496\n",
            "training error 0.02889018604579864, test error 0.0445315400941544\n",
            "Loss: 0.0\n",
            "training error 0.02887063154075201, test error 0.04445474365695239\n",
            "Loss: 0.0\n",
            "training error 0.028766929742150657, test error 0.04454791092892784\n",
            "Loss: 0.20957779600394577\n",
            "training error 0.028714630431066323, test error 0.044611176332706344\n",
            "Loss: 0.35189197571603437\n",
            "training error 0.028709551983329272, test error 0.04436777743193836\n",
            "Loss: 0.0\n",
            "training error 0.028680085757064187, test error 0.04440784138591885\n",
            "Loss: 0.09029966407929635\n",
            "training error 0.028642749034056607, test error 0.04418864108622456\n",
            "Loss: 0.0\n",
            "training error 0.028604502058395626, test error 0.044439521621927124\n",
            "Loss: 0.5677489271802338\n",
            "training error 0.02865843183176808, test error 0.044113152036054766\n",
            "Loss: 0.0\n",
            "training error 0.028493000964160423, test error 0.04434043412165616\n",
            "Loss: 0.5152252222095477\n",
            "training error 0.02849425253448344, test error 0.04439522168328501\n",
            "Loss: 0.6394230160649217\n",
            "training error 0.028429740166805717, test error 0.04426287507115375\n",
            "Loss: 0.33940679409307783\n",
            "training error 0.028386787022617332, test error 0.044082160952398156\n",
            "Loss: 0.0\n",
            "training error 0.028368293175436893, test error 0.044110799124951694\n",
            "Loss: 0.06496544619141176\n",
            "training error 0.02833580048840492, test error 0.04422449545873413\n",
            "Loss: 0.3228845938148872\n",
            "training error 0.02827009982402946, test error 0.044146873691278045\n",
            "Loss: 0.1468002871950258\n",
            "training error 0.028285925191193613, test error 0.04405073491443388\n",
            "Loss: 0.0\n",
            "training error 0.02819520695594317, test error 0.04418051794965603\n",
            "Loss: 0.2946217253225081\n",
            "training error 0.028144405558887694, test error 0.044149293628982914\n",
            "Loss: 0.22373909252701996\n",
            "training error 0.02812624709941021, test error 0.044239210376033906\n",
            "Loss: 0.4278599709315545\n",
            "training error 0.02809677712681468, test error 0.04422475849119427\n",
            "Loss: 0.39505260717767055\n",
            "training error 0.02808201045523065, test error 0.04413873084119107\n",
            "Loss: 0.1997604056507063\n",
            "training error 0.028037455878235792, test error 0.044058713489548026\n",
            "Loss: 0.018112240646273037\n",
            "training error 0.028026300797870763, test error 0.044110906589468674\n",
            "Loss: 0.1365963023129524\n",
            "training error 0.02794641390177374, test error 0.04388150246982521\n",
            "Loss: 0.0\n",
            "training error 0.02789279376062787, test error 0.043860742694873354\n",
            "Loss: 0.0\n",
            "training error 0.02795319970605142, test error 0.04394147560461801\n",
            "Loss: 0.1840664448075735\n",
            "training error 0.02787652637209973, test error 0.04376566800075211\n",
            "Loss: 0.0\n",
            "training error 0.027795528153466257, test error 0.043861476805937216\n",
            "Loss: 0.21891315627460273\n",
            "training error 0.027789843684919567, test error 0.04362651637965422\n",
            "Loss: 0.0\n",
            "training error 0.027777653518510876, test error 0.04357705712381678\n",
            "Loss: 0.0\n",
            "training error 0.02773520540666744, test error 0.043729145105183456\n",
            "Loss: 0.3490092984814064\n",
            "training error 0.027691002889951376, test error 0.04375636359009824\n",
            "Loss: 0.41146988373261806\n",
            "training error 0.027643624519638425, test error 0.043727834113498\n",
            "Loss: 0.3460008537355286\n",
            "training error 0.0276062805582053, test error 0.04378565073410338\n",
            "Loss: 0.4786775979247926\n",
            "training error 0.027583682495792832, test error 0.043835705502376296\n",
            "Loss: 0.593542555718285\n",
            "training error 0.027645254763447733, test error 0.043915936829818335\n",
            "Loss: 0.7776562447498137\n",
            "training error 0.027505845146318923, test error 0.04372490206872992\n",
            "Loss: 0.3392724398370017\n",
            "training error 0.02754325744389052, test error 0.04390870020414608\n",
            "Loss: 0.7610497408923056\n",
            "training error 0.027456950717118705, test error 0.04374528914110005\n",
            "Loss: 0.38605639845128437\n",
            "training error 0.02755588205741774, test error 0.043769842052836495\n",
            "Loss: 0.442400064951487\n",
            "training error 0.027388765515491683, test error 0.04371063458560488\n",
            "Loss: 0.306531626053963\n",
            "training error 0.027395819739976828, test error 0.04355394268948415\n",
            "Loss: 0.0\n",
            "training error 0.027347429417623883, test error 0.0437339470832514\n",
            "Loss: 0.41329069804445506\n",
            "training error 0.027319187535953166, test error 0.043598250528958396\n",
            "Loss: 0.10173094957244277\n",
            "training error 0.027318790099518323, test error 0.04385729735524865\n",
            "Loss: 0.6965033405293752\n",
            "training error 0.027291758393105693, test error 0.043606214419638904\n",
            "Loss: 0.12001606956095756\n",
            "training error 0.027222283181369773, test error 0.043471616005464835\n",
            "Loss: 0.0\n",
            "training error 0.027203889631326906, test error 0.04338426299508007\n",
            "Loss: 0.0\n",
            "training error 0.027220792265595373, test error 0.043603375941955624\n",
            "Loss: 0.5050516748444256\n",
            "training error 0.027226301987549897, test error 0.04337611600465244\n",
            "Loss: 0.0\n",
            "training error 0.02711800014463791, test error 0.04354778120332806\n",
            "Loss: 0.3957597279046654\n",
            "training error 0.027138545614923106, test error 0.043329340677171625\n",
            "Loss: 0.0\n",
            "training error 0.027078494319485977, test error 0.0432330596102531\n",
            "Loss: 0.0\n",
            "training error 0.027050375668326323, test error 0.04329933643211099\n",
            "Loss: 0.15330125245673898\n",
            "training error 0.02703066160827703, test error 0.04339976567844559\n",
            "Loss: 0.38559858981841444\n",
            "training error 0.02701499050211382, test error 0.043448394993381385\n",
            "Loss: 0.49808036967435854\n",
            "training error 0.02701105622012004, test error 0.042998007155760895\n",
            "Loss: 0.0\n",
            "training error 0.027037008967265273, test error 0.04326557659039625\n",
            "Loss: 0.6222833390070459\n",
            "training error 0.026945815119488622, test error 0.04309516476002077\n",
            "Loss: 0.22595838897350173\n",
            "training error 0.02689492563456127, test error 0.04330123020927865\n",
            "Loss: 0.7052025746665969\n",
            "training error 0.026868888513874895, test error 0.04316010339090074\n",
            "Loss: 0.37698546016946555\n",
            "training error 0.026845409276294568, test error 0.04296429809952294\n",
            "Loss: 0.0\n",
            "training error 0.026865266007123564, test error 0.04310456140004617\n",
            "Loss: 0.32646477826385745\n",
            "training error 0.026779249042986068, test error 0.04317893341889441\n",
            "Loss: 0.49956668412056704\n",
            "training error 0.0267573531024677, test error 0.04314510760127718\n",
            "Loss: 0.4208366242488326\n",
            "training error 0.026734929543709315, test error 0.04328882233164862\n",
            "Loss: 0.7553346533764937\n",
            "training error 0.026702835266165983, test error 0.04321142287627826\n",
            "Loss: 0.5751863470057739\n",
            "training error 0.02667930828639902, test error 0.043191904396479394\n",
            "Loss: 0.529756814435145\n",
            "training error 0.02667942903655902, test error 0.04326340111429228\n",
            "Loss: 0.6961664172343562\n",
            "training error 0.026666893677943712, test error 0.04355901380881413\n",
            "Loss: 1.384209065660924\n",
            "training error 0.026632644461882717, test error 0.04329861536453762\n",
            "Loss: 0.778128073313944\n",
            "training error 0.026567866601739158, test error 0.04330980581687429\n",
            "Loss: 0.8041740064064662\n",
            "training error 0.026585924860411718, test error 0.04332316836516374\n",
            "Loss: 0.8352755229691144\n",
            "training error 0.02664799834064917, test error 0.043682400280775996\n",
            "Loss: 1.6713927912650473\n",
            "training error 0.02661954784926202, test error 0.043066517959407365\n",
            "Loss: 0.23791814228557584\n",
            "training error 0.02659875714339902, test error 0.043519202156968854\n",
            "Loss: 1.2915468935638774\n",
            "training error 0.026506267755899923, test error 0.043217533938058154\n",
            "Loss: 0.5894099280956855\n",
            "training error 0.026461412905901125, test error 0.042980180928809356\n",
            "Loss: 0.03696750555455264\n",
            "training error 0.026474746039076135, test error 0.043038647007838944\n",
            "Loss: 0.17304811577225632\n",
            "training error 0.026447736774295714, test error 0.043223468395657494\n",
            "Loss: 0.6032224605047709\n",
            "training error 0.026387427038049863, test error 0.04308525291678263\n",
            "Loss: 0.2815240155431109\n",
            "training error 0.026337251017212154, test error 0.04310446193228678\n",
            "Loss: 0.32623326567364774\n",
            "training error 0.02633123191196846, test error 0.04311489928091049\n",
            "Loss: 0.350526339424162\n",
            "training error 0.02642002783469172, test error 0.042982837158608844\n",
            "Loss: 0.04314991727074613\n",
            "training error 0.02629887336899948, test error 0.0431623453068293\n",
            "Loss: 0.46095762311211264\n",
            "training error 0.02629173957278569, test error 0.04309267121286559\n",
            "Loss: 0.2987902026126088\n",
            "training error 0.026293123216834715, test error 0.04289348716673685\n",
            "Loss: 0.0\n",
            "training error 0.026382798358372265, test error 0.04276134670558909\n",
            "Loss: 0.0\n",
            "training error 0.026213872119597, test error 0.043208553318172103\n",
            "Loss: 1.045819757880917\n",
            "training error 0.02623066648960736, test error 0.043249396114126405\n",
            "Loss: 1.1413331107121616\n",
            "training error 0.026147961419566313, test error 0.043299516005993396\n",
            "Loss: 1.258541514395195\n",
            "training error 0.026237521007381737, test error 0.04314734261736456\n",
            "Loss: 0.9026748255452244\n",
            "training error 0.026112799401200062, test error 0.04329396377276915\n",
            "Loss: 1.2455572806139958\n",
            "training error 0.026135239970758773, test error 0.04331769055785683\n",
            "Loss: 1.3010438050470086\n",
            "training error 0.02607758834706026, test error 0.04327536251706419\n",
            "Loss: 1.2020571171766026\n",
            "training error 0.026138765682598188, test error 0.04349589517909302\n",
            "Loss: 1.717786108471464\n",
            "training error 0.02601230703341584, test error 0.04331801315272605\n",
            "Loss: 1.3017982126933392\n",
            "training error 0.026027074502886525, test error 0.043397709398810616\n",
            "Loss: 1.4881727126202726\n",
            "training error 0.02599555665921664, test error 0.04328714239552336\n",
            "Loss: 1.229605076646334\n",
            "training error 0.02597325532446128, test error 0.04301179102964134\n",
            "Loss: 0.5856792251575982\n",
            "training error 0.025979629470210815, test error 0.043151918559178916\n",
            "Loss: 0.9133759427150556\n",
            "training error 0.026093946857091384, test error 0.04291852745857596\n",
            "Loss: 0.3675767137762298\n",
            "training error 0.02591292119574009, test error 0.04319822219241646\n",
            "Loss: 1.021659794382157\n",
            "training error 0.025935525157909773, test error 0.04332768983710284\n",
            "Loss: 1.324427725377797\n",
            "training error 0.025884761171094308, test error 0.04341676632784706\n",
            "Loss: 1.5327384957506407\n",
            "training error 0.025850804311326143, test error 0.043275165945289226\n",
            "Loss: 1.2015974221714076\n",
            "training error 0.025924041964371983, test error 0.04312684222524354\n",
            "Loss: 0.8547334165382336\n",
            "training error 0.0258051539523635, test error 0.04340896632132\n",
            "Loss: 1.514497707917739\n",
            "training error 0.02578658320181582, test error 0.04333991545856694\n",
            "Loss: 1.3530180818702497\n",
            "training error 0.025777194535860765, test error 0.0434982586478601\n",
            "Loss: 1.7233132233758441\n",
            "training error 0.0257722749529746, test error 0.04346937001035043\n",
            "Loss: 1.6557553943192138\n",
            "training error 0.025754461306332646, test error 0.043264947955379225\n",
            "Loss: 1.1777020337021105\n",
            "training error 0.02570883484843075, test error 0.0432785157058844\n",
            "Loss: 1.209431040271025\n",
            "training error 0.025777198301664618, test error 0.043495208585204004\n",
            "Loss: 1.7161804670641878\n",
            "training error 0.025682443780667055, test error 0.04330268948953745\n",
            "Loss: 1.2659628979309057\n",
            "training error 0.02566203492955858, test error 0.04336618371095409\n",
            "Loss: 1.4144479815598077\n",
            "training error 0.025652037341953957, test error 0.04328744129890626\n",
            "Loss: 1.2303040803165466\n",
            "training error 0.02567772155783099, test error 0.04349476665861198\n",
            "Loss: 1.7151469949542753\n",
            "training error 0.025636494534217284, test error 0.04346392690154705\n",
            "Loss: 1.6430263546076018\n",
            "training error 0.025596004522257774, test error 0.04311043508987232\n",
            "Loss: 0.8163643364337725\n",
            "training error 0.02561689186559899, test error 0.042745769512131916\n",
            "Loss: 0.0\n",
            "training error 0.025583048201572114, test error 0.042909511595704185\n",
            "Loss: 0.38306032489552333\n",
            "training error 0.025533370410834956, test error 0.04288359326234366\n",
            "Loss: 0.3224266442849455\n",
            "training error 0.02557602329046002, test error 0.043063707854237965\n",
            "Loss: 0.7437890245859613\n",
            "training error 0.025591828784158922, test error 0.04277056422802372\n",
            "Loss: 0.05800507553097578\n",
            "training error 0.025508566818863983, test error 0.04288857064687005\n",
            "Loss: 0.3340708013166216\n",
            "training error 0.025461457694181483, test error 0.04296949053506327\n",
            "Loss: 0.5233758228819774\n",
            "training error 0.025442509573563178, test error 0.04305919987128439\n",
            "Loss: 0.7332429915983152\n",
            "training error 0.025447354328431624, test error 0.04326204909912249\n",
            "Loss: 1.2077910700474037\n",
            "training error 0.0254239922411837, test error 0.043068584188226185\n",
            "Loss: 0.7551967826959993\n",
            "training error 0.025453982096525107, test error 0.042791016199275916\n",
            "Loss: 0.10585067869970377\n",
            "training error 0.025391961277989202, test error 0.042787543968171596\n",
            "Loss: 0.09772769683751292\n",
            "training error 0.02541633226280628, test error 0.04274306325618041\n",
            "Loss: 0.0\n",
            "training error 0.025382686979014083, test error 0.04301382484532484\n",
            "Loss: 0.6334632300956544\n",
            "training error 0.025352663601840412, test error 0.04295782928859054\n",
            "Loss: 0.5024582143842604\n",
            "training error 0.02530892106596769, test error 0.04292063193770603\n",
            "Loss: 0.4154327462712937\n",
            "training error 0.025330967801460904, test error 0.04300681853462625\n",
            "Loss: 0.6170715394566484\n",
            "training error 0.025347075010636218, test error 0.043074811241005775\n",
            "Loss: 0.7761446175184927\n",
            "training error 0.025343690528532054, test error 0.042973634530836656\n",
            "Loss: 0.5394355413282481\n",
            "training error 0.02531624458788617, test error 0.04299301465192034\n",
            "Loss: 0.5847765150612805\n",
            "training error 0.02530100969206466, test error 0.04263625348919422\n",
            "Loss: 0.0\n",
            "training error 0.02529040312100396, test error 0.04274149949643345\n",
            "Loss: 0.24684628368181194\n",
            "training error 0.025247675613156856, test error 0.04283590961975576\n",
            "Loss: 0.4682778485969541\n",
            "training error 0.02520331708915711, test error 0.04282860574065042\n",
            "Loss: 0.45114717104530744\n",
            "training error 0.02519227294982802, test error 0.04288085611239053\n",
            "Loss: 0.5736963339386891\n",
            "training error 0.025201254193891475, test error 0.04298366159463884\n",
            "Loss: 0.8148185570119759\n",
            "training error 0.0251926448883326, test error 0.042831640811063595\n",
            "Loss: 0.4582656914705163\n",
            "training error 0.025161479977474227, test error 0.04274055061174424\n",
            "Loss: 0.2446207488105312\n",
            "training error 0.02513026211083927, test error 0.04295352735463102\n",
            "Loss: 0.7441410524430925\n",
            "training error 0.025170602505501703, test error 0.04280487479342541\n",
            "Loss: 0.3954880892007129\n",
            "training error 0.02514957914663046, test error 0.04281702817584778\n",
            "Loss: 0.42399289773285\n",
            "training error 0.02507216834271085, test error 0.04304720966157108\n",
            "Loss: 0.9638655809216745\n",
            "training error 0.025083736660936863, test error 0.04305708236364664\n",
            "Loss: 0.9870212319641913\n",
            "training error 0.025117112280023675, test error 0.042889459782228585\n",
            "Loss: 0.5938755690589437\n",
            "training error 0.025057575167299243, test error 0.04273588414461403\n",
            "Loss: 0.233675914899667\n",
            "training error 0.02506027673264411, test error 0.042789183717384605\n",
            "Loss: 0.358685896801747\n",
            "training error 0.024982360903440002, test error 0.04292448855868244\n",
            "Loss: 0.676032826292472\n",
            "training error 0.024985641309170026, test error 0.04297127924076015\n",
            "Loss: 0.7857767138260474\n",
            "training error 0.024989008597486147, test error 0.04301640737279749\n",
            "Loss: 0.8916212201890872\n",
            "training error 0.02498269149672796, test error 0.04280932432585805\n",
            "Loss: 0.40592411973461306\n",
            "training error 0.024998652514094475, test error 0.0431110400056546\n",
            "Loss: 1.1135746638262\n",
            "training error 0.02496596371342011, test error 0.04314150529701875\n",
            "Loss: 1.1850286234755147\n",
            "training error 0.024909434439192676, test error 0.04314001729155897\n",
            "Loss: 1.1815386229759195\n",
            "training error 0.024951183437877927, test error 0.04325144681718892\n",
            "Loss: 1.4428878657235078\n",
            "training error 0.024855920416568845, test error 0.04305951736068982\n",
            "Loss: 0.9927323272033606\n",
            "training error 0.024848162269477423, test error 0.043065287040083194\n",
            "Loss: 1.00626465924758\n",
            "training error 0.024874532529095553, test error 0.043139749532284076\n",
            "Loss: 1.18091061452541\n",
            "training error 0.024868263764644096, test error 0.043114353794294236\n",
            "Loss: 1.1213468960662443\n",
            "training error 0.02481508930164769, test error 0.04302424192765592\n",
            "Loss: 0.9099965562406442\n",
            "training error 0.02481393031581927, test error 0.042834555950030336\n",
            "Loss: 0.4651029220622549\n",
            "training error 0.024797129004028026, test error 0.042712958774828315\n",
            "Loss: 0.17990625197294996\n",
            "training error 0.02479623747388603, test error 0.04276829717137979\n",
            "Loss: 0.30969813569345916\n",
            "training error 0.02487884730229093, test error 0.042786754731535856\n",
            "Loss: 0.35298890034927144\n",
            "training error 0.024791381314457107, test error 0.04270997394501288\n",
            "Loss: 0.1729055669428048\n",
            "training error 0.024770864508473885, test error 0.04270557354266288\n",
            "Loss: 0.16258476717760928\n",
            "training error 0.024781583872128914, test error 0.04278957564650999\n",
            "Loss: 0.3596051359311625\n",
            "training error 0.024770432076506387, test error 0.042718274124739136\n",
            "Loss: 0.19237298972740735\n",
            "training error 0.024729101758961515, test error 0.042645225343466264\n",
            "Loss: 0.02104278293195705\n",
            "training error 0.024691607880812382, test error 0.04263675256116901\n",
            "Loss: 0.001170534308125859\n",
            "training error 0.02467370986065236, test error 0.042792790526230425\n",
            "Loss: 0.36714538503219707\n",
            "training error 0.02464913375882776, test error 0.04274011789970259\n",
            "Loss: 0.24360585653873112\n",
            "training error 0.024636045544907652, test error 0.042817764388176936\n",
            "Loss: 0.4257196262066554\n",
            "training error 0.02474582215212916, test error 0.04241669724028496\n",
            "Loss: 0.0\n",
            "training error 0.024649178614772047, test error 0.04237857244563007\n",
            "Loss: 0.0\n",
            "training error 0.024632321513030467, test error 0.04233988747125136\n",
            "Loss: 0.0\n",
            "training error 0.024683724161132875, test error 0.042357638597677695\n",
            "Loss: 0.04192530374198533\n",
            "training error 0.02459731821324528, test error 0.042668201072891536\n",
            "Loss: 0.7754238880844921\n",
            "training error 0.02465394954256312, test error 0.042546529379452726\n",
            "Loss: 0.48805493009795864\n",
            "training error 0.024572388997797852, test error 0.04296415149670482\n",
            "Loss: 1.4744111586912911\n",
            "training error 0.0246352413300546, test error 0.04260653866326275\n",
            "Loss: 0.629787200526799\n",
            "training error 0.024540490942963518, test error 0.04256827690551362\n",
            "Loss: 0.5394190865937842\n",
            "training error 0.02455373507570071, test error 0.042333341581846866\n",
            "Loss: 0.0\n",
            "training error 0.0245384719149519, test error 0.042185850414289176\n",
            "Loss: 0.0\n",
            "training error 0.024504655622196423, test error 0.04224518360269483\n",
            "Loss: 0.14064713126076267\n",
            "training error 0.024495491209076174, test error 0.042310036522496056\n",
            "Loss: 0.29437858188776644\n",
            "training error 0.02448132500189379, test error 0.04239930933092795\n",
            "Loss: 0.5059964764073221\n",
            "training error 0.024474684063360272, test error 0.0424098568323743\n",
            "Loss: 0.5309989389457703\n",
            "training error 0.02451344347660901, test error 0.04275847933546228\n",
            "Loss: 1.3573957038901963\n",
            "training error 0.02444378125085156, test error 0.042503772814694564\n",
            "Loss: 0.7536233056420816\n",
            "training error 0.024535781475325612, test error 0.042851203392194406\n",
            "Loss: 1.5771946550112936\n",
            "training error 0.024453608475634037, test error 0.04250259597700956\n",
            "Loss: 0.7508336553838868\n",
            "training error 0.02439200103789006, test error 0.042628151027800015\n",
            "Loss: 1.0484572650952817\n",
            "training error 0.02443548416694001, test error 0.04229433864441169\n",
            "Loss: 0.25716734179137557\n",
            "training error 0.024410063259414, test error 0.042530031197152124\n",
            "Loss: 0.8158678312346401\n",
            "training error 0.024390325919386283, test error 0.04244680938680245\n",
            "Loss: 0.6185936041362528\n",
            "training error 0.024423648247910625, test error 0.0423725917687674\n",
            "Loss: 0.44266348229160535\n",
            "training error 0.02436717389484421, test error 0.0426911398923792\n",
            "Loss: 1.197770041679358\n",
            "training error 0.02449409035076355, test error 0.042117109128568114\n",
            "Loss: 0.0\n",
            "training error 0.024367465095061262, test error 0.0425208132793412\n",
            "Loss: 0.9585276841787227\n",
            "training error 0.02431744650264065, test error 0.04250791213626883\n",
            "Loss: 0.9278960873305486\n",
            "training error 0.02443510687136824, test error 0.04281279774811329\n",
            "Loss: 1.6517957522238724\n",
            "training error 0.02434743983319426, test error 0.04248663168884412\n",
            "Loss: 0.8773692400111832\n",
            "training error 0.024310048407931804, test error 0.04279530099031328\n",
            "Loss: 1.6102526402628659\n",
            "training error 0.02425970058563638, test error 0.042897735296535136\n",
            "Loss: 1.8534656915413006\n",
            "training error 0.024256851248700882, test error 0.04283840669620184\n",
            "Loss: 1.7125998971863732\n",
            "training error 0.02428272955442767, test error 0.043190572645676745\n",
            "Loss: 2.548758780740945\n",
            "training error 0.02420911848883083, test error 0.04300564104333164\n",
            "Loss: 2.10966975926854\n",
            "training error 0.024193119675950687, test error 0.04298627728625005\n",
            "Loss: 2.063693771167152\n",
            "training error 0.024258964559007095, test error 0.04323592580196404\n",
            "Loss: 2.6564422310671576\n",
            "training error 0.024211651320651343, test error 0.04290007771623867\n",
            "Loss: 1.8590273736035279\n",
            "training error 0.024202609055650148, test error 0.04302273927159629\n",
            "Loss: 2.1502666298003126\n",
            "training error 0.02419044120474646, test error 0.0428583820606443\n",
            "Loss: 1.7600280442167993\n",
            "training error 0.024209730298182675, test error 0.042826604864825535\n",
            "Loss: 1.6845784312773837\n",
            "training error 0.024176245223587238, test error 0.0428135801364699\n",
            "Loss: 1.6536534019362836\n",
            "training error 0.02413548231593361, test error 0.04310392761721696\n",
            "Loss: 2.343034716928094\n",
            "training error 0.024153923363496713, test error 0.04295401857482908\n",
            "Loss: 1.9871008803243884\n",
            "training error 0.02413420485395405, test error 0.04325266890789623\n",
            "Loss: 2.696195923280653\n",
            "training error 0.024095233020837995, test error 0.043008461495983606\n",
            "Loss: 2.116366450257834\n",
            "training error 0.02412122907543362, test error 0.043083898586993685\n",
            "Loss: 2.2954791495169324\n",
            "training error 0.02415468239768129, test error 0.04304734493387768\n",
            "Loss: 2.2086886411645468\n",
            "training error 0.024081278618923934, test error 0.043091379871820865\n",
            "Loss: 2.3132422034918276\n",
            "training error 0.024060258704286203, test error 0.04310493702540618\n",
            "Loss: 2.345431387093999\n",
            "training error 0.02409543884040304, test error 0.04325417376448742\n",
            "Loss: 2.699768952442261\n",
            "training error 0.02403313658611224, test error 0.04321866185383304\n",
            "Loss: 2.615451886553477\n",
            "training error 0.0241246375066888, test error 0.042573580272977536\n",
            "Loss: 1.0838140457740897\n",
            "training error 0.02407494326206279, test error 0.04275228760336473\n",
            "Loss: 1.5081245791529696\n",
            "training error 0.024017355058434844, test error 0.04287488191502496\n",
            "Loss: 1.799204176487157\n",
            "training error 0.024079233919650122, test error 0.04268061808358463\n",
            "Loss: 1.3379573448318416\n",
            "training error 0.023985868185676218, test error 0.04305184640645999\n",
            "Loss: 2.2193766315690366\n",
            "training error 0.02398148030846547, test error 0.04312579169293464\n",
            "Loss: 2.3949472915802072\n",
            "training error 0.02397963298754723, test error 0.04322063278950773\n",
            "Loss: 2.620131542198112\n",
            "training error 0.023957680176658863, test error 0.0430399484890768\n",
            "Loss: 2.191127025579065\n",
            "training error 0.023993419029231553, test error 0.04326130198053754\n",
            "Loss: 2.7166937039212025\n",
            "training error 0.024086859745234074, test error 0.043045297565000445\n",
            "Loss: 2.2038275077211678\n",
            "training error 0.024007135046986084, test error 0.043435289440325715\n",
            "Loss: 3.129797697495529\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9606481481481481 0.042117109128568114\n",
            "training error 0.12510962893558936, test error 0.24998356774969135\n",
            "training error 0.1250625059089822, test error 0.250159119335797\n",
            "training error 0.12524336215094434, test error 0.2502731545496675\n",
            "training error 0.12497786085680497, test error 0.250167018426951\n",
            "training error 0.12505164425103213, test error 0.2502338474383879\n",
            "training error 0.12508167666466638, test error 0.25017134119401985\n",
            "training error 0.12499677937371458, test error 0.25023568874991026\n",
            "training error 0.12511365904188, test error 0.2501135972636859\n",
            "training error 0.12503169933037586, test error 0.25012469514109753\n",
            "training error 0.1250587339300916, test error 0.2502237837294142\n",
            "training error 0.12499380967810912, test error 0.25040170477307205\n",
            "training error 0.12501877064697572, test error 0.2504477019963638\n",
            "training error 0.12499218901630912, test error 0.25045180915674464\n",
            "training error 0.12500993311238454, test error 0.2504057094745052\n",
            "training error 0.12500874923287705, test error 0.2504414268718956\n",
            "training error 0.12499788818790893, test error 0.2504554197502054\n",
            "training error 0.12502314126800296, test error 0.250499063674537\n",
            "training error 0.12499702495225892, test error 0.2505005311550332\n",
            "training error 0.1249734910465785, test error 0.25052980580396744\n",
            "training error 0.12499141413010814, test error 0.2505553433093036\n",
            "training error 0.12504334313740112, test error 0.25047420870813286\n",
            "training error 0.1251709074647215, test error 0.250515640236399\n",
            "training error 0.1251072387545553, test error 0.2504935415543007\n",
            "training error 0.12505341531539668, test error 0.25041470959519235\n",
            "training error 0.12498738578076245, test error 0.2504318236344677\n",
            "training error 0.12501045045995446, test error 0.2503890873325711\n",
            "training error 0.12498401482926147, test error 0.2504432976208239\n",
            "training error 0.12503399033413506, test error 0.2503502573137875\n",
            "training error 0.1250486119642206, test error 0.2503691372457719\n",
            "training error 0.12498611588525764, test error 0.25039878074529553\n",
            "training error 0.1250053074870453, test error 0.2504819527497931\n",
            "training error 0.1250183920344208, test error 0.2505537071315764\n",
            "training error 0.12520208286074153, test error 0.2504965451694192\n",
            "training error 0.12496943462892492, test error 0.2505288020681775\n",
            "training error 0.1250446464897938, test error 0.2505072900724796\n",
            "training error 0.12497156201178222, test error 0.2506663541662389\n",
            "training error 0.125068382002476, test error 0.25078245075617644\n",
            "training error 0.12504004033081934, test error 0.25061158728155936\n",
            "training error 0.12499994066511881, test error 0.25058662041020013\n",
            "training error 0.12500551347879124, test error 0.2507068757472224\n",
            "training error 0.12507387211498305, test error 0.2506984748740014\n",
            "training error 0.12497946938978788, test error 0.25074313479804916\n",
            "training error 0.12501366694499066, test error 0.2507908648041936\n",
            "training error 0.1250504748051113, test error 0.25079725361265504\n",
            "training error 0.12498923380196221, test error 0.2507404923086668\n",
            "training error 0.12511694328918174, test error 0.2506759343522398\n",
            "training error 0.1250110279977242, test error 0.2507384355034714\n",
            "training error 0.12499980834643326, test error 0.25076985100628824\n",
            "training error 0.12499937445600323, test error 0.2506771016599733\n",
            "training error 0.12510187583801433, test error 0.2506233009348139\n",
            "Loss: 0.2559100947639559\n",
            "training error 0.1250197369893402, test error 0.2506155086820826\n",
            "Loss: 0.25279298878717604\n",
            "training error 0.12495333378769047, test error 0.250687192940299\n",
            "Loss: 0.28146857689150817\n",
            "training error 0.125048881557824, test error 0.2505373878078342\n",
            "Loss: 0.2215425850299768\n",
            "training error 0.12494381034054719, test error 0.2506305156108741\n",
            "Loss: 0.2587961548858875\n",
            "training error 0.12495316459924896, test error 0.2505604735715593\n",
            "Loss: 0.23077749752160592\n",
            "training error 0.12508363190050395, test error 0.2507033680900584\n",
            "Loss: 0.2879390620937894\n",
            "training error 0.12495825013762038, test error 0.2506301839460311\n",
            "Loss: 0.25866348022811625\n",
            "training error 0.12497691629313136, test error 0.2506000880001486\n",
            "Loss: 0.24662431055251233\n",
            "training error 0.12517596783154658, test error 0.2505680527197205\n",
            "Loss: 0.23380935606711617\n",
            "training error 0.12502528979804883, test error 0.25061805985833463\n",
            "Loss: 0.25381352636690835\n",
            "training error 0.12499313291328853, test error 0.25065942226627225\n",
            "Loss: 0.2703595770973344\n",
            "training error 0.12493824649980224, test error 0.25066219929689637\n",
            "Loss: 0.2714704623643538\n",
            "training error 0.12499832962000226, test error 0.2507384905100745\n",
            "Loss: 0.30198895358557465\n",
            "training error 0.12493173864590826, test error 0.2506677355029955\n",
            "Loss: 0.2736850903692911\n",
            "training error 0.12493899531507155, test error 0.25059775072870044\n",
            "Loss: 0.2456893405186067\n",
            "training error 0.12493979929901924, test error 0.2505861037302924\n",
            "Loss: 0.2410302349170168\n",
            "training error 0.12494412320265683, test error 0.250632686141166\n",
            "Loss: 0.25966442407312496\n",
            "training error 0.12496591572729077, test error 0.2505552069665165\n",
            "Loss: 0.2286707170279012\n",
            "training error 0.12519975835660385, test error 0.25052190964178017\n",
            "Loss: 0.21535091163586983\n",
            "training error 0.12499327882121435, test error 0.2505381604635944\n",
            "Loss: 0.22185166764976394\n",
            "training error 0.1250102788326616, test error 0.25062145257382157\n",
            "Loss: 0.2551707017674598\n",
            "training error 0.12517726407782065, test error 0.25048620090616924\n",
            "Loss: 0.20106647848996406\n",
            "training error 0.12497520841896338, test error 0.2506560367652933\n",
            "Loss: 0.269005287689672\n",
            "training error 0.12503167856092964, test error 0.25066771622900547\n",
            "Loss: 0.27367738026651267\n",
            "training error 0.124954728240271, test error 0.250666570179807\n",
            "Loss: 0.27321893045366696\n",
            "training error 0.12496951861590097, test error 0.25076367185841936\n",
            "Loss: 0.3120621550249725\n",
            "training error 0.12496409211649714, test error 0.2508222930174728\n",
            "Loss: 0.33551215999174655\n",
            "training error 0.1250220422446641, test error 0.2507411004719208\n",
            "Loss: 0.30303300694864976\n",
            "training error 0.12506425644474442, test error 0.2507704272212618\n",
            "Loss: 0.31476447778293615\n",
            "training error 0.12522584407378048, test error 0.25067463139567797\n",
            "Loss: 0.27644362875827166\n",
            "training error 0.1250769319607762, test error 0.2509466772923786\n",
            "Loss: 0.3852691404306885\n",
            "training error 0.12497391787929828, test error 0.2509332446850895\n",
            "Loss: 0.3798957443271078\n",
            "training error 0.12499009371333628, test error 0.2507889400074894\n",
            "Loss: 0.3221700790367432\n",
            "training error 0.12496206253180678, test error 0.250850756585514\n",
            "Loss: 0.3468983356101685\n",
            "training error 0.12497276161156397, test error 0.25076899973342004\n",
            "Loss: 0.3141934451128092\n",
            "training error 0.12491582199833022, test error 0.25074168422718335\n",
            "Loss: 0.30326652440255675\n",
            "training error 0.12493852872235055, test error 0.25078698204397615\n",
            "Loss: 0.3213868421500621\n",
            "training error 0.12505771149488928, test error 0.25088846915557433\n",
            "Loss: 0.3619843552233215\n",
            "training error 0.12492354283045067, test error 0.2508602286568532\n",
            "Loss: 0.35068741319814745\n",
            "training error 0.12494024894722092, test error 0.25081350874105907\n",
            "Loss: 0.33199821845839583\n",
            "training error 0.12502344910914034, test error 0.2507020747675749\n",
            "Loss: 0.28742169909463655\n",
            "training error 0.12496588382866838, test error 0.2506077023213741\n",
            "Loss: 0.24967023924855614\n",
            "training error 0.12491041318927586, test error 0.25057473063404256\n",
            "Loss: 0.23648069738053668\n",
            "training error 0.12492623671513499, test error 0.25062390677954605\n",
            "Loss: 0.25615244858647745\n",
            "training error 0.1249134497390869, test error 0.25072352191286945\n",
            "Loss: 0.29600112112928567\n",
            "training error 0.12503123956768197, test error 0.2507662023985276\n",
            "Loss: 0.3130744376045991\n",
            "training error 0.12492730951289321, test error 0.25084227018912586\n",
            "Loss: 0.3435035539193221\n",
            "training error 0.12500791703455497, test error 0.250864989062637\n",
            "Loss: 0.3525917006785839\n",
            "training error 0.12489196720971454, test error 0.2508732692543473\n",
            "Loss: 0.35590399507650083\n",
            "training error 0.12488600321859705, test error 0.25087625012657927\n",
            "Loss: 0.3570964223463591\n",
            "training error 0.12493390454789931, test error 0.25097225635971576\n",
            "Loss: 0.395501439924395\n",
            "training error 0.1249555679908591, test error 0.25088946838414566\n",
            "Loss: 0.36238407292490393\n",
            "training error 0.12499882757979959, test error 0.25078453967849823\n",
            "Loss: 0.32040983174097537\n",
            "training error 0.12487615996799213, test error 0.2507177455800826\n",
            "Loss: 0.2936904361355408\n",
            "training error 0.12488176214491709, test error 0.2507421495684114\n",
            "Loss: 0.30345267312914537\n",
            "training error 0.12495394269483469, test error 0.2507110599435142\n",
            "Loss: 0.29101600572054576\n",
            "training error 0.12487229778213778, test error 0.2506802459698523\n",
            "Loss: 0.2786896060538391\n",
            "training error 0.12490507465979812, test error 0.2506347254346066\n",
            "Loss: 0.260480195069146\n",
            "training error 0.12491706981042176, test error 0.2506919321491287\n",
            "Loss: 0.28336438503295813\n",
            "training error 0.12489688800886344, test error 0.25063924446531555\n",
            "Loss: 0.26228792617311036\n",
            "training error 0.12491596685401657, test error 0.2506301344594089\n",
            "Loss: 0.2586436842780593\n",
            "training error 0.1250787107005719, test error 0.250705165100176\n",
            "Loss: 0.288657913390189\n",
            "training error 0.12484867539764032, test error 0.2506577923485149\n",
            "Loss: 0.2697075671384219\n",
            "training error 0.1248944124714772, test error 0.25060537240187597\n",
            "Loss: 0.24873821018795983\n",
            "training error 0.12489521405420592, test error 0.2506157943718972\n",
            "Loss: 0.25290727222473386\n",
            "training error 0.12486263888379245, test error 0.25058184921299326\n",
            "Loss: 0.23932831613193706\n",
            "training error 0.12491070967970798, test error 0.25067576985932166\n",
            "Loss: 0.27689904414973654\n",
            "training error 0.12493598184781651, test error 0.25061344805518065\n",
            "Loss: 0.2519686838456492\n",
            "training error 0.12483277738267431, test error 0.2507371280384754\n",
            "Loss: 0.3014439291220139\n",
            "training error 0.12491428370226436, test error 0.25080688884918767\n",
            "Loss: 0.32935008765084817\n",
            "training error 0.12490617561511022, test error 0.2507681623051839\n",
            "Loss: 0.3138584517995868\n",
            "training error 0.1248580206978753, test error 0.2506912763577597\n",
            "Loss: 0.28310205124240095\n",
            "training error 0.124817966878822, test error 0.25073191831864605\n",
            "Loss: 0.2993599042093953\n",
            "training error 0.12496653938565144, test error 0.250706695829763\n",
            "Loss: 0.2892702454729834\n",
            "training error 0.1248874046431448, test error 0.25072835611705147\n",
            "Loss: 0.29793492990941584\n",
            "training error 0.12493735357420221, test error 0.2509738988883017\n",
            "Loss: 0.3961584945463059\n",
            "training error 0.12486454061280747, test error 0.2507650593572662\n",
            "Loss: 0.3126171910456721\n",
            "training error 0.12479035882525093, test error 0.25077788617615027\n",
            "Loss: 0.31774825585906274\n",
            "training error 0.12482318521022509, test error 0.2507299035682057\n",
            "Loss: 0.2985539510587554\n",
            "training error 0.12484624074209055, test error 0.25077193286241933\n",
            "Loss: 0.31536677383425804\n",
            "training error 0.12495303087658087, test error 0.25100399873271445\n",
            "Loss: 0.4081992237365162\n",
            "training error 0.1249104180236379, test error 0.25071314301915104\n",
            "Loss: 0.29184929074626\n",
            "training error 0.12484163817328431, test error 0.25084109811801997\n",
            "Loss: 0.3430346946593055\n",
            "training error 0.12473719593185502, test error 0.25079464112115385\n",
            "Loss: 0.3244506744037867\n",
            "training error 0.12495553135552336, test error 0.2507712981544978\n",
            "Loss: 0.31511287397705257\n",
            "training error 0.12485751957655787, test error 0.2506259386592258\n",
            "Loss: 0.25696525388325675\n",
            "training error 0.1247272082857738, test error 0.25062194706383545\n",
            "Loss: 0.2553685107747894\n",
            "training error 0.12468882848832849, test error 0.25059938885953054\n",
            "Loss: 0.2463446359225463\n",
            "training error 0.12471489526938326, test error 0.250543266001652\n",
            "Loss: 0.22389401711437884\n",
            "training error 0.12482457578614084, test error 0.25065415237226\n",
            "Loss: 0.26825148092939344\n",
            "training error 0.12469359893908048, test error 0.2505660359455946\n",
            "Loss: 0.23300259338905338\n",
            "training error 0.12471105502636612, test error 0.2504379238732538\n",
            "Loss: 0.18175439595988774\n",
            "training error 0.12470752228575932, test error 0.25040745319416946\n",
            "Loss: 0.16956532315057782\n",
            "training error 0.12463032098115381, test error 0.25027911957340726\n",
            "Loss: 0.11822850052762846\n",
            "training error 0.1246288515270798, test error 0.25038117497952184\n",
            "Loss: 0.15905334634980228\n",
            "training error 0.12487521504751348, test error 0.2501764791887325\n",
            "Loss: 0.07716964790034364\n",
            "training error 0.12466290465141933, test error 0.250180182748449\n",
            "Loss: 0.07865116916585535\n",
            "training error 0.12463669810776806, test error 0.2502109073224879\n",
            "Loss: 0.09094180663273832\n",
            "training error 0.12455126697512953, test error 0.2501384518222427\n",
            "Loss: 0.06195770143837542\n",
            "training error 0.12456560225869115, test error 0.2501549160393861\n",
            "Loss: 0.06854382119481084\n",
            "training error 0.12458945986547579, test error 0.2500730535053904\n",
            "Loss: 0.035796655158004675\n",
            "training error 0.12462394068577867, test error 0.25013392636979853\n",
            "Loss: 0.06014740147148867\n",
            "training error 0.12454153520901924, test error 0.24995481529661295\n",
            "Loss: 0.0\n",
            "training error 0.12453273167245515, test error 0.2498515518073055\n",
            "Loss: 0.0\n",
            "training error 0.12444300561733286, test error 0.24994064968708937\n",
            "Loss: 0.03566032675776931\n",
            "training error 0.12446173325627047, test error 0.24984066432190022\n",
            "Loss: 0.0\n",
            "training error 0.12440950727437475, test error 0.24984623034887998\n",
            "Loss: 0.002227830683554366\n",
            "training error 0.12434983022692796, test error 0.24983293675568574\n",
            "Loss: 0.0\n",
            "training error 0.1245715935732102, test error 0.24986684878513513\n",
            "Loss: 0.01357388256719716\n",
            "training error 0.12437840874701647, test error 0.24963905294102243\n",
            "Loss: 0.0\n",
            "training error 0.12425660590005289, test error 0.2495958092695726\n",
            "Loss: 0.0\n",
            "training error 0.12435890739605843, test error 0.24955440353309327\n",
            "Loss: 0.0\n",
            "training error 0.12419253481075618, test error 0.2493897206655672\n",
            "Loss: 0.0\n",
            "training error 0.12425189661095677, test error 0.24935544228814255\n",
            "Loss: 0.0\n",
            "training error 0.1241637125041652, test error 0.24943760271577495\n",
            "Loss: 0.03294912149438556\n",
            "training error 0.12413902063420637, test error 0.24944762292909128\n",
            "Loss: 0.03696756730187101\n",
            "training error 0.12409326738375243, test error 0.2492481148693172\n",
            "Loss: 0.0\n",
            "training error 0.12404856929681955, test error 0.24916726903922248\n",
            "Loss: 0.0\n",
            "training error 0.12395206112916442, test error 0.24908216869213484\n",
            "Loss: 0.0\n",
            "training error 0.12392417746985966, test error 0.24905498186549793\n",
            "Loss: 0.0\n",
            "training error 0.12389228426482592, test error 0.24892911044843535\n",
            "Loss: 0.0\n",
            "training error 0.1238180660413546, test error 0.24884769775458368\n",
            "Loss: 0.0\n",
            "training error 0.12378971101111683, test error 0.2489097475660095\n",
            "Loss: 0.024934854525771044\n",
            "training error 0.12384550186242652, test error 0.24877873642193704\n",
            "Loss: 0.0\n",
            "training error 0.12375882395268137, test error 0.24855367926137142\n",
            "Loss: 0.0\n",
            "training error 0.12358148859776885, test error 0.24830325968322534\n",
            "Loss: 0.0\n",
            "training error 0.12351555610099646, test error 0.24825874095377679\n",
            "Loss: 0.0\n",
            "training error 0.1234130332936221, test error 0.24801482587156018\n",
            "Loss: 0.0\n",
            "training error 0.12339794400736609, test error 0.24793573131622823\n",
            "Loss: 0.0\n",
            "training error 0.1233445779255762, test error 0.247904406447559\n",
            "Loss: 0.0\n",
            "training error 0.12338819781874517, test error 0.24771414306163\n",
            "Loss: 0.0\n",
            "training error 0.12311794347651969, test error 0.24760993593616762\n",
            "Loss: 0.0\n",
            "training error 0.12308137824199697, test error 0.24731625649144295\n",
            "Loss: 0.0\n",
            "training error 0.12297128475941549, test error 0.24724439549377725\n",
            "Loss: 0.0\n",
            "training error 0.12281369621580639, test error 0.2469250809640212\n",
            "Loss: 0.0\n",
            "training error 0.12269780635480437, test error 0.2467291073394064\n",
            "Loss: 0.0\n",
            "training error 0.12258549469825614, test error 0.24655425399561023\n",
            "Loss: 0.0\n",
            "training error 0.12248402340864856, test error 0.24626354946787393\n",
            "Loss: 0.0\n",
            "training error 0.12233546414509393, test error 0.24598343737828932\n",
            "Loss: 0.0\n",
            "training error 0.12226180113533149, test error 0.24584209663661724\n",
            "Loss: 0.0\n",
            "training error 0.12210175679025576, test error 0.24548249681000137\n",
            "Loss: 0.0\n",
            "training error 0.12192840010566415, test error 0.24520735368217944\n",
            "Loss: 0.0\n",
            "training error 0.12182972922456804, test error 0.24475726819620203\n",
            "Loss: 0.0\n",
            "training error 0.1216032079806334, test error 0.2444822798757347\n",
            "Loss: 0.0\n",
            "training error 0.12162787529628914, test error 0.24400080877257946\n",
            "Loss: 0.0\n",
            "training error 0.1214127805213435, test error 0.24365615822947131\n",
            "Loss: 0.0\n",
            "training error 0.12117961724540681, test error 0.24350775473531547\n",
            "Loss: 0.0\n",
            "training error 0.12089805714803402, test error 0.24313028518113902\n",
            "Loss: 0.0\n",
            "training error 0.12079876203626681, test error 0.24269228867677092\n",
            "Loss: 0.0\n",
            "training error 0.12062993303791514, test error 0.2421104044303686\n",
            "Loss: 0.0\n",
            "training error 0.12029896742682858, test error 0.24175378387186977\n",
            "Loss: 0.0\n",
            "training error 0.12015094137159893, test error 0.24114588537599183\n",
            "Loss: 0.0\n",
            "training error 0.1197896392316229, test error 0.24070953889645352\n",
            "Loss: 0.0\n",
            "training error 0.11968695346355404, test error 0.24012672362571405\n",
            "Loss: 0.0\n",
            "training error 0.11926981064209252, test error 0.23966466121877686\n",
            "Loss: 0.0\n",
            "training error 0.11901551973425062, test error 0.23923215237275317\n",
            "Loss: 0.0\n",
            "training error 0.11879670490653346, test error 0.23865292693634554\n",
            "Loss: 0.0\n",
            "training error 0.1184111091554956, test error 0.23803699982373314\n",
            "Loss: 0.0\n",
            "training error 0.11818785925202878, test error 0.23735665666512518\n",
            "Loss: 0.0\n",
            "training error 0.11777279516672905, test error 0.23649481731608746\n",
            "Loss: 0.0\n",
            "training error 0.11740689852801034, test error 0.23577016715338153\n",
            "Loss: 0.0\n",
            "training error 0.11718632493618415, test error 0.2349139101045643\n",
            "Loss: 0.0\n",
            "training error 0.11667366381988263, test error 0.23425536603054892\n",
            "Loss: 0.0\n",
            "training error 0.11635509769494264, test error 0.23333553315886602\n",
            "Loss: 0.0\n",
            "training error 0.11583411867235426, test error 0.23260881970340608\n",
            "Loss: 0.0\n",
            "training error 0.11542544862152576, test error 0.231700985774422\n",
            "Loss: 0.0\n",
            "training error 0.1150612012358677, test error 0.2307151051433181\n",
            "Loss: 0.0\n",
            "training error 0.11451482002479615, test error 0.22979539986983127\n",
            "Loss: 0.0\n",
            "training error 0.11445801678564962, test error 0.2289257016963791\n",
            "Loss: 0.0\n",
            "training error 0.1134937497183313, test error 0.22767912064485324\n",
            "Loss: 0.0\n",
            "training error 0.11306716637182689, test error 0.2266456181461355\n",
            "Loss: 0.0\n",
            "training error 0.11247029867158245, test error 0.22545020645364802\n",
            "Loss: 0.0\n",
            "training error 0.11190579855281443, test error 0.2242420094193383\n",
            "Loss: 0.0\n",
            "training error 0.11134611130012174, test error 0.2230333755621531\n",
            "Loss: 0.0\n",
            "training error 0.11088081580142893, test error 0.2219259176661571\n",
            "Loss: 0.0\n",
            "training error 0.11027325123633794, test error 0.22038811946686265\n",
            "Loss: 0.0\n",
            "training error 0.10978907634033276, test error 0.21921133376398805\n",
            "Loss: 0.0\n",
            "training error 0.10893645311362533, test error 0.21782406389844425\n",
            "Loss: 0.0\n",
            "training error 0.10813030523020993, test error 0.2164047689416366\n",
            "Loss: 0.0\n",
            "training error 0.10745374685014424, test error 0.21489724854338635\n",
            "Loss: 0.0\n",
            "training error 0.10684389980550707, test error 0.21333539659888434\n",
            "Loss: 0.0\n",
            "training error 0.10597755977258869, test error 0.21175368071044295\n",
            "Loss: 0.0\n",
            "training error 0.10529771942000635, test error 0.21017292026220188\n",
            "Loss: 0.0\n",
            "training error 0.10447625864411877, test error 0.2086118612911681\n",
            "Loss: 0.0\n",
            "training error 0.10367660533648819, test error 0.20686361282789767\n",
            "Loss: 0.0\n",
            "training error 0.10285583659299011, test error 0.20530546547311582\n",
            "Loss: 0.0\n",
            "training error 0.10202509424649606, test error 0.20357082776702562\n",
            "Loss: 0.0\n",
            "training error 0.1011751052653636, test error 0.20186773311708997\n",
            "Loss: 0.0\n",
            "training error 0.10035935281028452, test error 0.19996265188687273\n",
            "Loss: 0.0\n",
            "training error 0.09940958928523576, test error 0.19821197515239272\n",
            "Loss: 0.0\n",
            "training error 0.09856118854713575, test error 0.19631487677922507\n",
            "Loss: 0.0\n",
            "training error 0.09764637193726712, test error 0.19448056212948348\n",
            "Loss: 0.0\n",
            "training error 0.09678542289417359, test error 0.19253998065002528\n",
            "Loss: 0.0\n",
            "training error 0.09574297750597804, test error 0.19039916903149728\n",
            "Loss: 0.0\n",
            "training error 0.09491690613822897, test error 0.18833136569838413\n",
            "Loss: 0.0\n",
            "training error 0.09386788985102933, test error 0.18628414182499128\n",
            "Loss: 0.0\n",
            "training error 0.09293046122622077, test error 0.18419170011953542\n",
            "Loss: 0.0\n",
            "training error 0.09201026832900014, test error 0.18227948418940326\n",
            "Loss: 0.0\n",
            "training error 0.09099209308789194, test error 0.18002495900129775\n",
            "Loss: 0.0\n",
            "training error 0.08995646364337694, test error 0.17812350282779915\n",
            "Loss: 0.0\n",
            "training error 0.0889525638282564, test error 0.17597426964766497\n",
            "Loss: 0.0\n",
            "training error 0.08800833413596452, test error 0.17386503334425604\n",
            "Loss: 0.0\n",
            "training error 0.0869247026011028, test error 0.17164210748041675\n",
            "Loss: 0.0\n",
            "training error 0.08598811304414908, test error 0.16952699116118186\n",
            "Loss: 0.0\n",
            "training error 0.08505019752048758, test error 0.16734809156208474\n",
            "Loss: 0.0\n",
            "training error 0.08411281368494299, test error 0.16505244371788425\n",
            "Loss: 0.0\n",
            "training error 0.08313305300232966, test error 0.16316852154821482\n",
            "Loss: 0.0\n",
            "training error 0.08205981973386813, test error 0.16074773393589709\n",
            "Loss: 0.0\n",
            "training error 0.0809157472711411, test error 0.15859728187185052\n",
            "Loss: 0.0\n",
            "training error 0.07993472334434702, test error 0.15646789488539847\n",
            "Loss: 0.0\n",
            "training error 0.07892880396624595, test error 0.15444955399657764\n",
            "Loss: 0.0\n",
            "training error 0.07803953536747271, test error 0.15244886765937574\n",
            "Loss: 0.0\n",
            "training error 0.07710471495911139, test error 0.15035522232205406\n",
            "Loss: 0.0\n",
            "training error 0.07602679922871025, test error 0.1481385938062852\n",
            "Loss: 0.0\n",
            "training error 0.07510697691273181, test error 0.14594652000033936\n",
            "Loss: 0.0\n",
            "training error 0.07419843966974636, test error 0.14392166513149746\n",
            "Loss: 0.0\n",
            "training error 0.07329526047861049, test error 0.14200175232712636\n",
            "Loss: 0.0\n",
            "training error 0.07243628847968105, test error 0.1399669281608877\n",
            "Loss: 0.0\n",
            "training error 0.07165932683660087, test error 0.13816973305161454\n",
            "Loss: 0.0\n",
            "training error 0.07055220722658323, test error 0.13593547294387004\n",
            "Loss: 0.0\n",
            "training error 0.06965278279980967, test error 0.1340191546901551\n",
            "Loss: 0.0\n",
            "training error 0.06874031114752398, test error 0.1322291905127487\n",
            "Loss: 0.0\n",
            "training error 0.06788381225082002, test error 0.13045796277086205\n",
            "Loss: 0.0\n",
            "training error 0.06708433286530055, test error 0.12861669636645223\n",
            "Loss: 0.0\n",
            "training error 0.06622231435938356, test error 0.12677395610644984\n",
            "Loss: 0.0\n",
            "training error 0.06543812800199744, test error 0.1250784463466433\n",
            "Loss: 0.0\n",
            "training error 0.0646272076323303, test error 0.1233279145598058\n",
            "Loss: 0.0\n",
            "training error 0.06375039250867799, test error 0.1213645714406065\n",
            "Loss: 0.0\n",
            "training error 0.06307127371740134, test error 0.11959835051958838\n",
            "Loss: 0.0\n",
            "training error 0.06220841625845493, test error 0.11802689145596566\n",
            "Loss: 0.0\n",
            "training error 0.06146401302120928, test error 0.11633466778694519\n",
            "Loss: 0.0\n",
            "training error 0.0607656950363804, test error 0.11467844863519408\n",
            "Loss: 0.0\n",
            "training error 0.06005743867616894, test error 0.11292750837215593\n",
            "Loss: 0.0\n",
            "training error 0.05935452843719678, test error 0.11141438623585016\n",
            "Loss: 0.0\n",
            "training error 0.058658361119713946, test error 0.10982496453339011\n",
            "Loss: 0.0\n",
            "training error 0.0579622379443789, test error 0.10817487811924742\n",
            "Loss: 0.0\n",
            "training error 0.057236167714810104, test error 0.1067498697922215\n",
            "Loss: 0.0\n",
            "training error 0.05665631242555551, test error 0.10517717239786395\n",
            "Loss: 0.0\n",
            "training error 0.05591957001145051, test error 0.10396671611732879\n",
            "Loss: 0.0\n",
            "training error 0.05526951991774861, test error 0.10251171957522856\n",
            "Loss: 0.0\n",
            "training error 0.05477776321189054, test error 0.10100016575218401\n",
            "Loss: 0.0\n",
            "training error 0.054063332770449314, test error 0.09966242219636823\n",
            "Loss: 0.0\n",
            "training error 0.05350704208850614, test error 0.09826202899824854\n",
            "Loss: 0.0\n",
            "training error 0.05293557073061066, test error 0.09685473245436665\n",
            "Loss: 0.0\n",
            "training error 0.05246048126704028, test error 0.09565091814663641\n",
            "Loss: 0.0\n",
            "training error 0.05182622376149687, test error 0.09440635953256771\n",
            "Loss: 0.0\n",
            "training error 0.05133357890414269, test error 0.09329222440370577\n",
            "Loss: 0.0\n",
            "training error 0.05085785156423495, test error 0.09195233318224637\n",
            "Loss: 0.0\n",
            "training error 0.05028012092277306, test error 0.09080652840375167\n",
            "Loss: 0.0\n",
            "training error 0.04973885928399132, test error 0.08970327634759052\n",
            "Loss: 0.0\n",
            "training error 0.049249175116254214, test error 0.08850879021787426\n",
            "Loss: 0.0\n",
            "training error 0.0487741732963128, test error 0.08745318620057656\n",
            "Loss: 0.0\n",
            "training error 0.04831245875073263, test error 0.08631357990192402\n",
            "Loss: 0.0\n",
            "training error 0.04782912756819875, test error 0.08535830832996169\n",
            "Loss: 0.0\n",
            "training error 0.04740575896831535, test error 0.0841314112422261\n",
            "Loss: 0.0\n",
            "training error 0.046950514951483176, test error 0.08316363332731908\n",
            "Loss: 0.0\n",
            "training error 0.046521206534602524, test error 0.0820090329759443\n",
            "Loss: 0.0\n",
            "training error 0.046113917913530394, test error 0.08113516494030798\n",
            "Loss: 0.0\n",
            "training error 0.04568566985612754, test error 0.0801838408290795\n",
            "Loss: 0.0\n",
            "training error 0.04528430272614893, test error 0.07923096310880555\n",
            "Loss: 0.0\n",
            "training error 0.04491979366834784, test error 0.07841098657708549\n",
            "Loss: 0.0\n",
            "training error 0.04454876469515663, test error 0.0773132286653135\n",
            "Loss: 0.0\n",
            "training error 0.04415650822817012, test error 0.076453062173105\n",
            "Loss: 0.0\n",
            "training error 0.04385081935855356, test error 0.07555162871462016\n",
            "Loss: 0.0\n",
            "training error 0.04347220663745403, test error 0.07472058322629024\n",
            "Loss: 0.0\n",
            "training error 0.04321099378232841, test error 0.07398855650370433\n",
            "Loss: 0.0\n",
            "training error 0.04280577348957436, test error 0.07311149983895622\n",
            "Loss: 0.0\n",
            "training error 0.042583512867245606, test error 0.07250258997234006\n",
            "Loss: 0.0\n",
            "training error 0.04216227201105374, test error 0.07164723489847218\n",
            "Loss: 0.0\n",
            "training error 0.04194836576955983, test error 0.07103060360713159\n",
            "Loss: 0.0\n",
            "training error 0.04158595705770177, test error 0.07014101682864829\n",
            "Loss: 0.0\n",
            "training error 0.041285076856399754, test error 0.06947569279542544\n",
            "Loss: 0.0\n",
            "training error 0.04100228283296446, test error 0.06879007825683492\n",
            "Loss: 0.0\n",
            "training error 0.04075807907690992, test error 0.06815599815966335\n",
            "Loss: 0.0\n",
            "training error 0.04047413162214861, test error 0.06743748830510721\n",
            "Loss: 0.0\n",
            "training error 0.04019621194862107, test error 0.06686812581075778\n",
            "Loss: 0.0\n",
            "training error 0.039953537449776005, test error 0.06629481229390091\n",
            "Loss: 0.0\n",
            "training error 0.039686037794991594, test error 0.06568059996174147\n",
            "Loss: 0.0\n",
            "training error 0.03950497167257051, test error 0.06520804596708464\n",
            "Loss: 0.0\n",
            "training error 0.039188330549006375, test error 0.06448194476968556\n",
            "Loss: 0.0\n",
            "training error 0.038959775403517735, test error 0.06396829735072693\n",
            "Loss: 0.0\n",
            "training error 0.03876835705046939, test error 0.06342821913611488\n",
            "Loss: 0.0\n",
            "training error 0.03856300707154008, test error 0.0625964711089798\n",
            "Loss: 0.0\n",
            "training error 0.03831240584472666, test error 0.062117234451079126\n",
            "Loss: 0.0\n",
            "training error 0.03810909552099379, test error 0.06169537307021012\n",
            "Loss: 0.0\n",
            "training error 0.037906383495687285, test error 0.061223076465440555\n",
            "Loss: 0.0\n",
            "training error 0.03774974712514654, test error 0.0608345254377508\n",
            "Loss: 0.0\n",
            "training error 0.037527834912979466, test error 0.06026162808595313\n",
            "Loss: 0.0\n",
            "training error 0.03737623693528775, test error 0.0598355361422003\n",
            "Loss: 0.0\n",
            "training error 0.037148560491682074, test error 0.059497769890792\n",
            "Loss: 0.0\n",
            "training error 0.037008309598473656, test error 0.05888851522699306\n",
            "Loss: 0.0\n",
            "training error 0.03680049983512022, test error 0.05866689561538161\n",
            "Loss: 0.0\n",
            "training error 0.0366264688966829, test error 0.058226609117663426\n",
            "Loss: 0.0\n",
            "training error 0.03646781481555698, test error 0.057742584329792686\n",
            "Loss: 0.0\n",
            "training error 0.03627538372047842, test error 0.057503724974204626\n",
            "Loss: 0.0\n",
            "training error 0.03612019515880922, test error 0.05708224844111931\n",
            "Loss: 0.0\n",
            "training error 0.03597849871080669, test error 0.05670215034571928\n",
            "Loss: 0.0\n",
            "training error 0.03580705647660148, test error 0.05626395551641722\n",
            "Loss: 0.0\n",
            "training error 0.03567384474813234, test error 0.05597280951915487\n",
            "Loss: 0.0\n",
            "training error 0.03551509689304383, test error 0.05564477652170872\n",
            "Loss: 0.0\n",
            "training error 0.035368259707580084, test error 0.055313910598786896\n",
            "Loss: 0.0\n",
            "training error 0.03523250389739762, test error 0.054958379921968045\n",
            "Loss: 0.0\n",
            "training error 0.03509950878009899, test error 0.05470087970416321\n",
            "Loss: 0.0\n",
            "training error 0.035014642473794544, test error 0.05451228958786116\n",
            "Loss: 0.0\n",
            "training error 0.03487721270378429, test error 0.05401409024205432\n",
            "Loss: 0.0\n",
            "training error 0.034781775146579944, test error 0.05349717777468094\n",
            "Loss: 0.0\n",
            "training error 0.034587797601876795, test error 0.05332400092453578\n",
            "Loss: 0.0\n",
            "training error 0.03451608153577861, test error 0.053214054069464514\n",
            "Loss: 0.0\n",
            "training error 0.03438443410808072, test error 0.05290945133177456\n",
            "Loss: 0.0\n",
            "training error 0.03426748546532278, test error 0.0525123341121288\n",
            "Loss: 0.0\n",
            "training error 0.034091155835482584, test error 0.052386946654473285\n",
            "Loss: 0.0\n",
            "training error 0.034059934120213645, test error 0.05195813961684174\n",
            "Loss: 0.0\n",
            "training error 0.03387064600740602, test error 0.05187954652277452\n",
            "Loss: 0.0\n",
            "training error 0.03374542384290968, test error 0.051620837487979514\n",
            "Loss: 0.0\n",
            "training error 0.03368021120392192, test error 0.05151719179980588\n",
            "Loss: 0.0\n",
            "training error 0.0335589194849122, test error 0.05118772203595779\n",
            "Loss: 0.0\n",
            "training error 0.03344894824348124, test error 0.051112458942917945\n",
            "Loss: 0.0\n",
            "training error 0.03340581474858993, test error 0.050765240875077326\n",
            "Loss: 0.0\n",
            "training error 0.033268784557591004, test error 0.05041482126384764\n",
            "Loss: 0.0\n",
            "training error 0.03316091684479238, test error 0.0503321200040757\n",
            "Loss: 0.0\n",
            "training error 0.033048938991304636, test error 0.050081281539252294\n",
            "Loss: 0.0\n",
            "training error 0.03296453496713609, test error 0.04987742345542136\n",
            "Loss: 0.0\n",
            "training error 0.03288828212668809, test error 0.04984481438745704\n",
            "Loss: 0.0\n",
            "training error 0.032832505668856565, test error 0.04936510862684422\n",
            "Loss: 0.0\n",
            "training error 0.03269859154021978, test error 0.049153758277993766\n",
            "Loss: 0.0\n",
            "training error 0.03261570750918042, test error 0.04908520566663783\n",
            "Loss: 0.0\n",
            "training error 0.03251625418627087, test error 0.04885169842985595\n",
            "Loss: 0.0\n",
            "training error 0.032429496542260065, test error 0.04876226673751782\n",
            "Loss: 0.0\n",
            "training error 0.03236420914559807, test error 0.048645693006352915\n",
            "Loss: 0.0\n",
            "training error 0.0322908362572078, test error 0.04850809087009544\n",
            "Loss: 0.0\n",
            "training error 0.03220723990646848, test error 0.048352311727375245\n",
            "Loss: 0.0\n",
            "training error 0.032121375449988304, test error 0.048257515722149785\n",
            "Loss: 0.0\n",
            "training error 0.03206091624378774, test error 0.04804843663097663\n",
            "Loss: 0.0\n",
            "training error 0.03200231428551834, test error 0.04791944133230342\n",
            "Loss: 0.0\n",
            "training error 0.031882101690809767, test error 0.04789726923748677\n",
            "Loss: 0.0\n",
            "training error 0.03186145344633133, test error 0.04777663621069448\n",
            "Loss: 0.0\n",
            "training error 0.03181051599041311, test error 0.047768744430242184\n",
            "Loss: 0.0\n",
            "training error 0.03171070027770984, test error 0.04747136916518083\n",
            "Loss: 0.0\n",
            "training error 0.0316145458697207, test error 0.04751674622458614\n",
            "Loss: 0.09558826763016448\n",
            "training error 0.03154065979184021, test error 0.04746868879458235\n",
            "Loss: 0.0\n",
            "training error 0.03148398367865354, test error 0.04726629481773138\n",
            "Loss: 0.0\n",
            "training error 0.03138451605240779, test error 0.0471106064328453\n",
            "Loss: 0.0\n",
            "training error 0.03132602842708671, test error 0.04703624153883773\n",
            "Loss: 0.0\n",
            "training error 0.03130513157030253, test error 0.04694431103453535\n",
            "Loss: 0.0\n",
            "training error 0.031208853354728696, test error 0.04670901929688971\n",
            "Loss: 0.0\n",
            "training error 0.031158073539475997, test error 0.04667948181051578\n",
            "Loss: 0.0\n",
            "training error 0.031143480798348257, test error 0.04634221983787354\n",
            "Loss: 0.0\n",
            "training error 0.031051035286410008, test error 0.04647761662458373\n",
            "Loss: 0.29216724443470454\n",
            "training error 0.0309514164210905, test error 0.04629313136589138\n",
            "Loss: 0.0\n",
            "training error 0.030956664734488517, test error 0.04618239821418522\n",
            "Loss: 0.0\n",
            "training error 0.030857998966955978, test error 0.04611922925005247\n",
            "Loss: 0.0\n",
            "training error 0.030784737140255525, test error 0.045978349864178196\n",
            "Loss: 0.0\n",
            "training error 0.03071571749495823, test error 0.04582462270032657\n",
            "Loss: 0.0\n",
            "training error 0.030697549666295433, test error 0.04577769994401269\n",
            "Loss: 0.0\n",
            "training error 0.030688980464373416, test error 0.04556905249632413\n",
            "Loss: 0.0\n",
            "training error 0.030572095892165915, test error 0.04564915134566366\n",
            "Loss: 0.1757746649351466\n",
            "training error 0.03054802366274533, test error 0.04553940620878705\n",
            "Loss: 0.0\n",
            "training error 0.030506772599367498, test error 0.04552929997201243\n",
            "Loss: 0.0\n",
            "training error 0.030431597494110632, test error 0.04538895596119941\n",
            "Loss: 0.0\n",
            "training error 0.030346386984762705, test error 0.04534938197395403\n",
            "Loss: 0.0\n",
            "training error 0.03030369523368674, test error 0.04544199670177692\n",
            "Loss: 0.20422489522808895\n",
            "training error 0.030251574022502184, test error 0.04534534815855051\n",
            "Loss: 0.0\n",
            "training error 0.03019166639998795, test error 0.045189323941037286\n",
            "Loss: 0.0\n",
            "training error 0.03016585617886911, test error 0.04516896592628744\n",
            "Loss: 0.0\n",
            "training error 0.030094295693301683, test error 0.045028603372347785\n",
            "Loss: 0.0\n",
            "training error 0.030041906934761137, test error 0.04493867125638137\n",
            "Loss: 0.0\n",
            "training error 0.03001223634726744, test error 0.044981411612472696\n",
            "Loss: 0.09510818832958456\n",
            "training error 0.029968485654149172, test error 0.04493601717653874\n",
            "Loss: 0.0\n",
            "training error 0.02994126661736847, test error 0.044946029418161934\n",
            "Loss: 0.02228110600870359\n",
            "training error 0.02986117973167441, test error 0.04502108840580732\n",
            "Loss: 0.18931635381562728\n",
            "training error 0.029805128502942282, test error 0.04494795615301715\n",
            "Loss: 0.026568835487816855\n",
            "training error 0.02975443731583494, test error 0.0447438647235624\n",
            "Loss: 0.0\n",
            "training error 0.029715866368562126, test error 0.04472198258571593\n",
            "Loss: 0.0\n",
            "training error 0.029701898842529644, test error 0.04476454094593035\n",
            "Loss: 0.09516206069992705\n",
            "training error 0.02960855612077399, test error 0.044717083657749804\n",
            "Loss: 0.0\n",
            "training error 0.029607058851840447, test error 0.04468576851406986\n",
            "Loss: 0.0\n",
            "training error 0.029549404731349375, test error 0.044671039986928174\n",
            "Loss: 0.0\n",
            "training error 0.029500671703654873, test error 0.044552311290239906\n",
            "Loss: 0.0\n",
            "training error 0.029457796471497284, test error 0.04443446323018764\n",
            "Loss: 0.0\n",
            "training error 0.02944610772589855, test error 0.04445949947566444\n",
            "Loss: 0.05634420595361789\n",
            "training error 0.029374133009273343, test error 0.04423213470207291\n",
            "Loss: 0.0\n",
            "training error 0.029350243301290446, test error 0.04417252671510755\n",
            "Loss: 0.0\n",
            "training error 0.029276633968953893, test error 0.044281905831447164\n",
            "Loss: 0.2476179754105079\n",
            "training error 0.029274950310370747, test error 0.043967658838875\n",
            "Loss: 0.0\n",
            "training error 0.02922908633091453, test error 0.04396996174729707\n",
            "Loss: 0.005237732649154836\n",
            "training error 0.02916922977045644, test error 0.04387715653321133\n",
            "Loss: 0.0\n",
            "training error 0.0291472784268174, test error 0.043831155782997226\n",
            "Loss: 0.0\n",
            "training error 0.029094058704994215, test error 0.04386540435442069\n",
            "Loss: 0.07813750473071313\n",
            "training error 0.02907304153719275, test error 0.04391212800933065\n",
            "Loss: 0.18473668988858716\n",
            "training error 0.029036575402538727, test error 0.04376151506632865\n",
            "Loss: 0.0\n",
            "training error 0.028971528874003213, test error 0.04388814617443865\n",
            "Loss: 0.2893663711552641\n",
            "training error 0.0289444270123122, test error 0.043876517217321775\n",
            "Loss: 0.26279289192527866\n",
            "training error 0.028905123553959754, test error 0.04378524225291905\n",
            "Loss: 0.05421929874784848\n",
            "training error 0.02886071085175749, test error 0.04375169158217921\n",
            "Loss: 0.0\n",
            "training error 0.02884701754477711, test error 0.04371946650855055\n",
            "Loss: 0.0\n",
            "training error 0.028815107131641024, test error 0.043743095663455175\n",
            "Loss: 0.05404721693025305\n",
            "training error 0.028768792656851276, test error 0.043584338537489775\n",
            "Loss: 0.0\n",
            "training error 0.028737941560998632, test error 0.04359875085589356\n",
            "Loss: 0.033067654316676354\n",
            "training error 0.028673125606518232, test error 0.04360552199182293\n",
            "Loss: 0.04860336314369551\n",
            "training error 0.028699963095520392, test error 0.043534643400568354\n",
            "Loss: 0.0\n",
            "training error 0.02861170258167703, test error 0.04355056948753709\n",
            "Loss: 0.03658255982987857\n",
            "training error 0.028595373444121518, test error 0.04359819187866804\n",
            "Loss: 0.14597220313710135\n",
            "training error 0.028726110489786204, test error 0.04337618821060543\n",
            "Loss: 0.0\n",
            "training error 0.02858614207687693, test error 0.043460686944692586\n",
            "Loss: 0.19480442513040153\n",
            "training error 0.028465612590474275, test error 0.04362363926309013\n",
            "Loss: 0.5704767124378174\n",
            "training error 0.028500788530030418, test error 0.043442403918571196\n",
            "Loss: 0.15265451091337479\n",
            "training error 0.028401831031860378, test error 0.04353500917675614\n",
            "Loss: 0.3661478168150101\n",
            "training error 0.02836800134850343, test error 0.04357686842585152\n",
            "Loss: 0.462650646644458\n",
            "training error 0.02837457380840512, test error 0.043381154401003215\n",
            "Loss: 0.01144911667587678\n",
            "training error 0.028345568606136254, test error 0.04325956647611551\n",
            "Loss: 0.0\n",
            "training error 0.028280557824209112, test error 0.04325843293252573\n",
            "Loss: 0.0\n",
            "training error 0.02824552840439489, test error 0.043291115730695405\n",
            "Loss: 0.07555243210186635\n",
            "training error 0.028251732135267275, test error 0.04320814978129135\n",
            "Loss: 0.0\n",
            "training error 0.028180701988638316, test error 0.04339235169084986\n",
            "Loss: 0.4263128842380226\n",
            "training error 0.028234246754342652, test error 0.043186029038638864\n",
            "Loss: 0.0\n",
            "training error 0.02813050030991876, test error 0.04307389934562748\n",
            "Loss: 0.0\n",
            "training error 0.028145106488411113, test error 0.043122837972664384\n",
            "Loss: 0.1136155021495\n",
            "training error 0.028109373964118754, test error 0.04316641459820204\n",
            "Loss: 0.21478262702017137\n",
            "training error 0.028046781219019268, test error 0.043122915294719\n",
            "Loss: 0.1137950123767828\n",
            "training error 0.028009367556171356, test error 0.04320928550785536\n",
            "Loss: 0.31431136786930214\n",
            "training error 0.027982890787772365, test error 0.043083095680733985\n",
            "Loss: 0.021350133714892472\n",
            "training error 0.027967507956121615, test error 0.04320770217468825\n",
            "Loss: 0.31063551499512787\n",
            "training error 0.027946401533359536, test error 0.04311267773803089\n",
            "Loss: 0.09002758745442208\n",
            "training error 0.028061971842875996, test error 0.0433864722357316\n",
            "Loss: 0.7256665750087432\n",
            "training error 0.027837112969496678, test error 0.04314490300349064\n",
            "Loss: 0.16484149088389355\n",
            "training error 0.02781779611347298, test error 0.04320569674743015\n",
            "Loss: 0.30597973205332973\n",
            "training error 0.027842029077091008, test error 0.04288438013119016\n",
            "Loss: 0.0\n",
            "training error 0.02776445554306627, test error 0.04299484475032372\n",
            "Loss: 0.257587072019283\n",
            "training error 0.02774620449391996, test error 0.042897510202057766\n",
            "Loss: 0.030617373569197248\n",
            "training error 0.02773619084355034, test error 0.04287563990000439\n",
            "Loss: 0.0\n",
            "training error 0.027756306986356328, test error 0.042787940880572924\n",
            "Loss: 0.0\n",
            "training error 0.027660901475677423, test error 0.04288364032507049\n",
            "Loss: 0.22365985024770385\n",
            "training error 0.027670650853725363, test error 0.042914747647548844\n",
            "Loss: 0.29636099416388006\n",
            "training error 0.027618045483737384, test error 0.04270602089600747\n",
            "Loss: 0.0\n",
            "training error 0.027585942783602716, test error 0.04264148851324733\n",
            "Loss: 0.0\n",
            "training error 0.02756610174948972, test error 0.04264093234528275\n",
            "Loss: 0.0\n",
            "training error 0.027599847302484232, test error 0.04282879445362997\n",
            "Loss: 0.4405675439411505\n",
            "training error 0.02750257048467982, test error 0.04281459453488355\n",
            "Loss: 0.407266398854933\n",
            "training error 0.02748745123880874, test error 0.04291861157748034\n",
            "Loss: 0.651203472637718\n",
            "training error 0.02745868901389259, test error 0.042863109621023285\n",
            "Loss: 0.5210422556933381\n",
            "training error 0.02741930404590773, test error 0.042891250655958474\n",
            "Loss: 0.5870376112998121\n",
            "training error 0.02740475738253757, test error 0.042950811610903104\n",
            "Loss: 0.7267178473282954\n",
            "training error 0.027377870374497563, test error 0.04275183860491519\n",
            "Loss: 0.2600934208810912\n",
            "training error 0.027368430732352848, test error 0.04295743785438968\n",
            "Loss: 0.7422574781996705\n",
            "training error 0.027324702834846937, test error 0.04278518178777086\n",
            "Loss: 0.33828866901890464\n",
            "training error 0.02729273554549118, test error 0.042907036855459914\n",
            "Loss: 0.6240588456706275\n",
            "training error 0.027307835455508314, test error 0.04285906192818775\n",
            "Loss: 0.5115497502228727\n",
            "training error 0.027243468479327546, test error 0.0428886620689532\n",
            "Loss: 0.5809669489974301\n",
            "training error 0.027281928945654534, test error 0.04288138600842522\n",
            "Loss: 0.5639033902809842\n",
            "training error 0.02719215076717782, test error 0.04278369390726639\n",
            "Loss: 0.3347993445069086\n",
            "training error 0.02718415276475528, test error 0.04295858707532709\n",
            "Loss: 0.7449525903236554\n",
            "training error 0.027126259217852902, test error 0.04300844900933996\n",
            "Loss: 0.8618870269562873\n",
            "training error 0.027154600889514317, test error 0.04291422379001072\n",
            "Loss: 0.6409133893110042\n",
            "training error 0.02709737663729013, test error 0.04308274644870947\n",
            "Loss: 1.0361267428421916\n",
            "training error 0.027054452201246332, test error 0.04315837087254217\n",
            "Loss: 1.213478455558814\n",
            "training error 0.027055150044717945, test error 0.04320642848409374\n",
            "Loss: 1.3261814592418153\n",
            "training error 0.026996986197902167, test error 0.043118425302336505\n",
            "Loss: 1.1197995231137847\n",
            "training error 0.0269858878955569, test error 0.04304308769827863\n",
            "Loss: 0.9431204499457113\n",
            "training error 0.027005828652592236, test error 0.043129617485999826\n",
            "Loss: 1.1460470347129492\n",
            "training error 0.026982069302392817, test error 0.04292291550321304\n",
            "Loss: 0.6612968863976665\n",
            "training error 0.026954427747571983, test error 0.043100668504293546\n",
            "Loss: 1.0781569110358546\n",
            "training error 0.026931874322465396, test error 0.04296806687684899\n",
            "Loss: 0.7671842841457499\n",
            "training error 0.0269271312210219, test error 0.04293217102408849\n",
            "Loss: 0.6830026052138116\n",
            "training error 0.02685500004407756, test error 0.04310902541084424\n",
            "Loss: 1.0977552314549\n",
            "training error 0.02685599706723398, test error 0.04306983780142726\n",
            "Loss: 1.0058538417299756\n",
            "training error 0.026833271215813102, test error 0.043036048293870525\n",
            "Loss: 0.9266118887559571\n",
            "training error 0.026801511034666752, test error 0.04298835095397977\n",
            "Loss: 0.8147537813756323\n",
            "training error 0.02680478063213523, test error 0.042600770785661456\n",
            "Loss: 0.0\n",
            "training error 0.026763671765816392, test error 0.04269023694787933\n",
            "Loss: 0.21001066545958125\n",
            "training error 0.026741322587165405, test error 0.04276965532048927\n",
            "Loss: 0.39643539709064424\n",
            "training error 0.026704426454600975, test error 0.04282834638906863\n",
            "Loss: 0.5342053657953372\n",
            "training error 0.026746279832855883, test error 0.042885171681186614\n",
            "Loss: 0.6675956567923969\n",
            "training error 0.02667535345606546, test error 0.04253903683140397\n",
            "Loss: 0.0\n",
            "training error 0.026719225245925963, test error 0.042652681934941136\n",
            "Loss: 0.26715485822488283\n",
            "training error 0.02664591057955308, test error 0.042480895666241233\n",
            "Loss: 0.0\n",
            "training error 0.02664663285748224, test error 0.04271025325441331\n",
            "Loss: 0.5399076092323263\n",
            "training error 0.026598750666764308, test error 0.042664866153258575\n",
            "Loss: 0.43306640345519565\n",
            "training error 0.026565094527400766, test error 0.04279481834168316\n",
            "Loss: 0.7389737681340769\n",
            "training error 0.02652752113923123, test error 0.042818606785038175\n",
            "Loss: 0.7949717478892948\n",
            "training error 0.02650632700934436, test error 0.04282669293175323\n",
            "Loss: 0.8140065318509526\n",
            "training error 0.026531125415521505, test error 0.042894052380103104\n",
            "Loss: 0.9725706282370128\n",
            "training error 0.02650157724000426, test error 0.04281621841979874\n",
            "Loss: 0.7893495377122806\n",
            "training error 0.026542175351236427, test error 0.04304343981670016\n",
            "Loss: 1.324228554121487\n",
            "training error 0.026462952913249974, test error 0.0426518207236486\n",
            "Loss: 0.4023574708741373\n",
            "training error 0.026421679774144523, test error 0.0426574098345994\n",
            "Loss: 0.41551423431600565\n",
            "training error 0.02639746411573836, test error 0.042719538283557984\n",
            "Loss: 0.5617645616318612\n",
            "training error 0.026371078589597245, test error 0.04264367325843432\n",
            "Loss: 0.38317834320626964\n",
            "training error 0.026392370805140338, test error 0.04292154697887175\n",
            "Loss: 1.037292895358366\n",
            "training error 0.02640969715876811, test error 0.042746863469639955\n",
            "Loss: 0.6260880314020278\n",
            "training error 0.02642126540797464, test error 0.04300609789995089\n",
            "Loss: 1.2363257070566513\n",
            "training error 0.026295391213871568, test error 0.04282626398907299\n",
            "Loss: 0.8129968010684108\n",
            "training error 0.02629545949488319, test error 0.042945957588870855\n",
            "Loss: 1.094755454977836\n",
            "training error 0.026247504309046623, test error 0.04293661817475332\n",
            "Loss: 1.0727704803885185\n",
            "training error 0.026263425387364706, test error 0.04289961532844251\n",
            "Loss: 0.9856658049091527\n",
            "training error 0.02632430019942467, test error 0.04292240491910094\n",
            "Loss: 1.0393124860843317\n",
            "training error 0.026229039695469046, test error 0.04284657718243345\n",
            "Loss: 0.8608140446596524\n",
            "training error 0.026272765362211462, test error 0.0432133396251946\n",
            "Loss: 1.7241725897399807\n",
            "training error 0.026148271622545866, test error 0.043017139922355\n",
            "Loss: 1.262318620414371\n",
            "training error 0.0261453455352715, test error 0.04302804205030553\n",
            "Loss: 1.2879822223218929\n",
            "training error 0.02614725325684889, test error 0.04307709479538156\n",
            "Loss: 1.4034523514392827\n",
            "training error 0.026106899121093378, test error 0.04293431312975387\n",
            "Loss: 1.0673444060007364\n",
            "training error 0.02609714741595443, test error 0.04289769420651809\n",
            "Loss: 0.9811434851833223\n",
            "training error 0.026094039526380484, test error 0.04285205242853102\n",
            "Loss: 0.8737027703131339\n",
            "training error 0.02607738697395118, test error 0.04303688963221237\n",
            "Loss: 1.3088094242160064\n",
            "training error 0.026073781716651184, test error 0.04284022984364519\n",
            "Loss: 0.8458724133952655\n",
            "training error 0.026015229845569896, test error 0.04303422894680578\n",
            "Loss: 1.3025461725475695\n",
            "training error 0.025988484051030044, test error 0.04299268544095582\n",
            "Loss: 1.2047527875484576\n",
            "training error 0.026047341320175988, test error 0.042853850026581895\n",
            "Loss: 0.8779343149232233\n",
            "training error 0.026000627967354504, test error 0.04303694295696732\n",
            "Loss: 1.3089349506535175\n",
            "training error 0.02595596883280382, test error 0.04290040416737328\n",
            "Loss: 0.9875227312248436\n",
            "training error 0.025917906092777304, test error 0.04296618116938357\n",
            "Loss: 1.1423617499853655\n",
            "training error 0.02590031384115766, test error 0.043003602239430616\n",
            "Loss: 1.2304509238602712\n",
            "training error 0.02604272327557865, test error 0.04283089379312955\n",
            "Loss: 0.8238953567225593\n",
            "training error 0.025859389776351663, test error 0.04309658041459135\n",
            "Loss: 1.4493214860330506\n",
            "training error 0.02586231631102788, test error 0.043116230534038735\n",
            "Loss: 1.4955778540761466\n",
            "training error 0.025921380094199684, test error 0.04249283805399921\n",
            "Loss: 0.0281123727988275\n",
            "training error 0.025971690548462083, test error 0.04238018294264801\n",
            "Loss: 0.0\n",
            "training error 0.025825022333456914, test error 0.042673839650087315\n",
            "Loss: 0.6929104290009924\n",
            "training error 0.025810117971315764, test error 0.04267338740244797\n",
            "Loss: 0.6918433084556197\n",
            "training error 0.025781130423724815, test error 0.042763769440968956\n",
            "Loss: 0.9051081701087593\n",
            "training error 0.02580303119952366, test error 0.04277727170239703\n",
            "Loss: 0.9369680170715267\n",
            "training error 0.025757809185686643, test error 0.04254225823023288\n",
            "Loss: 0.382431779032677\n",
            "training error 0.02577689389017893, test error 0.042488429858243866\n",
            "Loss: 0.2554187077067116\n",
            "training error 0.025752667802073442, test error 0.0427120125159213\n",
            "Loss: 0.7829828713159293\n",
            "training error 0.025706945449997378, test error 0.04287943244272784\n",
            "Loss: 1.1780258257862108\n",
            "training error 0.025695428206713052, test error 0.04295103271519773\n",
            "Loss: 1.3469733562081032\n",
            "training error 0.025695237127121157, test error 0.04298070897925864\n",
            "Loss: 1.416997273049314\n",
            "training error 0.0256581889058021, test error 0.0429102255711255\n",
            "Loss: 1.2506850883460707\n",
            "training error 0.025657704088015133, test error 0.04290171147084223\n",
            "Loss: 1.2305952735031545\n",
            "training error 0.025623065216206862, test error 0.042904240040569024\n",
            "Loss: 1.2365616699441917\n",
            "training error 0.025594351924392803, test error 0.042871907449995625\n",
            "Loss: 1.1602699025935026\n",
            "training error 0.02560480409732437, test error 0.04290697212756785\n",
            "Loss: 1.2430082843972734\n",
            "training error 0.025568975901253784, test error 0.04288060347623608\n",
            "Loss: 1.1807889887244682\n",
            "training error 0.025585665460465833, test error 0.04270133833584883\n",
            "Loss: 0.7577961464570127\n",
            "training error 0.025561994683330105, test error 0.04264694145501635\n",
            "Loss: 0.6294416254156721\n",
            "training error 0.025573991926431897, test error 0.04292104365750812\n",
            "Loss: 1.276211373584779\n",
            "training error 0.025524611763294302, test error 0.04288809493106317\n",
            "Loss: 1.198465776097546\n",
            "training error 0.025499220881273883, test error 0.042884069857555315\n",
            "Loss: 1.1889682392102863\n",
            "training error 0.025484938925378117, test error 0.042832607377017724\n",
            "Loss: 1.0675377097403516\n",
            "training error 0.025474748587496904, test error 0.04292963887961635\n",
            "Loss: 1.2964926029505497\n",
            "training error 0.025465868584430153, test error 0.04291792366354045\n",
            "Loss: 1.268849456407839\n",
            "training error 0.02545469031783013, test error 0.043149437666509534\n",
            "Loss: 1.815128370027419\n",
            "training error 0.025422702267977035, test error 0.04312491005452854\n",
            "Loss: 1.7572531786574475\n",
            "training error 0.02547440115178345, test error 0.04324688044812322\n",
            "Loss: 2.045053714487466\n",
            "training error 0.025424076038797785, test error 0.04308464908449734\n",
            "Loss: 1.6622536594584014\n",
            "training error 0.025413428075631853, test error 0.042894416704707984\n",
            "Loss: 1.2133825914717455\n",
            "training error 0.025378686530755243, test error 0.04309065495645862\n",
            "Loss: 1.6764250753048326\n",
            "training error 0.025417343917128177, test error 0.04316671561023899\n",
            "Loss: 1.8558972920323935\n",
            "training error 0.02538962121853303, test error 0.043063725658441084\n",
            "Loss: 1.6128828814120366\n",
            "training error 0.025365958010521935, test error 0.04290445498113545\n",
            "Loss: 1.2370688422863152\n",
            "training error 0.025291011483587367, test error 0.04306389920202606\n",
            "Loss: 1.613292373709907\n",
            "training error 0.02528944102281801, test error 0.04304182698238992\n",
            "Loss: 1.5612109099134575\n",
            "training error 0.025289569425523857, test error 0.043164161853403604\n",
            "Loss: 1.8498714642561476\n",
            "training error 0.025262579456996784, test error 0.04304881410070289\n",
            "Loss: 1.5776976681759924\n",
            "training error 0.02533590268876321, test error 0.043308544215853347\n",
            "Loss: 2.1905551338975204\n",
            "training error 0.025248713554005594, test error 0.04287717404052314\n",
            "Loss: 1.1726969148474264\n",
            "training error 0.025222312381061634, test error 0.04284966686639292\n",
            "Loss: 1.1077911682926045\n",
            "training error 0.025236365829855376, test error 0.04290995561223412\n",
            "Loss: 1.2500480951274762\n",
            "training error 0.02518469193932276, test error 0.04288780240214313\n",
            "Loss: 1.1977755267882362\n",
            "training error 0.02520722046902562, test error 0.04296485401609191\n",
            "Loss: 1.379586006589717\n",
            "training error 0.02517380418803096, test error 0.04308319853940625\n",
            "Loss: 1.6588309628337683\n",
            "training error 0.025149149296082255, test error 0.04308975865860531\n",
            "Loss: 1.6743101768049273\n",
            "training error 0.02517072430969313, test error 0.04304853044850295\n",
            "Loss: 1.5770283643168792\n",
            "training error 0.025119381125549813, test error 0.04317785967682029\n",
            "Loss: 1.8821927579967257\n",
            "training error 0.025095594781579196, test error 0.04315481089906879\n",
            "Loss: 1.82780701411569\n",
            "training error 0.025179451265019696, test error 0.043236107099773644\n",
            "Loss: 2.0196329928163115\n",
            "training error 0.02513804383297342, test error 0.042702668851147395\n",
            "Loss: 0.7609356215752872\n",
            "training error 0.02514203307244433, test error 0.04271319095597067\n",
            "Loss: 0.78576351067976\n",
            "training error 0.025116143790959604, test error 0.042884775603637\n",
            "Loss: 1.1906335130073442\n",
            "training error 0.025092954747631454, test error 0.04261206575390994\n",
            "Loss: 0.5471491512335502\n",
            "training error 0.025051526450928317, test error 0.042811482647884\n",
            "Loss: 1.017691938280807\n",
            "training error 0.02504913789068213, test error 0.04264089318962244\n",
            "Loss: 0.6151701783053687\n",
            "training error 0.025032744697254944, test error 0.04257817438012351\n",
            "Loss: 0.4671792893000015\n",
            "training error 0.02501924868039743, test error 0.04281656700940783\n",
            "Loss: 1.0296889641801865\n",
            "training error 0.024978826356318064, test error 0.042784271901585985\n",
            "Loss: 0.9534856408827119\n",
            "training error 0.024970353653934012, test error 0.04279112946494119\n",
            "Loss: 0.9696667021218408\n",
            "training error 0.0250780297781222, test error 0.04307642859261264\n",
            "Loss: 1.6428566410551904\n",
            "training error 0.02499342185583103, test error 0.042844794375775576\n",
            "Loss: 1.0962940715860237\n",
            "training error 0.024975364694171182, test error 0.04313345023183279\n",
            "Loss: 1.7774045246670145\n",
            "training error 0.024970820705053614, test error 0.0429813791795559\n",
            "Loss: 1.4185786732479944\n",
            "training error 0.024902582839107356, test error 0.043118273504453075\n",
            "Loss: 1.7415936188947212\n",
            "training error 0.024925645557544708, test error 0.043121443693638115\n",
            "Loss: 1.749073976375315\n",
            "training error 0.02495103072860755, test error 0.042965602251474305\n",
            "Loss: 1.3813515378603425\n",
            "training error 0.024877376591499748, test error 0.043116136177575606\n",
            "Loss: 1.7365503964990925\n",
            "training error 0.024873108027461895, test error 0.04329105758974548\n",
            "Loss: 2.1492938063295552\n",
            "training error 0.024855499526002002, test error 0.043058723526550415\n",
            "Loss: 1.6010798840124352\n",
            "training error 0.024832493401366163, test error 0.04320312278674565\n",
            "Loss: 1.9418034254625738\n",
            "training error 0.024814310191206362, test error 0.043223241055601894\n",
            "Loss: 1.9892743598930895\n",
            "training error 0.024817171231436123, test error 0.04335998680994653\n",
            "Loss: 2.3119387394444857\n",
            "training error 0.02481365455782013, test error 0.04332495524588686\n",
            "Loss: 2.2292784920664\n",
            "training error 0.024818698760068213, test error 0.043296879877062884\n",
            "Loss: 2.163032037061785\n",
            "training error 0.02476529714104269, test error 0.043293107096062745\n",
            "Loss: 2.154129807910854\n",
            "training error 0.02475052891096706, test error 0.04319647911276717\n",
            "Loss: 1.9261270561852673\n",
            "training error 0.02476809716666325, test error 0.04322330472919257\n",
            "Loss: 1.9894246036774588\n",
            "training error 0.024748827340122855, test error 0.04336754744207843\n",
            "Loss: 2.329778757129475\n",
            "training error 0.02472734308780577, test error 0.04343199297202499\n",
            "Loss: 2.4818440043082646\n",
            "training error 0.02476704633276851, test error 0.04330019716246732\n",
            "Loss: 2.1708594818109717\n",
            "training error 0.02472102746636665, test error 0.04348284639706834\n",
            "Loss: 2.601837410453234\n",
            "training error 0.0247722065503891, test error 0.043626654282912375\n",
            "Loss: 2.9411655488867927\n",
            "training error 0.02465543998452293, test error 0.04339632245349323\n",
            "Loss: 2.3976760841743827\n",
            "training error 0.024675792761035095, test error 0.043439389635441604\n",
            "Loss: 2.4992971224947036\n",
            "training error 0.0247176714726346, test error 0.04360213968404487\n",
            "Loss: 2.8833210631735495\n",
            "training error 0.02464924262353414, test error 0.0435271099307503\n",
            "Loss: 2.7062813524292517\n",
            "training error 0.024617179989813328, test error 0.043403052794643494\n",
            "Loss: 2.4135569527382827\n",
            "training error 0.02465028761960269, test error 0.043434041996024464\n",
            "Loss: 2.4866788678156837\n",
            "training error 0.024692463909558062, test error 0.043414297106700576\n",
            "Loss: 2.440088957265729\n",
            "training error 0.024615531759291757, test error 0.04316555926052362\n",
            "Loss: 1.8531687768748917\n",
            "training error 0.024580069340990805, test error 0.04332974077426119\n",
            "Loss: 2.2405703932382393\n",
            "training error 0.024654168539219222, test error 0.04343291920103258\n",
            "Loss: 2.484029528162268\n",
            "training error 0.02455393093792369, test error 0.04331384745386076\n",
            "Loss: 2.20306861930315\n",
            "training error 0.024543953026410277, test error 0.043057158423122806\n",
            "Loss: 1.5973868762929389\n",
            "training error 0.024538430832329403, test error 0.04299821860557297\n",
            "Loss: 1.458312871752665\n",
            "training error 0.024555116904493565, test error 0.04301619096387194\n",
            "Loss: 1.5007203297933591\n",
            "training error 0.02449648864970572, test error 0.043110117534608756\n",
            "Loss: 1.7223488462722036\n",
            "training error 0.024532079610348347, test error 0.043181339683424066\n",
            "Loss: 1.890404158614034\n",
            "training error 0.024509827991695255, test error 0.04284343623472292\n",
            "Loss: 1.0930894109207\n",
            "training error 0.024541746070581395, test error 0.04284480480675638\n",
            "Loss: 1.0963186844594963\n",
            "training error 0.024472013499653868, test error 0.04297902294319914\n",
            "Loss: 1.413018913489661\n",
            "training error 0.024455046435245254, test error 0.04288595943212315\n",
            "Loss: 1.1934268668910564\n",
            "training error 0.024474824441306917, test error 0.0427234360951846\n",
            "Loss: 0.8099378735601626\n",
            "training error 0.02445420939609189, test error 0.04273811685029442\n",
            "Loss: 0.8445784864373751\n",
            "training error 0.024428329451725694, test error 0.04279358572370969\n",
            "Loss: 0.9754624741028728\n",
            "training error 0.024419655243037443, test error 0.042924763692820236\n",
            "Loss: 1.2849891443583283\n",
            "training error 0.02446303241209758, test error 0.043014387269694386\n",
            "Loss: 1.4964643449147719\n",
            "training error 0.024426299767582822, test error 0.04313738660396575\n",
            "Loss: 1.786692762375397\n",
            "training error 0.024397825952198193, test error 0.043181719422669546\n",
            "Loss: 1.8913001888317238\n",
            "training error 0.024421263302341803, test error 0.04268530886859765\n",
            "Loss: 0.7199731213113347\n",
            "training error 0.02439447122959384, test error 0.0430169107952991\n",
            "Loss: 1.5024188392786408\n",
            "training error 0.024426696299696266, test error 0.042779788931049244\n",
            "Loss: 0.94290765318783\n",
            "training error 0.024336645501802746, test error 0.0430378597372865\n",
            "Loss: 1.5518498245477241\n",
            "training error 0.02431068244523203, test error 0.0431362768013999\n",
            "Loss: 1.7840740795647125\n",
            "training error 0.02431308811658449, test error 0.04320258136427523\n",
            "Loss: 1.9405258885742738\n",
            "training error 0.02431512402203689, test error 0.043297079261286925\n",
            "Loss: 2.163502502761072\n",
            "training error 0.024298887894170264, test error 0.04314603218057438\n",
            "Loss: 1.807092807888\n",
            "training error 0.02432847456442578, test error 0.04312978590987429\n",
            "Loss: 1.7687582147549952\n",
            "training error 0.024284488937419323, test error 0.04335964674910704\n",
            "Loss: 2.3111363341317137\n",
            "training error 0.024300683345983388, test error 0.04321106673671782\n",
            "Loss: 1.9605479173938845\n",
            "training error 0.024317319823865823, test error 0.04290056229148462\n",
            "Loss: 1.227883677474506\n",
            "training error 0.024300328536133047, test error 0.043324836729006624\n",
            "Loss: 2.2289988404179173\n",
            "training error 0.024293405683227198, test error 0.043363181016864726\n",
            "Loss: 2.3194757690097267\n",
            "training error 0.024230966631778126, test error 0.04310413748131516\n",
            "Loss: 1.7082383519836597\n",
            "training error 0.024226957145828006, test error 0.04324103935821734\n",
            "Loss: 2.0312711172915554\n",
            "training error 0.024197899510720707, test error 0.043325162036631185\n",
            "Loss: 2.2297664341420775\n",
            "training error 0.024211834832058562, test error 0.04338709254347598\n",
            "Loss: 2.3758972494068553\n",
            "training error 0.02421427697755033, test error 0.04316938414141501\n",
            "Loss: 1.8621939405853993\n",
            "training error 0.024186324565034823, test error 0.04334017842970381\n",
            "Loss: 2.265199016141417\n",
            "training error 0.024163233569966195, test error 0.043360669803523814\n",
            "Loss: 2.313550326582292\n",
            "training error 0.024148561215683807, test error 0.043276423817637986\n",
            "Loss: 2.114764054234586\n",
            "training error 0.024139077740990573, test error 0.04335043573601854\n",
            "Loss: 2.2894020884325794\n",
            "training error 0.024130029780025007, test error 0.043376449581043544\n",
            "Loss: 2.350784185485355\n",
            "training error 0.024122405780641867, test error 0.043410045396380534\n",
            "Loss: 2.430056649652057\n",
            "training error 0.0241508694683298, test error 0.0432685364393901\n",
            "Loss: 2.096153048570537\n",
            "training error 0.02412987041814943, test error 0.043538694061237136\n",
            "Loss: 2.733615190281058\n",
            "training error 0.024125792489058225, test error 0.04360121813952029\n",
            "Loss: 2.8811465927947255\n",
            "training error 0.024080824564483145, test error 0.04342416799367554\n",
            "Loss: 2.463380237032786\n",
            "training error 0.024078687795772616, test error 0.04359078216400379\n",
            "Loss: 2.856521933834144\n",
            "training error 0.024068320701825553, test error 0.0434646216186227\n",
            "Loss: 2.5588343435001937\n",
            "training error 0.024104268274534116, test error 0.04332370087060663\n",
            "Loss: 2.2263186764329257\n",
            "training error 0.02407226774929199, test error 0.04319322806369499\n",
            "Loss: 1.9184559022485725\n",
            "training error 0.024042443442589184, test error 0.04323106857390079\n",
            "Loss: 2.0077441204165325\n",
            "training error 0.024039768429756355, test error 0.043416204863056876\n",
            "Loss: 2.4445904865745494\n",
            "training error 0.02402466623488965, test error 0.043417454143100706\n",
            "Loss: 2.4475382795218437\n",
            "training error 0.024034421681185615, test error 0.043347722450797384\n",
            "Loss: 2.2829998385299977\n",
            "training error 0.02400824553758489, test error 0.043567625366655635\n",
            "Loss: 2.8018813076256865\n",
            "training error 0.024084572583440152, test error 0.04306792084126956\n",
            "Loss: 1.6227818071296252\n",
            "training error 0.023995153817723408, test error 0.043228374563011644\n",
            "Loss: 2.0013873501005586\n",
            "training error 0.02397185743284667, test error 0.04322935681519933\n",
            "Loss: 2.0037050658806344\n",
            "training error 0.024053352954997324, test error 0.04321658508991044\n",
            "Loss: 1.9735689871709816\n",
            "training error 0.023961274227679276, test error 0.0431144355586234\n",
            "Loss: 1.7325376272420367\n",
            "training error 0.02402130355156871, test error 0.04314012950694641\n",
            "Loss: 1.7931648981478254\n",
            "training error 0.02395817432627378, test error 0.04340600581647307\n",
            "Loss: 2.4205248835600157\n",
            "training error 0.024016019536526257, test error 0.04324387847061664\n",
            "Loss: 2.037970268173339\n",
            "training error 0.023943878059609562, test error 0.04376580553197336\n",
            "Loss: 3.2695059178967645\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9606481481481481 0.04238018294264801\n",
            "training error 0.12504230176505654, test error 0.2501096696400113\n",
            "training error 0.1250189741400773, test error 0.250125717938614\n",
            "training error 0.12535028866757034, test error 0.250108528362774\n",
            "training error 0.12498933611297723, test error 0.2501372543575436\n",
            "training error 0.12506406874734416, test error 0.25012097917691495\n",
            "training error 0.1249781872778183, test error 0.2502361966707112\n",
            "training error 0.12512090856547103, test error 0.2502335590053515\n",
            "training error 0.12502765213789763, test error 0.2503883579500522\n",
            "training error 0.1250859349120094, test error 0.25032973416722865\n",
            "training error 0.12503243895528085, test error 0.2504213035178257\n",
            "training error 0.12500218416281442, test error 0.25040654251021954\n",
            "training error 0.1249873208598767, test error 0.25044644321135107\n",
            "training error 0.12496652598495692, test error 0.25045576956110527\n",
            "training error 0.12500247992492367, test error 0.2504723658494694\n",
            "training error 0.12503767822445952, test error 0.2503721980282715\n",
            "training error 0.12512713803045367, test error 0.2504736850988843\n",
            "training error 0.12497985222543122, test error 0.2503534174278884\n",
            "training error 0.12500263586512333, test error 0.25034619490903576\n",
            "training error 0.12497773548251004, test error 0.25037538725670944\n",
            "training error 0.12507659842672436, test error 0.2503496913207652\n",
            "training error 0.12498255864275107, test error 0.2503984608726646\n",
            "training error 0.12502740433994253, test error 0.25040750674205536\n",
            "training error 0.12496979548844124, test error 0.25043042694959444\n",
            "training error 0.12503725823189527, test error 0.2503930800100116\n",
            "training error 0.1253208007607411, test error 0.2505892513198219\n",
            "training error 0.12517647589621825, test error 0.25036807432732205\n",
            "training error 0.12504143933237005, test error 0.2504642404897542\n",
            "training error 0.12504927753706813, test error 0.25053555444797293\n",
            "training error 0.12504218071028475, test error 0.25047217623488505\n",
            "training error 0.12499363840592746, test error 0.25056358023742603\n",
            "training error 0.12497390115475093, test error 0.2504716368278357\n",
            "training error 0.12497270968685048, test error 0.2504505392839078\n",
            "training error 0.12496510157520686, test error 0.25047093137024634\n",
            "training error 0.12496051738989546, test error 0.25043689135302594\n",
            "training error 0.12496602311636272, test error 0.2505055094127044\n",
            "training error 0.12511495563546915, test error 0.2506277486760951\n",
            "training error 0.12495138455225466, test error 0.25054019619880474\n",
            "training error 0.1249704839533209, test error 0.2504910756260997\n",
            "training error 0.12500136792346558, test error 0.2505329840584201\n",
            "training error 0.1250308673432657, test error 0.25049249237617294\n",
            "training error 0.12498200707179655, test error 0.2505442326506198\n",
            "training error 0.12501758949432137, test error 0.25070364439336607\n",
            "training error 0.12501207784783752, test error 0.25071307541036325\n",
            "training error 0.1249656361537713, test error 0.25062984320749343\n",
            "training error 0.12496425582163145, test error 0.2507230426922732\n",
            "training error 0.12512835833653946, test error 0.25059685759089606\n",
            "training error 0.12496908628909417, test error 0.25073890644793634\n",
            "training error 0.12498732896305761, test error 0.25069104280623694\n",
            "training error 0.12497853873227843, test error 0.25079466939231604\n",
            "training error 0.12510913413451588, test error 0.2506754244810523\n",
            "Loss: 0.22666005113429755\n",
            "training error 0.12500157755479682, test error 0.250664151863063\n",
            "Loss: 0.22215296052725542\n",
            "training error 0.12497052787948847, test error 0.2506341024349336\n",
            "Loss: 0.21013840495567315\n",
            "training error 0.1250583525699357, test error 0.2505482400933646\n",
            "Loss: 0.1758083714573866\n",
            "training error 0.1249809343860698, test error 0.2505817919890583\n",
            "Loss: 0.1892233061312698\n",
            "training error 0.12502923179817121, test error 0.2507109017500851\n",
            "Loss: 0.24084480095671879\n",
            "training error 0.124955266817843, test error 0.2505878410717887\n",
            "Loss: 0.19164188928395554\n",
            "training error 0.12501678803389285, test error 0.2505473504525985\n",
            "Loss: 0.1754526695659342\n",
            "training error 0.12511740421586084, test error 0.25050999401721846\n",
            "Loss: 0.16051657937157238\n",
            "training error 0.12497932736021647, test error 0.2506509330558007\n",
            "Loss: 0.21686773201112786\n",
            "training error 0.12514537192081296, test error 0.2505026202175553\n",
            "Loss: 0.1575683393769456\n",
            "training error 0.12500849790238097, test error 0.2506071901654815\n",
            "Loss: 0.19937816833828226\n",
            "training error 0.12499752809168849, test error 0.25066536271838175\n",
            "Loss: 0.22263709248655505\n",
            "training error 0.1250721407805435, test error 0.25064907862322683\n",
            "Loss: 0.21612628085547758\n",
            "training error 0.12499291025994444, test error 0.2507144544654903\n",
            "Loss: 0.2422652704738848\n",
            "training error 0.12494272933321585, test error 0.25068587678258347\n",
            "Loss: 0.23083915754047446\n",
            "training error 0.12494403232692311, test error 0.2506825307765655\n",
            "Loss: 0.22950133589965116\n",
            "training error 0.1249593702607383, test error 0.25062733207362714\n",
            "Loss: 0.20743143556489763\n",
            "training error 0.12494821106058984, test error 0.2506386311309871\n",
            "Loss: 0.2119490973311544\n",
            "training error 0.1250287475054355, test error 0.2506385014604086\n",
            "Loss: 0.21189725160668527\n",
            "training error 0.1249519515080829, test error 0.2506727327694461\n",
            "Loss: 0.22558383369231105\n",
            "training error 0.12493702912032402, test error 0.25069845636042526\n",
            "Loss: 0.23586880523946974\n",
            "training error 0.1249360745897199, test error 0.2506474322867547\n",
            "Loss: 0.21546803202130427\n",
            "training error 0.12499081035836178, test error 0.2505918715639209\n",
            "Loss: 0.1932533865641739\n",
            "training error 0.12509083154985834, test error 0.2505818493089535\n",
            "Loss: 0.1892462241403381\n",
            "training error 0.12494204634047311, test error 0.25054951313531065\n",
            "Loss: 0.1763173672738816\n",
            "training error 0.12496412984255525, test error 0.2505583908513805\n",
            "Loss: 0.1798669127963448\n",
            "training error 0.12501679273594415, test error 0.25053680936069134\n",
            "Loss: 0.17123806242069772\n",
            "training error 0.12494503865020773, test error 0.2505320683217742\n",
            "Loss: 0.16934246975612854\n",
            "training error 0.12494341504961899, test error 0.25053612435973427\n",
            "Loss: 0.1709641809335194\n",
            "training error 0.12501453090430809, test error 0.25058697443710515\n",
            "Loss: 0.1912953858323574\n",
            "training error 0.12500522105409628, test error 0.250700870162352\n",
            "Loss: 0.23683390704647422\n",
            "training error 0.12503468875852627, test error 0.25067006109067375\n",
            "Loss: 0.22451562590672935\n",
            "training error 0.1249829978750783, test error 0.2506149016911851\n",
            "Loss: 0.20246144012996403\n",
            "training error 0.12496307402769397, test error 0.25057490110268854\n",
            "Loss: 0.18646814763474406\n",
            "training error 0.1249809281595982, test error 0.250641607422602\n",
            "Loss: 0.2131390973820757\n",
            "training error 0.12495813648281345, test error 0.25065820782203413\n",
            "Loss: 0.21977637582308862\n",
            "training error 0.12500275640462324, test error 0.25066755028969456\n",
            "Loss: 0.2235117413148524\n",
            "training error 0.1250383849770867, test error 0.2505712383290712\n",
            "Loss: 0.1850036739355243\n",
            "training error 0.12492835901746392, test error 0.25064114090261735\n",
            "Loss: 0.21295257036211268\n",
            "training error 0.12504479886212258, test error 0.25054402594923075\n",
            "Loss: 0.17412344525298273\n",
            "training error 0.12524720312031515, test error 0.2507882246071871\n",
            "Loss: 0.271760522866793\n",
            "training error 0.12497257351532838, test error 0.25071794525096736\n",
            "Loss: 0.24366097876895587\n",
            "training error 0.12503289438001997, test error 0.25068163731670856\n",
            "Loss: 0.22914410703473997\n",
            "training error 0.12495862713785803, test error 0.2506148310820596\n",
            "Loss: 0.2024332087353864\n",
            "training error 0.1250176930425726, test error 0.2507253489024603\n",
            "Loss: 0.24662115431410836\n",
            "training error 0.12494942866119656, test error 0.2506923420560017\n",
            "Loss: 0.23342414473006556\n",
            "training error 0.12503361646176858, test error 0.25059070960814406\n",
            "Loss: 0.19278880593416314\n",
            "training error 0.12490765616574526, test error 0.2506205243644059\n",
            "Loss: 0.20470953349072118\n",
            "training error 0.12498097225319622, test error 0.2507145490478692\n",
            "Loss: 0.24230308700878833\n",
            "training error 0.12494033305664462, test error 0.2507358806556405\n",
            "Loss: 0.2508320275894649\n",
            "training error 0.12491767258193735, test error 0.2507129321129553\n",
            "Loss: 0.2416565936946613\n",
            "training error 0.12491704394048357, test error 0.250811196442005\n",
            "Loss: 0.28094526957187504\n",
            "training error 0.1251282396401258, test error 0.25092355806871053\n",
            "Loss: 0.32587041764300384\n",
            "training error 0.12488894951942668, test error 0.250830711736639\n",
            "Loss: 0.2887480001551701\n",
            "training error 0.12491188146332996, test error 0.25079391294392267\n",
            "Loss: 0.2740348702362505\n",
            "training error 0.12497583235876841, test error 0.2507112596278666\n",
            "Loss: 0.240987889952482\n",
            "training error 0.1249204506722995, test error 0.25062647754489575\n",
            "Loss: 0.20708977239292103\n",
            "training error 0.12494602813440818, test error 0.25063191543347313\n",
            "Loss: 0.20926398396938772\n",
            "training error 0.12496718988158115, test error 0.25054702894272807\n",
            "Loss: 0.17532412142222675\n",
            "training error 0.12503874184374583, test error 0.2506495029476054\n",
            "Loss: 0.21629593695691174\n",
            "training error 0.12493390043718555, test error 0.2506061130719308\n",
            "Loss: 0.19894751786917464\n",
            "training error 0.12496474217286953, test error 0.2505932814824767\n",
            "Loss: 0.19381710926691387\n",
            "training error 0.12488885343997977, test error 0.25062259211196325\n",
            "Loss: 0.2055362736146371\n",
            "training error 0.12499182495716464, test error 0.2505785551792108\n",
            "Loss: 0.187929144005472\n",
            "training error 0.12492253318206623, test error 0.2507116702291198\n",
            "Loss: 0.2411520591856675\n",
            "training error 0.12490523195186899, test error 0.2506003558744649\n",
            "Loss: 0.19664563815973146\n",
            "training error 0.12490265645814319, test error 0.2506853785607032\n",
            "Loss: 0.23063995526475534\n",
            "training error 0.12488578105561916, test error 0.25069557895682676\n",
            "Loss: 0.23471834323109952\n",
            "training error 0.1249726202957833, test error 0.25069899545676705\n",
            "Loss: 0.23608435020521235\n",
            "training error 0.12489860063720606, test error 0.2506488335287697\n",
            "Loss: 0.2160282856136897\n",
            "training error 0.12488068481389641, test error 0.2507052366455011\n",
            "Loss: 0.23857974241550117\n",
            "training error 0.12487185096002182, test error 0.25071296070654814\n",
            "Loss: 0.24166802616880645\n",
            "training error 0.12490302614851248, test error 0.25081252982122415\n",
            "Loss: 0.28147838982486295\n",
            "training error 0.12493891338721556, test error 0.25067297835493385\n",
            "Loss: 0.22568202526112024\n",
            "training error 0.12493401279120599, test error 0.2507876100651532\n",
            "Loss: 0.2715148127193201\n",
            "training error 0.12487663397205416, test error 0.25069437853320226\n",
            "Loss: 0.23423838213887382\n",
            "training error 0.12488085454195055, test error 0.2505980108122408\n",
            "Loss: 0.1957080203026429\n",
            "training error 0.12491682732637224, test error 0.2506133245788551\n",
            "Loss: 0.20183086893739155\n",
            "training error 0.1249112660875191, test error 0.25064942469076557\n",
            "Loss: 0.21626464780402088\n",
            "training error 0.12491383152696434, test error 0.25084827618213573\n",
            "Loss: 0.2957707296925127\n",
            "training error 0.12491812661316955, test error 0.25067762441487157\n",
            "Loss: 0.22753964281942984\n",
            "training error 0.12485589537876202, test error 0.25071190439673385\n",
            "Loss: 0.2412456855868106\n",
            "training error 0.12485451015415082, test error 0.2507195006859895\n",
            "Loss: 0.24428288280091692\n",
            "training error 0.1248644031212075, test error 0.25067565040648065\n",
            "Loss: 0.22675038209176712\n",
            "training error 0.12486812023809823, test error 0.25063405923765053\n",
            "Loss: 0.21012113354019757\n",
            "training error 0.12510213272581214, test error 0.2505146167477457\n",
            "Loss: 0.1623648692149926\n",
            "training error 0.1248690019217712, test error 0.2506287620796828\n",
            "Loss: 0.20800318978098087\n",
            "training error 0.12485009516003893, test error 0.250592033487074\n",
            "Loss: 0.1933181277204099\n",
            "training error 0.1249012826714399, test error 0.2505650353112704\n",
            "Loss: 0.18252354347321198\n",
            "training error 0.12482850850714948, test error 0.25054553786204514\n",
            "Loss: 0.17472794795598823\n",
            "training error 0.12482390973943905, test error 0.2505389423835976\n",
            "Loss: 0.17209090135434923\n",
            "training error 0.1249291894260471, test error 0.250665614298432\n",
            "Loss: 0.22273768083989065\n",
            "training error 0.12487870111417312, test error 0.2505967233254867\n",
            "Loss: 0.1951932490701136\n",
            "training error 0.12484776699215674, test error 0.2505069377429503\n",
            "Loss: 0.15929460014192376\n",
            "training error 0.12491310416293003, test error 0.2506563936320401\n",
            "Loss: 0.21905101471448862\n",
            "training error 0.12481088493543088, test error 0.2505550431905329\n",
            "Loss: 0.17852842951091574\n",
            "training error 0.12491117001157437, test error 0.2505298162069406\n",
            "Loss: 0.16844201472232534\n",
            "training error 0.12481297674044814, test error 0.2506032145140606\n",
            "Loss: 0.19778859782386693\n",
            "training error 0.12492095032216509, test error 0.25064778563602225\n",
            "Loss: 0.21560931039747988\n",
            "training error 0.12478684676923953, test error 0.2504959223015307\n",
            "Loss: 0.15489033552460452\n",
            "training error 0.124782829816461, test error 0.2505031390675272\n",
            "Loss: 0.15777578930888936\n",
            "training error 0.12477274262946379, test error 0.2505248530158957\n",
            "Loss: 0.16645759976561258\n",
            "training error 0.12476893648968423, test error 0.25051627596226766\n",
            "Loss: 0.16302826703384898\n",
            "training error 0.12476968294420179, test error 0.2504459653460384\n",
            "Loss: 0.1349162243580171\n",
            "training error 0.12482579911612682, test error 0.2505285749561562\n",
            "Loss: 0.16794572985250333\n",
            "training error 0.12475912668366977, test error 0.25044013074615484\n",
            "Loss: 0.13258339711625933\n",
            "training error 0.12485082203766497, test error 0.2504587476247563\n",
            "Loss: 0.14002691722463378\n",
            "training error 0.12490775677339984, test error 0.25032846350886306\n",
            "Loss: 0.08793588428543941\n",
            "training error 0.1247166067524485, test error 0.2503648466312026\n",
            "Loss: 0.1024828182015769\n",
            "training error 0.12479908356394268, test error 0.25043834977233476\n",
            "Loss: 0.13187131671190677\n",
            "training error 0.12481856955833388, test error 0.25031360988828266\n",
            "Loss: 0.08199701419666638\n",
            "training error 0.12476261945795691, test error 0.2503665160498197\n",
            "Loss: 0.10315029588736646\n",
            "training error 0.12471543473561574, test error 0.2503665847922147\n",
            "Loss: 0.10317778091373242\n",
            "training error 0.12468297247765014, test error 0.2503414900087615\n",
            "Loss: 0.09314422323480898\n",
            "training error 0.12466377547561916, test error 0.2504525071426672\n",
            "Loss: 0.13753180754967342\n",
            "training error 0.12474052697894136, test error 0.2503847788535083\n",
            "Loss: 0.11045224748738036\n",
            "training error 0.12463283109999151, test error 0.25049124663256345\n",
            "Loss: 0.1530208794936927\n",
            "training error 0.1246395481470021, test error 0.2505846630792677\n",
            "Loss: 0.19037124387981663\n",
            "training error 0.12459783527882279, test error 0.25060121670233565\n",
            "Loss: 0.19698981989413333\n",
            "training error 0.12462760881567513, test error 0.25059424766991206\n",
            "Loss: 0.19420341653986206\n",
            "training error 0.12457034335793633, test error 0.2505641633305853\n",
            "Loss: 0.1821749025488817\n",
            "training error 0.12457158234032235, test error 0.2505565949195927\n",
            "Loss: 0.17914885180116968\n",
            "training error 0.12459798372073788, test error 0.25060404081534166\n",
            "Loss: 0.1981189749151513\n",
            "training error 0.12451419136983174, test error 0.25048588309362724\n",
            "Loss: 0.15087639486883742\n",
            "training error 0.12456074608091426, test error 0.25050089669819053\n",
            "Loss: 0.15687923078233634\n",
            "training error 0.12474806744227543, test error 0.2503839734934807\n",
            "Loss: 0.11013024326271825\n",
            "training error 0.12446125425759533, test error 0.25035247633631347\n",
            "Loss: 0.09753684735838153\n",
            "training error 0.12444001010557594, test error 0.2502540733761641\n",
            "Loss: 0.058192743103524514\n",
            "training error 0.1245562047856424, test error 0.25022620638542664\n",
            "Loss: 0.047050783682989206\n",
            "training error 0.1244130773927759, test error 0.25016631379391074\n",
            "Loss: 0.02310414263562155\n",
            "training error 0.12435736872423432, test error 0.2501571570870829\n",
            "Loss: 0.019443049234380005\n",
            "training error 0.12436448383607365, test error 0.25011450424946763\n",
            "Loss: 0.0023893174426303787\n",
            "training error 0.12436647137242021, test error 0.2501259285885815\n",
            "Loss: 0.0069570701652699185\n",
            "training error 0.12441279151396464, test error 0.25019932278043383\n",
            "Loss: 0.03630200787401172\n",
            "training error 0.12435419714600106, test error 0.24996853075462147\n",
            "Loss: 0.0\n",
            "training error 0.12428468056881785, test error 0.2498674000545154\n",
            "Loss: 0.0\n",
            "training error 0.12423426942731139, test error 0.24976960003975293\n",
            "Loss: 0.0\n",
            "training error 0.12415816375209816, test error 0.2497594739211864\n",
            "Loss: 0.0\n",
            "training error 0.12415519066134538, test error 0.24963355878071283\n",
            "Loss: 0.0\n",
            "training error 0.1241112498221125, test error 0.24956425754174114\n",
            "Loss: 0.0\n",
            "training error 0.12404770908986705, test error 0.24943459289480546\n",
            "Loss: 0.0\n",
            "training error 0.12401896726027585, test error 0.24930631039789541\n",
            "Loss: 0.0\n",
            "training error 0.1239977173279226, test error 0.24917688406962674\n",
            "Loss: 0.0\n",
            "training error 0.1239966584853823, test error 0.24914234406481672\n",
            "Loss: 0.0\n",
            "training error 0.12400203950320988, test error 0.24912136076140334\n",
            "Loss: 0.0\n",
            "training error 0.12381763857710607, test error 0.24903584517017327\n",
            "Loss: 0.0\n",
            "training error 0.12374873778953685, test error 0.2489107557013056\n",
            "Loss: 0.0\n",
            "training error 0.12371682631129796, test error 0.2488274092423286\n",
            "Loss: 0.0\n",
            "training error 0.12367593363670457, test error 0.2486986550009644\n",
            "Loss: 0.0\n",
            "training error 0.12366449309465072, test error 0.24845198487898323\n",
            "Loss: 0.0\n",
            "training error 0.12356486419100679, test error 0.24851746871571687\n",
            "Loss: 0.026356737204391578\n",
            "training error 0.1234694833077853, test error 0.24830388002863524\n",
            "Loss: 0.0\n",
            "training error 0.12342132613012796, test error 0.24821913584682703\n",
            "Loss: 0.0\n",
            "training error 0.1233015017929263, test error 0.24805798177588165\n",
            "Loss: 0.0\n",
            "training error 0.12328442721057603, test error 0.24781310890836736\n",
            "Loss: 0.0\n",
            "training error 0.12315753272914486, test error 0.24765658165089527\n",
            "Loss: 0.0\n",
            "training error 0.1230278333747601, test error 0.24745234355232135\n",
            "Loss: 0.0\n",
            "training error 0.12297169558203247, test error 0.24722173753384197\n",
            "Loss: 0.0\n",
            "training error 0.1228454161361466, test error 0.24701350264407887\n",
            "Loss: 0.0\n",
            "training error 0.12277184863938895, test error 0.24690009239990368\n",
            "Loss: 0.0\n",
            "training error 0.1226550338081913, test error 0.2467920122428074\n",
            "Loss: 0.0\n",
            "training error 0.12257637725203338, test error 0.246647606201439\n",
            "Loss: 0.0\n",
            "training error 0.12265301165222489, test error 0.24621434203656767\n",
            "Loss: 0.0\n",
            "training error 0.12226629574104024, test error 0.24610782617543633\n",
            "Loss: 0.0\n",
            "training error 0.1221549962566203, test error 0.2457933630578348\n",
            "Loss: 0.0\n",
            "training error 0.12206562386087258, test error 0.2455668147839117\n",
            "Loss: 0.0\n",
            "training error 0.12193224202719262, test error 0.2454439278669686\n",
            "Loss: 0.0\n",
            "training error 0.12178238651493435, test error 0.24505971117313774\n",
            "Loss: 0.0\n",
            "training error 0.12155157723118368, test error 0.24455796157582482\n",
            "Loss: 0.0\n",
            "training error 0.12145610986869229, test error 0.24426226961957978\n",
            "Loss: 0.0\n",
            "training error 0.12156451133108695, test error 0.24371030776855834\n",
            "Loss: 0.0\n",
            "training error 0.12104779281007697, test error 0.24343373573234853\n",
            "Loss: 0.0\n",
            "training error 0.12085904023795012, test error 0.2431283032881706\n",
            "Loss: 0.0\n",
            "training error 0.12077128303907944, test error 0.24284625357938025\n",
            "Loss: 0.0\n",
            "training error 0.12045602681165815, test error 0.2422890158511813\n",
            "Loss: 0.0\n",
            "training error 0.12029991863013735, test error 0.24183774807626557\n",
            "Loss: 0.0\n",
            "training error 0.12007898442402312, test error 0.24141831345734838\n",
            "Loss: 0.0\n",
            "training error 0.1198009786656612, test error 0.2408936802748905\n",
            "Loss: 0.0\n",
            "training error 0.11950915512281211, test error 0.2403300475357375\n",
            "Loss: 0.0\n",
            "training error 0.1191979702048944, test error 0.23971872711344888\n",
            "Loss: 0.0\n",
            "training error 0.11894782451332794, test error 0.23911752164637312\n",
            "Loss: 0.0\n",
            "training error 0.11870379482963458, test error 0.23855747440012554\n",
            "Loss: 0.0\n",
            "training error 0.11835340309254987, test error 0.23796088886383618\n",
            "Loss: 0.0\n",
            "training error 0.1180327588654227, test error 0.23728293019713861\n",
            "Loss: 0.0\n",
            "training error 0.11770436626082295, test error 0.23663397184583435\n",
            "Loss: 0.0\n",
            "training error 0.1173679638120992, test error 0.23598022377996686\n",
            "Loss: 0.0\n",
            "training error 0.117135348804597, test error 0.23540187735765639\n",
            "Loss: 0.0\n",
            "training error 0.11670488097149696, test error 0.2346296104393081\n",
            "Loss: 0.0\n",
            "training error 0.11627737503046286, test error 0.23383740116792126\n",
            "Loss: 0.0\n",
            "training error 0.11594379136391765, test error 0.23298092923227545\n",
            "Loss: 0.0\n",
            "training error 0.11554562686187275, test error 0.2321946117930862\n",
            "Loss: 0.0\n",
            "training error 0.11503054845576381, test error 0.23122886928684294\n",
            "Loss: 0.0\n",
            "training error 0.11456336961581547, test error 0.2301494018415795\n",
            "Loss: 0.0\n",
            "training error 0.11421218251583957, test error 0.2290578763929622\n",
            "Loss: 0.0\n",
            "training error 0.11361566120012433, test error 0.22796864351697654\n",
            "Loss: 0.0\n",
            "training error 0.11310218003660893, test error 0.22698096831645964\n",
            "Loss: 0.0\n",
            "training error 0.11257395379892063, test error 0.22593448336696137\n",
            "Loss: 0.0\n",
            "training error 0.11203068757546647, test error 0.22490232368689234\n",
            "Loss: 0.0\n",
            "training error 0.11155336827534784, test error 0.2237302108397665\n",
            "Loss: 0.0\n",
            "training error 0.11095363423557526, test error 0.2222882584306198\n",
            "Loss: 0.0\n",
            "training error 0.11038884786517432, test error 0.22112448416057448\n",
            "Loss: 0.0\n",
            "training error 0.10972483568368682, test error 0.219868687145718\n",
            "Loss: 0.0\n",
            "training error 0.10911636322688642, test error 0.21839841971539833\n",
            "Loss: 0.0\n",
            "training error 0.10842674135424207, test error 0.21692605254224445\n",
            "Loss: 0.0\n",
            "training error 0.10776345812289702, test error 0.21553849931085423\n",
            "Loss: 0.0\n",
            "training error 0.10701900158290488, test error 0.21415773800798477\n",
            "Loss: 0.0\n",
            "training error 0.10657925658548037, test error 0.212812675529691\n",
            "Loss: 0.0\n",
            "training error 0.10559586464318022, test error 0.21111205815245804\n",
            "Loss: 0.0\n",
            "training error 0.10491055833792287, test error 0.20937623092650243\n",
            "Loss: 0.0\n",
            "training error 0.1040776868751493, test error 0.2078520956295262\n",
            "Loss: 0.0\n",
            "training error 0.10335111802216944, test error 0.2062893567954233\n",
            "Loss: 0.0\n",
            "training error 0.1024811040554667, test error 0.20471323286890805\n",
            "Loss: 0.0\n",
            "training error 0.10170965963733132, test error 0.20303828338072805\n",
            "Loss: 0.0\n",
            "training error 0.10113518763390672, test error 0.20122692487231852\n",
            "Loss: 0.0\n",
            "training error 0.10000986087287818, test error 0.19959932546963458\n",
            "Loss: 0.0\n",
            "training error 0.09955644761003786, test error 0.19794106930228753\n",
            "Loss: 0.0\n",
            "training error 0.09839551094008979, test error 0.19593772718158714\n",
            "Loss: 0.0\n",
            "training error 0.0975870133319273, test error 0.1941775239434174\n",
            "Loss: 0.0\n",
            "training error 0.09656264964918272, test error 0.1922357212149055\n",
            "Loss: 0.0\n",
            "training error 0.09565764937877808, test error 0.19034991588068903\n",
            "Loss: 0.0\n",
            "training error 0.0947014030618573, test error 0.18840942736184838\n",
            "Loss: 0.0\n",
            "training error 0.09373561588403055, test error 0.18657224020635865\n",
            "Loss: 0.0\n",
            "training error 0.0928216384109961, test error 0.1844477610479943\n",
            "Loss: 0.0\n",
            "training error 0.09186919241074777, test error 0.18253854913134712\n",
            "Loss: 0.0\n",
            "training error 0.09089914142557819, test error 0.1805598694368484\n",
            "Loss: 0.0\n",
            "training error 0.09006835344122609, test error 0.1783353149019719\n",
            "Loss: 0.0\n",
            "training error 0.08898911275407427, test error 0.17629320235551355\n",
            "Loss: 0.0\n",
            "training error 0.08809941431203448, test error 0.17420180403471017\n",
            "Loss: 0.0\n",
            "training error 0.08709447758258704, test error 0.1720811084484278\n",
            "Loss: 0.0\n",
            "training error 0.0861291313468668, test error 0.1700043463146821\n",
            "Loss: 0.0\n",
            "training error 0.08530617306964117, test error 0.16794335696413115\n",
            "Loss: 0.0\n",
            "training error 0.08424201554947751, test error 0.16609761378320476\n",
            "Loss: 0.0\n",
            "training error 0.08329990161206607, test error 0.16391548240563558\n",
            "Loss: 0.0\n",
            "training error 0.08237049820401456, test error 0.16183101508486764\n",
            "Loss: 0.0\n",
            "training error 0.08140057117525176, test error 0.15970496963391329\n",
            "Loss: 0.0\n",
            "training error 0.0805972898248521, test error 0.15780189220107113\n",
            "Loss: 0.0\n",
            "training error 0.07944918226647632, test error 0.15555443630834093\n",
            "Loss: 0.0\n",
            "training error 0.07854349758428608, test error 0.15358728801958832\n",
            "Loss: 0.0\n",
            "training error 0.07754899746695365, test error 0.15145003960348752\n",
            "Loss: 0.0\n",
            "training error 0.07678114773748113, test error 0.14962464528832842\n",
            "Loss: 0.0\n",
            "training error 0.07582491706855808, test error 0.14743038673837794\n",
            "Loss: 0.0\n",
            "training error 0.07482608091989633, test error 0.14550793614311852\n",
            "Loss: 0.0\n",
            "training error 0.07397595180039516, test error 0.14343615513534408\n",
            "Loss: 0.0\n",
            "training error 0.07326900524460975, test error 0.14154736577792348\n",
            "Loss: 0.0\n",
            "training error 0.07205522602217031, test error 0.13971583694713868\n",
            "Loss: 0.0\n",
            "training error 0.07130500453959243, test error 0.13776147541305575\n",
            "Loss: 0.0\n",
            "training error 0.07035051564458981, test error 0.1359118873212894\n",
            "Loss: 0.0\n",
            "training error 0.06951843354606034, test error 0.1340850229538575\n",
            "Loss: 0.0\n",
            "training error 0.0687061886077189, test error 0.13223660549707136\n",
            "Loss: 0.0\n",
            "training error 0.06790975034327587, test error 0.13031460327995617\n",
            "Loss: 0.0\n",
            "training error 0.06699008508847872, test error 0.1285439107814764\n",
            "Loss: 0.0\n",
            "training error 0.06621770460782216, test error 0.12680343891593765\n",
            "Loss: 0.0\n",
            "training error 0.0654042171082924, test error 0.125006092625377\n",
            "Loss: 0.0\n",
            "training error 0.06464550110268515, test error 0.12320653802147605\n",
            "Loss: 0.0\n",
            "training error 0.06389153686858376, test error 0.12141294843239071\n",
            "Loss: 0.0\n",
            "training error 0.06318716226195528, test error 0.11969096763188353\n",
            "Loss: 0.0\n",
            "training error 0.06243058839112507, test error 0.11796350389360372\n",
            "Loss: 0.0\n",
            "training error 0.0617253124146166, test error 0.11628952187735632\n",
            "Loss: 0.0\n",
            "training error 0.06093296270698449, test error 0.11478256846866079\n",
            "Loss: 0.0\n",
            "training error 0.060219973192578935, test error 0.11331086504443759\n",
            "Loss: 0.0\n",
            "training error 0.059543020655503216, test error 0.1118214170993532\n",
            "Loss: 0.0\n",
            "training error 0.05889105776749002, test error 0.11021797926437381\n",
            "Loss: 0.0\n",
            "training error 0.058237867134035774, test error 0.10889141874322905\n",
            "Loss: 0.0\n",
            "training error 0.05756962271159828, test error 0.10729783204606286\n",
            "Loss: 0.0\n",
            "training error 0.05693275953518029, test error 0.10580521319145231\n",
            "Loss: 0.0\n",
            "training error 0.056273662882092276, test error 0.10447035551747431\n",
            "Loss: 0.0\n",
            "training error 0.0557840290220259, test error 0.10317765789646888\n",
            "Loss: 0.0\n",
            "training error 0.05509143851548037, test error 0.10160865538044382\n",
            "Loss: 0.0\n",
            "training error 0.05449809443279115, test error 0.10023731571526619\n",
            "Loss: 0.0\n",
            "training error 0.0539682048714011, test error 0.09870155941409096\n",
            "Loss: 0.0\n",
            "training error 0.05334265100434167, test error 0.09743934705395947\n",
            "Loss: 0.0\n",
            "training error 0.0528269215990203, test error 0.0963026838525705\n",
            "Loss: 0.0\n",
            "training error 0.052220192415366816, test error 0.09511685904342548\n",
            "Loss: 0.0\n",
            "training error 0.05171328767577481, test error 0.09384333999369568\n",
            "Loss: 0.0\n",
            "training error 0.05119797071804531, test error 0.09264707838712932\n",
            "Loss: 0.0\n",
            "training error 0.050683101393011826, test error 0.09163512499478559\n",
            "Loss: 0.0\n",
            "training error 0.050267559750124036, test error 0.0905635463326082\n",
            "Loss: 0.0\n",
            "training error 0.049672531522405215, test error 0.08929466332381587\n",
            "Loss: 0.0\n",
            "training error 0.049266435827658356, test error 0.08816589318855556\n",
            "Loss: 0.0\n",
            "training error 0.04882216756486932, test error 0.08716481716547655\n",
            "Loss: 0.0\n",
            "training error 0.04830763195343885, test error 0.08588708660002863\n",
            "Loss: 0.0\n",
            "training error 0.04790026672542253, test error 0.08492007264412785\n",
            "Loss: 0.0\n",
            "training error 0.04748516073639993, test error 0.08381567473883215\n",
            "Loss: 0.0\n",
            "training error 0.04710552803580876, test error 0.082789242417756\n",
            "Loss: 0.0\n",
            "training error 0.046688622959432116, test error 0.08188841016025136\n",
            "Loss: 0.0\n",
            "training error 0.04621360318427286, test error 0.08108364846004146\n",
            "Loss: 0.0\n",
            "training error 0.04583515054628471, test error 0.08023603813611914\n",
            "Loss: 0.0\n",
            "training error 0.045426454312349526, test error 0.07939765298431158\n",
            "Loss: 0.0\n",
            "training error 0.04507404863825129, test error 0.07850715592166937\n",
            "Loss: 0.0\n",
            "training error 0.04473993755774274, test error 0.07762766175707923\n",
            "Loss: 0.0\n",
            "training error 0.04435922239543066, test error 0.07673136627662615\n",
            "Loss: 0.0\n",
            "training error 0.043999434649784244, test error 0.07602170610373986\n",
            "Loss: 0.0\n",
            "training error 0.043712995398364186, test error 0.07529706592975866\n",
            "Loss: 0.0\n",
            "training error 0.04335099131340635, test error 0.07446964738605134\n",
            "Loss: 0.0\n",
            "training error 0.04304386024382962, test error 0.07356021029604463\n",
            "Loss: 0.0\n",
            "training error 0.04273367121854548, test error 0.07291743753101033\n",
            "Loss: 0.0\n",
            "training error 0.042421846416138546, test error 0.07217812388747953\n",
            "Loss: 0.0\n",
            "training error 0.04214678936818693, test error 0.07144109765695246\n",
            "Loss: 0.0\n",
            "training error 0.041838673596614806, test error 0.07059399816446543\n",
            "Loss: 0.0\n",
            "training error 0.04163482280062778, test error 0.06987449117073916\n",
            "Loss: 0.0\n",
            "training error 0.041295466415913, test error 0.06947575337891762\n",
            "Loss: 0.0\n",
            "training error 0.04099876065397568, test error 0.06876125654566652\n",
            "Loss: 0.0\n",
            "training error 0.0407886495786653, test error 0.06798901859411242\n",
            "Loss: 0.0\n",
            "training error 0.04049697040967364, test error 0.06761301072029742\n",
            "Loss: 0.0\n",
            "training error 0.04022056967360286, test error 0.06689868222880392\n",
            "Loss: 0.0\n",
            "training error 0.040002370660887, test error 0.06633904298832387\n",
            "Loss: 0.0\n",
            "training error 0.03976350300955666, test error 0.06587548191278005\n",
            "Loss: 0.0\n",
            "training error 0.039509212731549746, test error 0.06528383353826396\n",
            "Loss: 0.0\n",
            "training error 0.03929512688051787, test error 0.0647633765638835\n",
            "Loss: 0.0\n",
            "training error 0.03907917035797205, test error 0.06405975569923544\n",
            "Loss: 0.0\n",
            "training error 0.038854105346172646, test error 0.06355101377348613\n",
            "Loss: 0.0\n",
            "training error 0.03863366133365969, test error 0.06301843484601818\n",
            "Loss: 0.0\n",
            "training error 0.03845006758790041, test error 0.062370489924327414\n",
            "Loss: 0.0\n",
            "training error 0.03823037543396093, test error 0.061977290151096466\n",
            "Loss: 0.0\n",
            "training error 0.03809872286067993, test error 0.06149452754444469\n",
            "Loss: 0.0\n",
            "training error 0.03784841900507528, test error 0.06105858348883397\n",
            "Loss: 0.0\n",
            "training error 0.03767836806373388, test error 0.06053372981343986\n",
            "Loss: 0.0\n",
            "training error 0.03747480510176547, test error 0.060255678215253905\n",
            "Loss: 0.0\n",
            "training error 0.03731633905541864, test error 0.059700608021138316\n",
            "Loss: 0.0\n",
            "training error 0.03712280541082661, test error 0.05940208153150228\n",
            "Loss: 0.0\n",
            "training error 0.03694015131762741, test error 0.05901554936455208\n",
            "Loss: 0.0\n",
            "training error 0.036767652363911205, test error 0.05862277700447703\n",
            "Loss: 0.0\n",
            "training error 0.03663800786221524, test error 0.058092002214498735\n",
            "Loss: 0.0\n",
            "training error 0.03645974002963642, test error 0.057766406636380066\n",
            "Loss: 0.0\n",
            "training error 0.03629491054297644, test error 0.05747864404204057\n",
            "Loss: 0.0\n",
            "training error 0.03614527598596435, test error 0.05722224769005918\n",
            "Loss: 0.0\n",
            "training error 0.03601602635150909, test error 0.056759525728838585\n",
            "Loss: 0.0\n",
            "training error 0.03583333374985475, test error 0.05647361143340209\n",
            "Loss: 0.0\n",
            "training error 0.035698634571170534, test error 0.05609208942388832\n",
            "Loss: 0.0\n",
            "training error 0.03562502944835616, test error 0.0558485358330541\n",
            "Loss: 0.0\n",
            "training error 0.035413768263357154, test error 0.05549503901782759\n",
            "Loss: 0.0\n",
            "training error 0.03530656568110279, test error 0.055104643003584\n",
            "Loss: 0.0\n",
            "training error 0.03515132955928126, test error 0.05490666454237233\n",
            "Loss: 0.0\n",
            "training error 0.035051024969519395, test error 0.05448282178689027\n",
            "Loss: 0.0\n",
            "training error 0.03489074585778319, test error 0.05428780183791048\n",
            "Loss: 0.0\n",
            "training error 0.03478218370384177, test error 0.05385148983904572\n",
            "Loss: 0.0\n",
            "training error 0.03468948966522171, test error 0.053471270994119524\n",
            "Loss: 0.0\n",
            "training error 0.03453540110967741, test error 0.05340432050587515\n",
            "Loss: 0.0\n",
            "training error 0.03439568278535995, test error 0.05307595480126525\n",
            "Loss: 0.0\n",
            "training error 0.034284858191416016, test error 0.05281445309929232\n",
            "Loss: 0.0\n",
            "training error 0.03426325236348963, test error 0.05230145883945355\n",
            "Loss: 0.0\n",
            "training error 0.034080927988500154, test error 0.052183987055815964\n",
            "Loss: 0.0\n",
            "training error 0.033992626647101086, test error 0.05202752129652053\n",
            "Loss: 0.0\n",
            "training error 0.03394697789591783, test error 0.051710866918301464\n",
            "Loss: 0.0\n",
            "training error 0.0337679529336608, test error 0.051348972243042315\n",
            "Loss: 0.0\n",
            "training error 0.03368137104758067, test error 0.051335020724569094\n",
            "Loss: 0.0\n",
            "training error 0.033590839814666366, test error 0.05096049277413789\n",
            "Loss: 0.0\n",
            "training error 0.03347487013039696, test error 0.05091388422402428\n",
            "Loss: 0.0\n",
            "training error 0.03337760334702366, test error 0.050598817224204895\n",
            "Loss: 0.0\n",
            "training error 0.033273639885113816, test error 0.050493513907665155\n",
            "Loss: 0.0\n",
            "training error 0.03319621567035661, test error 0.05038476592256258\n",
            "Loss: 0.0\n",
            "training error 0.03309910496210676, test error 0.0502229669689285\n",
            "Loss: 0.0\n",
            "training error 0.033035184301868366, test error 0.05002354912816804\n",
            "Loss: 0.0\n",
            "training error 0.03290936856831683, test error 0.04986775582021639\n",
            "Loss: 0.0\n",
            "training error 0.032824360638445585, test error 0.04970516963372797\n",
            "Loss: 0.0\n",
            "training error 0.032730932070495225, test error 0.049552621934300684\n",
            "Loss: 0.0\n",
            "training error 0.032675988044119836, test error 0.049423813079741856\n",
            "Loss: 0.0\n",
            "training error 0.03263533334504508, test error 0.04936049971208445\n",
            "Loss: 0.0\n",
            "training error 0.03252830170050544, test error 0.04905349259708268\n",
            "Loss: 0.0\n",
            "training error 0.03241637151650824, test error 0.04902412878187172\n",
            "Loss: 0.0\n",
            "training error 0.0323266716503569, test error 0.04887938486683454\n",
            "Loss: 0.0\n",
            "training error 0.032302292374864164, test error 0.04884620644133808\n",
            "Loss: 0.0\n",
            "training error 0.03217732320765868, test error 0.048517196688117076\n",
            "Loss: 0.0\n",
            "training error 0.03211426792273811, test error 0.048451569506823076\n",
            "Loss: 0.0\n",
            "training error 0.032110575205670816, test error 0.048107393817364494\n",
            "Loss: 0.0\n",
            "training error 0.03196292915568029, test error 0.048252952727949286\n",
            "Loss: 0.30257076726583065\n",
            "training error 0.03188224338271313, test error 0.04810812202929268\n",
            "Loss: 0.0015137214270044552\n",
            "training error 0.03183274297955437, test error 0.04793635178953148\n",
            "Loss: 0.0\n",
            "training error 0.031735060228410175, test error 0.04783980847259299\n",
            "Loss: 0.0\n",
            "training error 0.03167206724180496, test error 0.04762717244533678\n",
            "Loss: 0.0\n",
            "training error 0.03159380458255254, test error 0.04755199663964394\n",
            "Loss: 0.0\n",
            "training error 0.03155277465984016, test error 0.04746617590056426\n",
            "Loss: 0.0\n",
            "training error 0.03145987917052693, test error 0.04738986363624137\n",
            "Loss: 0.0\n",
            "training error 0.03145321305670728, test error 0.04704925813412578\n",
            "Loss: 0.0\n",
            "training error 0.03134498724832619, test error 0.04706649125519647\n",
            "Loss: 0.03662782741771231\n",
            "training error 0.031279943545618576, test error 0.04696797880483378\n",
            "Loss: 0.0\n",
            "training error 0.031208396025942067, test error 0.0468734977473062\n",
            "Loss: 0.0\n",
            "training error 0.031148981298972925, test error 0.0467958158407752\n",
            "Loss: 0.0\n",
            "training error 0.03112199700894141, test error 0.04669637052852487\n",
            "Loss: 0.0\n",
            "training error 0.0310370380234016, test error 0.04658964120159841\n",
            "Loss: 0.0\n",
            "training error 0.030967692150018475, test error 0.046495663447507156\n",
            "Loss: 0.0\n",
            "training error 0.03097347812400755, test error 0.04633425556857491\n",
            "Loss: 0.0\n",
            "training error 0.030861604451055537, test error 0.04611256004320229\n",
            "Loss: 0.0\n",
            "training error 0.030823662062963542, test error 0.04608848947974576\n",
            "Loss: 0.0\n",
            "training error 0.03076865874300969, test error 0.04572977978672155\n",
            "Loss: 0.0\n",
            "training error 0.03071832569516788, test error 0.04581015943125702\n",
            "Loss: 0.17577089789269085\n",
            "training error 0.03068913927234966, test error 0.04580573484375272\n",
            "Loss: 0.16609539207366186\n",
            "training error 0.030591667421797874, test error 0.045678328684250875\n",
            "Loss: 0.0\n",
            "training error 0.030531507973284685, test error 0.04564190708845736\n",
            "Loss: 0.0\n",
            "training error 0.030502847125726505, test error 0.04555339755170291\n",
            "Loss: 0.0\n",
            "training error 0.030427462113927318, test error 0.04552817512788436\n",
            "Loss: 0.0\n",
            "training error 0.030372679897935198, test error 0.04549497191417993\n",
            "Loss: 0.0\n",
            "training error 0.030349959686972645, test error 0.04537800844273124\n",
            "Loss: 0.0\n",
            "training error 0.030282657230713408, test error 0.04548445134139397\n",
            "Loss: 0.23456934827157383\n",
            "training error 0.030264163775859016, test error 0.04547321342834546\n",
            "Loss: 0.20980423972192241\n",
            "training error 0.030195730468768365, test error 0.04544425399413503\n",
            "Loss: 0.14598602644140257\n",
            "training error 0.030160049913562432, test error 0.04525213658069583\n",
            "Loss: 0.0\n",
            "training error 0.030051161278766475, test error 0.04523069075408217\n",
            "Loss: 0.0\n",
            "training error 0.030032618347681506, test error 0.045272581518125896\n",
            "Loss: 0.09261579548163201\n",
            "training error 0.030031343426293027, test error 0.0450846201314941\n",
            "Loss: 0.0\n",
            "training error 0.02995862897851841, test error 0.04505757305066483\n",
            "Loss: 0.0\n",
            "training error 0.029887523539929815, test error 0.04515701296605677\n",
            "Loss: 0.2206952320315425\n",
            "training error 0.029839819465047504, test error 0.04508858846349368\n",
            "Loss: 0.06883507195110106\n",
            "training error 0.029761328973015335, test error 0.04495513536786966\n",
            "Loss: 0.0\n",
            "training error 0.029773089665531573, test error 0.04503111640557373\n",
            "Loss: 0.16901525728332256\n",
            "training error 0.02974629209150091, test error 0.04473012447232726\n",
            "Loss: 0.0\n",
            "training error 0.02963319006281828, test error 0.04482813415413958\n",
            "Loss: 0.21911336703961393\n",
            "training error 0.02960939129559032, test error 0.044728899908340274\n",
            "Loss: 0.0\n",
            "training error 0.029565964660891373, test error 0.04468382251546917\n",
            "Loss: 0.0\n",
            "training error 0.02950784636844901, test error 0.04457698554201667\n",
            "Loss: 0.0\n",
            "training error 0.029469083512455294, test error 0.044592082864578525\n",
            "Loss: 0.03386797554452148\n",
            "training error 0.029424862568397065, test error 0.044501843487617515\n",
            "Loss: 0.0\n",
            "training error 0.02939237389736974, test error 0.04445075704976757\n",
            "Loss: 0.0\n",
            "training error 0.029359635763588212, test error 0.04448772156726052\n",
            "Loss: 0.0831583530772173\n",
            "training error 0.029305196285287292, test error 0.044365441131419533\n",
            "Loss: 0.0\n",
            "training error 0.029263899123941454, test error 0.04445989653134304\n",
            "Loss: 0.2129031009602933\n",
            "training error 0.029209814395554336, test error 0.04421899838438488\n",
            "Loss: 0.0\n",
            "training error 0.02919838089854663, test error 0.044254120549770334\n",
            "Loss: 0.07942777237996967\n",
            "training error 0.029151211197202195, test error 0.04424214126185386\n",
            "Loss: 0.05233695541404515\n",
            "training error 0.029141429787517197, test error 0.04415721379756609\n",
            "Loss: 0.0\n",
            "training error 0.02907882834040853, test error 0.0439182629035021\n",
            "Loss: 0.0\n",
            "training error 0.029037960687412684, test error 0.04401302842493691\n",
            "Loss: 0.21577702570574342\n",
            "training error 0.02897667775560539, test error 0.04399921423371429\n",
            "Loss: 0.1843227050898122\n",
            "training error 0.028973269557703706, test error 0.04398246744423678\n",
            "Loss: 0.14619098409185405\n",
            "training error 0.02891445210711951, test error 0.04390834514522303\n",
            "Loss: 0.0\n",
            "training error 0.028899062297497793, test error 0.04405706963387204\n",
            "Loss: 0.3387157683969244\n",
            "training error 0.028904186618128925, test error 0.04382639733512934\n",
            "Loss: 0.0\n",
            "training error 0.028823510753643557, test error 0.0440859725267807\n",
            "Loss: 0.592280468929407\n",
            "training error 0.02875268426722292, test error 0.04397843079502831\n",
            "Loss: 0.3468992870584664\n",
            "training error 0.028726768250595676, test error 0.04374856646347819\n",
            "Loss: 0.0\n",
            "training error 0.028697764886519202, test error 0.043813412411875745\n",
            "Loss: 0.14822416741744338\n",
            "training error 0.0286474769301149, test error 0.043784575728065676\n",
            "Loss: 0.08230958748682848\n",
            "training error 0.028617193361150338, test error 0.04376889691742464\n",
            "Loss: 0.04647113171907247\n",
            "training error 0.02863749000330408, test error 0.04393427363515162\n",
            "Loss: 0.4244874442422386\n",
            "training error 0.028588780714265825, test error 0.04386457757045059\n",
            "Loss: 0.26517693344134674\n",
            "training error 0.02854648599077962, test error 0.04352571550467546\n",
            "Loss: 0.0\n",
            "training error 0.02847746317743858, test error 0.043569904609817404\n",
            "Loss: 0.10152413264108429\n",
            "training error 0.028470438204862794, test error 0.043563291630062534\n",
            "Loss: 0.08633086200049434\n",
            "training error 0.02845084915558821, test error 0.04373076462871621\n",
            "Loss: 0.47109880139413196\n",
            "training error 0.028375685548845543, test error 0.043890660595298975\n",
            "Loss: 0.8384585672906786\n",
            "training error 0.028333013883537848, test error 0.04397644892367963\n",
            "Loss: 1.0355565986175375\n",
            "training error 0.02830070300157103, test error 0.04394742125483985\n",
            "Loss: 0.9688657504529408\n",
            "training error 0.028323046274699643, test error 0.04394375541220704\n",
            "Loss: 0.9604435049130222\n",
            "training error 0.02823250970506605, test error 0.04390645341813849\n",
            "Loss: 0.8747424575298091\n",
            "training error 0.02819353625185973, test error 0.04377817690734974\n",
            "Loss: 0.5800281505933258\n",
            "training error 0.028163342598062673, test error 0.04372202383107505\n",
            "Loss: 0.4510168853594232\n",
            "training error 0.028117010849014648, test error 0.0437448627831577\n",
            "Loss: 0.503489203890739\n",
            "training error 0.028080979640298143, test error 0.043831848755493216\n",
            "Loss: 0.7033388130859519\n",
            "training error 0.028061039708303693, test error 0.04381943780820887\n",
            "Loss: 0.674824756187764\n",
            "training error 0.028040599123236358, test error 0.04370835614785667\n",
            "Loss: 0.4196154872206259\n",
            "training error 0.027967991047783615, test error 0.043794612700658146\n",
            "Loss: 0.6177892605896407\n",
            "training error 0.02797348337116145, test error 0.04369709850439713\n",
            "Loss: 0.39375113707955656\n",
            "training error 0.027951688251189485, test error 0.04363472173106617\n",
            "Loss: 0.25044097524140607\n",
            "training error 0.027909469348323484, test error 0.043685283595510827\n",
            "Loss: 0.36660647386308476\n",
            "training error 0.027866457845044386, test error 0.04387727174051794\n",
            "Loss: 0.8076977753638914\n",
            "training error 0.02784220435813352, test error 0.043791229876217606\n",
            "Loss: 0.6100172471917853\n",
            "training error 0.0279824521929507, test error 0.04337814768154468\n",
            "Loss: 0.0\n",
            "training error 0.02785513414429865, test error 0.04371385401779227\n",
            "Loss: 0.7739065732177863\n",
            "training error 0.02777682051097572, test error 0.043724306746320434\n",
            "Loss: 0.7980033341143056\n",
            "training error 0.02776682372455386, test error 0.043494867681854364\n",
            "Loss: 0.26907557502586776\n",
            "training error 0.027740942369752636, test error 0.04385086658167448\n",
            "Loss: 1.089762761656421\n",
            "training error 0.027638769076344216, test error 0.04369325946456475\n",
            "Loss: 0.7264297805739117\n",
            "training error 0.02763530072763058, test error 0.04370581740691305\n",
            "Loss: 0.7553797081745417\n",
            "training error 0.027605219423378874, test error 0.043428279560961604\n",
            "Loss: 0.11556943322006408\n",
            "training error 0.027720639098157484, test error 0.04332293351096288\n",
            "Loss: 0.0\n",
            "training error 0.02754496036307968, test error 0.043526960107708644\n",
            "Loss: 0.4709436324161542\n",
            "training error 0.02751199875977709, test error 0.04348541345613523\n",
            "Loss: 0.37504372858601087\n",
            "training error 0.027501244575303885, test error 0.04345522707123541\n",
            "Loss: 0.30536611801474933\n",
            "training error 0.02744359036670366, test error 0.043617430773803235\n",
            "Loss: 0.6797722106371973\n",
            "training error 0.027466516396707216, test error 0.043773396928827105\n",
            "Loss: 1.0397805073616206\n",
            "training error 0.02746069741873956, test error 0.04348980394700267\n",
            "Loss: 0.38517806278646205\n",
            "training error 0.027353420892482834, test error 0.04365391369288224\n",
            "Loss: 0.7639837727877019\n",
            "training error 0.027443759010270453, test error 0.043412934907486635\n",
            "Loss: 0.20774538848109536\n",
            "training error 0.027302414215224023, test error 0.04363200019480192\n",
            "Loss: 0.7134020224203841\n",
            "training error 0.027283838665307245, test error 0.04369016936231118\n",
            "Loss: 0.8476707867794309\n",
            "training error 0.027287021838628026, test error 0.04351710743362813\n",
            "Loss: 0.4482012341480157\n",
            "training error 0.02722200293860494, test error 0.043696595900997504\n",
            "Loss: 0.862504820778276\n",
            "training error 0.02720681428177897, test error 0.04368170439034034\n",
            "Loss: 0.8281315467399519\n",
            "training error 0.027208303024779786, test error 0.04372071493562373\n",
            "Loss: 0.9181774926672492\n",
            "training error 0.027168828383192494, test error 0.04353432965632546\n",
            "Loss: 0.4879543655765817\n",
            "training error 0.027132788832888062, test error 0.04361255568324234\n",
            "Loss: 0.6685193009983381\n",
            "training error 0.02712677150986221, test error 0.043550908536502626\n",
            "Loss: 0.52622250402794\n",
            "training error 0.02706925494515604, test error 0.043519964183322914\n",
            "Loss: 0.4547953159962592\n",
            "training error 0.027069375277497382, test error 0.043629238157715744\n",
            "Loss: 0.7070265605983339\n",
            "training error 0.027029322455126844, test error 0.04356891863424891\n",
            "Loss: 0.5677942451052687\n",
            "training error 0.027027474320243085, test error 0.043484257295780815\n",
            "Loss: 0.3723750257519365\n",
            "training error 0.026970605020132023, test error 0.04358984952207885\n",
            "Loss: 0.61610788902009\n",
            "training error 0.026950264743622037, test error 0.04364853923780878\n",
            "Loss: 0.7515782068716836\n",
            "training error 0.02698427618156667, test error 0.043491128684118495\n",
            "Loss: 0.38823588230252337\n",
            "training error 0.02693029731095099, test error 0.0437087682719233\n",
            "Loss: 0.8906016506541103\n",
            "training error 0.02686977224572875, test error 0.043641577832240694\n",
            "Loss: 0.7355095683841073\n",
            "training error 0.026879466343057747, test error 0.04352794383291133\n",
            "Loss: 0.473214312453174\n",
            "training error 0.02684782972483662, test error 0.0435685308682746\n",
            "Loss: 0.5668991857386008\n",
            "training error 0.026806515060429522, test error 0.04371119757750236\n",
            "Loss: 0.8962090862135064\n",
            "training error 0.026803153276531475, test error 0.04333081935935871\n",
            "Loss: 0.018202480203322047\n",
            "training error 0.02677668660805406, test error 0.043430240629706664\n",
            "Loss: 0.24769125737211795\n",
            "training error 0.026731959919806203, test error 0.04325665983919452\n",
            "Loss: 0.0\n",
            "training error 0.02674078144990621, test error 0.04340748387972176\n",
            "Loss: 0.3486724150406406\n",
            "training error 0.0267192545884749, test error 0.04327267991533651\n",
            "Loss: 0.037034935664337354\n",
            "training error 0.026691664973765183, test error 0.04306272483099716\n",
            "Loss: 0.0\n",
            "training error 0.02676355911220627, test error 0.04330922901347163\n",
            "Loss: 0.5724305264980245\n",
            "training error 0.02663119092518778, test error 0.043151798726187116\n",
            "Loss: 0.20684686243042094\n",
            "training error 0.02662240738749996, test error 0.0432061675441702\n",
            "Loss: 0.3331018037896927\n",
            "training error 0.02659028684425976, test error 0.04344156809286823\n",
            "Loss: 0.8797475388700349\n",
            "training error 0.026583902197931268, test error 0.04354967152219499\n",
            "Loss: 1.1307846707538483\n",
            "training error 0.026533718486589173, test error 0.04352576483013419\n",
            "Loss: 1.0752686945711742\n",
            "training error 0.026495593710898175, test error 0.043465714200143585\n",
            "Loss: 0.9358194836206701\n",
            "training error 0.026541247768398342, test error 0.04358837705533889\n",
            "Loss: 1.2206664264853062\n",
            "training error 0.026566037488646433, test error 0.043860595841847996\n",
            "Loss: 1.8528112514527129\n",
            "training error 0.026465680688072465, test error 0.043688504242074766\n",
            "Loss: 1.4531811759091573\n",
            "training error 0.026430576050932315, test error 0.0437037811225684\n",
            "Loss: 1.4886570556951861\n",
            "training error 0.026397596067013928, test error 0.04363331239340686\n",
            "Loss: 1.325014997655205\n",
            "training error 0.026441467779300423, test error 0.04381360598017975\n",
            "Loss: 1.74369167796391\n",
            "training error 0.026348379496752936, test error 0.04363801845123819\n",
            "Loss: 1.3359433767807793\n",
            "training error 0.026383435405545393, test error 0.043261616422781445\n",
            "Loss: 0.46186485542856204\n",
            "training error 0.026342932525691625, test error 0.043393258495721926\n",
            "Loss: 0.7675632836100732\n",
            "training error 0.02630197502803819, test error 0.043275377097448324\n",
            "Loss: 0.4938198111841263\n",
            "training error 0.026275437679434018, test error 0.043334172683918884\n",
            "Loss: 0.6303545676383582\n",
            "training error 0.026276294277769098, test error 0.043059345723757\n",
            "Loss: 0.0\n",
            "training error 0.026238499100663254, test error 0.04316703219858878\n",
            "Loss: 0.25008850697043616\n",
            "training error 0.026265613329601967, test error 0.04321974241801892\n",
            "Loss: 0.3725014664433868\n",
            "training error 0.026199282883302862, test error 0.043190844141216725\n",
            "Loss: 0.305388795973216\n",
            "training error 0.026203123815645935, test error 0.04308806054508878\n",
            "Loss: 0.0666866178506309\n",
            "training error 0.02616534263549631, test error 0.04303108558761292\n",
            "Loss: 0.0\n",
            "training error 0.02619005635235608, test error 0.04288272723668947\n",
            "Loss: 0.0\n",
            "training error 0.026142217528524792, test error 0.0429135924703465\n",
            "Loss: 0.07197591115573054\n",
            "training error 0.026102340871939942, test error 0.04284884308815146\n",
            "Loss: 0.0\n",
            "training error 0.0261468931507741, test error 0.042593143497150814\n",
            "Loss: 0.0\n",
            "training error 0.026079363861228114, test error 0.04286827123547002\n",
            "Loss: 0.6459437264535461\n",
            "training error 0.02606172890843965, test error 0.0428328742669277\n",
            "Loss: 0.5628388752122238\n",
            "training error 0.026064187137102846, test error 0.04291927929154229\n",
            "Loss: 0.7657002221808096\n",
            "training error 0.026053155659399533, test error 0.043033787990164364\n",
            "Loss: 1.0345432546978506\n",
            "training error 0.02602716106799485, test error 0.042912619987885973\n",
            "Loss: 0.7500655375589416\n",
            "training error 0.025998016353500384, test error 0.04293018507313095\n",
            "Loss: 0.7913047695168984\n",
            "training error 0.02595115746075454, test error 0.04304890441011465\n",
            "Loss: 1.0700335207574518\n",
            "training error 0.025928123264955943, test error 0.04317341949163962\n",
            "Loss: 1.3623694962256572\n",
            "training error 0.025950975299660763, test error 0.043301446422263405\n",
            "Loss: 1.662950575977029\n",
            "training error 0.025875259208122694, test error 0.04321561019076251\n",
            "Loss: 1.4614246390462826\n",
            "training error 0.025890715518767368, test error 0.04335630719825301\n",
            "Loss: 1.791752471036201\n",
            "training error 0.025928224545064056, test error 0.04340669041853669\n",
            "Loss: 1.9100419799733537\n",
            "training error 0.02586157727963874, test error 0.043295131914803525\n",
            "Loss: 1.648125402389411\n",
            "training error 0.025826523810805996, test error 0.04336573183539803\n",
            "Loss: 1.81387959378696\n",
            "training error 0.025835835069863554, test error 0.04349157979391514\n",
            "Loss: 2.109344892152465\n",
            "training error 0.025775590537525074, test error 0.04343106395358416\n",
            "Loss: 1.9672660612368276\n",
            "training error 0.025844415268441234, test error 0.043461715726757874\n",
            "Loss: 2.039230163101635\n",
            "training error 0.02576708578348145, test error 0.0431667145739036\n",
            "Loss: 1.3466277190627096\n",
            "training error 0.02586838137538259, test error 0.04335790238369562\n",
            "Loss: 1.7954976405908152\n",
            "training error 0.02570571655066223, test error 0.04331190455483392\n",
            "Loss: 1.687504134864315\n",
            "training error 0.025704887596914517, test error 0.04328882060380615\n",
            "Loss: 1.6333077334427681\n",
            "training error 0.025697079094645355, test error 0.043370428368000584\n",
            "Loss: 1.8249060929296457\n",
            "training error 0.025668472567101478, test error 0.043342004328873196\n",
            "Loss: 1.7581722555238821\n",
            "training error 0.025642353834837105, test error 0.0433375630951101\n",
            "Loss: 1.7477451459037896\n",
            "training error 0.02569614105370567, test error 0.04324178962023296\n",
            "Loss: 1.5228885915065993\n",
            "training error 0.025633407869249444, test error 0.04353511980402245\n",
            "Loss: 2.211567941527126\n",
            "training error 0.02557823341141216, test error 0.04345719131666185\n",
            "Loss: 2.0286077724430696\n",
            "training error 0.025583258063521668, test error 0.043578381732230345\n",
            "Loss: 2.3131381114085636\n",
            "training error 0.025603634363009567, test error 0.043541444133853635\n",
            "Loss: 2.2264161760360723\n",
            "training error 0.025528110139881315, test error 0.04357199025006145\n",
            "Loss: 2.298132216928561\n",
            "training error 0.025610527720479507, test error 0.04363997648495784\n",
            "Loss: 2.457750008230919\n",
            "training error 0.02550673384036092, test error 0.04345949120959023\n",
            "Loss: 2.0340074512165796\n",
            "training error 0.025507668028409987, test error 0.04332338731981952\n",
            "Loss: 1.7144633213502125\n",
            "training error 0.02551110009580177, test error 0.04309520984991084\n",
            "Loss: 1.1787492341193584\n",
            "training error 0.025540678802392882, test error 0.04302509337273138\n",
            "Loss: 1.0141300691024702\n",
            "training error 0.025445360121999314, test error 0.043326388504519635\n",
            "Loss: 1.7215094899437844\n",
            "training error 0.025456847492868768, test error 0.04324205025637339\n",
            "Loss: 1.5235005119215606\n",
            "training error 0.02541688821901704, test error 0.043157973646802815\n",
            "Loss: 1.3261058078274601\n",
            "training error 0.025398197146906786, test error 0.04313789851415418\n",
            "Loss: 1.2789734973184252\n",
            "training error 0.025403566340800203, test error 0.04303525825431091\n",
            "Loss: 1.0379951345682592\n",
            "training error 0.02538197073113917, test error 0.043220867781978284\n",
            "Loss: 1.4737683891996811\n",
            "training error 0.025364926519577877, test error 0.04331408257696989\n",
            "Loss: 1.6926176859130004\n",
            "training error 0.025332288637981343, test error 0.043389779029884154\n",
            "Loss: 1.8703374940772566\n",
            "training error 0.02542942973172642, test error 0.04351532899499074\n",
            "Loss: 2.165103164788995\n",
            "training error 0.02531003312533948, test error 0.043224686734284104\n",
            "Loss: 1.48273450907781\n",
            "training error 0.025316077905806353, test error 0.04320917569372495\n",
            "Loss: 1.4463177544417283\n",
            "training error 0.025315195415700043, test error 0.043219952400046245\n",
            "Loss: 1.471619259417567\n",
            "training error 0.025268871548632596, test error 0.04333381426608215\n",
            "Loss: 1.7389436611572995\n",
            "training error 0.025253661799228534, test error 0.043330105929644314\n",
            "Loss: 1.7302372447405734\n",
            "training error 0.025231969202573222, test error 0.043542161962771636\n",
            "Loss: 2.2281014916973874\n",
            "training error 0.025214603486629163, test error 0.04320247673139557\n",
            "Loss: 1.4305899593570093\n",
            "training error 0.025199643648522837, test error 0.04324129351436408\n",
            "Loss: 1.5217238362710273\n",
            "training error 0.02532645870610861, test error 0.043127817998973784\n",
            "Loss: 1.255306506923426\n",
            "training error 0.025168953034539306, test error 0.04339828102640088\n",
            "Loss: 1.8902984451098837\n",
            "training error 0.025155619230292293, test error 0.04313352928981487\n",
            "Loss: 1.2687154511152876\n",
            "training error 0.02515869732004417, test error 0.04316526871431879\n",
            "Loss: 1.343233136117905\n",
            "training error 0.025126699315929528, test error 0.04332473495019813\n",
            "Loss: 1.717627282185119\n",
            "training error 0.02510969855140447, test error 0.04326548354225293\n",
            "Loss: 1.5785170802129045\n",
            "training error 0.025094100819454204, test error 0.043389215937117934\n",
            "Loss: 1.8690154672908132\n",
            "training error 0.025075492199579493, test error 0.04337319482676189\n",
            "Loss: 1.8314011729687385\n",
            "training error 0.02510208826145689, test error 0.043455751368987744\n",
            "Loss: 2.0252270694569363\n",
            "training error 0.02513312010884061, test error 0.043267791086066196\n",
            "Loss: 1.5839347217011746\n",
            "training error 0.025048899434326706, test error 0.04336791939684005\n",
            "Loss: 1.8190155411775821\n",
            "training error 0.025044979384098328, test error 0.04368303132340082\n",
            "Loss: 2.558833973648622\n",
            "training error 0.025057750751283688, test error 0.04355019790711438\n",
            "Loss: 2.246968247430692\n",
            "training error 0.025021464189822554, test error 0.0437442635624948\n",
            "Loss: 2.702594762513799\n",
            "training error 0.025022780910063578, test error 0.0437831412923626\n",
            "Loss: 2.793871730297126\n",
            "training error 0.02498452138386064, test error 0.043495700880740326\n",
            "Loss: 2.1190203621620274\n",
            "training error 0.02502038951796317, test error 0.04346739006041285\n",
            "Loss: 2.0525523393701173\n",
            "training error 0.02492222427369927, test error 0.0435795714301728\n",
            "Loss: 2.315931279145822\n",
            "training error 0.02494035698896664, test error 0.043565067630257054\n",
            "Loss: 2.281879319781255\n",
            "training error 0.025005657183224304, test error 0.04346694357306274\n",
            "Loss: 2.0515040782805194\n",
            "training error 0.02488701432216908, test error 0.04360282430714648\n",
            "Loss: 2.3705242841801644\n",
            "training error 0.02491082688425224, test error 0.04378821337853895\n",
            "Loss: 2.8057799525129656\n",
            "training error 0.024882786557491685, test error 0.043726264539680426\n",
            "Loss: 2.660336733787716\n",
            "training error 0.02486946490036486, test error 0.0434479126342272\n",
            "Loss: 2.0068233215366327\n",
            "training error 0.024910959754273584, test error 0.04357765292698076\n",
            "Loss: 2.311427025562929\n",
            "training error 0.024883987576912, test error 0.04337631474840917\n",
            "Loss: 1.8387261116586506\n",
            "training error 0.024810787548509024, test error 0.04352687157580937\n",
            "Loss: 2.1922027866316363\n",
            "training error 0.024925286141551307, test error 0.04330149824426727\n",
            "Loss: 1.6630722434558987\n",
            "training error 0.024819053588780716, test error 0.043412802567197525\n",
            "Loss: 1.9243920564387063\n",
            "training error 0.024763961565734495, test error 0.04376766047628759\n",
            "Loss: 2.7575259365755667\n",
            "training error 0.024765902541064276, test error 0.04374482265376345\n",
            "Loss: 2.703907394601379\n",
            "training error 0.024768754600532707, test error 0.04366588577054812\n",
            "Loss: 2.5185797180456104\n",
            "training error 0.024727710850819436, test error 0.04370660540888213\n",
            "Loss: 2.6141811106423773\n",
            "training error 0.024720264244277364, test error 0.04356178385600494\n",
            "Loss: 2.2741696886469986\n",
            "training error 0.02474506702919526, test error 0.043424038791365874\n",
            "Loss: 1.9507724154490758\n",
            "training error 0.02472802774250559, test error 0.043396430795893885\n",
            "Loss: 1.8859544818447205\n",
            "training error 0.02468034430019891, test error 0.04341833649863208\n",
            "Loss: 1.9373845969750336\n",
            "training error 0.024676626254744375, test error 0.043481822125812614\n",
            "Loss: 2.0864358807450856\n",
            "training error 0.02472823418877484, test error 0.043297376078266434\n",
            "Loss: 1.6533942397623846\n",
            "training error 0.024695328976271722, test error 0.04333478835482223\n",
            "Loss: 1.7412306225320862\n",
            "training error 0.024658821274928716, test error 0.04343571259278657\n",
            "Loss: 1.9781801164596269\n",
            "training error 0.024605334367884805, test error 0.04353577508297802\n",
            "Loss: 2.2131064026543745\n",
            "training error 0.024651827764897275, test error 0.043565243937235786\n",
            "Loss: 2.282293252551315\n",
            "training error 0.024593325834149547, test error 0.04357696582080578\n",
            "Loss: 2.309813840626207\n",
            "training error 0.02462463720974578, test error 0.043611042023022774\n",
            "Loss: 2.3898178023419536\n",
            "training error 0.02456350945802459, test error 0.04360330922901139\n",
            "Loss: 2.3716627816591007\n",
            "training error 0.02454906685784458, test error 0.04357036229902126\n",
            "Loss: 2.2943101204441874\n",
            "training error 0.024539449949385027, test error 0.04350609601539272\n",
            "Loss: 2.1434260148067708\n",
            "training error 0.02457248885455474, test error 0.04359728660249962\n",
            "Loss: 2.3575228849121554\n",
            "training error 0.024503542839707054, test error 0.04359509447219285\n",
            "Loss: 2.352376210760454\n",
            "training error 0.0245241450207, test error 0.04326399595700244\n",
            "Loss: 1.575024533928815\n",
            "training error 0.0245166588510203, test error 0.043340551910300534\n",
            "Loss: 1.7547622734154888\n",
            "training error 0.02449982685029699, test error 0.043279067487622724\n",
            "Loss: 1.6104094090115595\n",
            "training error 0.0244702244686279, test error 0.04343834346462414\n",
            "Loss: 1.9843568660995015\n",
            "training error 0.024499243227067063, test error 0.04308909030963297\n",
            "Loss: 1.1643818036471743\n",
            "training error 0.02448356718070295, test error 0.043271677292289396\n",
            "Loss: 1.5930587400386997\n",
            "training error 0.024443932462189726, test error 0.043300645684215135\n",
            "Loss: 1.6610706066145342\n",
            "training error 0.024509321282544762, test error 0.04310266273435516\n",
            "Loss: 1.1962470843186912\n",
            "training error 0.024400628435038395, test error 0.04333151474570605\n",
            "Loss: 1.733544857060454\n",
            "training error 0.02441176403015384, test error 0.04363732580536222\n",
            "Loss: 2.4515267540214714\n",
            "training error 0.024449239571081423, test error 0.04345590835481914\n",
            "Loss: 2.0255956401199704\n",
            "training error 0.024403838298579214, test error 0.04349228340676164\n",
            "Loss: 2.1109968313819616\n",
            "training error 0.024383574336391076, test error 0.04382237997763518\n",
            "Loss: 2.8859961476348728\n",
            "training error 0.02437986393773178, test error 0.04359914740167763\n",
            "Loss: 2.361891661257909\n",
            "training error 0.024400110978084717, test error 0.04395524580046664\n",
            "Loss: 3.197937957800523\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xV5Zn3/8+VAIkCGoIoVALo1MGCSJB42Fgx1kM9FdHaUaqDVvsKeGjtWI06nZl27Alw+tP6m3qg1Voe6ZRpHRBP41RGBCVWQcADilobJD7GYsDIQU7J9fyx1g47yc5hh33O9+1rv7LX2mvtfa8Y8s19X2vdy9wdERGR7irIdANERCS3KDhERCQhCg4REUmIgkNERBKi4BARkYQoOEREJCEKDpEeMrNTzGx9ptshkm6m6zgkF5lZLfBNd38m020R6W3U4xDpgJkVZroN+ysfjkGyj4JD8oqZFZjZrWb2ZzNrMLP/NLPSmNd/b2b1ZtZoZsvMbGzMaw+Z2b1m9qSZbQdOM7NaM7vJzF4N91lgZsXh9pVmVhezf4fbhq9Xm9mHZvZ/zeybZuZm9vkOjqPUzH4dbrvFzBaF6680s+fbbNvyPnGO4abweAtjtr/QzF7tzvdLJB4Fh+SbbwFTgVOBzwFbgF/EvP4UcBRwKPAKML/N/l8HfgwMBKK/oP8OOBs4AjgWuLKTz4+7rZmdDdwInAF8Hqjs4jj+D3AgMDZs651dbN/RMfwc2A58qc3rvw2fd/X9EmlHwSH5ZibwPXevc/ddwA+Ai82sD4C7P+juW2NeG29mB8fs/6i7v+Duze6+M1x3t7v/X3ffDDwGlHfy+R1t+3fAr939DXffEX52XGY2DDgHmOnuW9x9j7s/l8D3oO0x/AcwLXzvgcC54Tro4vslEo+CQ/LNSGChmX1iZp8AbwJNwGFmVmhms8JhmU+B2nCfQ2L23xjnPetjnu8ABnTy+R1t+7k27x3vc6LKgM3uvqWTbTrT9r1/C1xkZkXARcAr7r4hfK3D71cPP1t6AQWH5JuNwDnuXhLzKHb3DwiGaC4gGC46GBgV7mMx+6fqNMMPgeExy2WdbLsRKDWzkjivbScYwgLAzIbG2abVMbj7OmADQS8mdpgq+lkdfb9E4lJwSC7ra2bFMY8+wH3Aj81sJICZDTGzC8LtBwK7gAaCX74/SWNb/xP4hpl9wcwOBP65ow3d/UOCWsw9ZjbIzPqa2eTw5bXAWDMrDwvvP+jm5/8WuAGYDPw+Zn1n3y+RuBQcksueBD6LefyAoBi8GPgfM9sKvAicGG4/j+Av7w+AdeFraeHuTwF3A88C78Z89q4Odvl7YA/wFvBX4Dvh+7wN3A48A7zDvgJ+V/6DoAD+v+7+ccz6zr5fInHpAkCRDDCzLwCvA0XuvjfT7RFJhHocImkSXj9RZGaDgNnAYwoNyUUKDpH0mUEw7PRngjOXrslsc0R6RkNVIiKSEPU4REQkIXlzdeghhxzio0aNynQzRERyyqpVqz529yGJ7JM3wTFq1ChWrlyZ6WaIiOQUM9vQ9VataahKREQSouAQEZGEKDhERCQheVPjEJHssGfPHurq6ti5c2fXG0vaFBcXM3z4cPr27bvf76XgEJGkqqurY+DAgYwaNQoz63oHSTl3p6Ghgbq6Oo444oj9fj8NVYlIUu3cuZPBgwcrNLKImTF48OCk9QJT2uMIb5f5c6AQ+JW7z2rz+o3AN4G9wCbgqugNZsysCXgt3PR9d5+SyrYmW83GGuatnUf9tno2f7aZTTs2UdSniF17dzGk/xDGHDKGCcMm0LCjgU92fcJj6x/js72fMeLgEeCwoXED/fv154YTb6BqYhUAc1fN5ZF1j1A+rJy3P36b9Q3rGdJ/CDhs2rGJIf2HUFpcyubPNrOhcQNmRklxCf0K+lF5RCUlRSVUjqoEYM4Lc1hdv5pdTbso7lNMSXFJS9uin7+raRelB5S2tKFmYw1La5cy+MDBNOxoaPlaOaqSSFkkg99tyTYKjeyTzP8nKZtyxMwKgbeBM4E64GVgWnhTmeg2pwF/cvcdZnYNUOnul4SvbXP3zu601kpFRYX39DqOuavmcteLd7FlZ3jDtfBb0regLwcVHcSnuz5lT/Oelteaw/8Mw3GKCos4uOhgGnc1smvvLnY37ebT3Z/2qC3x9O/TH8fZsXdH0t6zJ23YsXcH3sF9jkqKSuhb2JcD+x7IoOJBbNm5hZ17dtJMM44T/TkzM4oKixh8wODg+9XUflbx2LCS3PPmm2/yhS98IdPNkDji/b8xs1XuXpHI+6Syx3EC8K67vwdgZr8juPtaS3C4+7Mx278IXJ7C9sR154o7ufGPN3a8wdbuvc8HW1N3w7Tte7en7L2T1YZPdn3S8nxDY9fXE3X2/arfVs+Mx2dw25LbGHHwCLZ8tqVd70ekIw0NDZx++ukA1NfXU1hYyJAhwYXRL730Ev369etw35UrVzJv3jzuvvvuTj9j0qRJrFixYr/bunTpUi644IJWdYd/+7d/44wzztjv906lVAbH4bS+93Ednd8g5mqCu55FFZvZSoJhrFnuvqjtDmZWBVQBjBgxokeNfPzdx3u0n6Te5s82s/mzzS3L0UC56X9uYvCBgykfWk71pGoNk0krgwcPZs2aNQD84Ac/YMCAAdx0000tr+/du5c+feL/6quoqKCious/vpMRGlGnnHIKjz/e8e8h96DHXlBQEHe5I50d5/7KiuK4mV0OVAB3xKweGXafvg7cZWZ/03Y/d5/r7hXuXhH9iyJRl4y9pEf79cTwgcO73ggwjIH9Bu735w0dMLTLzyw9oJRRJaM4qvSodq8d2OfAOHtk3tbdW6n9pJZFby1i0oOTGPazYVy44EJqNtZkumnSQzU18NOfBl9T4corr2TmzJmceOKJVFdX89JLLxGJRJgwYQKTJk1i/fr1QNADOP/884EgdK666ioqKys58sgjW/VCBgwY0LJ9ZWUlF198MUcffTSXXXZZy7Dsk08+ydFHH83EiRP59re/3fK+3VFbW8vo0aOZPn06xxxzDMuXL2+1vHHjRm6++WaOOeYYxo0bx4IFC1rac8oppzBlyhTGjBmTlO9dPKnscXwAlMUsDw/XtWJmZwDfA05195YBb3f/IPz6npktBSYQ3McgqaLDHq1qHKFo0Tg6VNKReNsV9ylmxMEjKC0uZeiAoUwfP51IWaSlaL5u07qWgnm/gn5cfdzVjDt0HEtrl7YUm9vWXkoPKGXC0Als2r6Jr475KuMOHce8tfMAmDBsAqs/XE39tvpWnwdBDeeBVx5gd/Nutny2BTOL+9d6tG1Ay/7RNsQW7mML/UV9itodd1ffs8622b57O1t3d3N8MFS/rZ5Fby1i0VuLGFUyitu+eJuGs7LEd74D4R//HWpshFdfheZmKCiAY4+Fgw/uePvycrjrrsTbUldXx4oVKygsLOTTTz9l+fLl9OnTh2eeeYZ//Md/5JFHHmm3z1tvvcWzzz7L1q1bGT16NNdcc0276yBWr17NG2+8wec+9zlOPvlkXnjhBSoqKpgxYwbLli3jiCOOYNq0aR22a/ny5ZSXl7csP/LIIxQWFvLOO+/wm9/8hpNOOona2tpWy4888ghr1qxh7dq1fPzxxxx//PFMnhzclv6VV17h9ddfT8pptx1JZXC8DBxlZkcQBMalBL2HFmY2AbgfONvd/xqzfhCww913mdkhwMnAnFQ1tGpiVdp+0UTKIp0OrcS+1p12dWeYprvHF69t6fzeRMWGVTRgNn+2uVuBUvtJLTMen8GyDct4+KKH09Ba2V+NjUFoQPC1sbHz4Oipr33taxQWFoaf2cgVV1zBO++8g5mxZ8+euPucd955FBUVUVRUxKGHHspHH33E8OGte/EnnHBCy7ry8nJqa2sZMGAARx55ZMsv72nTpjF37ty4nxFvqKq2tpaRI0dy0kkntayLXX7++eeZNm0ahYWFHHbYYZx66qm8/PLLHHTQQZxwwgkpDQ1IYXC4+14zux54muB03Afd/Q0zux1Y6e6LCYamBgC/D08Vi552+wXgfjNrJhhOmxV7Npbkt47CKhooH23/qFXtI575r81nyV+W8K+V/6reRwZ1p2dQUwOnnw67d0O/fjB/PkRSULbq379/y/N//ud/5rTTTmPhwoXU1tZSWVkZd5+ioqKW54WFhezd2/5Ov93ZZn/bG2+5u/ulQkprHO7+pLv/rbv/jbv/OFz3L2Fo4O5nuPth7l4ePqaE61e4+zh3Hx9+fSCV7ZTcUDWxinXXraOhuoEVV61g6uipjDx4JKUHlMbdPlpMv/y/0n6yniQgEoElS+CHPwy+piI02mpsbOTwww8H4KGHHkr6+48ePZr33nuP2tpagJYaRLKccsopLFiwgKamJjZt2sSyZcs44YQTkvoZndGUI5KTImURFl66sGV57qq5fP/Z71O/vb7dtvNfmw+goassFomkJzCiqqurueKKK/jRj37Eeeedl/T3P+CAA7jnnns4++yz6d+/P8cff3yH27atcfzTP/1Tl2d2XXjhhdTU1DB+/HjMjDlz5jB06FDeeuutpB1DZ/LmnuP7cwGg5I/L/+vylqBo67Jxlyk80kAXAAa2bdvGgAEDcHeuu+46jjrqKP7hH/4ho21K1gWAWXE6rkiyPHzRw9x//v0M7T+03WvzX5vPqQ+dqtN2JS1++ctfUl5eztixY2lsbGTGjBmZblLSqMcheauj3keBFXDvefeqaJ4i6nFkL/U4RLrw8EUPc9m4y9qtb/ZmZj4+k7mr4p8eKSKdU3BIXnv4ooepPrm63XrHuebxazRsJdIDCg7Je7PPmM3959+P0Xpa6WaaufWZWzPUKpHcpeCQXqFqYhX3nX9fu/BY9v4ybnnmlgy1SiQ3KTik14iGR1t3vHCHhqzySENDA+Xl5ZSXlzN06FAOP/zwluXdu3d3uf/SpUs7nP32oYceYsiQIS3vV15ezrp1vW9SC10AKL1K1cQq/rzlz8x5Yd/UZ44z54U5rS4olNzV1bTqXVm6dCkDBgxg0qRJcV+/5JJL+Pd///cO9287nXl3pzdP5TToyaYeh/Q6s8+YzeSRk1ute3T9ozrLKoNqNtbw0+U/TVnPb9WqVZx66qlMnDiRL3/5y3z44YcA3H333YwZM4Zjjz2WSy+9lNraWu677z7uvPNOysvLWb58ebfev+105m2Xd+7cyTe+8Q3GjRvHhAkTePbZ4B52Dz30EFOmTOFLX/pSy82nckFuxJtIks06fRZffPCLNBNMyxo9y2rcoeN0Y6gk+s5/f4c19Z3Pq964q5FXP3qVZm+mwAo49rBjObio4+lxy4eWc9fZ3Z9X3d351re+xaOPPsqQIUNYsGAB3/ve93jwwQeZNWsWf/nLXygqKuKTTz6hpKSEmTNndtpLWbBgAc8//3zLck14E5HY6cyXLl3aavlnP/sZZsZrr73GW2+9xVlnncXbb7/dst+rr75KaWn8OdeykXoc0itFyiJMOXpKq3XNNLcawpL0aNzZSLMHAd7szTTubEzq++/atYvXX3+dM888k/Lycn70ox9RV1cHwLHHHstll13Gww8/3O1hoksuuYQ1a9a0PA444ACAdtOZxy4///zzXH55MNnm0UcfzciRI1uC48wzz8yp0AD1OKQXq55UzeL1i1t+aQEsXr+Ymo016nUkSXd6BjUbazh93unsbtpNv8J+zL9oflK//+7O2LFjW3oGsZ544gmWLVvGY489xo9//GNee+21Hn9ONk+DnmzqcUivFSmLcO9597Zap15H+kXKIiyZvoQfnvZDlkxfkvTQLioqYtOmTS3BsWfPHt544w2am5vZuHEjp512GrNnz6axsZFt27YxcOBAtm5N7C6UXTnllFOYPz+Y/ubtt9/m/fffZ/To0Un9jHRScEivVjWxiqlHT221LtrrkPSJlEW47ZTbUtLTKygo4A9/+AO33HIL48ePp7y8nBUrVtDU1MTll1/eUrD+9re/TUlJCV/5yldYuHBhh8XxBQsWtDodt6NTd2Nde+21NDc3M27cOC655BIeeuihVjeAyjWa5FB6vZqNNa0K5QBTR0/V6bk9pEkOs5cmORRJkniFcp2eK9IxBYcIQaG80Apblh3n+iev15CVSBwKDhGCXsc9593Tai6rvc17WVq7NHONymH5MgSeT5L5/0TBIRKqmljFzZNubll2nE92fZLBFuWm4uJiGhoaFB5ZxN1paGiguLg4Ke+n6zhEYpQUl7Ra/tmKnzF19FRd15GA4cOHU1dXx6ZNmzLdFIlRXFzM8OHDk/JeCg6RGJWjKim0Qpq8CYAmb9IEiAnq27dvqyuoJf9oqEokRqQswldGf6XVusfefkxFcpEYCg6RNtqeYdXszSqSi8RQcIi0ESmL8N1J321ZVpFcpDUFh0gcJUXti+QarhIJKDhE4ogWyaOavIl5a+dlsEUi2UPBIRJHvCJ5/bb6DLVGJLsoOEQ6UD2pmj6274z1J955QsNVIig4RDoUKYtw/t+e37K8p3mP7tUhgoJDpFNDBwxttaxrOkQUHCKdmj5+uq7pEGlDwSHSCV3TIdKegkOkC22v6biz5k4NV0mvltLgMLOzzWy9mb1rZrfGef1GM1tnZq+a2RIzGxnz2hVm9k74uCKV7RTpTOWoSvoU7Du7SvfpkN4uZcFhZoXAL4BzgDHANDMb02az1UCFux8L/AGYE+5bCnwfOBE4Afi+mQ1KVVtFOhMpi3Bj5MaWZQ1XSW+Xyh7HCcC77v6eu+8GfgdcELuBuz/r7jvCxReB6GTxXwb+6O6b3X0L8Efg7BS2VaRTGq4S2SeVwXE4sDFmuS5c15GrgacS2dfMqsxspZmt1E1jJJU0XCWyT1YUx83scqACuCOR/dx9rrtXuHvFkCFDUtM4EeIPVw0+cHAGWySSOakMjg+Aspjl4eG6VszsDOB7wBR335XIviLpVFJUQkHMP5nVH67OYGtEMieVwfEycJSZHWFm/YBLgcWxG5jZBOB+gtD4a8xLTwNnmdmgsCh+VrhOJGMqR1XSp3DfcNUvX/klc1fNzWCLRDIjZcHh7nuB6wl+4b8J/Ke7v2Fmt5vZlHCzO4ABwO/NbI2ZLQ733Qz8kCB8XgZuD9eJZEykLMJV5Ve1LDd5E9c/eb2K5NLr9Ol6k55z9yeBJ9us+5eY52d0su+DwIOpa51I4qaPn86vXvkVe30vsK9IHimLZLhlIumTFcVxkVyhIrmIgkMkYSXF+67pMExFcul1FBwiCaocVUnfgr5A0ON4YPUDqnNIr6LgEElQpCzCuUed27K8p3mP7kcuvYqCQ6QHhg0Y1mpZ9yOX3kTBIdIDbW/w9NS7T2m4SnoNBYdID0TKIlwxft9s/3ua9mjuKuk1FBwiPXTi8BNbnjfTrKnWpddQcIj0UMOOhlbLmmpdegsFh0gPaap16a0UHCI9pKvIpbdScIjsh9g7A+oqcuktFBwi+6HtVeS/XvNr1Tkk7yk4RPZDpCzCNyZ8o2VZp+VKb6DgENlPE4dNbHneTLPqHJL3FBwi+6lhRwOGAapzSO+g4BDZT5WjKulbqDqH9B4KDpH91PaWsqpzSL5TcIgkwYRhE1qea/oRyXcKDpEkiK1zgKYfkfym4BBJgspRlRQW7JtmXdOPSD5TcIgkgaYfkd5EwSGSJCVFJa2Gq3RaruQrBYdIksSelgvotFzJWwoOkSTRabnSWyg4RJKo7Wm5qnNIPlJwiCRRw44GCmL+WanOIflIwSGSRJWjKulTuO+ugA+sfkB1Dsk7Cg6RJIqURTj38+e2LO9p3sO8tfMy2CKR5FNwiCTZ0AFDWy3Xb6vPUEtEUkPBIZJk08dPb7krIMBT7z6l4SrJKwoOkSSLlEW4esLVLcu7m3ZruEryioJDJAWmj59On4KgSK57dEi+UXCIpECkLML0Y6e3LOtiQMknCg6RFDlx+Iktz3UxoOSTlAaHmZ1tZuvN7F0zuzXO65PN7BUz22tmF7d5rcnM1oSPxalsp0gq6F7kkq9SFhxmVgj8AjgHGANMM7MxbTZ7H7gS+G2ct/jM3cvDx5RUtVMkVdrei1wXA0q+SGWP4wTgXXd/z913A78DLojdwN1r3f1VoDmF7RDJCF0MKPmqy+AwswIzm9SD9z4c2BizXBeu665iM1tpZi+a2dQO2lYVbrNy06ZNPWiiSGrpYkDJR10Gh7s3Eww5pdtId68Avg7cZWZ/03YDd5/r7hXuXjFkyJD0t1CkC9PHT6fQ9t1SVhcDSj7o7lDVEjP7qplZ15u2+AAoi1keHq7rFnf/IPz6HrAUmNDpDiJZKFIW4erj9l0MqNNyJR90NzhmAL8HdpvZp2a21cw+7WKfl4GjzOwIM+sHXAp06+woMxtkZkXh80OAk4F13WyrSFaZOGxiy3Odliv5oE/Xm4C7D0z0jd19r5ldDzwNFAIPuvsbZnY7sNLdF5vZ8cBCYBDwFTP7V3cfC3wBuN/MmgnCbZa7KzgkJ0VPy3Vcp+VKXjB3796GZlOAyeHiUnd/PGWt6oGKigpfuXJlppsh0k7Nxhoqf1PJ7qbdAPQt6MtzVz5HpCyS4ZaJgJmtCuvJ3datoSozmwXcQDBctA64wcx+mngTRXofnZYr+aZbQ1XAuUB5eIYVZvYbYDVwW6oaJpJPdFqu5JNELgAsiXl+cLIbIpLPYmfLBZ2WK7mtu8HxE2C1mT0U9jZWAT9OXbNE8kvbe3TotFzJZV0OVZlZAcGUICcBx4erb3F39bVFEnDcsONanjfTzCe7Pslga0R6rrtXjle7+4fuvjh8KDREEtSwo6HV8p01d2q4SnJSd4eqnjGzm8yszMxKo4+Utkwkz1SOqmxV59jbvFfDVZKTuhsclwDXAcsI6hurAF00IZKASFmEGyM3tiw7rqvIJSd1t8Zxq7svSEN7RPJaSVFJy1XkgK4il5zU3RrHzWloi0jei725E6CbO0lOUo1DJI10Fbnkg+5eOX5J+PW6mHUOHJnc5ojkP11FLrmuWz0Odz8izkOhIdID08dPp2/BvuGqJ955QsNVklM6DQ4zq455/rU2r/0kVY0SyWeRsgjnHXVey7KGqyTXdDVUdSkwJ3x+G8HNnKLOBv4xFY1Kt7lz4a67YMuW1uuLi6GkJFi/a1fH+3dnu2x9r0y3v39/OP/8YLmyEiK9ZKZxDVdJLusqOKyD5/GWc9LPfw7f+U6mW9G7rYu5RdegQVBUFDzvKIRKS+GGG6CqKr3tTKbp46fzwOoH2NO8B9g36aHu0SG5oKsah3fwPN5yTnr00Uy3QGJt2QL19cGjthbWrIENG/atq68PgmbGDDjoIBg2DI44Ak49Fa65BmpypFSgSQ8ll3UVHOOj9xgHjg2fR5fHpaF9KXfppZlugfTU1q37AmbZMrjvPpg0CQYPhrFjgyHIbDZh2ISW55r0UHJJp0NV7l6YroZkSnS4QzWO9Lf//fdh8+aO36unNm8OHjNmwG23weTJUF2dffWT2HuRQzDp4dTRUzVcJVmvu9dx5LWqqtweL89lc+fCAw/A7t3twyZeCG3fHvQ0umvzZli0KHiUl8NJJ8H06dkRIpWjKiksKGRv814gmPRw3tp5Cg7JeuaeF6UKKioqfOVKzbvYG8Q7C64+gZOSzODmm2H27OS3LVFzV83lmsevoZlmAPoW9OW5K59TeEjamNkqd69IZJ9Ebh0rkhWqqoIC+Ycf7nusWAFTp8LIkTBwYOf7u8OcOUEPJNPF9KqJVXxl9FdalnVNh+QCBYfkhUgEFi4MCuWffgr33x+ESGfWrg2K6bfckpYmdmjYgGGtlnVNh2Q7BYfkpaqqIERieyIdyXTvY/r46fSxfeVGTUEi2U7BIXkttieyYkUQEPGsXQsnn5yZU3gjZRHO+9vWU5DMeWFOJ3uIZJaCQ3qNSARWrw5OzY3HHWbOzEx4tB2ueuztx9TrkKyl4JBeZ/bsjnsfmQqP6eOnU2j7Lptq9mZdSS5ZS8EhvVJnvQ/34OLBdBbNI2URvjvpu/vagOtKcslaCg7p1WbPDs7AsjhTds6Zk97wKCkqabX8sxU/03CVZCUFh/R6VVXBPFcFcf41zJmTvmGrylGVrYarmrxJ13RIVlJwiBCEx/PPB/NatZWumkekLNLqYkDQNR2SnRQcIqFIBJ57rn14uKdvyvbqSdW6raxkPQWHSBuzZrUftmpuhltvTf1nx7utrK7pkGyj4BBpIxKBe+9tXzBftiw9xfK2t5VdvH6xeh2SVRQcInFEC+ZtpaNYPn38dApi/mk206wiuWSVlAaHmZ1tZuvN7F0za9fRN7PJZvaKme01s4vbvHaFmb0TPq5IZTtF4qmqin+dR6rrHZGyCFOOntJqnYrkkk1SFhxmVgj8AjgHGANMM7MxbTZ7H7gS+G2bfUuB7wMnAicA3zezQalqq0hHZs9uXyxPR71DRXLJZqnscZwAvOvu77n7buB3wAWxG7h7rbu/CuFdbPb5MvBHd9/s7luAPwJnp7CtIh2KVyxPdb1DRXLJZqkMjsOBjTHLdeG6pO1rZlVmttLMVm7atKnHDRXpTLRY3tYdd6S23qEiuWSrnC6Ou/tcd69w94ohQ4ZkujmSx+LVO9zh2mtTV++IVyRXr0OyQSqD4wOgLGZ5eLgu1fuKpMTs2e3Do6kpONMqFeIVyR9d/yhzV2Vg3neRGKkMjpeBo8zsCDPrB1wKLO7mvk8DZ5nZoLAofla4TiSjZs8O7igY69FHUzdkVT2putX8VY5z7RPXashKMiplweHue4HrCX7hvwn8p7u/YWa3m9kUADM73szqgK8B95vZG+G+m4EfEoTPy8Dt4TqRjKuubl0sT+WUJJGyCPecdw/GvqsRm7xJQ1aSUebumW5DUlRUVPjKlSsz3QzpJS68EBYtar1u6tTgNrUp+bwFF7LorX0fWEABz1/1PJGySGo+UHoNM1vl7hWJ7JPTxXGRTGnb6wBYvDh1hfLqSdUqlEvWUHCI9EC8U3Sbm9NbKNfpuZIpCg6RHqqqSn+hXL0OyQYKDpH9kO5CuXodkg0UHCL7IRKBKa1/l6d0yEq9DskGCg6R/ZTOQrl6HZINFBwi+yndhaGE5DYAAA/mSURBVHL1OiTTFBwiSZDOQrl6HZJpCg6RJElnoVy9DskkBYdIkqSzUK4JECWTFBwiSRSvUJ6qIau2vQ7HuebxazRkJSmn4BBJomih3PbNSZiy+3bE63VoyErSQcEhkmRVVXDffa3Do6kpNfcpr55UTYG1/mesIStJNQWHSApUVcEFF7Rel4r7lEfKItx7XutzgR1n5uMzFR6SMgoOkRSprm7d64DgPuXJHrKqmljF1KNbnwus8JBUUnCIpEgkAjff3Hqde+qGrPoW9G39WSqWS4ooOERSaPZsmDy59bply5J/llWkLMJzVz7HmEPGtFrfTDO3PpOCpJJeTcEhkmKzZrUfsrrrruR/TqQswq+m/KpdsXzZ+8u45ZkkF1ekV1NwiKRYvCGrN99MfqEc4hfLAea8MEf1DkkaBYdIGsyeDWNajyIxZ05qLgysmlhF9cnV7darWC7JouAQSZMbbmi/LlVzWc0+YzaTR7YuruhMK0kWBYdImlRVtS+Up3L69Vmnz2pX79CZVpIMCg6RNJo1K31zWUXrHUbrynwzzXz9ka8rPKTHFBwiaRTvpk+pnH69amIV951/X7vwqG2s5Yu//qKGraRHFBwiaRbvpk/NzfDNb6Y2PNpq9mZmPD5Dp+pKwhQcIhkQb/r1devg1FNTFx7xzrSC4FTdy//r8uR/qOQtBYdIBsSbfh1gz57UTEkCwZlW959/P0MOHNLutfmvzVd4SLcpOEQyJN7065CaWXRbPnNiFY9e+iiFVtjutfmvzaf8vnIVzaVLCg6RDIqGR1upujgQgrOtln9jOeWHlbd7be1Ha5n04CTVPaRTCg6RDKuqCmoebc2cmdrwWD1zNZeNuyzu63NemKPeh3RIwSGSBeLNouue2vAAePiihzsMj2jv48IFFypApBUFh0iWiHdxYLrCo6MzrgAWvbWISQ9O4tSHTlWACKDgEMkaHZ1plY7wmH3GbFZctSJu3SNq2YZlqn8IoOAQySodnWmVjvCI1j2qT65ud6V5rDkvzGHwnMEawurFFBwiWaaz8JgxI3Wn6kbNPmM2L1z1AlNHT+1wm82fbdYQVi+W0uAws7PNbL2ZvWtm7S5rMrMiM1sQvv4nMxsVrh9lZp+Z2ZrwEeeERZH81VF4QHCq7uUpvlYvUhZh4aULWXHVCiaPmNzpttEhrMFzBjP2nrGa/6oXMHdPzRubFQJvA2cCdcDLwDR3XxezzbXAse4+08wuBS5090vCAHnc3Y/p7udVVFT4ypUrk3kIIhk3d24wAWJzc/vXLrsMHn44Pe2o2VjDtU9cy5qP1nRr+4H9BtK/X39KDyjlhhNvoGpiVYpbKD1lZqvcvSKhfVIYHBHgB+7+5XD5NgB3/2nMNk+H29SYWR+gHhgCjETBIQIEc1fdemtwRXlb48cHBfVIJD1tmbtqLj9Z/hM2NG5IaL/SA0o55tBjKC0uZeiAoUwfP51IWZoaLZ3KtuC4GDjb3b8ZLv89cKK7Xx+zzevhNnXh8p+BE4EBwBsEPZZPgX9y9+VxPqMKqAIYMWLExA0bEvthFskll18O8+fHf626OrgWJF1qNtYw54U5vFj3IvXb63v0HqUHlNKvsJ96JRmWT8GxFRjg7g1mNhFYBIx19087+jz1OKQ36Cw80jl0FWvuqrnc9eJd1H1ax9bdW3v8PgP7DWTwgYMpKS6hX0E/rj7uaoVJGmRbcPR4qMrbNMrMlgI3uXuHyaDgkN6is/BI99BVW9EQ2bJzC9t3b9+vIIF9tZJYxX2KKSkuYctnW9jVtIviPsWUDy2nelK1hr96INuCow/BUNPpwAcExfGvu/sbMdtcB4yLKY5f5O5/Z2ZDgM3u3mRmRwLLw+02d/R5Cg7pTW65pfN7lY8ZAzfcEJydlUmxQfLRto9wUvP7Jqq0uJR+ffoB7QMmnmjonPP5c3jqnadYXb+63bbFfYoZcfAIcNi0YxND+g9peT76kNFUTwquup+3dh4AE4ZNYPWHq1m3aR079+5s1XOq2VjDvLXzqN8WDO91Vu+JDgeub1jPkP5DGHPImJTUhrIqOADM7FzgLqAQeNDdf2xmtwMr3X2xmRUD/weYAGwGLnX398zsq8DtwB6gGfi+uz/W2WcpOKS3qamBa6+FNZ2c6JTpHkis6C/NdZvWsaFxA7uadiWlV5ILDuxzIH0L+tK4uzHu67GB19zczM69O/l0d/yR+WhtCPaF4/4M7WVdcKSTgkN6q86GrqImTw7mwsqGAGkrtldS3KeYvU17qdtal+lm5aT7z78/4fBQcCg4pJe65Ra4447g6vLOZHOAxIoNk7Zih6C27t7K5s86HMHudc468iye/vunE9pHwaHgkF6spgbmzYMlS+CddzrftrwcTjoJpk/P/hDpSrQW0LY+0Z0ax+6m3e2CJ3YoaHfTbrZ8tiXltZlkUY8jQQoOkX26Kp7HGjo0CJHq6twPkZ6Yu2ouD7zyAJ876HNxz8yq2VjD0tqlVI6qBFoXwaMFdTNjxMEjKC0ubdlv6IChHFR8EI+tf6yl5xRbjI8W0KP1nrai17eMO3Rcu9pQlGoc+0nBIdJaZ1ecd6S3h0hv1JPg0Oy4InkqEoHnnoMVK9rfXbAj9fWwaBFMmgTDhsGECXDiiamdzl1yj4JDJM/FBsjUqTByZPf2q68PTvV96aVgOveDDgrCZOxYBUlvp6EqkV6opiaogbz4YhAQPTFwIAweDCUlsGUL9O+fHRcdSmJU41BwiCQsGSESa+DAIERilZYqVLKVgkPBIbJfoiGyenVwE6m9e6EuidfiDRoERUXt1xcXB6cIqyiffgoOBYdI0s2dC3fdFQxHbd8OW1M8Q8iRR0JTE+yKf+kFxcUwYkQwH1c+XIeSaQoOBYdIysUGSXFxUON4/33YnKELuEtLoV+/9uujbduyZV8IqWfTnoJDwSGSMbGBEmv37syFSmcGDQrOFBs0qHW4xNM2hKIBdM450NAAlZW5G0QKDgWHSFaKrZ20/QWdrcGSqJKSIIhKSqCxsfOhtrY9obavp7NXpOBQcIjkpGiwrF8fFM+76gEk4+yvXDByZHCCwp49UBDnqrtoCPXrB1df3bOz1noSHH0S/xgRkeSKRGDhwu5v31kPJqrtX/a52LPZsKH72770UvA1Hac8KzhEJOckGjRRbQOnq2GjqNjttm7N3gB65BEFh4hIUvU0cNrqaGitO0HU2Tb72yv66ld7vm8iFBwiIglKVgDFk0i9Jxk1jp5QcIiIZJFUhlKyaHZcERFJiIJDREQSouAQEZGEKDhERCQhCg4REUmIgkNERBKSN3NVmdkmIIEL9Ns5BPg4Sc3JBvl2PKBjyhX5dkz5djzQ+phGuvuQRHbOm+DYX2a2MtGJvrJZvh0P6JhyRb4dU74dD+z/MWmoSkREEqLgEBGRhCg49pmb6QYkWb4dD+iYckW+HVO+HQ/s5zGpxiEiIglRj0NERBKi4BARkYT0+uAws7PNbL2ZvWtmt2a6Pd1lZg+a2V/N7PWYdaVm9kczeyf8Oihcb2Z2d3iMr5rZcZlreXxmVmZmz5rZOjN7w8xuCNfn8jEVm9lLZrY2PKZ/DdcfYWZ/Ctu+wMz6heuLwuV3w9dHZbL9nTGzQjNbbWaPh8s5fUxmVmtmr5nZGjNbGa7L5Z+9EjP7g5m9ZWZvmlkkmcfTq4PDzAqBXwDnAGOAaWY2JrOt6raHgLPbrLsVWOLuRwFLwmUIju+o8FEF3JumNiZiL/Bddx8DnARcF/6/yOVj2gV8yd3HA+XA2WZ2EjAbuNPdPw9sAa4Ot78a2BKuvzPcLlvdALwZs5wPx3Sau5fHXN+Qyz97Pwf+292PBsYT/L9K3vG4e699ABHg6Zjl24DbMt2uBNo/Cng9Znk9MCx8PgxYHz6/H5gWb7tsfQCPAmfmyzEBBwKvACcSXLHbJ1zf8jMIPA1Ewud9wu0s022PcyzDw188XwIeBywPjqkWOKTNupz82QMOBv7S9vuczOPp1T0O4HBgY8xyXbguVx3m7h+Gz+uBw8LnOXWc4XDGBOBP5PgxhUM6a4C/An8E/gx84u57w01i291yTOHrjcDg9La4W+4CqoHmcHkwuX9MDvyPma0ys+gNWHP1Z+8IYBPw63A48Vdm1p8kHk9vD4685cGfDjl3rrWZDQAeAb7j7p/GvpaLx+TuTe5eTvBX+gnA0Rlu0n4xs/OBv7r7qky3Jcm+6O7HEQzbXGdmk2NfzLGfvT7AccC97j4B2M6+YSlg/4+ntwfHB0BZzPLwcF2u+sjMhgGEX/8ars+J4zSzvgShMd/d/ytcndPHFOXunwDPEgzjlJhZn/Cl2Ha3HFP4+sFAQ5qb2pWTgSlmVgv8jmC46ufk9jHh7h+EX/8KLCQI+Vz92asD6tz9T+HyHwiCJGnH09uD42XgqPCMkH7ApcDiDLdpfywGrgifX0FQJ4iunx6ePXES0BjTZc0KZmbAA8Cb7v7/xbyUy8c0xMxKwucHENRs3iQIkIvDzdoeU/RYLwb+N/zLMGu4+23uPtzdRxH8e/lfd7+MHD4mM+tvZgOjz4GzgNfJ0Z89d68HNprZ6HDV6cA6knk8mS7kZPoBnAu8TTD2/L1MtyeBdv8H8CGwh+AvjKsJxo6XAO8AzwCl4bZGcPbYn4HXgIpMtz/O8XyRoOv8KrAmfJyb48d0LLA6PKbXgX8J1x8JvAS8C/weKArXF4fL74avH5npY+ji+CqBx3P9mMK2rw0fb0R/D+T4z145sDL82VsEDErm8WjKERERSUhvH6oSEZEEKThERCQhCg4REUmIgkNERBKi4BARkYQoOES6YGZN4ayp0UfSZlE2s1EWM8OxSC7o0/UmIr3eZx5MGyIiqMch0mPhPRzmhPdxeMnMPh+uH2Vm/xve22CJmY0I1x9mZgstuD/HWjObFL5VoZn90oJ7dvxPeJU5ZvZtC+5P8qqZ/S5DhynSjoJDpGsHtBmquiTmtUZ3Hwf8O8GssQD/P/Abdz8WmA/cHa6/G3jOg/tzHEdwlTIE90H4hbuPBT4BvhquvxWYEL7PzFQdnEiidOW4SBfMbJu7D4izvpbgRk3vhRM01rv7YDP7mOB+BnvC9R+6+yFmtgkY7u67Yt5jFPBHD26ug5ndAvR19x+Z2X8D2wimjFjk7ttSfKgi3aIeh8j+8Q6eJ2JXzPMm9tUezyOYQ+g44OWY2WdFMkrBIbJ/Lon5WhM+X0EwcyzAZcDy8PkS4BpoucHTwR29qZkVAGXu/ixwC8F05O16PSKZoL9gRLp2QHgXv6j/dvfoKbmDzOxVgl7DtHDdtwjuvnYzwZ3YvhGuvwGYa2ZXE/QsriGY4TieQuDhMFwMuNuDe3qIZJxqHCI9FNY4Ktz940y3RSSdNFQlIiIJUY9DREQSoh6HiIgkRMEhIiIJUXCIiEhCFBwiIpIQBYeIiCTk/wGPn+JuFT3e1wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9606481481481481 0.042593143497150814\n",
            "training error 0.12514096964174928, test error 0.25002808558635703\n",
            "training error 0.12500632795960998, test error 0.2500659681012442\n",
            "training error 0.12498763257965274, test error 0.2501499479911209\n",
            "training error 0.1250004456648163, test error 0.2501617833351307\n",
            "training error 0.12499752530094829, test error 0.25022596459337804\n",
            "training error 0.1249855033135872, test error 0.2502012485578077\n",
            "training error 0.1250761183859055, test error 0.250328915017666\n",
            "training error 0.12499143775337997, test error 0.25032118791327457\n",
            "training error 0.12503314624608614, test error 0.2503351514464269\n",
            "training error 0.12498909847839919, test error 0.25034697415594553\n",
            "training error 0.1251063935749746, test error 0.2504014893385028\n",
            "training error 0.1252018040896415, test error 0.25042852406253796\n",
            "training error 0.12498641294835845, test error 0.25044972919672154\n",
            "training error 0.1251810064897117, test error 0.2505066548099154\n",
            "training error 0.12507996149262574, test error 0.25044115655132604\n",
            "training error 0.12497328955321728, test error 0.25046254275836743\n",
            "training error 0.1250524395635481, test error 0.25052014241508685\n",
            "training error 0.12497202856412115, test error 0.250498611446582\n",
            "training error 0.12516506961917842, test error 0.25038333882884406\n",
            "training error 0.12506591257352448, test error 0.2504574510020757\n",
            "training error 0.12498384583732144, test error 0.2505653789513604\n",
            "training error 0.12508934372357228, test error 0.25049122718781763\n",
            "training error 0.12512166008088713, test error 0.25040599759281557\n",
            "training error 0.1250782884061591, test error 0.25056274195282413\n",
            "training error 0.12501594446307546, test error 0.2505557436007285\n",
            "training error 0.1250237104482638, test error 0.25045581765074304\n",
            "training error 0.12499680161294299, test error 0.2504717790194385\n",
            "training error 0.1250962215000152, test error 0.25052876756036846\n",
            "training error 0.12498571785079537, test error 0.25066157562018665\n",
            "training error 0.12501906915097674, test error 0.25065194741851327\n",
            "training error 0.12502383075077292, test error 0.2506868175044441\n",
            "training error 0.12500542691035577, test error 0.25066777538806906\n",
            "training error 0.12496654906576811, test error 0.25054639634674836\n",
            "training error 0.12514412433346325, test error 0.25042214442404137\n",
            "training error 0.1250355544343561, test error 0.2505687346429607\n",
            "training error 0.12504249769459128, test error 0.250674629793657\n",
            "training error 0.12495791817803804, test error 0.25062236348308203\n",
            "training error 0.12525882578442504, test error 0.2504930576997955\n",
            "training error 0.12496776624429824, test error 0.25063492779419033\n",
            "training error 0.12498156601004964, test error 0.25066909882219457\n",
            "training error 0.12496957342175694, test error 0.2506057439529545\n",
            "training error 0.12500175911392375, test error 0.25065251954364887\n",
            "training error 0.12503801076947513, test error 0.25058012352924763\n",
            "training error 0.12500406801329048, test error 0.2505958026889356\n",
            "training error 0.12504635121193397, test error 0.2505797747083173\n",
            "training error 0.12497109851086693, test error 0.2506815589514904\n",
            "training error 0.12500786258703472, test error 0.2507492086060463\n",
            "training error 0.12501541052044526, test error 0.25076573049294587\n",
            "training error 0.12503830031625893, test error 0.25065811914746006\n",
            "training error 0.1250666274237145, test error 0.25069157899043065\n",
            "Loss: 0.2653675496165242\n",
            "training error 0.1249854691576555, test error 0.2506112115944584\n",
            "Loss: 0.23322420228666374\n",
            "training error 0.12511435373303567, test error 0.25040268880579786\n",
            "Loss: 0.14982445614553086\n",
            "training error 0.12539467836639728, test error 0.2504904932415075\n",
            "Loss: 0.18494228521011014\n",
            "training error 0.1250700023202697, test error 0.2505247138001251\n",
            "Loss: 0.19862897106275312\n",
            "training error 0.12542376155017906, test error 0.2504154735771842\n",
            "Loss: 0.15493779025610444\n",
            "training error 0.12501310990097608, test error 0.25052925343527915\n",
            "Loss: 0.20044462114998662\n",
            "training error 0.1250212062624529, test error 0.25045574947301125\n",
            "Loss: 0.17104633891480425\n",
            "training error 0.12499072537567973, test error 0.2505416250025116\n",
            "Loss: 0.20539269216506284\n",
            "training error 0.12510212903949258, test error 0.2504841011319822\n",
            "Loss: 0.1823857286095576\n",
            "training error 0.125059417838145, test error 0.2505740765150821\n",
            "Loss: 0.21837183908546276\n",
            "training error 0.12501590311684882, test error 0.2507676627727862\n",
            "Loss: 0.2957976439705856\n",
            "training error 0.1250850388879499, test error 0.2507297429611933\n",
            "Loss: 0.2806314231422524\n",
            "training error 0.1249536229911026, test error 0.25068360157583025\n",
            "Loss: 0.2621769422166853\n",
            "training error 0.12501696594194475, test error 0.25066659004070463\n",
            "Loss: 0.255373092526856\n",
            "training error 0.12497947900134569, test error 0.25061150679652633\n",
            "Loss: 0.23334226984983442\n",
            "training error 0.125039944577965, test error 0.25067504201954777\n",
            "Loss: 0.25875350430073496\n",
            "training error 0.12493516044590132, test error 0.25057766840602375\n",
            "Loss: 0.2198084340716555\n",
            "training error 0.12497409863007707, test error 0.25064239280118417\n",
            "Loss: 0.24569528394640106\n",
            "training error 0.12506132845606124, test error 0.25051759350297037\n",
            "Loss: 0.19578117212926127\n",
            "training error 0.12494358848640247, test error 0.2505461241011753\n",
            "Loss: 0.20719212947752919\n",
            "training error 0.12503223073962452, test error 0.25057390159701076\n",
            "Loss: 0.21830187971632053\n",
            "training error 0.12505698045604863, test error 0.25072308948487926\n",
            "Loss: 0.2779703315698878\n",
            "training error 0.12492933080761554, test error 0.25067964974700363\n",
            "Loss: 0.2605963882491791\n",
            "training error 0.1249591833176054, test error 0.25060812606175387\n",
            "Loss: 0.23199012784365092\n",
            "training error 0.1250298776871059, test error 0.25056137429118797\n",
            "Loss: 0.21329152026272613\n",
            "training error 0.12500613303818034, test error 0.25062789465460933\n",
            "Loss: 0.239896676745599\n",
            "training error 0.12493192371297983, test error 0.25064407842415243\n",
            "Loss: 0.24636945739546778\n",
            "training error 0.12511018284258707, test error 0.25078765250927376\n",
            "Loss: 0.3037926403889557\n",
            "training error 0.12502691177608544, test error 0.25059946433657593\n",
            "Loss: 0.22852582696015755\n",
            "training error 0.12498125325335709, test error 0.2507347407918892\n",
            "Loss: 0.282630330858602\n",
            "training error 0.1250791437152893, test error 0.2507061386849668\n",
            "Loss: 0.27119077323636276\n",
            "training error 0.12493994493764773, test error 0.2506623501129362\n",
            "Loss: 0.25367731192746934\n",
            "training error 0.12504437411908512, test error 0.2507078640068976\n",
            "Loss: 0.27188082448672546\n",
            "training error 0.12502175258582138, test error 0.25053858663469647\n",
            "Loss: 0.20417748155860949\n",
            "training error 0.1249709017962614, test error 0.25057171517161064\n",
            "Loss: 0.21742740779648884\n",
            "training error 0.12493310087314585, test error 0.25059617065568307\n",
            "Loss: 0.22720850259432268\n",
            "training error 0.12498162838886985, test error 0.2506034205970584\n",
            "Loss: 0.23010815339090662\n",
            "training error 0.1249460215522926, test error 0.25072361816919625\n",
            "Loss: 0.27818178154188633\n",
            "training error 0.12494931647245691, test error 0.25083707505609515\n",
            "Loss: 0.32355943846904545\n",
            "training error 0.1249519347064338, test error 0.25069789452732477\n",
            "Loss: 0.2678934806051636\n",
            "training error 0.12492365451846084, test error 0.25072215510429546\n",
            "Loss: 0.2775966213198533\n",
            "training error 0.12512321667392873, test error 0.25070418572727854\n",
            "Loss: 0.2704096779111653\n",
            "training error 0.12522478439534182, test error 0.2509014918293876\n",
            "Loss: 0.3493232534186319\n",
            "training error 0.1249534699056711, test error 0.2507825882802993\n",
            "Loss: 0.3017671763445584\n",
            "training error 0.12491924335817839, test error 0.2506908978725404\n",
            "Loss: 0.2650951330643414\n",
            "training error 0.12505216118399584, test error 0.2507328707775544\n",
            "Loss: 0.281882409147971\n",
            "training error 0.12492068749187586, test error 0.25064941434811855\n",
            "Loss: 0.24850358722876553\n",
            "training error 0.1249508742910704, test error 0.2506927963054363\n",
            "Loss: 0.26585442092250666\n",
            "training error 0.12499787926704577, test error 0.2506057591081588\n",
            "Loss: 0.23104345275732907\n",
            "training error 0.12498432865823499, test error 0.2505783593358586\n",
            "Loss: 0.2200847749608048\n",
            "training error 0.12504629294074177, test error 0.2507325715585394\n",
            "Loss: 0.2817627349864349\n",
            "training error 0.12497011507546738, test error 0.2507449122658087\n",
            "Loss: 0.2866984634028569\n",
            "training error 0.12498707744235035, test error 0.25070957630045543\n",
            "Loss: 0.27256566497326773\n",
            "training error 0.12496842627573682, test error 0.2506791115153387\n",
            "Loss: 0.26038111976698364\n",
            "training error 0.12499092040875341, test error 0.2506345107315159\n",
            "Loss: 0.24254281023536883\n",
            "training error 0.12497345693736349, test error 0.25075724749350736\n",
            "Loss: 0.2916320002372297\n",
            "training error 0.12495203571321846, test error 0.2507603064272947\n",
            "Loss: 0.2928554363084812\n",
            "training error 0.12490534118248846, test error 0.2507662953365703\n",
            "Loss: 0.2952507309257113\n",
            "training error 0.12490403673728265, test error 0.25088158762426394\n",
            "Loss: 0.34136246570273254\n",
            "training error 0.12491770385598557, test error 0.2509085852814211\n",
            "Loss: 0.35216031550981963\n",
            "training error 0.1249346822153748, test error 0.25077501342064873\n",
            "Loss: 0.29873757283707114\n",
            "training error 0.12488672599695913, test error 0.25077501435974126\n",
            "Loss: 0.29873794843189305\n",
            "training error 0.12492003853082179, test error 0.2507715371846113\n",
            "Loss: 0.29734723461596335\n",
            "training error 0.12499307588668128, test error 0.2507841703444694\n",
            "Loss: 0.3023999309274661\n",
            "training error 0.12498833452415634, test error 0.2507182737480759\n",
            "Loss: 0.2760442532287133\n",
            "training error 0.12499479338603348, test error 0.2507350995489264\n",
            "Loss: 0.2827738175538608\n",
            "training error 0.12494122331977804, test error 0.250646933816844\n",
            "Loss: 0.2475114861739014\n",
            "training error 0.12488498295116303, test error 0.2506651581709912\n",
            "Loss: 0.2548004089781175\n",
            "training error 0.12489040910809812, test error 0.2507699874947118\n",
            "Loss: 0.2967274282866583\n",
            "training error 0.12491188015507343, test error 0.2507221418215939\n",
            "Loss: 0.2775913088360449\n",
            "training error 0.12487320045201314, test error 0.25079383094740443\n",
            "Loss: 0.30626373803230855\n",
            "training error 0.12497004469452162, test error 0.2508703311735096\n",
            "Loss: 0.3368603911745982\n",
            "training error 0.1252837652565992, test error 0.2506726141611129\n",
            "Loss: 0.2577824700150666\n",
            "training error 0.12486498891278668, test error 0.2508755113383045\n",
            "Loss: 0.3389322243379622\n",
            "training error 0.12496498234222386, test error 0.2509629385527628\n",
            "Loss: 0.37389918185126714\n",
            "training error 0.12486238065773421, test error 0.25086462640417284\n",
            "Loss: 0.3345787397659672\n",
            "training error 0.12485704156610257, test error 0.2508413555434888\n",
            "Loss: 0.32527144109610884\n",
            "training error 0.12485987458529692, test error 0.25079007528701314\n",
            "Loss: 0.30476164262471794\n",
            "training error 0.12485921448081862, test error 0.25080971239424416\n",
            "Loss: 0.31261560318476267\n",
            "training error 0.12485712733356791, test error 0.25083649714118583\n",
            "Loss: 0.3233282984721253\n",
            "training error 0.12490156034578624, test error 0.25082413132989106\n",
            "Loss: 0.3183825295735021\n",
            "training error 0.12484170884489579, test error 0.25084426447448893\n",
            "Loss: 0.3264348827924035\n",
            "training error 0.12484576873125257, test error 0.2508442600458201\n",
            "Loss: 0.32643311152384946\n",
            "training error 0.12490745540034963, test error 0.25094038129806195\n",
            "Loss: 0.3648772935110234\n",
            "training error 0.12494674062481116, test error 0.25087593073297304\n",
            "Loss: 0.3390999633611891\n",
            "training error 0.12483245896184965, test error 0.25084293243033395\n",
            "Loss: 0.3259021249816696\n",
            "training error 0.12500962331592722, test error 0.2508799061908756\n",
            "Loss: 0.3406899678973563\n",
            "training error 0.1248407759637547, test error 0.2507751364895616\n",
            "Loss: 0.2987867948725187\n",
            "training error 0.12492936253350434, test error 0.2506030845607882\n",
            "Loss: 0.2299737539815716\n",
            "training error 0.1249202641019466, test error 0.2507004429770987\n",
            "Loss: 0.26891274600806536\n",
            "training error 0.12490225627240062, test error 0.25075653635504563\n",
            "Loss: 0.2913475768053342\n",
            "training error 0.12493014973786289, test error 0.25073061773223015\n",
            "Loss: 0.2809812922518473\n",
            "training error 0.12483869108993764, test error 0.25080748217437226\n",
            "Loss: 0.31172361544400395\n",
            "training error 0.12494864055839056, test error 0.25070343476671764\n",
            "Loss: 0.27010932742888016\n",
            "training error 0.12497214416008509, test error 0.25088618719649286\n",
            "Loss: 0.3432020879268194\n",
            "training error 0.12499697086893415, test error 0.2506430598158325\n",
            "Loss: 0.2459620598355139\n",
            "training error 0.12484987044142518, test error 0.250842860058019\n",
            "Loss: 0.3258731793075009\n",
            "training error 0.12484513683100261, test error 0.2509047781983244\n",
            "Loss: 0.3506376533305877\n",
            "training error 0.12511006611344155, test error 0.25075025077091634\n",
            "Loss: 0.2888336255767898\n",
            "training error 0.12481934783696773, test error 0.25086222626876087\n",
            "Loss: 0.33361879344380174\n",
            "training error 0.12498324178821273, test error 0.2507569859192136\n",
            "Loss: 0.2915273822727382\n",
            "training error 0.12475717011886688, test error 0.25083469065465597\n",
            "Loss: 0.3226057850290287\n",
            "training error 0.12484248585726396, test error 0.2508511998802257\n",
            "Loss: 0.32920873346622415\n",
            "training error 0.12478165196891022, test error 0.2507770633966105\n",
            "Loss: 0.2995574711124993\n",
            "training error 0.12474953002717877, test error 0.2507870271490458\n",
            "Loss: 0.30354252439639406\n",
            "training error 0.12488199727431343, test error 0.2507529293567097\n",
            "Loss: 0.2899049395401976\n",
            "training error 0.12490960687488459, test error 0.25075542145943297\n",
            "Loss: 0.2909016686546195\n",
            "training error 0.12471585188402474, test error 0.2507852222839934\n",
            "Loss: 0.30282065947142556\n",
            "training error 0.1250131789190567, test error 0.250925746450227\n",
            "Loss: 0.3590240119484278\n",
            "training error 0.12470124033120757, test error 0.25078212943965783\n",
            "Loss: 0.3015836607045408\n",
            "training error 0.12471286501763319, test error 0.2507352512408199\n",
            "Loss: 0.2828344874954647\n",
            "training error 0.12468992528909885, test error 0.25069654125628904\n",
            "Loss: 0.26735223299589883\n",
            "training error 0.12473746540920896, test error 0.2506538403035108\n",
            "Loss: 0.25027377051911603\n",
            "training error 0.12468648285624735, test error 0.2506902697892616\n",
            "Loss: 0.2648439279737902\n",
            "training error 0.12466265983809184, test error 0.2507056087185618\n",
            "Loss: 0.2709788104867661\n",
            "training error 0.12468789309530116, test error 0.25081889328838175\n",
            "Loss: 0.3162875483248895\n",
            "training error 0.12470465127691432, test error 0.2507304301931836\n",
            "Loss: 0.28090628505972326\n",
            "training error 0.12465035182872175, test error 0.2506838163976911\n",
            "Loss: 0.2622628613086819\n",
            "training error 0.12466486837536522, test error 0.2506294339772439\n",
            "Loss: 0.24051233663475724\n",
            "training error 0.12460770051985143, test error 0.25068968889566756\n",
            "Loss: 0.2646115966368212\n",
            "training error 0.1246060386420328, test error 0.25067733396414354\n",
            "Loss: 0.259670179157645\n",
            "training error 0.12463058008076705, test error 0.25059588970748015\n",
            "Loss: 0.22709613593669964\n",
            "training error 0.12457514410969701, test error 0.2506077247895441\n",
            "Loss: 0.23182963698966663\n",
            "training error 0.12460820795748391, test error 0.25043071523463867\n",
            "Loss: 0.16103376840141959\n",
            "training error 0.12451755910780153, test error 0.2504507766350649\n",
            "Loss: 0.1690574271752565\n",
            "training error 0.12459131212053855, test error 0.25038766084729897\n",
            "Loss: 0.14381394798055336\n",
            "training error 0.12451514233548333, test error 0.25042423365425354\n",
            "Loss: 0.1584414274770296\n",
            "training error 0.12455405369250384, test error 0.25048705448861946\n",
            "Loss: 0.18356693856456108\n",
            "training error 0.12447398786805175, test error 0.25045461110284784\n",
            "Loss: 0.1705910419985468\n",
            "training error 0.12452983809955451, test error 0.2503861226403295\n",
            "Loss: 0.1431987343073038\n",
            "training error 0.12450608598139677, test error 0.2503923840731525\n",
            "Loss: 0.14570302609850927\n",
            "training error 0.12442837891358058, test error 0.25034439721733326\n",
            "Loss: 0.1265104399109429\n",
            "training error 0.12436831708370756, test error 0.25032739088710665\n",
            "Loss: 0.11970867194688317\n",
            "training error 0.12438707771401546, test error 0.25025976193752814\n",
            "Loss: 0.09266013081201674\n",
            "training error 0.12431521294860805, test error 0.25022315230847714\n",
            "Loss: 0.07801792413146824\n",
            "training error 0.12429207804096012, test error 0.25023271613184117\n",
            "Loss: 0.08184302375640051\n",
            "training error 0.12426098299760768, test error 0.2501895543112951\n",
            "Loss: 0.06458023488016273\n",
            "training error 0.12422669967671017, test error 0.25020499853496386\n",
            "Loss: 0.07075723040952386\n",
            "training error 0.12422908724556722, test error 0.2500798757075571\n",
            "Loss: 0.020713721451959266\n",
            "training error 0.1242305527291473, test error 0.2500325919579788\n",
            "Loss: 0.001802346168910951\n",
            "training error 0.12412134720130051, test error 0.2499031122067618\n",
            "Loss: 0.0\n",
            "training error 0.12413268200322787, test error 0.24980657923812505\n",
            "Loss: 0.0\n",
            "training error 0.12412947594848467, test error 0.24965275789364824\n",
            "Loss: 0.0\n",
            "training error 0.12410642365090463, test error 0.24960489815376014\n",
            "Loss: 0.0\n",
            "training error 0.12407183620119414, test error 0.249446087507179\n",
            "Loss: 0.0\n",
            "training error 0.1239489469850919, test error 0.2492439103761363\n",
            "Loss: 0.0\n",
            "training error 0.12389398268725846, test error 0.24912344409745757\n",
            "Loss: 0.0\n",
            "training error 0.12391833528522361, test error 0.24906101368260425\n",
            "Loss: 0.0\n",
            "training error 0.12392857359885369, test error 0.2489214233306639\n",
            "Loss: 0.0\n",
            "training error 0.12375053688007583, test error 0.2489246012752436\n",
            "Loss: 0.001276685846152148\n",
            "training error 0.1237843157284262, test error 0.24876179606707027\n",
            "Loss: 0.0\n",
            "training error 0.12373515547488491, test error 0.24879804544018022\n",
            "Loss: 0.014571921284955991\n",
            "training error 0.1236135312788012, test error 0.24866706766890095\n",
            "Loss: 0.0\n",
            "training error 0.12358453434736971, test error 0.24841525467278397\n",
            "Loss: 0.0\n",
            "training error 0.12340134293925185, test error 0.24833274535692657\n",
            "Loss: 0.0\n",
            "training error 0.12333155199316359, test error 0.24819735928295747\n",
            "Loss: 0.0\n",
            "training error 0.12326754163340786, test error 0.2480894188888116\n",
            "Loss: 0.0\n",
            "training error 0.12320278331190596, test error 0.24792458335377193\n",
            "Loss: 0.0\n",
            "training error 0.12310227197086171, test error 0.24772511667511804\n",
            "Loss: 0.0\n",
            "training error 0.12302278935193968, test error 0.24754474112678615\n",
            "Loss: 0.0\n",
            "training error 0.12292582135859417, test error 0.2472931743870635\n",
            "Loss: 0.0\n",
            "training error 0.12294182458727135, test error 0.24702324022792513\n",
            "Loss: 0.0\n",
            "training error 0.12272348615377991, test error 0.246963863876139\n",
            "Loss: 0.0\n",
            "training error 0.1226817943364262, test error 0.2467047331113694\n",
            "Loss: 0.0\n",
            "training error 0.12252480652697903, test error 0.2465580181668859\n",
            "Loss: 0.0\n",
            "training error 0.12248827970252917, test error 0.24627652561796376\n",
            "Loss: 0.0\n",
            "training error 0.122280062311539, test error 0.2461327134713688\n",
            "Loss: 0.0\n",
            "training error 0.12218627306509453, test error 0.24584089385348432\n",
            "Loss: 0.0\n",
            "training error 0.12199078405484269, test error 0.24560092016855278\n",
            "Loss: 0.0\n",
            "training error 0.12203856986884884, test error 0.24528008027610737\n",
            "Loss: 0.0\n",
            "training error 0.12170686027561127, test error 0.244995331839666\n",
            "Loss: 0.0\n",
            "training error 0.12158052780564851, test error 0.2446365684331253\n",
            "Loss: 0.0\n",
            "training error 0.12137095543642472, test error 0.24427272864980126\n",
            "Loss: 0.0\n",
            "training error 0.1212727863302805, test error 0.24384881219452198\n",
            "Loss: 0.0\n",
            "training error 0.12113267290224967, test error 0.24356877993658196\n",
            "Loss: 0.0\n",
            "training error 0.12104967871450772, test error 0.2430389599107623\n",
            "Loss: 0.0\n",
            "training error 0.12066685181136212, test error 0.24264713869357862\n",
            "Loss: 0.0\n",
            "training error 0.12048297238622277, test error 0.24224722644010818\n",
            "Loss: 0.0\n",
            "training error 0.12026517717682655, test error 0.24176696671297457\n",
            "Loss: 0.0\n",
            "training error 0.12007261163513892, test error 0.2413800382905327\n",
            "Loss: 0.0\n",
            "training error 0.1199733449546627, test error 0.24092513261940957\n",
            "Loss: 0.0\n",
            "training error 0.11951104344871041, test error 0.24037291404953098\n",
            "Loss: 0.0\n",
            "training error 0.11921464803326882, test error 0.23973303725070905\n",
            "Loss: 0.0\n",
            "training error 0.11896926512016318, test error 0.23918839105019105\n",
            "Loss: 0.0\n",
            "training error 0.11882125019055911, test error 0.23846967379333361\n",
            "Loss: 0.0\n",
            "training error 0.11838064392783812, test error 0.23799355294710767\n",
            "Loss: 0.0\n",
            "training error 0.11813845147334935, test error 0.2373222499664885\n",
            "Loss: 0.0\n",
            "training error 0.1177262484976933, test error 0.2367165393075233\n",
            "Loss: 0.0\n",
            "training error 0.11742691848661618, test error 0.23609380335178615\n",
            "Loss: 0.0\n",
            "training error 0.11700260903870939, test error 0.23533897042197477\n",
            "Loss: 0.0\n",
            "training error 0.1168391591312597, test error 0.23447561568683997\n",
            "Loss: 0.0\n",
            "training error 0.11624394053813275, test error 0.2337517323881337\n",
            "Loss: 0.0\n",
            "training error 0.11584700584290591, test error 0.23284936752004867\n",
            "Loss: 0.0\n",
            "training error 0.11564887444219389, test error 0.23211432255792241\n",
            "Loss: 0.0\n",
            "training error 0.11508392665091043, test error 0.23115467704086815\n",
            "Loss: 0.0\n",
            "training error 0.11454600173815788, test error 0.23018438568592536\n",
            "Loss: 0.0\n",
            "training error 0.11406047970125524, test error 0.22906222683299796\n",
            "Loss: 0.0\n",
            "training error 0.11362425431156314, test error 0.2281153366994394\n",
            "Loss: 0.0\n",
            "training error 0.11307402376941394, test error 0.22702874903363054\n",
            "Loss: 0.0\n",
            "training error 0.11250803935003885, test error 0.2259377578466442\n",
            "Loss: 0.0\n",
            "training error 0.11201138634415599, test error 0.22488063246629694\n",
            "Loss: 0.0\n",
            "training error 0.11147083099189939, test error 0.2237360795416761\n",
            "Loss: 0.0\n",
            "training error 0.11089535311798975, test error 0.2224733410834205\n",
            "Loss: 0.0\n",
            "training error 0.11023018147774638, test error 0.22119575897976723\n",
            "Loss: 0.0\n",
            "training error 0.10956767443818383, test error 0.21985171165794898\n",
            "Loss: 0.0\n",
            "training error 0.10894774056052067, test error 0.21838060016172334\n",
            "Loss: 0.0\n",
            "training error 0.1083009651966726, test error 0.2169052340457655\n",
            "Loss: 0.0\n",
            "training error 0.10766597615113392, test error 0.21555431594206348\n",
            "Loss: 0.0\n",
            "training error 0.10691051099754091, test error 0.2140188540013784\n",
            "Loss: 0.0\n",
            "training error 0.10611186420988004, test error 0.21247595069692693\n",
            "Loss: 0.0\n",
            "training error 0.10545145832897246, test error 0.2108300578346718\n",
            "Loss: 0.0\n",
            "training error 0.10467629240067024, test error 0.2092921346945096\n",
            "Loss: 0.0\n",
            "training error 0.10400812728497559, test error 0.20769975884611871\n",
            "Loss: 0.0\n",
            "training error 0.10310657893954686, test error 0.2059555055506914\n",
            "Loss: 0.0\n",
            "training error 0.1021803831218587, test error 0.20425200724567355\n",
            "Loss: 0.0\n",
            "training error 0.10134950117863362, test error 0.2024987216943503\n",
            "Loss: 0.0\n",
            "training error 0.10055381963728022, test error 0.20071723249063833\n",
            "Loss: 0.0\n",
            "training error 0.09969622302309446, test error 0.19905249121514404\n",
            "Loss: 0.0\n",
            "training error 0.09875618155262886, test error 0.19708692731939387\n",
            "Loss: 0.0\n",
            "training error 0.09789707697793537, test error 0.19515347940897562\n",
            "Loss: 0.0\n",
            "training error 0.09696056503846111, test error 0.19306477819954596\n",
            "Loss: 0.0\n",
            "training error 0.0959895247181063, test error 0.19102900562225683\n",
            "Loss: 0.0\n",
            "training error 0.09504905167822017, test error 0.18907272251196072\n",
            "Loss: 0.0\n",
            "training error 0.0941008347635189, test error 0.1870083767615175\n",
            "Loss: 0.0\n",
            "training error 0.09315606836791288, test error 0.18496405414182004\n",
            "Loss: 0.0\n",
            "training error 0.09223366020962002, test error 0.18273191028316269\n",
            "Loss: 0.0\n",
            "training error 0.09127650844622597, test error 0.1809428691341398\n",
            "Loss: 0.0\n",
            "training error 0.0901989743786954, test error 0.17889565054936418\n",
            "Loss: 0.0\n",
            "training error 0.08923664159556466, test error 0.17683608570524026\n",
            "Loss: 0.0\n",
            "training error 0.0882642537669027, test error 0.1747572485989549\n",
            "Loss: 0.0\n",
            "training error 0.0872710482345859, test error 0.1722850673626422\n",
            "Loss: 0.0\n",
            "training error 0.0862686747841617, test error 0.17038872921841014\n",
            "Loss: 0.0\n",
            "training error 0.08527359942718464, test error 0.16819228056071073\n",
            "Loss: 0.0\n",
            "training error 0.08428992284202046, test error 0.16612078780344175\n",
            "Loss: 0.0\n",
            "training error 0.08319655726154009, test error 0.1639740459628804\n",
            "Loss: 0.0\n",
            "training error 0.0822708622141363, test error 0.16182281855633673\n",
            "Loss: 0.0\n",
            "training error 0.08126554972388153, test error 0.1596553551001202\n",
            "Loss: 0.0\n",
            "training error 0.08021928249914846, test error 0.15748050914344836\n",
            "Loss: 0.0\n",
            "training error 0.07931571528768198, test error 0.1554169665864256\n",
            "Loss: 0.0\n",
            "training error 0.07826803371115755, test error 0.1532908126644272\n",
            "Loss: 0.0\n",
            "training error 0.07735534957732888, test error 0.15137328772876563\n",
            "Loss: 0.0\n",
            "training error 0.0763358268766695, test error 0.1491760505231802\n",
            "Loss: 0.0\n",
            "training error 0.07544683249254264, test error 0.14693125209694488\n",
            "Loss: 0.0\n",
            "training error 0.07461669357853548, test error 0.14509297481001387\n",
            "Loss: 0.0\n",
            "training error 0.07355428283582563, test error 0.14294442642610136\n",
            "Loss: 0.0\n",
            "training error 0.07261959606505622, test error 0.1410121133024318\n",
            "Loss: 0.0\n",
            "training error 0.07173542180882819, test error 0.13897393547666634\n",
            "Loss: 0.0\n",
            "training error 0.07111696097458829, test error 0.13681382143298362\n",
            "Loss: 0.0\n",
            "training error 0.06987678500743569, test error 0.1349350084927467\n",
            "Loss: 0.0\n",
            "training error 0.06909858593028316, test error 0.1330834808766458\n",
            "Loss: 0.0\n",
            "training error 0.06823567003961042, test error 0.13101332037257382\n",
            "Loss: 0.0\n",
            "training error 0.06727482233054437, test error 0.12909663159331272\n",
            "Loss: 0.0\n",
            "training error 0.0664514300166867, test error 0.12725756471331298\n",
            "Loss: 0.0\n",
            "training error 0.06560318279148504, test error 0.12562652511987105\n",
            "Loss: 0.0\n",
            "training error 0.06479754337046613, test error 0.12370942673675478\n",
            "Loss: 0.0\n",
            "training error 0.06407380648281426, test error 0.1219950365093217\n",
            "Loss: 0.0\n",
            "training error 0.06327660740369818, test error 0.12031634587549875\n",
            "Loss: 0.0\n",
            "training error 0.06253879884363531, test error 0.11837268629691743\n",
            "Loss: 0.0\n",
            "training error 0.06178961060686798, test error 0.1168270667673955\n",
            "Loss: 0.0\n",
            "training error 0.060973619652347606, test error 0.11520289750777253\n",
            "Loss: 0.0\n",
            "training error 0.06023508416600336, test error 0.11345379989413656\n",
            "Loss: 0.0\n",
            "training error 0.05955313143843675, test error 0.11193134853206747\n",
            "Loss: 0.0\n",
            "training error 0.05881557170279201, test error 0.11039673015118222\n",
            "Loss: 0.0\n",
            "training error 0.05818744298838184, test error 0.10892080432953052\n",
            "Loss: 0.0\n",
            "training error 0.057488833940579506, test error 0.10744751158404997\n",
            "Loss: 0.0\n",
            "training error 0.05684476504217862, test error 0.10574984175289606\n",
            "Loss: 0.0\n",
            "training error 0.05621065352075024, test error 0.10411310591459717\n",
            "Loss: 0.0\n",
            "training error 0.05558826150593145, test error 0.10276294329985494\n",
            "Loss: 0.0\n",
            "training error 0.054939443849014995, test error 0.10130800097750828\n",
            "Loss: 0.0\n",
            "training error 0.05430707940748253, test error 0.10007303345895581\n",
            "Loss: 0.0\n",
            "training error 0.05372052076791217, test error 0.09848259271717325\n",
            "Loss: 0.0\n",
            "training error 0.05311889359272745, test error 0.09717334490508138\n",
            "Loss: 0.0\n",
            "training error 0.05268997473356116, test error 0.09581613438163933\n",
            "Loss: 0.0\n",
            "training error 0.052014970957078244, test error 0.09464739491222293\n",
            "Loss: 0.0\n",
            "training error 0.051596577009041064, test error 0.09331644497939103\n",
            "Loss: 0.0\n",
            "training error 0.050998561788594465, test error 0.09206252885925986\n",
            "Loss: 0.0\n",
            "training error 0.050440224619661515, test error 0.09077804458423516\n",
            "Loss: 0.0\n",
            "training error 0.049935766048208007, test error 0.08965319104405818\n",
            "Loss: 0.0\n",
            "training error 0.049439331276882104, test error 0.08843462416069886\n",
            "Loss: 0.0\n",
            "training error 0.049002918741915945, test error 0.08721977998555876\n",
            "Loss: 0.0\n",
            "training error 0.04848234282905371, test error 0.08621232009996256\n",
            "Loss: 0.0\n",
            "training error 0.04802165133304355, test error 0.08526291163493303\n",
            "Loss: 0.0\n",
            "training error 0.04761191011428053, test error 0.08412406164743881\n",
            "Loss: 0.0\n",
            "training error 0.047200331044653134, test error 0.08341787570523053\n",
            "Loss: 0.0\n",
            "training error 0.0467302595600805, test error 0.08235094684828342\n",
            "Loss: 0.0\n",
            "training error 0.046313432412203044, test error 0.0814286276079485\n",
            "Loss: 0.0\n",
            "training error 0.04589572863591906, test error 0.08038243508200667\n",
            "Loss: 0.0\n",
            "training error 0.04556031782399176, test error 0.07954055407324788\n",
            "Loss: 0.0\n",
            "training error 0.04508456918486409, test error 0.07844081630401738\n",
            "Loss: 0.0\n",
            "training error 0.04475139075343187, test error 0.07756159471099164\n",
            "Loss: 0.0\n",
            "training error 0.044407026373350554, test error 0.0765994330070312\n",
            "Loss: 0.0\n",
            "training error 0.04401239564499581, test error 0.07570061973184711\n",
            "Loss: 0.0\n",
            "training error 0.04364093994532521, test error 0.0747977614801946\n",
            "Loss: 0.0\n",
            "training error 0.043317505358360785, test error 0.07414815333386945\n",
            "Loss: 0.0\n",
            "training error 0.042991587540975644, test error 0.07322609687132532\n",
            "Loss: 0.0\n",
            "training error 0.04271269934446878, test error 0.0725586286706829\n",
            "Loss: 0.0\n",
            "training error 0.042332714824118406, test error 0.07183300479389478\n",
            "Loss: 0.0\n",
            "training error 0.0420548527655214, test error 0.07114353284240764\n",
            "Loss: 0.0\n",
            "training error 0.0417242993537631, test error 0.07037717314902532\n",
            "Loss: 0.0\n",
            "training error 0.04143781425565701, test error 0.06973815335909865\n",
            "Loss: 0.0\n",
            "training error 0.04115029851634192, test error 0.06910804323811327\n",
            "Loss: 0.0\n",
            "training error 0.04086193261503114, test error 0.06837900001139485\n",
            "Loss: 0.0\n",
            "training error 0.04070356553093897, test error 0.06786277998865119\n",
            "Loss: 0.0\n",
            "training error 0.040355705265907774, test error 0.06715046488963776\n",
            "Loss: 0.0\n",
            "training error 0.04005089248512324, test error 0.06649756523066251\n",
            "Loss: 0.0\n",
            "training error 0.039809253287721606, test error 0.06586865924879931\n",
            "Loss: 0.0\n",
            "training error 0.03956649422917878, test error 0.06535461869066263\n",
            "Loss: 0.0\n",
            "training error 0.039336783857914795, test error 0.06478747113408885\n",
            "Loss: 0.0\n",
            "training error 0.03907927541751993, test error 0.06413564444202054\n",
            "Loss: 0.0\n",
            "training error 0.03887590058993595, test error 0.06358609070305905\n",
            "Loss: 0.0\n",
            "training error 0.0386395475750763, test error 0.06307351434292792\n",
            "Loss: 0.0\n",
            "training error 0.0384441459803134, test error 0.06241798591473629\n",
            "Loss: 0.0\n",
            "training error 0.03820418474837522, test error 0.06199222659671351\n",
            "Loss: 0.0\n",
            "training error 0.03803592966395941, test error 0.06129839224798609\n",
            "Loss: 0.0\n",
            "training error 0.0378386136002095, test error 0.06072899319941786\n",
            "Loss: 0.0\n",
            "training error 0.03762574034674574, test error 0.0603810635380909\n",
            "Loss: 0.0\n",
            "training error 0.03740756979803831, test error 0.06001054687984577\n",
            "Loss: 0.0\n",
            "training error 0.037225844876958715, test error 0.059576116305054425\n",
            "Loss: 0.0\n",
            "training error 0.03712189591594464, test error 0.059143239049328666\n",
            "Loss: 0.0\n",
            "training error 0.03686841682148201, test error 0.05868775009654184\n",
            "Loss: 0.0\n",
            "training error 0.03673078331520112, test error 0.05840532836308584\n",
            "Loss: 0.0\n",
            "training error 0.03651068093015748, test error 0.058022741452878786\n",
            "Loss: 0.0\n",
            "training error 0.0363590531169829, test error 0.057626721963509125\n",
            "Loss: 0.0\n",
            "training error 0.03623482122657389, test error 0.05716220434399161\n",
            "Loss: 0.0\n",
            "training error 0.0360777932199516, test error 0.05661722057201592\n",
            "Loss: 0.0\n",
            "training error 0.035879263062883464, test error 0.05647036944139288\n",
            "Loss: 0.0\n",
            "training error 0.03575786251387032, test error 0.056052529417852236\n",
            "Loss: 0.0\n",
            "training error 0.035608806161347414, test error 0.055904970753221386\n",
            "Loss: 0.0\n",
            "training error 0.03546206552886009, test error 0.05527833924881\n",
            "Loss: 0.0\n",
            "training error 0.03529858435564582, test error 0.05483179548682594\n",
            "Loss: 0.0\n",
            "training error 0.03515352868898005, test error 0.054493238416192406\n",
            "Loss: 0.0\n",
            "training error 0.03500828977247776, test error 0.0543181207430052\n",
            "Loss: 0.0\n",
            "training error 0.03487851694785635, test error 0.05414651956334824\n",
            "Loss: 0.0\n",
            "training error 0.03477082160857111, test error 0.05370102386466651\n",
            "Loss: 0.0\n",
            "training error 0.03464094043818067, test error 0.05360342175650341\n",
            "Loss: 0.0\n",
            "training error 0.034479697361321275, test error 0.05333236929410767\n",
            "Loss: 0.0\n",
            "training error 0.03437817360602815, test error 0.053080076156999535\n",
            "Loss: 0.0\n",
            "training error 0.03426510019015751, test error 0.05279204855034515\n",
            "Loss: 0.0\n",
            "training error 0.034155464155299124, test error 0.05258373635186406\n",
            "Loss: 0.0\n",
            "training error 0.03399471840582873, test error 0.05229651817050725\n",
            "Loss: 0.0\n",
            "training error 0.03390123218270259, test error 0.05202593073621971\n",
            "Loss: 0.0\n",
            "training error 0.033814161732107044, test error 0.05167168157897476\n",
            "Loss: 0.0\n",
            "training error 0.03377247053539065, test error 0.05118403712841474\n",
            "Loss: 0.0\n",
            "training error 0.03365201332651215, test error 0.05126483559456863\n",
            "Loss: 0.15785872058349248\n",
            "training error 0.0334725261573717, test error 0.05096206583859636\n",
            "Loss: 0.0\n",
            "training error 0.0333953529092279, test error 0.05081871272856965\n",
            "Loss: 0.0\n",
            "training error 0.0333150195697834, test error 0.050146748955702855\n",
            "Loss: 0.0\n",
            "training error 0.03318446236752027, test error 0.04997965627115589\n",
            "Loss: 0.0\n",
            "training error 0.03310887435704726, test error 0.04985316047804847\n",
            "Loss: 0.0\n",
            "training error 0.03301031241002414, test error 0.04976110858362207\n",
            "Loss: 0.0\n",
            "training error 0.03292062972128088, test error 0.04951046712723449\n",
            "Loss: 0.0\n",
            "training error 0.03282889329468331, test error 0.04942005017695486\n",
            "Loss: 0.0\n",
            "training error 0.03273568541210603, test error 0.049258990226985225\n",
            "Loss: 0.0\n",
            "training error 0.03265564744341693, test error 0.048903933654354866\n",
            "Loss: 0.0\n",
            "training error 0.032549818642074034, test error 0.04861060844033456\n",
            "Loss: 0.0\n",
            "training error 0.032465999953121306, test error 0.04851865519919663\n",
            "Loss: 0.0\n",
            "training error 0.03244679956450157, test error 0.04845366082349742\n",
            "Loss: 0.0\n",
            "training error 0.03230658653613329, test error 0.04823358725845952\n",
            "Loss: 0.0\n",
            "training error 0.032246469222330626, test error 0.048067852070131645\n",
            "Loss: 0.0\n",
            "training error 0.03219112465148559, test error 0.04783780154858951\n",
            "Loss: 0.0\n",
            "training error 0.03207145912358456, test error 0.04780867463411219\n",
            "Loss: 0.0\n",
            "training error 0.03205087970362962, test error 0.04769172437853814\n",
            "Loss: 0.0\n",
            "training error 0.03193901554181608, test error 0.04756887284952399\n",
            "Loss: 0.0\n",
            "training error 0.031847488522405126, test error 0.047352631654531324\n",
            "Loss: 0.0\n",
            "training error 0.03177931670469205, test error 0.04728835910792237\n",
            "Loss: 0.0\n",
            "training error 0.031710585762923856, test error 0.04706622440093841\n",
            "Loss: 0.0\n",
            "training error 0.03164454186528822, test error 0.0470556525856366\n",
            "Loss: 0.0\n",
            "training error 0.03158713840977422, test error 0.04689860016144985\n",
            "Loss: 0.0\n",
            "training error 0.03152566706539614, test error 0.0468606963519262\n",
            "Loss: 0.0\n",
            "training error 0.03143533425791956, test error 0.04661169061302301\n",
            "Loss: 0.0\n",
            "training error 0.03135876233843504, test error 0.046520309283076905\n",
            "Loss: 0.0\n",
            "training error 0.03131182118409187, test error 0.04642145211132547\n",
            "Loss: 0.0\n",
            "training error 0.03126256513433949, test error 0.04645498319055057\n",
            "Loss: 0.07223186199492027\n",
            "training error 0.031225439684966817, test error 0.046253492926913005\n",
            "Loss: 0.0\n",
            "training error 0.031115755433528236, test error 0.04613824038584156\n",
            "Loss: 0.0\n",
            "training error 0.031068309921056112, test error 0.046163031422238716\n",
            "Loss: 0.053732080352086875\n",
            "training error 0.031010483897911157, test error 0.04608025080733115\n",
            "Loss: 0.0\n",
            "training error 0.030937846381902633, test error 0.04595072078086743\n",
            "Loss: 0.0\n",
            "training error 0.03088755490455604, test error 0.045711816557572726\n",
            "Loss: 0.0\n",
            "training error 0.030855874402919134, test error 0.045479865860952026\n",
            "Loss: 0.0\n",
            "training error 0.03080747083349225, test error 0.04559863958169208\n",
            "Loss: 0.2611567085601951\n",
            "training error 0.030737702225899464, test error 0.04551638380097843\n",
            "Loss: 0.080294739958231\n",
            "training error 0.030658888700306964, test error 0.04542919963640216\n",
            "Loss: 0.0\n",
            "training error 0.03068441008104805, test error 0.045377008329071024\n",
            "Loss: 0.0\n",
            "training error 0.030584328898803772, test error 0.045318231100492594\n",
            "Loss: 0.0\n",
            "training error 0.03057435599864651, test error 0.04521878362744178\n",
            "Loss: 0.0\n",
            "training error 0.030439268507051778, test error 0.045128495769125795\n",
            "Loss: 0.0\n",
            "training error 0.030402228892976772, test error 0.04498244572221708\n",
            "Loss: 0.0\n",
            "training error 0.03035587234795788, test error 0.04483630571536961\n",
            "Loss: 0.0\n",
            "training error 0.030291620958742528, test error 0.04477330221691794\n",
            "Loss: 0.0\n",
            "training error 0.030229479873319958, test error 0.04474711861097809\n",
            "Loss: 0.0\n",
            "training error 0.030188325440668504, test error 0.04473902915536405\n",
            "Loss: 0.0\n",
            "training error 0.030149429935877315, test error 0.04454266677637233\n",
            "Loss: 0.0\n",
            "training error 0.030170484057386485, test error 0.044710040220347774\n",
            "Loss: 0.37575981881765586\n",
            "training error 0.03002575748165401, test error 0.04452032499328452\n",
            "Loss: 0.0\n",
            "training error 0.030102461652685392, test error 0.04442611678461358\n",
            "Loss: 0.0\n",
            "training error 0.02995115775506164, test error 0.04455175975886336\n",
            "Loss: 0.2828133164528568\n",
            "training error 0.029890857448074303, test error 0.04449633932400482\n",
            "Loss: 0.15806589563454043\n",
            "training error 0.02986285556734391, test error 0.04431063256702157\n",
            "Loss: 0.0\n",
            "training error 0.029811191971431988, test error 0.04421045815423897\n",
            "Loss: 0.0\n",
            "training error 0.02977213481856728, test error 0.04407262333278052\n",
            "Loss: 0.0\n",
            "training error 0.029719349025445617, test error 0.04400702110548685\n",
            "Loss: 0.0\n",
            "training error 0.02970494884988657, test error 0.044058907149482206\n",
            "Loss: 0.1179040132504916\n",
            "training error 0.029632452397393517, test error 0.04393175398969886\n",
            "Loss: 0.0\n",
            "training error 0.029665559916953622, test error 0.044074667157338134\n",
            "Loss: 0.32530722008683455\n",
            "training error 0.02958629961028847, test error 0.04386574513538833\n",
            "Loss: 0.0\n",
            "training error 0.029609078599398805, test error 0.04383373440615639\n",
            "Loss: 0.0\n",
            "training error 0.02953086458265866, test error 0.043618572259680945\n",
            "Loss: 0.0\n",
            "training error 0.02946146554066613, test error 0.04351444940494878\n",
            "Loss: 0.0\n",
            "training error 0.029435246732509565, test error 0.043618528164153875\n",
            "Loss: 0.2391820662523525\n",
            "training error 0.029357986751709073, test error 0.043650034745341244\n",
            "Loss: 0.31158693777944\n",
            "training error 0.02933707380554308, test error 0.043526016051893164\n",
            "Loss: 0.026581163504446614\n",
            "training error 0.029269754758763828, test error 0.04356289938963555\n",
            "Loss: 0.11134229054787781\n",
            "training error 0.029310998773454656, test error 0.04350437237282171\n",
            "Loss: 0.0\n",
            "training error 0.029184286465277296, test error 0.04371213898371206\n",
            "Loss: 0.47757638958640936\n",
            "training error 0.029225755452859235, test error 0.043771446590191145\n",
            "Loss: 0.6139020121487482\n",
            "training error 0.02912381200476206, test error 0.04357093410900276\n",
            "Loss: 0.15300010677232745\n",
            "training error 0.029095821984623866, test error 0.043723619293646473\n",
            "Loss: 0.5039652542182971\n",
            "training error 0.029040936026806685, test error 0.043487352150001365\n",
            "Loss: 0.0\n",
            "training error 0.02902691389308802, test error 0.04327260275376519\n",
            "Loss: 0.0\n",
            "training error 0.02899334225362846, test error 0.04327505377270008\n",
            "Loss: 0.005664135686123117\n",
            "training error 0.028943373261979932, test error 0.043243462921284816\n",
            "Loss: 0.0\n",
            "training error 0.028913840996015964, test error 0.043239506707627454\n",
            "Loss: 0.0\n",
            "training error 0.028847904759261533, test error 0.04331065555023007\n",
            "Loss: 0.16454591650110206\n",
            "training error 0.028807538832287892, test error 0.04330370041154358\n",
            "Loss: 0.1484607684129724\n",
            "training error 0.02877927485312533, test error 0.0432516347717458\n",
            "Loss: 0.02804857187745924\n",
            "training error 0.02874288107312424, test error 0.043294361971540976\n",
            "Loss: 0.12686375976589392\n",
            "training error 0.028712472414911636, test error 0.043307504367200635\n",
            "Loss: 0.15725817603091663\n",
            "training error 0.028684189864243276, test error 0.04331810008447204\n",
            "Loss: 0.18176288960929377\n",
            "training error 0.028636166969232033, test error 0.043276029695467434\n",
            "Loss: 0.08446670792741084\n",
            "training error 0.028633091735707446, test error 0.04317219466346776\n",
            "Loss: 0.0\n",
            "training error 0.028562367532062802, test error 0.04326631332294316\n",
            "Loss: 0.2180075861536901\n",
            "training error 0.028544441729154522, test error 0.04332747669921399\n",
            "Loss: 0.3596806624186488\n",
            "training error 0.028560798235028897, test error 0.043368438398835904\n",
            "Loss: 0.4545604801837966\n",
            "training error 0.028465258268228103, test error 0.04327323705628108\n",
            "Loss: 0.2340450690565099\n",
            "training error 0.028438233193370084, test error 0.04329279894957385\n",
            "Loss: 0.2793563937303123\n",
            "training error 0.028405594829658494, test error 0.0432001126607813\n",
            "Loss: 0.06466661593453882\n",
            "training error 0.028373209892933078, test error 0.04321497478667452\n",
            "Loss: 0.0990918426553078\n",
            "training error 0.02835101835486792, test error 0.0430611614110204\n",
            "Loss: 0.0\n",
            "training error 0.028388819395559442, test error 0.042938552185410116\n",
            "Loss: 0.0\n",
            "training error 0.028276107255992763, test error 0.04304537043183157\n",
            "Loss: 0.24877002363798972\n",
            "training error 0.028385998834694972, test error 0.04287496325764083\n",
            "Loss: 0.0\n",
            "training error 0.02820073666368754, test error 0.04309758649045322\n",
            "Loss: 0.5192383057557981\n",
            "training error 0.028214277719104813, test error 0.04316012871162366\n",
            "Loss: 0.6651095005475449\n",
            "training error 0.028189977840856903, test error 0.04314865033140531\n",
            "Loss: 0.6383377453173855\n",
            "training error 0.028150067345097635, test error 0.04306546397534619\n",
            "Loss: 0.44431692351691865\n",
            "training error 0.02808877877711518, test error 0.04305508283060235\n",
            "Loss: 0.42010432027466127\n",
            "training error 0.02807530444968078, test error 0.043055325338038714\n",
            "Loss: 0.42066993577130063\n",
            "training error 0.02802510093996771, test error 0.04300018390428518\n",
            "Loss: 0.2920600675314633\n",
            "training error 0.028050924916786773, test error 0.043103033045916124\n",
            "Loss: 0.5319416529987464\n",
            "training error 0.02799020554389639, test error 0.043008443190160645\n",
            "Loss: 0.3113237245655931\n",
            "training error 0.027952661916577897, test error 0.042945921893281114\n",
            "Loss: 0.16550133282653423\n",
            "training error 0.027941061485153806, test error 0.04290559638666481\n",
            "Loss: 0.07144759247932342\n",
            "training error 0.027873470068491495, test error 0.04296015537470616\n",
            "Loss: 0.1986989855907284\n",
            "training error 0.027905720366359816, test error 0.04291374238880425\n",
            "Loss: 0.09044703066074522\n",
            "training error 0.02784648007458965, test error 0.04295458137952946\n",
            "Loss: 0.1856984026090025\n",
            "training error 0.027797260085729227, test error 0.043064542305523146\n",
            "Loss: 0.4421672544489885\n",
            "training error 0.02777180735441494, test error 0.04305324731023645\n",
            "Loss: 0.41582321954258816\n",
            "training error 0.027741728817649795, test error 0.043010624240542396\n",
            "Loss: 0.31641072689989613\n",
            "training error 0.027721495433176466, test error 0.043053668915210896\n",
            "Loss: 0.41680655560263347\n",
            "training error 0.027805086202639814, test error 0.0432515585102404\n",
            "Loss: 0.8783570270056273\n",
            "training error 0.027656042434554304, test error 0.04311471314983632\n",
            "Loss: 0.5591839012311528\n",
            "training error 0.027631933540383753, test error 0.04312232453574317\n",
            "Loss: 0.5769364200172378\n",
            "training error 0.02759365111211458, test error 0.043231379958804265\n",
            "Loss: 0.8312933098547193\n",
            "training error 0.027562390294437994, test error 0.04317286040515273\n",
            "Loss: 0.6948044380161944\n",
            "training error 0.027571568233681863, test error 0.04328814490399125\n",
            "Loss: 0.9636897969277802\n",
            "training error 0.02753748958570158, test error 0.04320477465005036\n",
            "Loss: 0.769240058417453\n",
            "training error 0.02750590811207927, test error 0.04310447416378818\n",
            "Loss: 0.535302863744036\n",
            "training error 0.02754154511874627, test error 0.043256588182464285\n",
            "Loss: 0.8900880509919773\n",
            "training error 0.02743254522008563, test error 0.04306790880645041\n",
            "Loss: 0.45001915838422946\n",
            "training error 0.027445854623214697, test error 0.04315704729503889\n",
            "Loss: 0.657922516931353\n",
            "training error 0.027414578868876483, test error 0.04315996878354133\n",
            "Loss: 0.6647364901233077\n",
            "training error 0.02737783514462523, test error 0.043136108196173154\n",
            "Loss: 0.609084926704373\n",
            "training error 0.027327044876846725, test error 0.04312734637872232\n",
            "Loss: 0.5886491833588137\n",
            "training error 0.027325401173848417, test error 0.043122173713439474\n",
            "Loss: 0.5765846475788816\n",
            "training error 0.02729879699838114, test error 0.043085024529853785\n",
            "Loss: 0.4899392471794606\n",
            "training error 0.02728756985024703, test error 0.04303846808391922\n",
            "Loss: 0.3813526913034826\n",
            "training error 0.027242156557746002, test error 0.04309446074038248\n",
            "Loss: 0.5119479203344435\n",
            "training error 0.027314835204704027, test error 0.042830064405958726\n",
            "Loss: 0.0\n",
            "training error 0.027272009129160897, test error 0.04315701331786622\n",
            "Loss: 0.7633631105677496\n",
            "training error 0.02717240909014626, test error 0.04287345824632479\n",
            "Loss: 0.10131630892440402\n",
            "training error 0.027160045847260127, test error 0.04287980516996525\n",
            "Loss: 0.11613516042157102\n",
            "training error 0.027112097700757773, test error 0.04304317222050354\n",
            "Loss: 0.49756594462455617\n",
            "training error 0.027105598051108428, test error 0.04311294442813318\n",
            "Loss: 0.6604706906186752\n",
            "training error 0.02710420735505558, test error 0.04314773941856925\n",
            "Loss: 0.7417103313212214\n",
            "training error 0.027047091191970466, test error 0.043034850306610764\n",
            "Loss: 0.47813586902649696\n",
            "training error 0.027030011688765645, test error 0.04301918995527915\n",
            "Loss: 0.44157194705061187\n",
            "training error 0.02698570848087118, test error 0.0429864985008687\n",
            "Loss: 0.36524366021781773\n",
            "training error 0.02713521985670217, test error 0.042779129859389045\n",
            "Loss: 0.0\n",
            "training error 0.026998603245278304, test error 0.042859041989228706\n",
            "Loss: 0.18680167198894537\n",
            "training error 0.02696577180797324, test error 0.0430057151894278\n",
            "Loss: 0.52966325117767\n",
            "training error 0.02692790508972234, test error 0.04294540125222444\n",
            "Loss: 0.3886740880001849\n",
            "training error 0.02690943779392473, test error 0.04311927083634306\n",
            "Loss: 0.7951096202097219\n",
            "training error 0.02688625859232096, test error 0.04302722938443669\n",
            "Loss: 0.5799545850117216\n",
            "training error 0.026889511872474802, test error 0.04285588122860473\n",
            "Loss: 0.17941311445079933\n",
            "training error 0.026879372757219265, test error 0.043027989432113764\n",
            "Loss: 0.5817312636855698\n",
            "training error 0.026822701019256247, test error 0.04304192894268202\n",
            "Loss: 0.6143161026340938\n",
            "training error 0.026775206650902928, test error 0.04305741344179043\n",
            "Loss: 0.6505124889544911\n",
            "training error 0.026848088894674427, test error 0.04280282401652366\n",
            "Loss: 0.05538718812769794\n",
            "training error 0.0268053401392361, test error 0.043125618761445265\n",
            "Loss: 0.8099484566308268\n",
            "training error 0.02675285048555078, test error 0.04267814603430867\n",
            "Loss: 0.0\n",
            "training error 0.026723066584837286, test error 0.042731569787583905\n",
            "Loss: 0.12517824282312073\n",
            "training error 0.02666795393789726, test error 0.04285849541361382\n",
            "Loss: 0.42258016353422523\n",
            "training error 0.026650033141277178, test error 0.042930853163475106\n",
            "Loss: 0.5921230246583198\n",
            "training error 0.02664339209490254, test error 0.042964813002510954\n",
            "Loss: 0.6716949887463297\n",
            "training error 0.02661220370568024, test error 0.04294338263986181\n",
            "Loss: 0.6214810862213316\n",
            "training error 0.02658500356883361, test error 0.042917886062103894\n",
            "Loss: 0.5617395554214033\n",
            "training error 0.026576491438322816, test error 0.04271537176171794\n",
            "Loss: 0.08722433111163497\n",
            "training error 0.02657922482149987, test error 0.042847946176948724\n",
            "Loss: 0.3978620404540445\n",
            "training error 0.026542942106805432, test error 0.0428208409254283\n",
            "Loss: 0.33435119464870766\n",
            "training error 0.026595489769279397, test error 0.042596294704137665\n",
            "Loss: 0.0\n",
            "training error 0.02653332023440002, test error 0.04290233001601591\n",
            "Loss: 0.7184552412454748\n",
            "training error 0.02648315750997694, test error 0.04276525388182186\n",
            "Loss: 0.3966522883216417\n",
            "training error 0.026457099318418765, test error 0.0428329768969432\n",
            "Loss: 0.5556403308068614\n",
            "training error 0.0264479149481908, test error 0.04269254383734866\n",
            "Loss: 0.22595658584747547\n",
            "training error 0.026415100809660422, test error 0.04267126865476896\n",
            "Loss: 0.17601049845308303\n",
            "training error 0.02639751606831968, test error 0.0427604433587189\n",
            "Loss: 0.38535899829166276\n",
            "training error 0.026404939601312776, test error 0.042826655586869204\n",
            "Loss: 0.5408002839955017\n",
            "training error 0.026352130796284637, test error 0.04275851672445396\n",
            "Loss: 0.38083598924048\n",
            "training error 0.026373583443065993, test error 0.04289794854736417\n",
            "Loss: 0.7081692088988456\n",
            "training error 0.026347336178456318, test error 0.04272212411693573\n",
            "Loss: 0.2953999019680964\n",
            "training error 0.026327664658019712, test error 0.042852526710778556\n",
            "Loss: 0.6015359045208335\n",
            "training error 0.02627247955792628, test error 0.04285058726509019\n",
            "Loss: 0.5969828190897086\n",
            "training error 0.026312997980163212, test error 0.04258239961908595\n",
            "Loss: 0.0\n",
            "training error 0.02632953731521276, test error 0.04227876706176087\n",
            "Loss: 0.0\n",
            "training error 0.026243919323931445, test error 0.04251797465803105\n",
            "Loss: 0.5657865942986229\n",
            "training error 0.0262757693050229, test error 0.0427016879286722\n",
            "Loss: 1.0003150429943375\n",
            "training error 0.02622148271779938, test error 0.04283156309897859\n",
            "Loss: 1.307502738691979\n",
            "training error 0.026165509216860602, test error 0.042709330424540025\n",
            "Loss: 1.0183914827747609\n",
            "training error 0.026151498537979732, test error 0.04271752837415952\n",
            "Loss: 1.0377817114621823\n",
            "training error 0.026164517230331353, test error 0.042874171619673204\n",
            "Loss: 1.4082826896124168\n",
            "training error 0.02611943844927172, test error 0.04291268553144613\n",
            "Loss: 1.4993778526209978\n",
            "training error 0.026136897945069907, test error 0.04259724875352488\n",
            "Loss: 0.7532899228086976\n",
            "training error 0.02610831292238496, test error 0.042300169467800075\n",
            "Loss: 0.05062211489739177\n",
            "training error 0.02612926212410102, test error 0.042064713083538784\n",
            "Loss: 0.0\n",
            "training error 0.026081169928813455, test error 0.04222973986416547\n",
            "Loss: 0.39231642992298976\n",
            "training error 0.026070204573941352, test error 0.0423579259787904\n",
            "Loss: 0.6970519320298463\n",
            "training error 0.02604752586484324, test error 0.0425045642089138\n",
            "Loss: 1.0456534542420126\n",
            "training error 0.02603203764040131, test error 0.04268275609371877\n",
            "Loss: 1.469267147864728\n",
            "training error 0.02601222131087521, test error 0.04271181060876389\n",
            "Loss: 1.5383381409020735\n",
            "training error 0.02599531979604978, test error 0.04265894043984548\n",
            "Loss: 1.412650444391672\n",
            "training error 0.02600177590740441, test error 0.04268816142307506\n",
            "Loss: 1.4821171804931588\n",
            "training error 0.02597089838014161, test error 0.04252723295228551\n",
            "Loss: 1.0995436194422226\n",
            "training error 0.025918863977765834, test error 0.04263499892718344\n",
            "Loss: 1.3557345381438601\n",
            "training error 0.02590578976469597, test error 0.042840347523373276\n",
            "Loss: 1.8439075961224738\n",
            "training error 0.025906489462132278, test error 0.042910292248035505\n",
            "Loss: 2.0101864544218806\n",
            "training error 0.025863281938174824, test error 0.04292754228988452\n",
            "Loss: 2.05119479748308\n",
            "training error 0.025886045850178292, test error 0.043078175896463455\n",
            "Loss: 2.409294486121838\n",
            "training error 0.0259773994023248, test error 0.04274747229916133\n",
            "Loss: 1.6231163024138962\n",
            "training error 0.025984890432581006, test error 0.04311667325195976\n",
            "Loss: 2.500813844449201\n",
            "training error 0.025833862882007043, test error 0.042717331846349064\n",
            "Loss: 1.5514637209439908\n",
            "training error 0.025793648502419293, test error 0.042806377132857905\n",
            "Loss: 1.763150144032144\n",
            "training error 0.02574799311330673, test error 0.0429436568701212\n",
            "Loss: 2.089503819595473\n",
            "training error 0.02576603082178044, test error 0.04278157588981075\n",
            "Loss: 1.7041904097819582\n",
            "training error 0.025746537282713318, test error 0.043053085834567455\n",
            "Loss: 2.3496481458599394\n",
            "training error 0.0258618689641817, test error 0.043166152778941445\n",
            "Loss: 2.618441003544336\n",
            "training error 0.025738348541772316, test error 0.043265377792051814\n",
            "Loss: 2.854327583617544\n",
            "training error 0.025754233395925658, test error 0.042941746313287794\n",
            "Loss: 2.0849618729295916\n",
            "training error 0.02564016619400298, test error 0.04314126305469131\n",
            "Loss: 2.5592709238609235\n",
            "training error 0.02566009905146656, test error 0.043078861280719236\n",
            "Loss: 2.410923842904622\n",
            "training error 0.025682400180845995, test error 0.04270297195695649\n",
            "Loss: 1.5173261069204358\n",
            "training error 0.025681165155517452, test error 0.04270337701200442\n",
            "Loss: 1.5182890400257332\n",
            "training error 0.025603238710520684, test error 0.04287873465935809\n",
            "Loss: 1.9351649307643992\n",
            "training error 0.025595229234714873, test error 0.042894586293405534\n",
            "Loss: 1.97284885366662\n",
            "training error 0.02558425174429579, test error 0.04297711772030701\n",
            "Loss: 2.1690499468193902\n",
            "training error 0.02557100807574622, test error 0.042982203563849845\n",
            "Loss: 2.1811404691836733\n",
            "training error 0.025554552440843363, test error 0.04293883406090818\n",
            "Loss: 2.0780386059770173\n",
            "training error 0.02555042395453464, test error 0.04292621036465139\n",
            "Loss: 2.048028425634829\n",
            "training error 0.025529123636782947, test error 0.04290748603896541\n",
            "Loss: 2.003515283113688\n",
            "training error 0.025556960402673766, test error 0.0425387678585264\n",
            "Loss: 1.1269654307308974\n",
            "training error 0.02549220927518162, test error 0.04269674286069232\n",
            "Loss: 1.5025177418858116\n",
            "training error 0.025506932316128637, test error 0.0425772709329184\n",
            "Loss: 1.2184983845288544\n",
            "training error 0.025487620924067086, test error 0.042784130065013255\n",
            "Loss: 1.7102624236274622\n",
            "training error 0.025447001903887326, test error 0.04272009620847006\n",
            "Loss: 1.558035409940195\n",
            "training error 0.0255607517026731, test error 0.04258204574565924\n",
            "Loss: 1.2298494966388018\n",
            "training error 0.025517264362311592, test error 0.0429717004004626\n",
            "Loss: 2.1561714093296658\n",
            "training error 0.02545016624457295, test error 0.04297094117905104\n",
            "Loss: 2.154366520253048\n",
            "training error 0.025430562601267, test error 0.04277028690396727\n",
            "Loss: 1.6773532224675813\n",
            "training error 0.02538603846623035, test error 0.043080860542908626\n",
            "Loss: 2.4156766678803177\n",
            "training error 0.02540421032417398, test error 0.0429923852275409\n",
            "Loss: 2.205345231191269\n",
            "training error 0.025433554011980358, test error 0.04308231891466405\n",
            "Loss: 2.419143639716115\n",
            "training error 0.025446194155459328, test error 0.04291688048632394\n",
            "Loss: 2.0258486040134915\n",
            "training error 0.02531746262797268, test error 0.04309934519990768\n",
            "Loss: 2.459620048552713\n",
            "training error 0.0253633819474103, test error 0.043062959515854746\n",
            "Loss: 2.3731207445382596\n",
            "training error 0.025299891320060164, test error 0.0431467172401813\n",
            "Loss: 2.572237101662145\n",
            "training error 0.02526271440528267, test error 0.04308242577145709\n",
            "Loss: 2.4193976692463703\n",
            "training error 0.02524884043329366, test error 0.043112748422163824\n",
            "Loss: 2.4914833878545206\n",
            "training error 0.025250512249763414, test error 0.04316968597194636\n",
            "Loss: 2.6268404261147626\n",
            "training error 0.025232598430809026, test error 0.04311150711317603\n",
            "Loss: 2.488532437053248\n",
            "training error 0.025233517286905996, test error 0.04315027108328606\n",
            "Loss: 2.5806856154977353\n",
            "training error 0.025201740848789442, test error 0.04312431035293029\n",
            "Loss: 2.518969444263619\n",
            "training error 0.025193266583514822, test error 0.04298831603440889\n",
            "Loss: 2.195671581156078\n",
            "training error 0.025199903153823998, test error 0.0431956350637559\n",
            "Loss: 2.6885289291552983\n",
            "training error 0.025183534633702428, test error 0.04271894600143687\n",
            "Loss: 1.555301034857437\n",
            "training error 0.0251639452645853, test error 0.04291079862535032\n",
            "Loss: 2.011390259886592\n",
            "training error 0.02514600621044929, test error 0.0427619353180511\n",
            "Loss: 1.657499085106462\n",
            "training error 0.025157436723082807, test error 0.04255772411483036\n",
            "Loss: 1.1720299394708311\n",
            "training error 0.0251439987999602, test error 0.04283275238743655\n",
            "Loss: 1.8258517593415347\n",
            "training error 0.025111535936761197, test error 0.0428528345768873\n",
            "Loss: 1.873592937109403\n",
            "training error 0.025099049571919465, test error 0.0425290421599953\n",
            "Loss: 1.1038446299024507\n",
            "training error 0.02507821404459284, test error 0.042545645636651926\n",
            "Loss: 1.1433159003320092\n",
            "training error 0.025067424276332694, test error 0.042817168481040016\n",
            "Loss: 1.7888043025680256\n",
            "training error 0.02504541056582622, test error 0.04286530943504406\n",
            "Loss: 1.9032492862017802\n",
            "training error 0.025026937674804246, test error 0.04296246256707498\n",
            "Loss: 2.1342104051757183\n",
            "training error 0.02508258426010792, test error 0.043165410417992565\n",
            "Loss: 2.6166761966683216\n",
            "training error 0.025002774515181285, test error 0.0430437172571029\n",
            "Loss: 2.3273763251869806\n",
            "training error 0.025019413582439566, test error 0.04317543373247365\n",
            "Loss: 2.640504516764497\n",
            "training error 0.024991610587309185, test error 0.04330849004621589\n",
            "Loss: 2.9568178920108545\n",
            "training error 0.024986826046902583, test error 0.04312560503744508\n",
            "Loss: 2.522047284143847\n",
            "training error 0.024972484786783856, test error 0.04329075855836758\n",
            "Loss: 2.9146650124390883\n",
            "training error 0.024949399688612595, test error 0.043265631589989015\n",
            "Loss: 2.85493093478435\n",
            "training error 0.025025461903966026, test error 0.042983450146082805\n",
            "Loss: 2.1841039560152264\n",
            "training error 0.024909913273258748, test error 0.04324591106480164\n",
            "Loss: 2.8080495376660464\n",
            "training error 0.024920605080454343, test error 0.043352214252632394\n",
            "Loss: 3.060762988057686\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9606481481481481 0.042064713083538784\n",
            "training error 0.12508784860829747, test error 0.2500330741885373\n",
            "training error 0.12503652959696374, test error 0.2500086137462041\n",
            "training error 0.1250681727900866, test error 0.2500799516443416\n",
            "training error 0.12509215022633607, test error 0.25019666666734564\n",
            "training error 0.12503364581344595, test error 0.2502070493018522\n",
            "training error 0.12512781529662964, test error 0.2502904306255652\n",
            "training error 0.1250490164338748, test error 0.2502668759109859\n",
            "training error 0.12503293045046707, test error 0.25031312797233746\n",
            "training error 0.12498900346188171, test error 0.250250824398161\n",
            "training error 0.12497663643021936, test error 0.2502731492445097\n",
            "training error 0.12506977673516395, test error 0.2503416902730832\n",
            "training error 0.1251803412605109, test error 0.2503394119675832\n",
            "training error 0.12498662535701456, test error 0.2502934716289592\n",
            "training error 0.12501418688443947, test error 0.25018784549058487\n",
            "training error 0.12516985756417556, test error 0.25022657466427733\n",
            "training error 0.12519000360735993, test error 0.2502462114494188\n",
            "training error 0.12497686293881466, test error 0.2504041922844714\n",
            "training error 0.12501957015997067, test error 0.2504546632805717\n",
            "training error 0.1250253963472475, test error 0.2504506712972849\n",
            "training error 0.1250336091730376, test error 0.25041046742809336\n",
            "training error 0.12506897841471945, test error 0.25040976168060847\n",
            "training error 0.12508163779448375, test error 0.25052156046587437\n",
            "training error 0.12499856005009778, test error 0.25049281707429805\n",
            "training error 0.12498817065307004, test error 0.25053533108146664\n",
            "training error 0.12507256413225354, test error 0.2506651921731493\n",
            "training error 0.12507461209993015, test error 0.2504736076359331\n",
            "training error 0.124962777931367, test error 0.2505474126039401\n",
            "training error 0.12498097817978389, test error 0.2505105051649923\n",
            "training error 0.1249799581607485, test error 0.25055244139694727\n",
            "training error 0.12504153257815415, test error 0.2504852700508224\n",
            "training error 0.12496992601460173, test error 0.2505540361451777\n",
            "training error 0.12504262897115137, test error 0.25048027919868426\n",
            "training error 0.12501413168825593, test error 0.2504823983401724\n",
            "training error 0.12498283681357113, test error 0.25060921011237164\n",
            "training error 0.125080806373746, test error 0.25051739138541723\n",
            "training error 0.12507119910094028, test error 0.2506684277629786\n",
            "training error 0.12507910509418194, test error 0.2507463710261592\n",
            "training error 0.12497125396952895, test error 0.25057515255886925\n",
            "training error 0.12501639883594534, test error 0.2505878667125673\n",
            "training error 0.1252086784141807, test error 0.25045224769299396\n",
            "training error 0.12505463561501412, test error 0.2504766432222738\n",
            "training error 0.12500336681126167, test error 0.2505625056861577\n",
            "training error 0.12496181690715537, test error 0.25062827582456876\n",
            "training error 0.12503760816020482, test error 0.250557998778168\n",
            "training error 0.1250692560822524, test error 0.2505998455111573\n",
            "training error 0.12508009854062266, test error 0.25065524525892713\n",
            "training error 0.12514373741391427, test error 0.25043987094989006\n",
            "training error 0.12508531446966767, test error 0.25056009159173603\n",
            "training error 0.12494636070298026, test error 0.2504518113971315\n",
            "training error 0.12508302527616605, test error 0.25040162613624656\n",
            "Loss: 0.15719953970922962\n",
            "training error 0.12495339085970458, test error 0.2504505318620559\n",
            "Loss: 0.17676115603777465\n",
            "training error 0.12497195241166147, test error 0.2504517196844004\n",
            "Loss: 0.17723626860557395\n",
            "training error 0.1250080515695471, test error 0.25061259814388975\n",
            "Loss: 0.24158543525176768\n",
            "training error 0.12498738861820838, test error 0.2505630409714724\n",
            "Loss: 0.22176324925795576\n",
            "training error 0.1250150834024432, test error 0.2506499143107742\n",
            "Loss: 0.2565113877320835\n",
            "training error 0.1249465598422101, test error 0.2505893928781582\n",
            "Loss: 0.23230364876294374\n",
            "training error 0.1250164803233041, test error 0.25052376680047933\n",
            "Loss: 0.20605412211844865\n",
            "training error 0.12500522821270676, test error 0.25058822747557924\n",
            "Loss: 0.231837503792387\n",
            "training error 0.1250272467309675, test error 0.250716990589236\n",
            "Loss: 0.28334097470379405\n",
            "training error 0.12505436451699614, test error 0.25077071747826446\n",
            "Loss: 0.30483098987701407\n",
            "training error 0.125012608786975, test error 0.2507113523663675\n",
            "Loss: 0.2810857632596475\n",
            "training error 0.12497367518389113, test error 0.25076134529259564\n",
            "Loss: 0.3010822447724548\n",
            "training error 0.1249668508340627, test error 0.25067057080526767\n",
            "Loss: 0.2647737008515838\n",
            "training error 0.1250902461551524, test error 0.25076228741715556\n",
            "Loss: 0.30145908161249846\n",
            "training error 0.12507125474903955, test error 0.25055200336413086\n",
            "Loss: 0.21734835843632005\n",
            "training error 0.12495239888101212, test error 0.2506291161052867\n",
            "Loss: 0.2481923921679341\n",
            "training error 0.12498921148729521, test error 0.2505849371141218\n",
            "Loss: 0.23052140455557257\n",
            "training error 0.12527123789201472, test error 0.2504530872762748\n",
            "Loss: 0.1777832865078377\n",
            "training error 0.12495175833987911, test error 0.25058996303525455\n",
            "Loss: 0.23253170374386567\n",
            "training error 0.12493961372004989, test error 0.25051008914980605\n",
            "Loss: 0.2005832503479299\n",
            "training error 0.1249411915278863, test error 0.25054455608372983\n",
            "Loss: 0.21436954891074222\n",
            "training error 0.1250191219262551, test error 0.2504735020286227\n",
            "Loss: 0.18594890610070802\n",
            "training error 0.12504898598637418, test error 0.25064222794331226\n",
            "Loss: 0.25343694667712136\n",
            "training error 0.12498366417089861, test error 0.25050997783763346\n",
            "Loss: 0.2005387270129555\n",
            "training error 0.12505506602123434, test error 0.25050822298545233\n",
            "Loss: 0.19983681032502698\n",
            "training error 0.12496039110588933, test error 0.25058828121062354\n",
            "Loss: 0.23185899706956725\n",
            "training error 0.12492882858521409, test error 0.25057800089873705\n",
            "Loss: 0.22774701399328467\n",
            "training error 0.12500164139035597, test error 0.250563831389032\n",
            "Loss: 0.22207940538860882\n",
            "training error 0.12520136654899097, test error 0.2507997339390403\n",
            "Loss: 0.3164371742964578\n",
            "training error 0.12495163182432233, test error 0.2506351530458157\n",
            "Loss: 0.25060708518132024\n",
            "training error 0.12517443818010185, test error 0.25071987441640853\n",
            "Loss: 0.28449446582927695\n",
            "training error 0.12511239069801333, test error 0.25057480319902226\n",
            "Loss: 0.22646797817651443\n",
            "training error 0.12492197668003788, test error 0.2505458763060416\n",
            "Loss: 0.2148976196407748\n",
            "training error 0.12491842580341476, test error 0.2505258898416687\n",
            "Loss: 0.20690330933545997\n",
            "training error 0.12492111271110326, test error 0.25054446104873596\n",
            "Loss: 0.21433153622292256\n",
            "training error 0.12491476090381909, test error 0.25051301006501003\n",
            "Loss: 0.20175157617488004\n",
            "training error 0.12492137923506115, test error 0.25054962018591276\n",
            "Loss: 0.21639511999289596\n",
            "training error 0.12491171171500866, test error 0.25054758504072794\n",
            "Loss: 0.2155810899663546\n",
            "training error 0.12497048722292978, test error 0.250623347739512\n",
            "Loss: 0.24588512535490548\n",
            "training error 0.12493989429321989, test error 0.2506738029096856\n",
            "Loss: 0.2660664980754568\n",
            "training error 0.12493055168806354, test error 0.2507316172248608\n",
            "Loss: 0.2891914273764451\n",
            "training error 0.12511408774676508, test error 0.2507885549256172\n",
            "Loss: 0.31196572299099223\n",
            "training error 0.12497672980584076, test error 0.25060322107182503\n",
            "Loss: 0.23783473565617275\n",
            "training error 0.12489594699374666, test error 0.25069379616586845\n",
            "Loss: 0.27406352501115894\n",
            "training error 0.12492175012107054, test error 0.25073046117918374\n",
            "Loss: 0.2887290250376928\n",
            "training error 0.12489916324830744, test error 0.25063438595998616\n",
            "Loss: 0.2503002614211214\n",
            "training error 0.12496585667457455, test error 0.2505516385329686\n",
            "Loss: 0.21720243099934144\n",
            "training error 0.12493067292116947, test error 0.25072842125581446\n",
            "Loss: 0.2879130838032262\n",
            "training error 0.12490330089025606, test error 0.25066930754174777\n",
            "Loss: 0.26426841285331903\n",
            "training error 0.12496420779343904, test error 0.2506033228811285\n",
            "Loss: 0.23787545797449017\n",
            "training error 0.12494819097406507, test error 0.2507041831318995\n",
            "Loss: 0.27821816827540147\n",
            "training error 0.124871715305918, test error 0.2506417354156053\n",
            "Loss: 0.2532399423821108\n",
            "training error 0.12488560277396792, test error 0.25067482370404187\n",
            "Loss: 0.2664748017498608\n",
            "training error 0.12486925138787511, test error 0.25069625996222394\n",
            "Loss: 0.27504900959849365\n",
            "training error 0.12503442315480018, test error 0.2507891424733077\n",
            "Loss: 0.3122007339698918\n",
            "training error 0.12488984953049798, test error 0.2507913397638137\n",
            "Loss: 0.31307961989028854\n",
            "training error 0.12496655279670671, test error 0.25073075467633005\n",
            "Loss: 0.28884641985136295\n",
            "training error 0.12500806135186518, test error 0.25060057733911134\n",
            "Loss: 0.23677727900535128\n",
            "training error 0.12506035896171303, test error 0.2505973771593914\n",
            "Loss: 0.23549725122071763\n",
            "training error 0.1249720359759257, test error 0.2507471717704059\n",
            "Loss: 0.29541303122921914\n",
            "training error 0.12490640957376861, test error 0.2509066851159632\n",
            "Loss: 0.35921617111591697\n",
            "training error 0.1249062576883013, test error 0.25076680578490484\n",
            "Loss: 0.30326636644224525\n",
            "training error 0.12484139454923575, test error 0.2508095095443252\n",
            "Loss: 0.3203472816877051\n",
            "training error 0.12487041006762341, test error 0.2508762665825211\n",
            "Loss: 0.347049176952674\n",
            "training error 0.12488643999758708, test error 0.2508336682813758\n",
            "Loss: 0.3300104435638662\n",
            "training error 0.12492352754652114, test error 0.25088678155127353\n",
            "Loss: 0.3512550195414077\n",
            "training error 0.12481937001125107, test error 0.25079017540928283\n",
            "Loss: 0.3126138941245138\n",
            "training error 0.12499419811920173, test error 0.2507886554877145\n",
            "Loss: 0.31200594644402724\n",
            "training error 0.12488895499082348, test error 0.2507603276502938\n",
            "Loss: 0.300675201876377\n",
            "training error 0.12481986575744572, test error 0.25075364140130335\n",
            "Loss: 0.2980007944268559\n",
            "training error 0.12493139029223, test error 0.2506121850383096\n",
            "Loss: 0.2414201987129383\n",
            "training error 0.12481705726894742, test error 0.25077251482999147\n",
            "Loss: 0.30554990579758634\n",
            "training error 0.12488091485798673, test error 0.25082770086301853\n",
            "Loss: 0.3276235584610543\n",
            "training error 0.12478930718691074, test error 0.25068023847423937\n",
            "Loss: 0.2686406352051085\n",
            "training error 0.12477336605251972, test error 0.2505903262862729\n",
            "Loss: 0.232676999145065\n",
            "training error 0.12484927162301825, test error 0.25053201103492867\n",
            "Loss: 0.2093517022801139\n",
            "training error 0.12475402079656192, test error 0.25062552722170417\n",
            "Loss: 0.2467568881951987\n",
            "training error 0.12486013123418953, test error 0.2504944856067355\n",
            "Loss: 0.19434204816024536\n",
            "training error 0.12479875651134778, test error 0.25056670653042573\n",
            "Loss: 0.22322942232229082\n",
            "training error 0.12475538035082265, test error 0.25058204754940266\n",
            "Loss: 0.22936561849051795\n",
            "training error 0.12470968020811263, test error 0.25054340175380474\n",
            "Loss: 0.21390783284913617\n",
            "training error 0.1247081074995216, test error 0.2504584922006436\n",
            "Loss: 0.17994518176729368\n",
            "training error 0.1247618396354234, test error 0.2503776716324016\n",
            "Loss: 0.14761806830070334\n",
            "training error 0.12478270456605078, test error 0.2503605124488163\n",
            "Loss: 0.14075463134619604\n",
            "training error 0.12475734088477734, test error 0.25039037106520573\n",
            "Loss: 0.15269766640488847\n",
            "training error 0.12466680283441271, test error 0.250383344302702\n",
            "Loss: 0.14988705824283688\n",
            "training error 0.1246548115069952, test error 0.2503116714305981\n",
            "Loss: 0.12121889716234513\n",
            "training error 0.12480863786853309, test error 0.250321482836781\n",
            "Loss: 0.1251433244194189\n",
            "training error 0.12486642209936075, test error 0.2501479046221\n",
            "Loss: 0.05571443071850091\n",
            "training error 0.12469092021657752, test error 0.250329358381547\n",
            "Loss: 0.12829343378886193\n",
            "training error 0.12473459934676379, test error 0.250136718108998\n",
            "Loss: 0.051239979644845945\n",
            "training error 0.12459907404930948, test error 0.250241396037699\n",
            "Loss: 0.09310970850437972\n",
            "training error 0.1245538902825567, test error 0.25017095190861316\n",
            "Loss: 0.06493302769714493\n",
            "training error 0.12491650461466151, test error 0.25009817238643534\n",
            "Loss: 0.03582222183839967\n",
            "training error 0.12458049610408871, test error 0.2501119568154758\n",
            "Loss: 0.0413358034842215\n",
            "training error 0.12449682474336474, test error 0.25014103665277115\n",
            "Loss: 0.052967337638021306\n",
            "training error 0.1245564371572825, test error 0.25016616208015874\n",
            "Loss: 0.06301716232648413\n",
            "training error 0.1246557673492672, test error 0.25031381246722934\n",
            "Loss: 0.12207528230809839\n",
            "training error 0.12444050823728166, test error 0.2501736256532649\n",
            "Loss: 0.06600248870958225\n",
            "training error 0.12441022009336382, test error 0.25017468051467917\n",
            "Loss: 0.06642441873769833\n",
            "training error 0.12444139501906583, test error 0.25001983791813714\n",
            "Loss: 0.0044895140870915284\n",
            "training error 0.12437885758486186, test error 0.24985401707020036\n",
            "Loss: 0.0\n",
            "training error 0.124328967672544, test error 0.24979125034436267\n",
            "Loss: 0.0\n",
            "training error 0.12430040305530085, test error 0.24965909967073835\n",
            "Loss: 0.0\n",
            "training error 0.12433909479370496, test error 0.24954922766709103\n",
            "Loss: 0.0\n",
            "training error 0.12424096042754074, test error 0.24953145094844864\n",
            "Loss: 0.0\n",
            "training error 0.12419455116913483, test error 0.24944399037209378\n",
            "Loss: 0.0\n",
            "training error 0.12454597427639018, test error 0.249550204947156\n",
            "Loss: 0.04258053076515722\n",
            "training error 0.12413315385980098, test error 0.24924034047756546\n",
            "Loss: 0.0\n",
            "training error 0.12412472432091211, test error 0.24913217440304034\n",
            "Loss: 0.0\n",
            "training error 0.12414524671259351, test error 0.24896829614759786\n",
            "Loss: 0.0\n",
            "training error 0.12418470741293033, test error 0.2490701755294099\n",
            "Loss: 0.04092062458893597\n",
            "training error 0.12396354998722878, test error 0.24898591930849692\n",
            "Loss: 0.007078475923139038\n",
            "training error 0.12399268946417911, test error 0.24886756853537964\n",
            "Loss: 0.0\n",
            "training error 0.12400427524374887, test error 0.24883216163974659\n",
            "Loss: 0.0\n",
            "training error 0.12389298872427704, test error 0.24873104037660781\n",
            "Loss: 0.0\n",
            "training error 0.12386987177557926, test error 0.24870181919724865\n",
            "Loss: 0.0\n",
            "training error 0.12369837062072213, test error 0.24851958108114755\n",
            "Loss: 0.0\n",
            "training error 0.12368679507207393, test error 0.24844807889170917\n",
            "Loss: 0.0\n",
            "training error 0.1237827142627229, test error 0.24822434523172002\n",
            "Loss: 0.0\n",
            "training error 0.12360021029151437, test error 0.24805415343876674\n",
            "Loss: 0.0\n",
            "training error 0.1233957960267826, test error 0.24792721770894968\n",
            "Loss: 0.0\n",
            "training error 0.12348547983603446, test error 0.24785382149608223\n",
            "Loss: 0.0\n",
            "training error 0.12323422224613859, test error 0.24759237982277527\n",
            "Loss: 0.0\n",
            "training error 0.12320593165056634, test error 0.2473157477577153\n",
            "Loss: 0.0\n",
            "training error 0.12311014246879776, test error 0.24725820087091266\n",
            "Loss: 0.0\n",
            "training error 0.12296856788361457, test error 0.24716673809767423\n",
            "Loss: 0.0\n",
            "training error 0.12287160158472626, test error 0.2470009827212977\n",
            "Loss: 0.0\n",
            "training error 0.12288821520347436, test error 0.24678408359405551\n",
            "Loss: 0.0\n",
            "training error 0.12267459199261811, test error 0.2467127856063998\n",
            "Loss: 0.0\n",
            "training error 0.12260181863874574, test error 0.24650082494008477\n",
            "Loss: 0.0\n",
            "training error 0.12244668891004315, test error 0.24605728009604688\n",
            "Loss: 0.0\n",
            "training error 0.12233006747325179, test error 0.2459038367524862\n",
            "Loss: 0.0\n",
            "training error 0.122158056849975, test error 0.24565452150222536\n",
            "Loss: 0.0\n",
            "training error 0.12201088084618382, test error 0.24545314588903458\n",
            "Loss: 0.0\n",
            "training error 0.12193861111433672, test error 0.24523540975232425\n",
            "Loss: 0.0\n",
            "training error 0.12174277891701302, test error 0.24495977334183136\n",
            "Loss: 0.0\n",
            "training error 0.12156959002743944, test error 0.24452676794814013\n",
            "Loss: 0.0\n",
            "training error 0.1214965559102707, test error 0.2442657353232929\n",
            "Loss: 0.0\n",
            "training error 0.12119302843619188, test error 0.2437711436754318\n",
            "Loss: 0.0\n",
            "training error 0.12106405667000171, test error 0.24339128007441654\n",
            "Loss: 0.0\n",
            "training error 0.12092412385150995, test error 0.24314110946453701\n",
            "Loss: 0.0\n",
            "training error 0.12061674416689719, test error 0.24264707650101916\n",
            "Loss: 0.0\n",
            "training error 0.12048410161971704, test error 0.2420641731011812\n",
            "Loss: 0.0\n",
            "training error 0.12023784577644028, test error 0.2417351723973247\n",
            "Loss: 0.0\n",
            "training error 0.11997146731729627, test error 0.24104676487484802\n",
            "Loss: 0.0\n",
            "training error 0.11972470600169385, test error 0.24050830576102616\n",
            "Loss: 0.0\n",
            "training error 0.11954056260153907, test error 0.24011896661715973\n",
            "Loss: 0.0\n",
            "training error 0.11919862324670424, test error 0.23946047616072855\n",
            "Loss: 0.0\n",
            "training error 0.11887160814117029, test error 0.23901798782297304\n",
            "Loss: 0.0\n",
            "training error 0.11858178379659953, test error 0.2384428471947108\n",
            "Loss: 0.0\n",
            "training error 0.118266605264316, test error 0.23782601380456347\n",
            "Loss: 0.0\n",
            "training error 0.11797684569891176, test error 0.2370690082910238\n",
            "Loss: 0.0\n",
            "training error 0.11766476628304924, test error 0.23642481134749724\n",
            "Loss: 0.0\n",
            "training error 0.11728406351682606, test error 0.23561848800104107\n",
            "Loss: 0.0\n",
            "training error 0.11689196847016656, test error 0.23477467449267292\n",
            "Loss: 0.0\n",
            "training error 0.11650305457538206, test error 0.23403643649046815\n",
            "Loss: 0.0\n",
            "training error 0.1161489127884204, test error 0.23319979021469545\n",
            "Loss: 0.0\n",
            "training error 0.11572633538794672, test error 0.23224652155845282\n",
            "Loss: 0.0\n",
            "training error 0.11526634067108711, test error 0.23141513648985823\n",
            "Loss: 0.0\n",
            "training error 0.11482127528290648, test error 0.2305542258976283\n",
            "Loss: 0.0\n",
            "training error 0.11433397517346167, test error 0.22961927752984326\n",
            "Loss: 0.0\n",
            "training error 0.11392984954445323, test error 0.22861971744845275\n",
            "Loss: 0.0\n",
            "training error 0.1134467085973096, test error 0.22755030791534286\n",
            "Loss: 0.0\n",
            "training error 0.11286979589099935, test error 0.2264026331050197\n",
            "Loss: 0.0\n",
            "training error 0.11235671883175864, test error 0.2251815069131444\n",
            "Loss: 0.0\n",
            "training error 0.11178838502337109, test error 0.22404549828154557\n",
            "Loss: 0.0\n",
            "training error 0.11114075453157314, test error 0.22278277760445714\n",
            "Loss: 0.0\n",
            "training error 0.11055031476788227, test error 0.2216852308713057\n",
            "Loss: 0.0\n",
            "training error 0.11014403734746059, test error 0.2204159805547374\n",
            "Loss: 0.0\n",
            "training error 0.10937235705252679, test error 0.21905098269122822\n",
            "Loss: 0.0\n",
            "training error 0.10873800837767446, test error 0.21743393039128278\n",
            "Loss: 0.0\n",
            "training error 0.10789923455534227, test error 0.21614900547874016\n",
            "Loss: 0.0\n",
            "training error 0.1072560440334673, test error 0.21465681121347416\n",
            "Loss: 0.0\n",
            "training error 0.1065654781363275, test error 0.21315178842155993\n",
            "Loss: 0.0\n",
            "training error 0.10574779014507174, test error 0.2115232618863139\n",
            "Loss: 0.0\n",
            "training error 0.10499439899528802, test error 0.20990196065644925\n",
            "Loss: 0.0\n",
            "training error 0.1042015239970808, test error 0.2082914739805084\n",
            "Loss: 0.0\n",
            "training error 0.10349545848315536, test error 0.20646516208641455\n",
            "Loss: 0.0\n",
            "training error 0.1028241862653874, test error 0.20473815404060003\n",
            "Loss: 0.0\n",
            "training error 0.10190142045723034, test error 0.20320037376742575\n",
            "Loss: 0.0\n",
            "training error 0.10092345630754414, test error 0.20131722853860418\n",
            "Loss: 0.0\n",
            "training error 0.10009659922242845, test error 0.19949830604988483\n",
            "Loss: 0.0\n",
            "training error 0.09922800232207897, test error 0.19763644141889067\n",
            "Loss: 0.0\n",
            "training error 0.09836855763619318, test error 0.19562113340490536\n",
            "Loss: 0.0\n",
            "training error 0.0973975486489628, test error 0.1937671742416654\n",
            "Loss: 0.0\n",
            "training error 0.09647701091088856, test error 0.19182303600646428\n",
            "Loss: 0.0\n",
            "training error 0.09558811761011408, test error 0.1897927203809029\n",
            "Loss: 0.0\n",
            "training error 0.09459563129893003, test error 0.1878562134126239\n",
            "Loss: 0.0\n",
            "training error 0.09368194617976662, test error 0.1859039007110325\n",
            "Loss: 0.0\n",
            "training error 0.09269875361726507, test error 0.18379437626298661\n",
            "Loss: 0.0\n",
            "training error 0.09190996962084019, test error 0.18161632479207607\n",
            "Loss: 0.0\n",
            "training error 0.09069588960494156, test error 0.17970955892446341\n",
            "Loss: 0.0\n",
            "training error 0.08974288007778405, test error 0.17768756621852336\n",
            "Loss: 0.0\n",
            "training error 0.08875333357199586, test error 0.1755508182648724\n",
            "Loss: 0.0\n",
            "training error 0.08780249689137785, test error 0.17342887205798688\n",
            "Loss: 0.0\n",
            "training error 0.08684897320519336, test error 0.1714966590802812\n",
            "Loss: 0.0\n",
            "training error 0.08580621747897128, test error 0.16928732515937575\n",
            "Loss: 0.0\n",
            "training error 0.08485813648834459, test error 0.16704479127844177\n",
            "Loss: 0.0\n",
            "training error 0.08392493350885499, test error 0.16506088072681518\n",
            "Loss: 0.0\n",
            "training error 0.08282427021881525, test error 0.1629582618032198\n",
            "Loss: 0.0\n",
            "training error 0.08181325371602742, test error 0.1609130062338901\n",
            "Loss: 0.0\n",
            "training error 0.08087844988064607, test error 0.1586582662484591\n",
            "Loss: 0.0\n",
            "training error 0.0798560662701127, test error 0.15670650253881777\n",
            "Loss: 0.0\n",
            "training error 0.07889064480790485, test error 0.15460229752936194\n",
            "Loss: 0.0\n",
            "training error 0.07795435735931977, test error 0.15254876180716417\n",
            "Loss: 0.0\n",
            "training error 0.07693905900857476, test error 0.15053306838171512\n",
            "Loss: 0.0\n",
            "training error 0.07603175336182878, test error 0.14842416315040363\n",
            "Loss: 0.0\n",
            "training error 0.07506509632255369, test error 0.14629944082268778\n",
            "Loss: 0.0\n",
            "training error 0.07413920757722658, test error 0.14417486925090983\n",
            "Loss: 0.0\n",
            "training error 0.07324333141669914, test error 0.14230021356241734\n",
            "Loss: 0.0\n",
            "training error 0.07232054572516634, test error 0.14029562354932593\n",
            "Loss: 0.0\n",
            "training error 0.0713501252224486, test error 0.13835500181953653\n",
            "Loss: 0.0\n",
            "training error 0.07051632549546608, test error 0.13633014739510543\n",
            "Loss: 0.0\n",
            "training error 0.06960051091565257, test error 0.13445705915065925\n",
            "Loss: 0.0\n",
            "training error 0.06894814314543801, test error 0.13266325979964155\n",
            "Loss: 0.0\n",
            "training error 0.06787620200895286, test error 0.1306141343205301\n",
            "Loss: 0.0\n",
            "training error 0.06706104296520733, test error 0.12876315578927158\n",
            "Loss: 0.0\n",
            "training error 0.06626192496645014, test error 0.1270974345441492\n",
            "Loss: 0.0\n",
            "training error 0.06548854092828078, test error 0.12505922766241806\n",
            "Loss: 0.0\n",
            "training error 0.0645634155482353, test error 0.12346767167239263\n",
            "Loss: 0.0\n",
            "training error 0.06391195707439483, test error 0.12178017289238796\n",
            "Loss: 0.0\n",
            "training error 0.06299528597004693, test error 0.1199024561244235\n",
            "Loss: 0.0\n",
            "training error 0.062258776171596406, test error 0.11824721069885402\n",
            "Loss: 0.0\n",
            "training error 0.06150857551409193, test error 0.11663803913228311\n",
            "Loss: 0.0\n",
            "training error 0.06084656930199597, test error 0.11503226697052127\n",
            "Loss: 0.0\n",
            "training error 0.06006125773179935, test error 0.11342949094376523\n",
            "Loss: 0.0\n",
            "training error 0.059390140935629256, test error 0.1116274583596136\n",
            "Loss: 0.0\n",
            "training error 0.05867415387296369, test error 0.1101008338235044\n",
            "Loss: 0.0\n",
            "training error 0.05802952601908019, test error 0.10846080326300044\n",
            "Loss: 0.0\n",
            "training error 0.0573230738151212, test error 0.10710159668760713\n",
            "Loss: 0.0\n",
            "training error 0.05675180281926086, test error 0.10573020457666767\n",
            "Loss: 0.0\n",
            "training error 0.05606179708658172, test error 0.10421299641351539\n",
            "Loss: 0.0\n",
            "training error 0.05544172324249175, test error 0.10288651735592232\n",
            "Loss: 0.0\n",
            "training error 0.054796416630076575, test error 0.10142251909436245\n",
            "Loss: 0.0\n",
            "training error 0.054200826380430764, test error 0.09994735054977408\n",
            "Loss: 0.0\n",
            "training error 0.053543346492472235, test error 0.09865403832104387\n",
            "Loss: 0.0\n",
            "training error 0.05300460150758199, test error 0.09728805144719484\n",
            "Loss: 0.0\n",
            "training error 0.052482161645556406, test error 0.09610697126284669\n",
            "Loss: 0.0\n",
            "training error 0.05193121201856978, test error 0.09477161029483798\n",
            "Loss: 0.0\n",
            "training error 0.05139837228837784, test error 0.09353491696800954\n",
            "Loss: 0.0\n",
            "training error 0.05089090203392698, test error 0.09232522926282559\n",
            "Loss: 0.0\n",
            "training error 0.05032035656780738, test error 0.09119180696763068\n",
            "Loss: 0.0\n",
            "training error 0.049901391065322924, test error 0.08974231709608019\n",
            "Loss: 0.0\n",
            "training error 0.04941194771886298, test error 0.0888314972982268\n",
            "Loss: 0.0\n",
            "training error 0.04904555548887464, test error 0.08777079734146909\n",
            "Loss: 0.0\n",
            "training error 0.048393201241432766, test error 0.08654605967389163\n",
            "Loss: 0.0\n",
            "training error 0.047924261388359646, test error 0.08547943952870983\n",
            "Loss: 0.0\n",
            "training error 0.04753255236465634, test error 0.0843590801100146\n",
            "Loss: 0.0\n",
            "training error 0.04707486066451401, test error 0.08334630343143631\n",
            "Loss: 0.0\n",
            "training error 0.04664959628645136, test error 0.08233493234553854\n",
            "Loss: 0.0\n",
            "training error 0.04624635907084239, test error 0.08142964467858657\n",
            "Loss: 0.0\n",
            "training error 0.045849369463015266, test error 0.08048289965775401\n",
            "Loss: 0.0\n",
            "training error 0.04546344062317714, test error 0.07971707788578293\n",
            "Loss: 0.0\n",
            "training error 0.04503272747937099, test error 0.07871460777520405\n",
            "Loss: 0.0\n",
            "training error 0.04465590286498278, test error 0.0778463932977628\n",
            "Loss: 0.0\n",
            "training error 0.04430538880927238, test error 0.07693957281621656\n",
            "Loss: 0.0\n",
            "training error 0.04395383212493887, test error 0.07617087234265583\n",
            "Loss: 0.0\n",
            "training error 0.043609307921141205, test error 0.07530167273709823\n",
            "Loss: 0.0\n",
            "training error 0.04328665499935726, test error 0.07445273977183225\n",
            "Loss: 0.0\n",
            "training error 0.04296244125258368, test error 0.07377106775290636\n",
            "Loss: 0.0\n",
            "training error 0.04268179970348012, test error 0.0730354476695975\n",
            "Loss: 0.0\n",
            "training error 0.042355842318915836, test error 0.07219698774038855\n",
            "Loss: 0.0\n",
            "training error 0.042003448531841535, test error 0.07127365434026389\n",
            "Loss: 0.0\n",
            "training error 0.04175752169999156, test error 0.07067752993332568\n",
            "Loss: 0.0\n",
            "training error 0.04139978964484069, test error 0.06989910990889082\n",
            "Loss: 0.0\n",
            "training error 0.04114044054940878, test error 0.0693115459948053\n",
            "Loss: 0.0\n",
            "training error 0.04090326263997144, test error 0.06854291196858996\n",
            "Loss: 0.0\n",
            "training error 0.040593938135514526, test error 0.06794493979368611\n",
            "Loss: 0.0\n",
            "training error 0.040304122694752455, test error 0.06742979712057288\n",
            "Loss: 0.0\n",
            "training error 0.040070045478549615, test error 0.06670505283563995\n",
            "Loss: 0.0\n",
            "training error 0.03984303417456589, test error 0.06597359037615223\n",
            "Loss: 0.0\n",
            "training error 0.03957901761417118, test error 0.06551388247710725\n",
            "Loss: 0.0\n",
            "training error 0.03934190354714697, test error 0.06489470093399625\n",
            "Loss: 0.0\n",
            "training error 0.039082270390321486, test error 0.06430660069831616\n",
            "Loss: 0.0\n",
            "training error 0.03885767095501501, test error 0.06372517704905122\n",
            "Loss: 0.0\n",
            "training error 0.038623373911848516, test error 0.06330834939252833\n",
            "Loss: 0.0\n",
            "training error 0.038440862642991035, test error 0.06291114712139004\n",
            "Loss: 0.0\n",
            "training error 0.038267274931765516, test error 0.0624219390254758\n",
            "Loss: 0.0\n",
            "training error 0.03800184478281391, test error 0.061854468191070155\n",
            "Loss: 0.0\n",
            "training error 0.037808443786601593, test error 0.06145140686660839\n",
            "Loss: 0.0\n",
            "training error 0.03763141805920329, test error 0.06098892894195894\n",
            "Loss: 0.0\n",
            "training error 0.03746576122067086, test error 0.060431741649604805\n",
            "Loss: 0.0\n",
            "training error 0.03725457562342813, test error 0.06012278313152122\n",
            "Loss: 0.0\n",
            "training error 0.03719005175745029, test error 0.059343623308661476\n",
            "Loss: 0.0\n",
            "training error 0.036934784765163445, test error 0.05925037924342103\n",
            "Loss: 0.0\n",
            "training error 0.036691965871449427, test error 0.05875748637436773\n",
            "Loss: 0.0\n",
            "training error 0.03662372270590531, test error 0.05820023591521457\n",
            "Loss: 0.0\n",
            "training error 0.03637828097539069, test error 0.057946699618177624\n",
            "Loss: 0.0\n",
            "training error 0.036214098847407464, test error 0.057515747227941755\n",
            "Loss: 0.0\n",
            "training error 0.03605676102790692, test error 0.05711788163556063\n",
            "Loss: 0.0\n",
            "training error 0.035881756766872955, test error 0.056888680916324125\n",
            "Loss: 0.0\n",
            "training error 0.035744558590370346, test error 0.05651832278555826\n",
            "Loss: 0.0\n",
            "training error 0.0355842024426306, test error 0.056089040808926446\n",
            "Loss: 0.0\n",
            "training error 0.03546413130720838, test error 0.055728084702687246\n",
            "Loss: 0.0\n",
            "training error 0.03530118318662833, test error 0.05544089702415773\n",
            "Loss: 0.0\n",
            "training error 0.03522548633810619, test error 0.05497999870055834\n",
            "Loss: 0.0\n",
            "training error 0.03503205000071885, test error 0.054775457094895626\n",
            "Loss: 0.0\n",
            "training error 0.034930657478680835, test error 0.05430047866670407\n",
            "Loss: 0.0\n",
            "training error 0.034827590469423755, test error 0.05404825759420236\n",
            "Loss: 0.0\n",
            "training error 0.034697933627064026, test error 0.053653208963458\n",
            "Loss: 0.0\n",
            "training error 0.03454730044613099, test error 0.05359346609263791\n",
            "Loss: 0.0\n",
            "training error 0.03440590298581934, test error 0.05313335788284333\n",
            "Loss: 0.0\n",
            "training error 0.034293332579542105, test error 0.05298205788150485\n",
            "Loss: 0.0\n",
            "training error 0.03415985085916046, test error 0.052586750864726056\n",
            "Loss: 0.0\n",
            "training error 0.0340570108652247, test error 0.05234432162649962\n",
            "Loss: 0.0\n",
            "training error 0.03400429821967153, test error 0.051981130225090245\n",
            "Loss: 0.0\n",
            "training error 0.033866134612406104, test error 0.05190481317851217\n",
            "Loss: 0.0\n",
            "training error 0.03375847992543901, test error 0.051556236797574886\n",
            "Loss: 0.0\n",
            "training error 0.03362791552949802, test error 0.051545987910302134\n",
            "Loss: 0.0\n",
            "training error 0.03350325796770547, test error 0.05127049855530328\n",
            "Loss: 0.0\n",
            "training error 0.03343073880736757, test error 0.050987185018480985\n",
            "Loss: 0.0\n",
            "training error 0.03330398479638736, test error 0.05089969227147771\n",
            "Loss: 0.0\n",
            "training error 0.03321565523407806, test error 0.05062644602030507\n",
            "Loss: 0.0\n",
            "training error 0.033125201635997754, test error 0.05041867244921955\n",
            "Loss: 0.0\n",
            "training error 0.03306537115119925, test error 0.050186561030409305\n",
            "Loss: 0.0\n",
            "training error 0.03295722658135139, test error 0.05005910696947575\n",
            "Loss: 0.0\n",
            "training error 0.032853404741562786, test error 0.049757587975805914\n",
            "Loss: 0.0\n",
            "training error 0.03275051960871148, test error 0.0495450717336175\n",
            "Loss: 0.0\n",
            "training error 0.03272503564404377, test error 0.04921259625457675\n",
            "Loss: 0.0\n",
            "training error 0.03257299453908073, test error 0.04917037482265511\n",
            "Loss: 0.0\n",
            "training error 0.03255032130166832, test error 0.04914802461366198\n",
            "Loss: 0.0\n",
            "training error 0.03243477635161955, test error 0.048866946291308805\n",
            "Loss: 0.0\n",
            "training error 0.03240693473551865, test error 0.04867626819008732\n",
            "Loss: 0.0\n",
            "training error 0.03227582737319368, test error 0.04852302904278128\n",
            "Loss: 0.0\n",
            "training error 0.03219764830133172, test error 0.048380815599707035\n",
            "Loss: 0.0\n",
            "training error 0.03209386916578104, test error 0.04839106251273143\n",
            "Loss: 0.02117970294088778\n",
            "training error 0.03204880083188534, test error 0.048198115174129266\n",
            "Loss: 0.0\n",
            "training error 0.032028209434115655, test error 0.04810295990460336\n",
            "Loss: 0.0\n",
            "training error 0.031896372226139225, test error 0.047961817079719506\n",
            "Loss: 0.0\n",
            "training error 0.031784000320809226, test error 0.04789106478329203\n",
            "Loss: 0.0\n",
            "training error 0.03173230778414135, test error 0.04772733243637137\n",
            "Loss: 0.0\n",
            "training error 0.031699028267107854, test error 0.04752756999576551\n",
            "Loss: 0.0\n",
            "training error 0.031597764909840245, test error 0.04743869945312021\n",
            "Loss: 0.0\n",
            "training error 0.031520073363358964, test error 0.04745084760471487\n",
            "Loss: 0.02560810421599058\n",
            "training error 0.03144657270511361, test error 0.04723993645960114\n",
            "Loss: 0.0\n",
            "training error 0.03138039354723847, test error 0.047118793336977854\n",
            "Loss: 0.0\n",
            "training error 0.03133847976441868, test error 0.04697636114864455\n",
            "Loss: 0.0\n",
            "training error 0.03125199621621476, test error 0.046813738151059124\n",
            "Loss: 0.0\n",
            "training error 0.031231216774393656, test error 0.04685287538365979\n",
            "Loss: 0.08360202399213534\n",
            "training error 0.031126321485820268, test error 0.0466182534734114\n",
            "Loss: 0.0\n",
            "training error 0.03108988584941604, test error 0.04656689038621937\n",
            "Loss: 0.0\n",
            "training error 0.031030640168711616, test error 0.0465388207541111\n",
            "Loss: 0.0\n",
            "training error 0.03095654342460593, test error 0.046269279296039995\n",
            "Loss: 0.0\n",
            "training error 0.030887825865707017, test error 0.04627828464302378\n",
            "Loss: 0.019462907399470097\n",
            "training error 0.030842811186842647, test error 0.04617107712277906\n",
            "Loss: 0.0\n",
            "training error 0.030810507691651976, test error 0.046330099798975274\n",
            "Loss: 0.3444205465974681\n",
            "training error 0.03070350211887377, test error 0.046083494131126855\n",
            "Loss: 0.0\n",
            "training error 0.030691516433995012, test error 0.045790337648559644\n",
            "Loss: 0.0\n",
            "training error 0.03059965020383103, test error 0.04577429861275176\n",
            "Loss: 0.0\n",
            "training error 0.030538168699682202, test error 0.04573449662832887\n",
            "Loss: 0.0\n",
            "training error 0.03050091741857246, test error 0.045617028198357785\n",
            "Loss: 0.0\n",
            "training error 0.03045086430307843, test error 0.04562170494822728\n",
            "Loss: 0.010252201982896558\n",
            "training error 0.030386147183906805, test error 0.04541414151503236\n",
            "Loss: 0.0\n",
            "training error 0.030346729497223238, test error 0.045466682280093\n",
            "Loss: 0.11569252067276103\n",
            "training error 0.03032838881129202, test error 0.0454302806567979\n",
            "Loss: 0.035537700872745326\n",
            "training error 0.03027776083769601, test error 0.04508235056563698\n",
            "Loss: 0.0\n",
            "training error 0.030196159096575862, test error 0.04516144795871747\n",
            "Loss: 0.1754509072576571\n",
            "training error 0.030138233836083266, test error 0.04513660736419759\n",
            "Loss: 0.12035042068538893\n",
            "training error 0.030104746677887723, test error 0.04494520648744125\n",
            "Loss: 0.0\n",
            "training error 0.030024983940231408, test error 0.04498360204897467\n",
            "Loss: 0.08542748945685741\n",
            "training error 0.029975278551801574, test error 0.044855874470945634\n",
            "Loss: 0.0\n",
            "training error 0.029936856802221568, test error 0.044788877513017626\n",
            "Loss: 0.0\n",
            "training error 0.029869104325844994, test error 0.04484173107685837\n",
            "Loss: 0.1180060023280971\n",
            "training error 0.029833430989519308, test error 0.044842266402045194\n",
            "Loss: 0.11920122135691269\n",
            "training error 0.029783612892771236, test error 0.04472094459122945\n",
            "Loss: 0.0\n",
            "training error 0.02978985084867854, test error 0.04478140572314653\n",
            "Loss: 0.13519645541864023\n",
            "training error 0.02970005563235651, test error 0.044608322643000235\n",
            "Loss: 0.0\n",
            "training error 0.02964383881847149, test error 0.04452412619021077\n",
            "Loss: 0.0\n",
            "training error 0.02961588220854316, test error 0.044506859094720526\n",
            "Loss: 0.0\n",
            "training error 0.02957712397904839, test error 0.044511397775410126\n",
            "Loss: 0.010197710604420429\n",
            "training error 0.029527574666012815, test error 0.04435389224793926\n",
            "Loss: 0.0\n",
            "training error 0.029496873235117547, test error 0.04437004647368915\n",
            "Loss: 0.036421213406900854\n",
            "training error 0.029485062386340966, test error 0.04422241922503408\n",
            "Loss: 0.0\n",
            "training error 0.029402930375391836, test error 0.04435437676000908\n",
            "Loss: 0.2983951065714141\n",
            "training error 0.02935945531508377, test error 0.044290889955721106\n",
            "Loss: 0.154832620844636\n",
            "training error 0.029302988915707748, test error 0.0442129123805877\n",
            "Loss: 0.0\n",
            "training error 0.029315793702004416, test error 0.04419017005188773\n",
            "Loss: 0.0\n",
            "training error 0.02922661147199945, test error 0.04409029047610424\n",
            "Loss: 0.0\n",
            "training error 0.02921291368778029, test error 0.04421623749143068\n",
            "Loss: 0.2856570323452656\n",
            "training error 0.029140207531207118, test error 0.044179249864393075\n",
            "Loss: 0.201766391938496\n",
            "training error 0.029098941964477334, test error 0.044040594479649815\n",
            "Loss: 0.0\n",
            "training error 0.02906780566595107, test error 0.043981677861930864\n",
            "Loss: 0.0\n",
            "training error 0.029024749169062475, test error 0.043941578026308235\n",
            "Loss: 0.0\n",
            "training error 0.028993463483065904, test error 0.043984000902986924\n",
            "Loss: 0.09654381700467951\n",
            "training error 0.028947065082392025, test error 0.04398865090434518\n",
            "Loss: 0.10712605270744024\n",
            "training error 0.028935090547644874, test error 0.0437765141473632\n",
            "Loss: 0.0\n",
            "training error 0.028884786468202663, test error 0.043691026031424995\n",
            "Loss: 0.0\n",
            "training error 0.028881671226856993, test error 0.043692376980637174\n",
            "Loss: 0.003092051926656403\n",
            "training error 0.02883672150901989, test error 0.04360426611697987\n",
            "Loss: 0.0\n",
            "training error 0.028759166296785342, test error 0.043767651451683064\n",
            "Loss: 0.37470034300053356\n",
            "training error 0.028726729703594255, test error 0.04366803727896405\n",
            "Loss: 0.14624982292581468\n",
            "training error 0.02871764225844458, test error 0.043744137051121894\n",
            "Loss: 0.32077350818560113\n",
            "training error 0.028640168120224126, test error 0.04366544056673521\n",
            "Loss: 0.14029464362779187\n",
            "training error 0.02864962704887887, test error 0.04373719029317416\n",
            "Loss: 0.30484213594532683\n",
            "training error 0.02858106967079683, test error 0.043673641536695416\n",
            "Loss: 0.15910236748264417\n",
            "training error 0.02861292799585746, test error 0.04373997208528799\n",
            "Loss: 0.31122176886098085\n",
            "training error 0.028553790980348544, test error 0.04352101173874862\n",
            "Loss: 0.0\n",
            "training error 0.028466617388878044, test error 0.04362244452438005\n",
            "Loss: 0.233066239912616\n",
            "training error 0.028451732580993497, test error 0.043562409029722325\n",
            "Loss: 0.09512024036160138\n",
            "training error 0.028420423123952817, test error 0.04339469172882688\n",
            "Loss: 0.0\n",
            "training error 0.028384653997411847, test error 0.04345950594437912\n",
            "Loss: 0.1493597787426637\n",
            "training error 0.028345871491139672, test error 0.043599628267345515\n",
            "Loss: 0.4722617683270647\n",
            "training error 0.028303490062201557, test error 0.043579880535919095\n",
            "Loss: 0.4267545170028031\n",
            "training error 0.028263140710317942, test error 0.04351246234798113\n",
            "Loss: 0.2713940679431426\n",
            "training error 0.028242207491171834, test error 0.043526419768959705\n",
            "Loss: 0.3035579580931147\n",
            "training error 0.028284452527404236, test error 0.0436366973383042\n",
            "Loss: 0.5576848223501907\n",
            "training error 0.028181241080672857, test error 0.04359134921078306\n",
            "Loss: 0.4531832676334835\n",
            "training error 0.02814495577277521, test error 0.04348264194592215\n",
            "Loss: 0.20267505907143324\n",
            "training error 0.028106696357526197, test error 0.043427987129276716\n",
            "Loss: 0.07672689705435776\n",
            "training error 0.0281310590660357, test error 0.043548843520552036\n",
            "Loss: 0.3552319087515299\n",
            "training error 0.028073676318504517, test error 0.043514869826043315\n",
            "Loss: 0.2769419309795573\n",
            "training error 0.02801632515305217, test error 0.04332322702001296\n",
            "Loss: 0.0\n",
            "training error 0.02799005428319928, test error 0.04342919197658574\n",
            "Loss: 0.24459155945106836\n",
            "training error 0.027954927955662758, test error 0.04339531669133539\n",
            "Loss: 0.1663995881219238\n",
            "training error 0.027967692433470175, test error 0.0434869353757865\n",
            "Loss: 0.3778766427023239\n",
            "training error 0.027982146070993176, test error 0.043239871212155855\n",
            "Loss: 0.0\n",
            "training error 0.027959897022632487, test error 0.04348846833956203\n",
            "Loss: 0.5749256888079923\n",
            "training error 0.027832129800005392, test error 0.04341678020136527\n",
            "Loss: 0.40913394108277323\n",
            "training error 0.02783085932961757, test error 0.04347791143122351\n",
            "Loss: 0.5505109344561054\n",
            "training error 0.027782682054987174, test error 0.04346475132260103\n",
            "Loss: 0.5200758099898284\n",
            "training error 0.02775278315564913, test error 0.04341307885245666\n",
            "Loss: 0.4005739042352019\n",
            "training error 0.027716225244451703, test error 0.04335187260315173\n",
            "Loss: 0.2590234148625026\n",
            "training error 0.027680835018768647, test error 0.0434828327698909\n",
            "Loss: 0.561892417632226\n",
            "training error 0.02776512592229339, test error 0.04354979619423156\n",
            "Loss: 0.7167574125164888\n",
            "training error 0.027684140618732065, test error 0.043587489001840465\n",
            "Loss: 0.8039288275837553\n",
            "training error 0.027626567566964237, test error 0.04336231861781144\n",
            "Loss: 0.28318170758372396\n",
            "training error 0.02762220947510598, test error 0.043291510783560354\n",
            "Loss: 0.11942582148576353\n",
            "training error 0.02757364847948593, test error 0.043300633406122835\n",
            "Loss: 0.14052353132332218\n",
            "training error 0.02752961019763089, test error 0.043422004302051356\n",
            "Loss: 0.42121561602685986\n",
            "training error 0.02750808069469477, test error 0.04328298096826888\n",
            "Loss: 0.09969908536846539\n",
            "training error 0.027481793499081128, test error 0.043455926323314395\n",
            "Loss: 0.4996664076506452\n",
            "training error 0.02745878345046121, test error 0.043414300905345544\n",
            "Loss: 0.40340012192416896\n",
            "training error 0.027462670626064268, test error 0.04348834433458727\n",
            "Loss: 0.5746389049409606\n",
            "training error 0.027400841881980033, test error 0.04329245167976011\n",
            "Loss: 0.12160181362768085\n",
            "training error 0.027428692816473452, test error 0.04307114425664334\n",
            "Loss: 0.0\n",
            "training error 0.02735579738967461, test error 0.04309654225012213\n",
            "Loss: 0.05896753828378287\n",
            "training error 0.027323630916499336, test error 0.04323776276953787\n",
            "Loss: 0.38684487206033236\n",
            "training error 0.027304403247601628, test error 0.04318259138603911\n",
            "Loss: 0.25875126217149713\n",
            "training error 0.027330096514734583, test error 0.043130414569954936\n",
            "Loss: 0.13761025933842763\n",
            "training error 0.027220118866967905, test error 0.043268911373755055\n",
            "Loss: 0.45916383352460066\n",
            "training error 0.02725583714225134, test error 0.04353358885848458\n",
            "Loss: 1.073676146344571\n",
            "training error 0.02716597450338073, test error 0.04347663562723062\n",
            "Loss: 0.9414455491851603\n",
            "training error 0.027156981152782617, test error 0.04348418969512327\n",
            "Loss: 0.9589841310432057\n",
            "training error 0.027133843483296227, test error 0.04342252051487241\n",
            "Loss: 0.8158043262917802\n",
            "training error 0.027104130549280292, test error 0.04346762095197278\n",
            "Loss: 0.9205158167310268\n",
            "training error 0.027133938611794993, test error 0.04313373004042783\n",
            "Loss: 0.14530791987221736\n",
            "training error 0.027075547978766788, test error 0.04321674497275974\n",
            "Loss: 0.3380470118203194\n",
            "training error 0.027032975618092803, test error 0.04318371276495579\n",
            "Loss: 0.26135481249744963\n",
            "training error 0.02702743459680679, test error 0.042891690983458394\n",
            "Loss: 0.0\n",
            "training error 0.026995748221374828, test error 0.0429190672421436\n",
            "Loss: 0.06382648493798992\n",
            "training error 0.026999885835643483, test error 0.04309665121938242\n",
            "Loss: 0.47785534033404353\n",
            "training error 0.02695668617321987, test error 0.04285255855104588\n",
            "Loss: 0.0\n",
            "training error 0.026943374486783224, test error 0.042899941477374234\n",
            "Loss: 0.11057198900250587\n",
            "training error 0.026909987570721997, test error 0.04285651269089469\n",
            "Loss: 0.00922731333321547\n",
            "training error 0.02691792710459783, test error 0.042916815665059914\n",
            "Loss: 0.1499493056814627\n",
            "training error 0.026919947986193304, test error 0.042987223996327276\n",
            "Loss: 0.3142529870672206\n",
            "training error 0.026864418583429903, test error 0.04278065315550732\n",
            "Loss: 0.0\n",
            "training error 0.0269134207440401, test error 0.04273738799035737\n",
            "Loss: 0.0\n",
            "training error 0.026796047335795912, test error 0.04295896633963048\n",
            "Loss: 0.518464884477976\n",
            "training error 0.026761238387529265, test error 0.04293154669476894\n",
            "Loss: 0.45430643645179014\n",
            "training error 0.026746422928477204, test error 0.04273002211389427\n",
            "Loss: 0.0\n",
            "training error 0.0267408488834508, test error 0.04265261836286263\n",
            "Loss: 0.0\n",
            "training error 0.02671665955804657, test error 0.04286782520442905\n",
            "Loss: 0.5045571639601842\n",
            "training error 0.026680687042245962, test error 0.04285213878608842\n",
            "Loss: 0.4677800118351172\n",
            "training error 0.026677835367150734, test error 0.04256066140775207\n",
            "Loss: 0.0\n",
            "training error 0.02668486931526397, test error 0.04263649226599567\n",
            "Loss: 0.17817124014380958\n",
            "training error 0.02663379917325416, test error 0.04271876611151468\n",
            "Loss: 0.3714808429500005\n",
            "training error 0.0266092881424034, test error 0.04258671106577963\n",
            "Loss: 0.0612059520832986\n",
            "training error 0.026655334034497023, test error 0.04266108680483614\n",
            "Loss: 0.2359582623069567\n",
            "training error 0.02657573509486349, test error 0.042905419216438895\n",
            "Loss: 0.8100386537320814\n",
            "training error 0.026557179236094002, test error 0.04291267930409486\n",
            "Loss: 0.8270968652725808\n",
            "training error 0.026519635688198248, test error 0.042807617426325696\n",
            "Loss: 0.5802447856899207\n",
            "training error 0.026532755096844607, test error 0.04278496100510229\n",
            "Loss: 0.5270115405428433\n",
            "training error 0.0264976378073948, test error 0.04295645717828712\n",
            "Loss: 0.9299568132720815\n",
            "training error 0.02643986957598941, test error 0.04292356920682264\n",
            "Loss: 0.8526836451006492\n",
            "training error 0.026458505552980214, test error 0.04292709940545854\n",
            "Loss: 0.8609781558510443\n",
            "training error 0.02647192127000408, test error 0.04278410668379468\n",
            "Loss: 0.5250042378380781\n",
            "training error 0.02639636282097442, test error 0.042902446381693986\n",
            "Loss: 0.8030537182386555\n",
            "training error 0.02642910375857344, test error 0.0428407335344897\n",
            "Loss: 0.6580539810093766\n",
            "training error 0.026377374429086643, test error 0.04291557938240856\n",
            "Loss: 0.8339108531613348\n",
            "training error 0.026381424660986218, test error 0.042922218453195\n",
            "Loss: 0.8495099311992327\n",
            "training error 0.02634213975277016, test error 0.04277065913869129\n",
            "Loss: 0.4934080533367302\n",
            "training error 0.026287472463450583, test error 0.042786940345058075\n",
            "Loss: 0.5316621730525828\n",
            "training error 0.026256409104147137, test error 0.04285548512902472\n",
            "Loss: 0.6927141438148654\n",
            "training error 0.026277888898143028, test error 0.04277907878562883\n",
            "Loss: 0.5131907509242417\n",
            "training error 0.02623309596281513, test error 0.0428189571276366\n",
            "Loss: 0.6068884066672053\n",
            "training error 0.02621512675568647, test error 0.04289570333553132\n",
            "Loss: 0.7872103409516606\n",
            "training error 0.02619262745263163, test error 0.04288853224539023\n",
            "Loss: 0.7703612368637813\n",
            "training error 0.026168587827953463, test error 0.042969836949455574\n",
            "Loss: 0.9613937569799669\n",
            "training error 0.026176283355118075, test error 0.04275759350060915\n",
            "Loss: 0.46270919281627965\n",
            "training error 0.026133943856965592, test error 0.04274295038942373\n",
            "Loss: 0.42830392113799576\n",
            "training error 0.026150264307762818, test error 0.04278949676254271\n",
            "Loss: 0.5376686997372815\n",
            "training error 0.026105591554117548, test error 0.04281021596496374\n",
            "Loss: 0.5863502797121001\n",
            "training error 0.026120635822996605, test error 0.04316191458172373\n",
            "Loss: 1.41269696965316\n",
            "training error 0.026082579045500386, test error 0.04314667718862812\n",
            "Loss: 1.3768953805997741\n",
            "training error 0.026064274381838917, test error 0.043141914660366154\n",
            "Loss: 1.3657054035072225\n",
            "training error 0.026016099605430824, test error 0.04322736235293155\n",
            "Loss: 1.566472237807015\n",
            "training error 0.02605777105122743, test error 0.043046097974234546\n",
            "Loss: 1.140575711058056\n",
            "training error 0.025995922493230714, test error 0.0432868405852697\n",
            "Loss: 1.7062215517763768\n",
            "training error 0.025976658374444284, test error 0.0433345599436983\n",
            "Loss: 1.818342362050962\n",
            "training error 0.025950375321284174, test error 0.04315747505951904\n",
            "Loss: 1.402265923570134\n",
            "training error 0.02593867145400575, test error 0.04326718676190138\n",
            "Loss: 1.6600431731557341\n",
            "training error 0.025980830941919292, test error 0.04317075218530273\n",
            "Loss: 1.4334616929603028\n",
            "training error 0.025967756878240558, test error 0.0431217607992382\n",
            "Loss: 1.3183521423939304\n",
            "training error 0.02587965675563351, test error 0.04321929950817064\n",
            "Loss: 1.547527878170163\n",
            "training error 0.025921121380466747, test error 0.042939333598542975\n",
            "Loss: 0.8897234635595552\n",
            "training error 0.025901747736947724, test error 0.043083471334987355\n",
            "Loss: 1.228387694041011\n",
            "training error 0.025827539417523016, test error 0.042987573245081206\n",
            "Loss: 1.0030667362969536\n",
            "training error 0.02581838493536287, test error 0.04311290887157263\n",
            "Loss: 1.2975537633914236\n",
            "training error 0.02583036044148116, test error 0.04286058136654063\n",
            "Loss: 0.7046882000145249\n",
            "training error 0.025840958283485337, test error 0.042755699016265636\n",
            "Loss: 0.45825793599636455\n",
            "training error 0.025800690741337444, test error 0.04310169849289756\n",
            "Loss: 1.2712139972687275\n",
            "training error 0.025740665580834817, test error 0.043123489873305436\n",
            "Loss: 1.3224147532887143\n",
            "training error 0.025728965805213014, test error 0.043024720010415414\n",
            "Loss: 1.0903463135063518\n",
            "training error 0.025722767931902247, test error 0.0430852958723909\n",
            "Loss: 1.2326746044018844\n",
            "training error 0.02570777297571965, test error 0.04298829432802217\n",
            "Loss: 1.0047609837947835\n",
            "training error 0.02571042763819532, test error 0.04311625987477905\n",
            "Loss: 1.305427238792367\n",
            "training error 0.025658863733528377, test error 0.043178434243368105\n",
            "Loss: 1.4515113609195884\n",
            "training error 0.02566784402715171, test error 0.043350228303661774\n",
            "Loss: 1.8551565454898888\n",
            "training error 0.025628713509128324, test error 0.043306125331631834\n",
            "Loss: 1.7515327516596901\n",
            "training error 0.025635113861174716, test error 0.0433193003171584\n",
            "Loss: 1.7824885335737672\n",
            "training error 0.02562174875747326, test error 0.043180742337118674\n",
            "Loss: 1.4569344292512998\n",
            "training error 0.025631779304966386, test error 0.043319360128816486\n",
            "Loss: 1.7826290663007116\n",
            "training error 0.025585728164640727, test error 0.0431261574240265\n",
            "Loss: 1.328682397241665\n",
            "training error 0.02560550852180878, test error 0.043268248420329045\n",
            "Loss: 1.6625376325756447\n",
            "training error 0.02555284775479588, test error 0.043063194129770964\n",
            "Loss: 1.1807446251936415\n",
            "training error 0.02558960918431878, test error 0.04330783310204551\n",
            "Loss: 1.7555453077553684\n",
            "training error 0.025543059854853292, test error 0.04329980862179613\n",
            "Loss: 1.7366910888970244\n",
            "training error 0.02551618964387264, test error 0.04326506905420478\n",
            "Loss: 1.6550674335253923\n",
            "training error 0.025458254620390218, test error 0.04318356060951572\n",
            "Loss: 1.463556206977068\n",
            "training error 0.025458511973115598, test error 0.043150588099174624\n",
            "Loss: 1.3860844073140033\n",
            "training error 0.025468285101691423, test error 0.04310081176623627\n",
            "Loss: 1.2691305553485188\n",
            "training error 0.025444221209765895, test error 0.04292643878420174\n",
            "Loss: 0.8594259683733396\n",
            "training error 0.02542886025225607, test error 0.043044155204882556\n",
            "Loss: 1.1360110043835636\n",
            "training error 0.025431608108458584, test error 0.042941146885545144\n",
            "Loss: 0.8939839401174599\n",
            "training error 0.025386195228060838, test error 0.0430176670687484\n",
            "Loss: 1.0737748096017352\n",
            "training error 0.025423849336282306, test error 0.04317859786044829\n",
            "Loss: 1.451895793573521\n",
            "training error 0.025360329939202085, test error 0.04297158888240674\n",
            "Loss: 0.9655100768237279\n",
            "training error 0.02552184101762028, test error 0.04316430123621527\n",
            "Loss: 1.4183046233234853\n",
            "training error 0.02533578342042503, test error 0.04311317935504547\n",
            "Loss: 1.2981892879906365\n",
            "training error 0.02530827515142324, test error 0.04300022130551891\n",
            "Loss: 1.0327844615844572\n",
            "training error 0.02530480381015394, test error 0.04304062481615535\n",
            "Loss: 1.1277160469970182\n",
            "training error 0.025272944951985506, test error 0.04304343330320846\n",
            "Loss: 1.1343148332005448\n",
            "training error 0.02527730603075822, test error 0.04302715325529577\n",
            "Loss: 1.096063435373984\n",
            "training error 0.025340068551506306, test error 0.04327464092267692\n",
            "Loss: 1.6775573764810092\n",
            "training error 0.025246411182758444, test error 0.043141502528889285\n",
            "Loss: 1.3647370645218038\n",
            "training error 0.025222611673770434, test error 0.04304153858660233\n",
            "Loss: 1.129863030659295\n",
            "training error 0.025207170431642265, test error 0.04312086530808865\n",
            "Loss: 1.3162481075412735\n",
            "training error 0.025204364082143212, test error 0.04331438260759755\n",
            "Loss: 1.77093394443395\n",
            "training error 0.02524196814878797, test error 0.042981600466140316\n",
            "Loss: 0.9890331692814724\n",
            "training error 0.02520239867276985, test error 0.04295227652145644\n",
            "Loss: 0.9201339940479336\n",
            "training error 0.02523597211656253, test error 0.042868314903072934\n",
            "Loss: 0.7228588211386056\n",
            "training error 0.025197097325854616, test error 0.043083624108654434\n",
            "Loss: 1.2287466491465615\n",
            "training error 0.025131985346250777, test error 0.042942654597596976\n",
            "Loss: 0.8975264415776563\n",
            "training error 0.025126555874484643, test error 0.04297495995697247\n",
            "Loss: 0.9734307116405327\n",
            "training error 0.025174545941373208, test error 0.04306966660285971\n",
            "Loss: 1.1959522673558265\n",
            "training error 0.02513463971367158, test error 0.042994542346328726\n",
            "Loss: 1.0194412497960714\n",
            "training error 0.02513375032451054, test error 0.042738757782991604\n",
            "Loss: 0.41845302527909745\n",
            "training error 0.025073098636153635, test error 0.042997395512541815\n",
            "Loss: 1.026145013597457\n",
            "training error 0.025111324622990414, test error 0.04328197315977635\n",
            "Loss: 1.6947851094553323\n",
            "training error 0.02520424874279521, test error 0.04297520423501445\n",
            "Loss: 0.9740046642857658\n",
            "training error 0.025055353678863402, test error 0.043102247254906356\n",
            "Loss: 1.27250336165039\n",
            "training error 0.025052136713683906, test error 0.043103884318039964\n",
            "Loss: 1.2763497848014005\n",
            "training error 0.02501564352837207, test error 0.0429437979364832\n",
            "Loss: 0.9002128163857703\n",
            "training error 0.024986835755947553, test error 0.042756456314961604\n",
            "Loss: 0.4600372755811488\n",
            "training error 0.024966183056056936, test error 0.04277472821956767\n",
            "Loss: 0.5029687150881701\n",
            "training error 0.0250549063887636, test error 0.0429822808720125\n",
            "Loss: 0.9906318424450999\n",
            "training error 0.024950131898603223, test error 0.04278892856541387\n",
            "Loss: 0.5363336708395838\n",
            "training error 0.025031485416554936, test error 0.042720532882796466\n",
            "Loss: 0.37563202675059415\n",
            "training error 0.024962081212401132, test error 0.04311846041586993\n",
            "Loss: 1.3105976027343091\n",
            "training error 0.024948918313197014, test error 0.042907659589043495\n",
            "Loss: 0.8153026053026258\n",
            "training error 0.024899060705338948, test error 0.043153873489815295\n",
            "Loss: 1.3938037202475773\n",
            "training error 0.02487067309179514, test error 0.04318481680206608\n",
            "Loss: 1.4665077413489769\n",
            "training error 0.02486491896375379, test error 0.043138396862208515\n",
            "Loss: 1.3574400287661437\n",
            "training error 0.024861021309691294, test error 0.04325404683547322\n",
            "Loss: 1.6291697656626747\n",
            "training error 0.024824708712815398, test error 0.04317409458514261\n",
            "Loss: 1.4413149539983738\n",
            "training error 0.024830689855115502, test error 0.04319702444112855\n",
            "Loss: 1.4951906580581875\n",
            "training error 0.02482540204556294, test error 0.043298805223241164\n",
            "Loss: 1.7343335161484452\n",
            "training error 0.024816469198153078, test error 0.04313452052539743\n",
            "Loss: 1.3483322360700933\n",
            "training error 0.024813993954057752, test error 0.04329393036450962\n",
            "Loss: 1.7228796087835008\n",
            "training error 0.024889970609488908, test error 0.04352611806247882\n",
            "Loss: 2.26842493230357\n",
            "training error 0.02481206756120365, test error 0.0432729122443897\n",
            "Loss: 1.6734956955060332\n",
            "training error 0.024737636831912834, test error 0.043507754729269495\n",
            "Loss: 2.225278673288944\n",
            "training error 0.024756998140415196, test error 0.043510129649736225\n",
            "Loss: 2.2308587568407034\n",
            "training error 0.0247448124868151, test error 0.04327695856449405\n",
            "Loss: 1.683002878831008\n",
            "training error 0.02474589251907746, test error 0.04330489654396187\n",
            "Loss: 1.7486456074534695\n",
            "training error 0.024698933827944584, test error 0.043392932049893036\n",
            "Loss: 1.955492735809261\n",
            "training error 0.02467223384539153, test error 0.04333933152158827\n",
            "Loss: 1.8295536020367598\n",
            "training error 0.024683162330632495, test error 0.04326491692832391\n",
            "Loss: 1.6547100004502546\n",
            "training error 0.02468787753183322, test error 0.0434868632204828\n",
            "Loss: 2.176192244423225\n",
            "training error 0.02464086766048733, test error 0.04338212674996946\n",
            "Loss: 1.930104737676297\n",
            "training error 0.02466178833392226, test error 0.043435763917874425\n",
            "Loss: 2.056129959397124\n",
            "training error 0.02462280081910674, test error 0.04339954649845331\n",
            "Loss: 1.971033961771207\n",
            "training error 0.024621984135505995, test error 0.043245952877760604\n",
            "Loss: 1.6101523034219412\n",
            "training error 0.02464959251412938, test error 0.04317099838655412\n",
            "Loss: 1.4340401643543998\n",
            "training error 0.024684056230088097, test error 0.04345492358967524\n",
            "Loss: 2.10114728564883\n",
            "training error 0.024589048504271845, test error 0.0433615065899466\n",
            "Loss: 1.8816558664867644\n",
            "training error 0.02456692114866506, test error 0.043413142729817306\n",
            "Loss: 2.0029794976587523\n",
            "training error 0.024560665283033214, test error 0.04335972426873177\n",
            "Loss: 1.877468146757133\n",
            "training error 0.024578585748167712, test error 0.04351416372323852\n",
            "Loss: 2.2403371657019777\n",
            "training error 0.02452336309079155, test error 0.043311938370459324\n",
            "Loss: 1.765190995294108\n",
            "training error 0.024538955548121008, test error 0.04329098913675004\n",
            "Loss: 1.7159689366691655\n",
            "training error 0.02457238530335675, test error 0.04349906473233189\n",
            "Loss: 2.204860764708183\n",
            "training error 0.02456848753145734, test error 0.04313451037376772\n",
            "Loss: 1.3483083839273435\n",
            "training error 0.02449830083992638, test error 0.0431773100162717\n",
            "Loss: 1.448869890934823\n",
            "training error 0.024526153799592103, test error 0.04330080527439516\n",
            "Loss: 1.7390328114315512\n",
            "training error 0.024496481745394854, test error 0.043205470130673856\n",
            "Loss: 1.5150345450325675\n",
            "training error 0.024467706986840053, test error 0.043140358699193186\n",
            "Loss: 1.3620495365129104\n",
            "training error 0.024466649941614255, test error 0.043406332912240574\n",
            "Loss: 1.9869792350888371\n",
            "training error 0.024421229719227274, test error 0.04332629486873003\n",
            "Loss: 1.798922844837425\n",
            "training error 0.02441664652543814, test error 0.04326761730602759\n",
            "Loss: 1.661054774272741\n",
            "training error 0.02446611236801692, test error 0.04335798363404497\n",
            "Loss: 1.8733783731746279\n",
            "training error 0.024437173555724532, test error 0.04345832408574789\n",
            "Loss: 2.109137048871901\n",
            "training error 0.0244023803863072, test error 0.043288693250568624\n",
            "Loss: 1.7105745510899162\n",
            "training error 0.024436146143433125, test error 0.0429912975912853\n",
            "Loss: 1.011817413755689\n",
            "training error 0.024420494928073164, test error 0.04316844152748044\n",
            "Loss: 1.4280326001176125\n",
            "training error 0.024440572938648184, test error 0.04307294787562047\n",
            "Loss: 1.2036619049700548\n",
            "training error 0.024331222910513085, test error 0.04332447297578319\n",
            "Loss: 1.7946421478591157\n",
            "training error 0.02433883644884782, test error 0.04342985929234564\n",
            "Loss: 2.042256524789954\n",
            "training error 0.024351044389168967, test error 0.0432113256502088\n",
            "Loss: 1.5287926008081598\n",
            "training error 0.02441575802855923, test error 0.043370084641031426\n",
            "Loss: 1.901810748485988\n",
            "training error 0.024329289279069646, test error 0.0434827377498204\n",
            "Loss: 2.166499089932805\n",
            "training error 0.024276232134299552, test error 0.04332878493000119\n",
            "Loss: 1.804773461789333\n",
            "training error 0.024274636718943917, test error 0.04335395190199021\n",
            "Loss: 1.8639054657493048\n",
            "training error 0.024265059395719844, test error 0.04345037594983071\n",
            "Loss: 2.090462207705701\n",
            "training error 0.024276636946479295, test error 0.043398873746201126\n",
            "Loss: 1.9694532714578106\n",
            "training error 0.024259184740707825, test error 0.04350650218489735\n",
            "Loss: 2.2223357106311514\n",
            "training error 0.024249029810993392, test error 0.04331911564838639\n",
            "Loss: 1.7820546381268842\n",
            "training error 0.024251440918007582, test error 0.04334853350477555\n",
            "Loss: 1.8511744671335872\n",
            "training error 0.024230907885526738, test error 0.043496540556052905\n",
            "Loss: 2.1989299915587734\n",
            "training error 0.02420734204206874, test error 0.043662858234156235\n",
            "Loss: 2.5897079367366516\n",
            "training error 0.024196893355995664, test error 0.043646246555071565\n",
            "Loss: 2.550677342438501\n",
            "training error 0.024190537595107753, test error 0.04360345633861599\n",
            "Loss: 2.4501379827569725\n",
            "training error 0.024174623843314626, test error 0.04372162399857331\n",
            "Loss: 2.727783244951598\n",
            "training error 0.02415354768262863, test error 0.04368991345929029\n",
            "Loss: 2.65327655677019\n",
            "training error 0.024193083316584287, test error 0.043776423547978484\n",
            "Loss: 2.856539583769191\n",
            "training error 0.024222064786221853, test error 0.04346159853822556\n",
            "Loss: 2.1168306616339283\n",
            "training error 0.02415629347711285, test error 0.043737966839154895\n",
            "Loss: 2.7661821796509667\n",
            "training error 0.024164290233613536, test error 0.04345089920211718\n",
            "Loss: 2.0916916347614833\n",
            "training error 0.02411958203395267, test error 0.04365989079854007\n",
            "Loss: 2.5827356869688733\n",
            "training error 0.0241082358019276, test error 0.043553828937988835\n",
            "Loss: 2.333534060295106\n",
            "training error 0.024094216258663228, test error 0.043681141855536176\n",
            "Loss: 2.6326669058296615\n",
            "training error 0.024138849445714934, test error 0.04369079764684378\n",
            "Loss: 2.655354032834345\n",
            "training error 0.024101015152271466, test error 0.043777881179971494\n",
            "Loss: 2.859964417746852\n",
            "training error 0.024073041564535887, test error 0.04373869505818757\n",
            "Loss: 2.7678931940210205\n",
            "training error 0.02411830047395375, test error 0.043551031807599866\n",
            "Loss: 2.3269619575681855\n",
            "training error 0.02414504163639338, test error 0.043538386325420675\n",
            "Loss: 2.297250290124775\n",
            "training error 0.024032058623346095, test error 0.04378843661901561\n",
            "Loss: 2.884765345869167\n",
            "training error 0.02402633654975397, test error 0.04372326363211362\n",
            "Loss: 2.73163570749817\n",
            "training error 0.024051008654474227, test error 0.043753086369162655\n",
            "Loss: 2.8017068390610023\n",
            "training error 0.024055822523396976, test error 0.04323779277141568\n",
            "Loss: 1.590979419178562\n",
            "training error 0.024010002862440125, test error 0.04333042361936897\n",
            "Loss: 1.8086237059198984\n",
            "training error 0.024010688062718794, test error 0.043401225649962495\n",
            "Loss: 1.9749792752453033\n",
            "training error 0.02396816981540793, test error 0.04356532311558601\n",
            "Loss: 2.360540636830777\n",
            "training error 0.02405568824244915, test error 0.04369669674160221\n",
            "Loss: 2.6692144724123734\n",
            "training error 0.02395249707924171, test error 0.04358274521349528\n",
            "Loss: 2.4014753811063994\n",
            "training error 0.02398783949443245, test error 0.043448885659328806\n",
            "Loss: 2.086960639702262\n",
            "training error 0.023953040528488498, test error 0.043663853026122405\n",
            "Loss: 2.592045287551392\n",
            "training error 0.023958132124390755, test error 0.04390682716970928\n",
            "Loss: 3.162934309362053\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9606481481481481 0.04256066140775207\n",
            "training error 0.12500928193055644, test error 0.24997731149512162\n",
            "training error 0.1250743507194259, test error 0.24995756893063487\n",
            "training error 0.12501778129615562, test error 0.24998114561878992\n",
            "training error 0.1250715606986857, test error 0.2501145536273122\n",
            "training error 0.12504154083597682, test error 0.2500834030221186\n",
            "training error 0.12501160290466146, test error 0.25022389251039473\n",
            "training error 0.12500655757849646, test error 0.25024668363022007\n",
            "training error 0.12497407115753353, test error 0.25029825856817434\n",
            "training error 0.1251266896247145, test error 0.2501810983940044\n",
            "training error 0.12512153935388526, test error 0.2502111994326977\n",
            "training error 0.12511070939491364, test error 0.2504732125664249\n",
            "training error 0.12512961093483368, test error 0.25052037959307144\n",
            "training error 0.12501316394507914, test error 0.2504858712942631\n",
            "training error 0.1250210907538457, test error 0.2504399607249071\n",
            "training error 0.12497419343133577, test error 0.250520998701742\n",
            "training error 0.12517205619008212, test error 0.25060885783400094\n",
            "training error 0.12495532616208609, test error 0.2504963855331234\n",
            "training error 0.12497025968293025, test error 0.2505551748525987\n",
            "training error 0.12502448915319284, test error 0.2506325667560817\n",
            "training error 0.12504706424480727, test error 0.25048932626751613\n",
            "training error 0.12496003259272039, test error 0.2505581168061249\n",
            "training error 0.124994144330446, test error 0.2505026983464011\n",
            "training error 0.12500185728631463, test error 0.2505926287062821\n",
            "training error 0.12502440916194105, test error 0.2505482380149484\n",
            "training error 0.12512810558219045, test error 0.25055235219654337\n",
            "training error 0.12501098151546808, test error 0.2503831738808819\n",
            "training error 0.12496188592773205, test error 0.2504241910447346\n",
            "training error 0.1250761560520529, test error 0.25044865861549626\n",
            "training error 0.1250847464179372, test error 0.25048721951960207\n",
            "training error 0.12502260367419565, test error 0.2504780330981896\n",
            "training error 0.1249726896835116, test error 0.2505657950562544\n",
            "training error 0.12501003587016796, test error 0.250513746805961\n",
            "training error 0.12500375423368648, test error 0.2504912856268406\n",
            "training error 0.12509468222327313, test error 0.2504791614003519\n",
            "training error 0.12518218465820607, test error 0.2506560480279555\n",
            "training error 0.12503431859879793, test error 0.2505934588101641\n",
            "training error 0.12497997403398445, test error 0.25061134708384725\n",
            "training error 0.12512356350734533, test error 0.2506019751000904\n",
            "training error 0.12497791644531535, test error 0.25064791417629745\n",
            "training error 0.1250291391933774, test error 0.2506488780149183\n",
            "training error 0.12496501639947469, test error 0.2506904113336502\n",
            "training error 0.12497608505128978, test error 0.2506882762429071\n",
            "training error 0.12506972471267244, test error 0.25056549672926753\n",
            "training error 0.12505600340988574, test error 0.25065945329337197\n",
            "training error 0.12512085936239598, test error 0.2505389304528938\n",
            "training error 0.12500220473804946, test error 0.2505783434837757\n",
            "training error 0.1249455197820347, test error 0.25064388307092766\n",
            "training error 0.12503616261046943, test error 0.25059958967936957\n",
            "training error 0.12496581386482333, test error 0.25056669117606056\n",
            "training error 0.12496934088554852, test error 0.25052014519317006\n",
            "Loss: 0.2250687046373434\n",
            "training error 0.12495303875540065, test error 0.25058175681787453\n",
            "Loss: 0.2497175380245853\n",
            "training error 0.12495075753052977, test error 0.25053862028033436\n",
            "Loss: 0.23245999398431305\n",
            "training error 0.12497992655567461, test error 0.25043181293797495\n",
            "Loss: 0.18972980469005307\n",
            "training error 0.12495206779083488, test error 0.25049712657511936\n",
            "Loss: 0.2158596944244584\n",
            "training error 0.12515356683250162, test error 0.250669335590756\n",
            "Loss: 0.284754993884051\n",
            "training error 0.1253254391504447, test error 0.2506599567296327\n",
            "Loss: 0.28100281259846316\n",
            "training error 0.12506633972718365, test error 0.25063499338936224\n",
            "Loss: 0.2710157814486358\n",
            "training error 0.1250869646984455, test error 0.25054396028558307\n",
            "Loss: 0.234596358676753\n",
            "training error 0.12494444741237934, test error 0.2505534481970712\n",
            "Loss: 0.23839216751291303\n",
            "training error 0.12512485566174333, test error 0.2506708225137285\n",
            "Loss: 0.2853498640369656\n",
            "training error 0.12492948998945443, test error 0.25057056286265506\n",
            "Loss: 0.24523919585339726\n",
            "training error 0.12498558791588633, test error 0.2506118106736487\n",
            "Loss: 0.2617411210281695\n",
            "training error 0.12497619633806728, test error 0.250499338920225\n",
            "Loss: 0.21674478268769448\n",
            "training error 0.12506043837510872, test error 0.2506540892298094\n",
            "Loss: 0.27865541425866613\n",
            "training error 0.12497523809092766, test error 0.2506066921796705\n",
            "Loss: 0.25969337588485075\n",
            "training error 0.1250018576699732, test error 0.25067118112921644\n",
            "Loss: 0.285493334582565\n",
            "training error 0.12496324692155644, test error 0.2507375357491934\n",
            "Loss: 0.31203968813402483\n",
            "training error 0.1250189922486148, test error 0.2507010971341987\n",
            "Loss: 0.29746176790916845\n",
            "training error 0.12494445393689968, test error 0.25068219056161795\n",
            "Loss: 0.289897855097232\n",
            "training error 0.12494970231514355, test error 0.25079120830422585\n",
            "Loss: 0.3335123545797902\n",
            "training error 0.12496592565106152, test error 0.25073091396969416\n",
            "Loss: 0.3093905267073094\n",
            "training error 0.12494675004613007, test error 0.25061501513030143\n",
            "Loss: 0.2630231212758405\n",
            "training error 0.1249482346783425, test error 0.25056208419084036\n",
            "Loss: 0.24184715141521984\n",
            "training error 0.12493979116551131, test error 0.25056314635372\n",
            "Loss: 0.24227208868925487\n",
            "training error 0.12497642519215306, test error 0.25055945895183923\n",
            "Loss: 0.24079687755780288\n",
            "training error 0.12494875054158575, test error 0.250678828402943\n",
            "Loss: 0.2885527633325058\n",
            "training error 0.12494916710181389, test error 0.25072849283355686\n",
            "Loss: 0.3084219078542505\n",
            "training error 0.12494800475120608, test error 0.25074092940421006\n",
            "Loss: 0.3133973805740542\n",
            "training error 0.12496058603903012, test error 0.25074752034371456\n",
            "Loss: 0.3160342039087993\n",
            "training error 0.12493957882819128, test error 0.2506849507294563\n",
            "Loss: 0.29100210965136153\n",
            "training error 0.124960200516227, test error 0.2505590807714429\n",
            "Loss: 0.2406455797203444\n",
            "training error 0.12537856628448776, test error 0.25042485707245377\n",
            "Loss: 0.1869469861697004\n",
            "training error 0.12510261269294162, test error 0.25055787515389766\n",
            "Loss: 0.24016325083933143\n",
            "training error 0.12503275180274406, test error 0.2507612450462667\n",
            "Loss: 0.32152501685389634\n",
            "training error 0.1249787807432279, test error 0.2507423181396324\n",
            "Loss: 0.3139529690398435\n",
            "training error 0.1249590931739923, test error 0.25066651159220465\n",
            "Loss: 0.2836252027105246\n",
            "training error 0.12520547605380064, test error 0.25051878616604956\n",
            "Loss: 0.22452500150953458\n",
            "training error 0.12492035115664152, test error 0.2506256031026329\n",
            "Loss: 0.26725902914481736\n",
            "training error 0.12492404951113419, test error 0.2506014624745683\n",
            "Loss: 0.25760113874053747\n",
            "training error 0.12490725775453528, test error 0.2506637490930393\n",
            "Loss: 0.2825200154672691\n",
            "training error 0.12496389928479586, test error 0.250800658295747\n",
            "Loss: 0.33729299285436554\n",
            "training error 0.12490142549103654, test error 0.2507454081912483\n",
            "Loss: 0.3151891995045286\n",
            "training error 0.12490924784256785, test error 0.2507033977572771\n",
            "Loss: 0.2983821733556935\n",
            "training error 0.12504439975619716, test error 0.25088853605462313\n",
            "Loss: 0.37245006341322107\n",
            "training error 0.12490678000393203, test error 0.25082001732482556\n",
            "Loss: 0.34503791898776726\n",
            "training error 0.12492112832390839, test error 0.2507235321166695\n",
            "Loss: 0.30643728426049677\n",
            "training error 0.12492046287881134, test error 0.25080165091339596\n",
            "Loss: 0.3376901073139038\n",
            "training error 0.1249789175542011, test error 0.2506797650295235\n",
            "Loss: 0.288927477562817\n",
            "training error 0.12491961736976238, test error 0.2507397219405072\n",
            "Loss: 0.3129143131046286\n",
            "training error 0.12488557251963751, test error 0.2507346244753075\n",
            "Loss: 0.3108749809005662\n",
            "training error 0.12491453566788821, test error 0.2506938173736\n",
            "Loss: 0.29454936936494924\n",
            "training error 0.1249712477202374, test error 0.2507642941673331\n",
            "Loss: 0.32274487231955806\n",
            "training error 0.12489334116058855, test error 0.25070521794486333\n",
            "Loss: 0.2991103719831578\n",
            "training error 0.12490346611707392, test error 0.2506889577732982\n",
            "Loss: 0.2926051992713674\n",
            "training error 0.12488684907053535, test error 0.25063849347199546\n",
            "Loss: 0.27241605216186127\n",
            "training error 0.12501087620603987, test error 0.25068122977345925\n",
            "Loss: 0.28951347459504895\n",
            "training error 0.12490529295561324, test error 0.25050722048731583\n",
            "Loss: 0.21989794469217117\n",
            "training error 0.12485947003897378, test error 0.25054772905688233\n",
            "Loss: 0.23610412310068885\n",
            "training error 0.12486652379762761, test error 0.2506499688551068\n",
            "Loss: 0.277006984599093\n",
            "training error 0.12494084938425197, test error 0.2507500273542896\n",
            "Loss: 0.3170371783679071\n",
            "training error 0.12487914862455454, test error 0.2508162840531818\n",
            "Loss: 0.3435443568365004\n",
            "training error 0.12487223831058147, test error 0.2507929270693399\n",
            "Loss: 0.33419997733170614\n",
            "training error 0.12496533161141805, test error 0.25066154469906876\n",
            "Loss: 0.281638108197968\n",
            "training error 0.12490648467074605, test error 0.2508782146062471\n",
            "Loss: 0.36832078322370787\n",
            "training error 0.12483409879205766, test error 0.25080740111194866\n",
            "Loss: 0.33999057718057113\n",
            "training error 0.124832842437184, test error 0.25077354195088225\n",
            "Loss: 0.32644461367514754\n",
            "training error 0.12487621905889006, test error 0.2507485292475323\n",
            "Loss: 0.3164378339417073\n",
            "training error 0.12482630698339006, test error 0.25073897919228316\n",
            "Loss: 0.31261716338149004\n",
            "training error 0.12482062856421458, test error 0.2507289626815486\n",
            "Loss: 0.30860987895422465\n",
            "training error 0.12485648517851669, test error 0.25069546766891837\n",
            "Loss: 0.2952095995493842\n",
            "training error 0.1247956931653825, test error 0.2507282800193906\n",
            "Loss: 0.30833676773742713\n",
            "training error 0.12483397612883487, test error 0.25074591568754684\n",
            "Loss: 0.31539223248355786\n",
            "training error 0.1248108983364251, test error 0.2507465016829068\n",
            "Loss: 0.31562667041735715\n",
            "training error 0.12489222066360679, test error 0.2505504059603724\n",
            "Loss: 0.23717506626177443\n",
            "training error 0.12477863720806708, test error 0.25062653842018573\n",
            "Loss: 0.2676332196751785\n",
            "training error 0.12482033194950791, test error 0.25066109736128167\n",
            "Loss: 0.28145914270836236\n",
            "training error 0.12486291349365679, test error 0.2506044182940534\n",
            "Loss: 0.2587836672383492\n",
            "training error 0.1247874213847536, test error 0.25060679822861603\n",
            "Loss: 0.2597358026638963\n",
            "training error 0.12481637074404699, test error 0.2505047659176128\n",
            "Loss: 0.21891595014262855\n",
            "training error 0.12474033347346768, test error 0.2505783507603134\n",
            "Loss: 0.24835488372458858\n",
            "training error 0.12503451747268013, test error 0.25056542706013735\n",
            "Loss: 0.243184526118978\n",
            "training error 0.12475584265584402, test error 0.25048212135560477\n",
            "Loss: 0.2098565877456826\n",
            "training error 0.12475822639456825, test error 0.2504919678583306\n",
            "Loss: 0.21379585742571372\n",
            "training error 0.12484679548570798, test error 0.2505900165538918\n",
            "Loss: 0.25302199327776886\n",
            "training error 0.12500200069388914, test error 0.2505578318429072\n",
            "Loss: 0.24014592350227737\n",
            "training error 0.12494176959445476, test error 0.2503605585909759\n",
            "Loss: 0.1612232276322345\n",
            "training error 0.12471028539131114, test error 0.2503039375024932\n",
            "Loss: 0.13857094759728295\n",
            "training error 0.12478542943718703, test error 0.2503382063040036\n",
            "Loss: 0.15228079509541637\n",
            "training error 0.12465576315327731, test error 0.25034822233240783\n",
            "Loss: 0.15628788655781545\n",
            "training error 0.124735693593563, test error 0.2502136551320292\n",
            "Loss: 0.10245186912718474\n",
            "training error 0.12481442771143177, test error 0.2502626617723035\n",
            "Loss: 0.12205785284833404\n",
            "training error 0.12459487472377229, test error 0.2501762151950211\n",
            "Loss: 0.08747335210597118\n",
            "training error 0.12470650550608796, test error 0.2500501118333799\n",
            "Loss: 0.03702344487543918\n",
            "training error 0.12456700579359171, test error 0.2501132320786233\n",
            "Loss: 0.06227582891544792\n",
            "training error 0.1245638873940237, test error 0.2500830771118787\n",
            "Loss: 0.05021179465811709\n",
            "training error 0.12457866774588375, test error 0.25001794484373463\n",
            "Loss: 0.024154464838987977\n",
            "training error 0.12459131747739693, test error 0.24997004384314173\n",
            "Loss: 0.0049908120647090826\n",
            "training error 0.12463172283107334, test error 0.2501248836368211\n",
            "Loss: 0.06693724334976281\n",
            "training error 0.12472030012833071, test error 0.2500687464655489\n",
            "Loss: 0.04447856305758169\n",
            "training error 0.1246273698797903, test error 0.24988775551700798\n",
            "Loss: 0.0\n",
            "training error 0.12453322911838424, test error 0.2498674318912723\n",
            "Loss: 0.0\n",
            "training error 0.124430038994116, test error 0.2498891565502474\n",
            "Loss: 0.00869447402995771\n",
            "training error 0.124557412045401, test error 0.24983532536540373\n",
            "Loss: 0.0\n",
            "training error 0.12435340923509443, test error 0.24969480702109395\n",
            "Loss: 0.0\n",
            "training error 0.12438585702136856, test error 0.24963604133665473\n",
            "Loss: 0.0\n",
            "training error 0.12453101978739038, test error 0.24944086377587074\n",
            "Loss: 0.0\n",
            "training error 0.12460332904651344, test error 0.24965226841863705\n",
            "Loss: 0.08475140743429144\n",
            "training error 0.1244973618881802, test error 0.24936478484626545\n",
            "Loss: 0.0\n",
            "training error 0.12419101554686907, test error 0.2494011480495892\n",
            "Loss: 0.014582333005108694\n",
            "training error 0.12418728663214461, test error 0.24933032296815855\n",
            "Loss: 0.0\n",
            "training error 0.12417084767543266, test error 0.24926809971168978\n",
            "Loss: 0.0\n",
            "training error 0.12422309211155401, test error 0.24926409442520508\n",
            "Loss: 0.0\n",
            "training error 0.12438577724134542, test error 0.24917258366525097\n",
            "Loss: 0.0\n",
            "training error 0.12403852788124266, test error 0.24900314550897992\n",
            "Loss: 0.0\n",
            "training error 0.1240702989987432, test error 0.24886899435799165\n",
            "Loss: 0.0\n",
            "training error 0.12429468311216282, test error 0.24897940762612866\n",
            "Loss: 0.044366020131136885\n",
            "training error 0.12390555824680945, test error 0.2488462592627308\n",
            "Loss: 0.0\n",
            "training error 0.12389006131025888, test error 0.24863253168821822\n",
            "Loss: 0.0\n",
            "training error 0.12379363649490169, test error 0.24866716520137877\n",
            "Loss: 0.013929598401851706\n",
            "training error 0.12372310376347645, test error 0.2484855630910395\n",
            "Loss: 0.0\n",
            "training error 0.12368270344244249, test error 0.24843817468317878\n",
            "Loss: 0.0\n",
            "training error 0.12359954135052177, test error 0.2483798182454444\n",
            "Loss: 0.0\n",
            "training error 0.12351945820806781, test error 0.24817353578632986\n",
            "Loss: 0.0\n",
            "training error 0.12347428459460143, test error 0.24801422908909923\n",
            "Loss: 0.0\n",
            "training error 0.12343194899013267, test error 0.2478662069163854\n",
            "Loss: 0.0\n",
            "training error 0.12332143705248101, test error 0.24782441860649995\n",
            "Loss: 0.0\n",
            "training error 0.12319419201143854, test error 0.24767062645122298\n",
            "Loss: 0.0\n",
            "training error 0.1231740045834734, test error 0.247593724955585\n",
            "Loss: 0.0\n",
            "training error 0.1231061426536052, test error 0.2473083482768505\n",
            "Loss: 0.0\n",
            "training error 0.12305327308735307, test error 0.2470727066793824\n",
            "Loss: 0.0\n",
            "training error 0.12281026731026283, test error 0.24693930646480264\n",
            "Loss: 0.0\n",
            "training error 0.12276884489624498, test error 0.24668394689601816\n",
            "Loss: 0.0\n",
            "training error 0.12259191919881406, test error 0.24645176842421423\n",
            "Loss: 0.0\n",
            "training error 0.12247987316530673, test error 0.2462879238816549\n",
            "Loss: 0.0\n",
            "training error 0.12237311845053327, test error 0.24596650052255287\n",
            "Loss: 0.0\n",
            "training error 0.12229736380610945, test error 0.24560840676232917\n",
            "Loss: 0.0\n",
            "training error 0.12238491544880885, test error 0.24543668401824484\n",
            "Loss: 0.0\n",
            "training error 0.12210289093677618, test error 0.2449544742520938\n",
            "Loss: 0.0\n",
            "training error 0.12180125936178157, test error 0.2447188442377511\n",
            "Loss: 0.0\n",
            "training error 0.12165177067987279, test error 0.244457927756857\n",
            "Loss: 0.0\n",
            "training error 0.12155219007686097, test error 0.24409851446429132\n",
            "Loss: 0.0\n",
            "training error 0.12141774708164527, test error 0.24382475214049873\n",
            "Loss: 0.0\n",
            "training error 0.12112332329100715, test error 0.2433315364699314\n",
            "Loss: 0.0\n",
            "training error 0.12091324969507693, test error 0.24298425874972635\n",
            "Loss: 0.0\n",
            "training error 0.1207955416222608, test error 0.24263018795059438\n",
            "Loss: 0.0\n",
            "training error 0.12054303459057852, test error 0.24226410520114205\n",
            "Loss: 0.0\n",
            "training error 0.12039657725749207, test error 0.24162111664112282\n",
            "Loss: 0.0\n",
            "training error 0.12013664275191382, test error 0.24121591262607442\n",
            "Loss: 0.0\n",
            "training error 0.11988596823538987, test error 0.2408173722880558\n",
            "Loss: 0.0\n",
            "training error 0.11957090349410143, test error 0.24022433820675668\n",
            "Loss: 0.0\n",
            "training error 0.1193479183665904, test error 0.23962944100013647\n",
            "Loss: 0.0\n",
            "training error 0.11916864018710265, test error 0.23915645871071955\n",
            "Loss: 0.0\n",
            "training error 0.11875530850125814, test error 0.23846626526144668\n",
            "Loss: 0.0\n",
            "training error 0.11854924598081637, test error 0.23791380799782982\n",
            "Loss: 0.0\n",
            "training error 0.11815090294633558, test error 0.2371165698139484\n",
            "Loss: 0.0\n",
            "training error 0.11789077517808547, test error 0.23653156757017307\n",
            "Loss: 0.0\n",
            "training error 0.11751671079588158, test error 0.23561303263052716\n",
            "Loss: 0.0\n",
            "training error 0.11712082076229717, test error 0.23493156266161147\n",
            "Loss: 0.0\n",
            "training error 0.1168576110038628, test error 0.23408486186849634\n",
            "Loss: 0.0\n",
            "training error 0.11638907924472632, test error 0.2333393247241344\n",
            "Loss: 0.0\n",
            "training error 0.11610527045506451, test error 0.23240721048222734\n",
            "Loss: 0.0\n",
            "training error 0.11553763280970475, test error 0.23149145709594132\n",
            "Loss: 0.0\n",
            "training error 0.11517701502745879, test error 0.23068962788441888\n",
            "Loss: 0.0\n",
            "training error 0.11457955046062838, test error 0.22965613839232382\n",
            "Loss: 0.0\n",
            "training error 0.11411290811407118, test error 0.2286421948633698\n",
            "Loss: 0.0\n",
            "training error 0.11367605491455123, test error 0.22770131341951283\n",
            "Loss: 0.0\n",
            "training error 0.11318230888526891, test error 0.22656051697027157\n",
            "Loss: 0.0\n",
            "training error 0.11285035843298043, test error 0.22536440106792027\n",
            "Loss: 0.0\n",
            "training error 0.11206894230757233, test error 0.22432718127620763\n",
            "Loss: 0.0\n",
            "training error 0.11146430691250286, test error 0.2230391374372417\n",
            "Loss: 0.0\n",
            "training error 0.11099026106320803, test error 0.2220317970352504\n",
            "Loss: 0.0\n",
            "training error 0.11023919275945536, test error 0.22066473853172383\n",
            "Loss: 0.0\n",
            "training error 0.10972968763808433, test error 0.21958098334519452\n",
            "Loss: 0.0\n",
            "training error 0.1089595508867346, test error 0.21811871841570502\n",
            "Loss: 0.0\n",
            "training error 0.10824548914827026, test error 0.2167764373321777\n",
            "Loss: 0.0\n",
            "training error 0.1077724398371893, test error 0.21518276369426517\n",
            "Loss: 0.0\n",
            "training error 0.10680310072944867, test error 0.21374156208781742\n",
            "Loss: 0.0\n",
            "training error 0.10611004767705995, test error 0.21204679696065654\n",
            "Loss: 0.0\n",
            "training error 0.10531000335166177, test error 0.21048795314920538\n",
            "Loss: 0.0\n",
            "training error 0.10456567637180242, test error 0.20885258603646506\n",
            "Loss: 0.0\n",
            "training error 0.10386723135348838, test error 0.20723310718291804\n",
            "Loss: 0.0\n",
            "training error 0.10302886284773491, test error 0.2053345027746749\n",
            "Loss: 0.0\n",
            "training error 0.10215159021827767, test error 0.20372608377820559\n",
            "Loss: 0.0\n",
            "training error 0.10138300958966967, test error 0.20177468620824074\n",
            "Loss: 0.0\n",
            "training error 0.1003990190147767, test error 0.20016459911234372\n",
            "Loss: 0.0\n",
            "training error 0.09963144586739517, test error 0.19827402616548634\n",
            "Loss: 0.0\n",
            "training error 0.09865673335213447, test error 0.19625851516386697\n",
            "Loss: 0.0\n",
            "training error 0.09774003708578541, test error 0.1943104970033591\n",
            "Loss: 0.0\n",
            "training error 0.09673647828956289, test error 0.19245276225311034\n",
            "Loss: 0.0\n",
            "training error 0.0958796605718679, test error 0.19033848595259098\n",
            "Loss: 0.0\n",
            "training error 0.09494663840136934, test error 0.1882417962166372\n",
            "Loss: 0.0\n",
            "training error 0.09385381936008479, test error 0.18632057257581885\n",
            "Loss: 0.0\n",
            "training error 0.09290401169458631, test error 0.18426190666257306\n",
            "Loss: 0.0\n",
            "training error 0.09189561712792, test error 0.18227712668692808\n",
            "Loss: 0.0\n",
            "training error 0.09094133857015994, test error 0.18035175722522803\n",
            "Loss: 0.0\n",
            "training error 0.08994337648188591, test error 0.17824655947923754\n",
            "Loss: 0.0\n",
            "training error 0.08900053516000168, test error 0.17602787223228203\n",
            "Loss: 0.0\n",
            "training error 0.0880048487353795, test error 0.1740447929004275\n",
            "Loss: 0.0\n",
            "training error 0.08693985505107667, test error 0.1718509696053139\n",
            "Loss: 0.0\n",
            "training error 0.08592078942482109, test error 0.16977650728440533\n",
            "Loss: 0.0\n",
            "training error 0.0849368579158352, test error 0.16756274844388053\n",
            "Loss: 0.0\n",
            "training error 0.08405671751072272, test error 0.1651772333645162\n",
            "Loss: 0.0\n",
            "training error 0.08286422819340052, test error 0.16323069597463802\n",
            "Loss: 0.0\n",
            "training error 0.08190156948982881, test error 0.1610136832939657\n",
            "Loss: 0.0\n",
            "training error 0.08084676368478742, test error 0.1587929219313126\n",
            "Loss: 0.0\n",
            "training error 0.07993972091155137, test error 0.15644506940794223\n",
            "Loss: 0.0\n",
            "training error 0.07886517184657901, test error 0.15425757352950692\n",
            "Loss: 0.0\n",
            "training error 0.07785022359268534, test error 0.1521085877505501\n",
            "Loss: 0.0\n",
            "training error 0.07703186460395398, test error 0.1499741991321753\n",
            "Loss: 0.0\n",
            "training error 0.07592111967594353, test error 0.1478431525489271\n",
            "Loss: 0.0\n",
            "training error 0.07506846318390531, test error 0.14588892857376712\n",
            "Loss: 0.0\n",
            "training error 0.0740284156463663, test error 0.14380804159144722\n",
            "Loss: 0.0\n",
            "training error 0.07310455601607414, test error 0.14168054315197076\n",
            "Loss: 0.0\n",
            "training error 0.07221878906362494, test error 0.13956541065238318\n",
            "Loss: 0.0\n",
            "training error 0.07126608476906424, test error 0.13774332971478603\n",
            "Loss: 0.0\n",
            "training error 0.07039375097301576, test error 0.13585179724400911\n",
            "Loss: 0.0\n",
            "training error 0.0694646937359859, test error 0.13390140609617202\n",
            "Loss: 0.0\n",
            "training error 0.06858308306286089, test error 0.13203803544808007\n",
            "Loss: 0.0\n",
            "training error 0.06771796991142376, test error 0.13002641320613686\n",
            "Loss: 0.0\n",
            "training error 0.0668684467520135, test error 0.12812816228271565\n",
            "Loss: 0.0\n",
            "training error 0.06602370657340606, test error 0.12633192767343998\n",
            "Loss: 0.0\n",
            "training error 0.06523890010509287, test error 0.12438579768425184\n",
            "Loss: 0.0\n",
            "training error 0.06442978711200822, test error 0.12282845594443942\n",
            "Loss: 0.0\n",
            "training error 0.06359438162320959, test error 0.12094816578639524\n",
            "Loss: 0.0\n",
            "training error 0.06278141653075432, test error 0.11921278718021652\n",
            "Loss: 0.0\n",
            "training error 0.062060088600567206, test error 0.11739652161056903\n",
            "Loss: 0.0\n",
            "training error 0.061376070836226634, test error 0.11586400274307244\n",
            "Loss: 0.0\n",
            "training error 0.06052271045949965, test error 0.11413264526015697\n",
            "Loss: 0.0\n",
            "training error 0.05980172167650592, test error 0.11252069643734795\n",
            "Loss: 0.0\n",
            "training error 0.05912196945245835, test error 0.11082866123984263\n",
            "Loss: 0.0\n",
            "training error 0.058383103502788046, test error 0.109253523900495\n",
            "Loss: 0.0\n",
            "training error 0.05786930124769944, test error 0.10765375606826111\n",
            "Loss: 0.0\n",
            "training error 0.056983982801122775, test error 0.10629172835113417\n",
            "Loss: 0.0\n",
            "training error 0.056353963473992005, test error 0.10477149531465811\n",
            "Loss: 0.0\n",
            "training error 0.05585327392679581, test error 0.10345800387048724\n",
            "Loss: 0.0\n",
            "training error 0.05512470117636735, test error 0.10207560719193282\n",
            "Loss: 0.0\n",
            "training error 0.05450121378143257, test error 0.1007211407835049\n",
            "Loss: 0.0\n",
            "training error 0.05390532181888124, test error 0.09927670313521915\n",
            "Loss: 0.0\n",
            "training error 0.053431763304500376, test error 0.09778979838618752\n",
            "Loss: 0.0\n",
            "training error 0.05278771880085518, test error 0.09632329432381277\n",
            "Loss: 0.0\n",
            "training error 0.052189824783575915, test error 0.09484817072183292\n",
            "Loss: 0.0\n",
            "training error 0.051599132852403924, test error 0.09375644144560094\n",
            "Loss: 0.0\n",
            "training error 0.05106788845210397, test error 0.09248916762481874\n",
            "Loss: 0.0\n",
            "training error 0.05053565886687902, test error 0.09126718881842691\n",
            "Loss: 0.0\n",
            "training error 0.05010414238250315, test error 0.08987643932684773\n",
            "Loss: 0.0\n",
            "training error 0.04950574985585637, test error 0.08890975714043767\n",
            "Loss: 0.0\n",
            "training error 0.04908836329262616, test error 0.08782981317673233\n",
            "Loss: 0.0\n",
            "training error 0.048530900291386576, test error 0.08660408577802296\n",
            "Loss: 0.0\n",
            "training error 0.04809567961078466, test error 0.08569420750299019\n",
            "Loss: 0.0\n",
            "training error 0.0476196654026797, test error 0.08465354430154715\n",
            "Loss: 0.0\n",
            "training error 0.0472181866387504, test error 0.08359738488557844\n",
            "Loss: 0.0\n",
            "training error 0.046762645741125805, test error 0.08272216339000062\n",
            "Loss: 0.0\n",
            "training error 0.0463835537251498, test error 0.08161722595177086\n",
            "Loss: 0.0\n",
            "training error 0.04590018570760894, test error 0.0806638259103771\n",
            "Loss: 0.0\n",
            "training error 0.045532478763583815, test error 0.07975486022125494\n",
            "Loss: 0.0\n",
            "training error 0.045196135493639204, test error 0.0786588525420838\n",
            "Loss: 0.0\n",
            "training error 0.04481349442523932, test error 0.07790137459842285\n",
            "Loss: 0.0\n",
            "training error 0.044378170856930015, test error 0.0770441988908594\n",
            "Loss: 0.0\n",
            "training error 0.04399318294548097, test error 0.07599390663074627\n",
            "Loss: 0.0\n",
            "training error 0.04366142801887829, test error 0.07508401712007977\n",
            "Loss: 0.0\n",
            "training error 0.04331231593570062, test error 0.07432245721213389\n",
            "Loss: 0.0\n",
            "training error 0.042959173685921566, test error 0.07355756540899555\n",
            "Loss: 0.0\n",
            "training error 0.04265164012214957, test error 0.0728838974531832\n",
            "Loss: 0.0\n",
            "training error 0.04232570363228077, test error 0.07203974024868488\n",
            "Loss: 0.0\n",
            "training error 0.04199085210079938, test error 0.07134325242939066\n",
            "Loss: 0.0\n",
            "training error 0.04171960436816887, test error 0.07069439925453037\n",
            "Loss: 0.0\n",
            "training error 0.04141506179866228, test error 0.07006230921622193\n",
            "Loss: 0.0\n",
            "training error 0.0411016630807343, test error 0.0693606007965864\n",
            "Loss: 0.0\n",
            "training error 0.040827526653274694, test error 0.06868128982805484\n",
            "Loss: 0.0\n",
            "training error 0.040654058684687, test error 0.06768599709503662\n",
            "Loss: 0.0\n",
            "training error 0.04029673442107753, test error 0.06712939067134216\n",
            "Loss: 0.0\n",
            "training error 0.04002730951450641, test error 0.06649568604825527\n",
            "Loss: 0.0\n",
            "training error 0.03975871882016819, test error 0.06584326077493749\n",
            "Loss: 0.0\n",
            "training error 0.03957458393954595, test error 0.0654760795307912\n",
            "Loss: 0.0\n",
            "training error 0.039275363916108495, test error 0.06481400867047116\n",
            "Loss: 0.0\n",
            "training error 0.039032161372998905, test error 0.06429425681952479\n",
            "Loss: 0.0\n",
            "training error 0.038824080812221815, test error 0.06389212488610711\n",
            "Loss: 0.0\n",
            "training error 0.038587552627354735, test error 0.06333745397141298\n",
            "Loss: 0.0\n",
            "training error 0.03840539099004505, test error 0.0624870164031987\n",
            "Loss: 0.0\n",
            "training error 0.03813692157391349, test error 0.06200596020319068\n",
            "Loss: 0.0\n",
            "training error 0.03793187557205478, test error 0.06150928850644102\n",
            "Loss: 0.0\n",
            "training error 0.03779316794445821, test error 0.060919298035026735\n",
            "Loss: 0.0\n",
            "training error 0.0375370077293156, test error 0.06063828517536549\n",
            "Loss: 0.0\n",
            "training error 0.03741399428237667, test error 0.06002990980456418\n",
            "Loss: 0.0\n",
            "training error 0.03720001379070079, test error 0.05946320209609843\n",
            "Loss: 0.0\n",
            "training error 0.03698770773007606, test error 0.059104067226477444\n",
            "Loss: 0.0\n",
            "training error 0.036819433905990115, test error 0.05847476044525195\n",
            "Loss: 0.0\n",
            "training error 0.036606994496765896, test error 0.058045729460700045\n",
            "Loss: 0.0\n",
            "training error 0.03646910849176626, test error 0.057579300956508914\n",
            "Loss: 0.0\n",
            "training error 0.036263756637237944, test error 0.05713909253727461\n",
            "Loss: 0.0\n",
            "training error 0.03610967366110532, test error 0.05682084944202672\n",
            "Loss: 0.0\n",
            "training error 0.03595519866352234, test error 0.05643200802106644\n",
            "Loss: 0.0\n",
            "training error 0.03581353919756491, test error 0.05621337317622415\n",
            "Loss: 0.0\n",
            "training error 0.03565596446928842, test error 0.055917372378921325\n",
            "Loss: 0.0\n",
            "training error 0.0354960700282465, test error 0.05543213633096832\n",
            "Loss: 0.0\n",
            "training error 0.03538812033462297, test error 0.055016891999321665\n",
            "Loss: 0.0\n",
            "training error 0.035214531872553134, test error 0.054882260059982875\n",
            "Loss: 0.0\n",
            "training error 0.03508219229361882, test error 0.05457767315097085\n",
            "Loss: 0.0\n",
            "training error 0.03501983570436899, test error 0.05406150405331262\n",
            "Loss: 0.0\n",
            "training error 0.034803667306286215, test error 0.053990860561713705\n",
            "Loss: 0.0\n",
            "training error 0.034671820267332766, test error 0.053611733633384395\n",
            "Loss: 0.0\n",
            "training error 0.03451763567364588, test error 0.053319111614512776\n",
            "Loss: 0.0\n",
            "training error 0.03446309268974825, test error 0.05304338311639986\n",
            "Loss: 0.0\n",
            "training error 0.03429369104329165, test error 0.05276754339171347\n",
            "Loss: 0.0\n",
            "training error 0.03418623740027161, test error 0.052598498652316264\n",
            "Loss: 0.0\n",
            "training error 0.03407645125721621, test error 0.05229007139900036\n",
            "Loss: 0.0\n",
            "training error 0.033921855005268, test error 0.05203427910267189\n",
            "Loss: 0.0\n",
            "training error 0.03380822813167321, test error 0.051810590555487486\n",
            "Loss: 0.0\n",
            "training error 0.03374763157081, test error 0.0515198855174986\n",
            "Loss: 0.0\n",
            "training error 0.03362452264901494, test error 0.05142897398884429\n",
            "Loss: 0.0\n",
            "training error 0.033558497949650386, test error 0.05112648107350376\n",
            "Loss: 0.0\n",
            "training error 0.033442424611721415, test error 0.05065772996557323\n",
            "Loss: 0.0\n",
            "training error 0.03329315314758651, test error 0.0505623116961041\n",
            "Loss: 0.0\n",
            "training error 0.03319128758297421, test error 0.05039725510465836\n",
            "Loss: 0.0\n",
            "training error 0.03308514776396586, test error 0.050158810231854305\n",
            "Loss: 0.0\n",
            "training error 0.033019082373878496, test error 0.050051165754567875\n",
            "Loss: 0.0\n",
            "training error 0.03291947382013028, test error 0.04979502077045433\n",
            "Loss: 0.0\n",
            "training error 0.03279346980916269, test error 0.04950314468364393\n",
            "Loss: 0.0\n",
            "training error 0.03273401568901842, test error 0.04939014241613789\n",
            "Loss: 0.0\n",
            "training error 0.03263156678578885, test error 0.04909451291607007\n",
            "Loss: 0.0\n",
            "training error 0.032565389297126865, test error 0.04899805354657941\n",
            "Loss: 0.0\n",
            "training error 0.03246234288516707, test error 0.048855260847399135\n",
            "Loss: 0.0\n",
            "training error 0.03237576767184656, test error 0.04864876436085629\n",
            "Loss: 0.0\n",
            "training error 0.03229845731180022, test error 0.048486597949744886\n",
            "Loss: 0.0\n",
            "training error 0.0322549890939464, test error 0.04835614111150608\n",
            "Loss: 0.0\n",
            "training error 0.032168390036846045, test error 0.04827231109633072\n",
            "Loss: 0.0\n",
            "training error 0.03207691607461537, test error 0.047932184713428354\n",
            "Loss: 0.0\n",
            "training error 0.031978205990592326, test error 0.04782339988912705\n",
            "Loss: 0.0\n",
            "training error 0.03191576444208079, test error 0.047849560823210964\n",
            "Loss: 0.05470320835525566\n",
            "training error 0.031872617386796816, test error 0.047778568325506185\n",
            "Loss: 0.0\n",
            "training error 0.0317640269912905, test error 0.047508710407516606\n",
            "Loss: 0.0\n",
            "training error 0.03170935144376573, test error 0.04732231487742656\n",
            "Loss: 0.0\n",
            "training error 0.03161738074307629, test error 0.04730325544764155\n",
            "Loss: 0.0\n",
            "training error 0.03158403764083514, test error 0.04722227185314493\n",
            "Loss: 0.0\n",
            "training error 0.03147303187968644, test error 0.04710534214242529\n",
            "Loss: 0.0\n",
            "training error 0.031440344302742514, test error 0.0468278914862864\n",
            "Loss: 0.0\n",
            "training error 0.03136041449383748, test error 0.046768087154082226\n",
            "Loss: 0.0\n",
            "training error 0.03128099259414192, test error 0.046794272112417885\n",
            "Loss: 0.05598894444707181\n",
            "training error 0.03125356843891666, test error 0.046614597314650175\n",
            "Loss: 0.0\n",
            "training error 0.03116090002358817, test error 0.046529540473556855\n",
            "Loss: 0.0\n",
            "training error 0.031112968007948875, test error 0.04648625375477105\n",
            "Loss: 0.0\n",
            "training error 0.031035823158991102, test error 0.04627081923549164\n",
            "Loss: 0.0\n",
            "training error 0.030979302786129995, test error 0.046255775563247274\n",
            "Loss: 0.0\n",
            "training error 0.03089346768606209, test error 0.04601567790695204\n",
            "Loss: 0.0\n",
            "training error 0.03082059372431275, test error 0.04594464682365559\n",
            "Loss: 0.0\n",
            "training error 0.03076463348971616, test error 0.045923972482382856\n",
            "Loss: 0.0\n",
            "training error 0.030731295240852625, test error 0.045816939725446926\n",
            "Loss: 0.0\n",
            "training error 0.030656495353122867, test error 0.04567826626734361\n",
            "Loss: 0.0\n",
            "training error 0.03063219154562532, test error 0.04553932933334089\n",
            "Loss: 0.0\n",
            "training error 0.030550189323776376, test error 0.04552537533448404\n",
            "Loss: 0.0\n",
            "training error 0.030556122784278657, test error 0.04541953298116399\n",
            "Loss: 0.0\n",
            "training error 0.030453162353000756, test error 0.04544150904166123\n",
            "Loss: 0.048384602515283426\n",
            "training error 0.030457341270652655, test error 0.045025950105797206\n",
            "Loss: 0.0\n",
            "training error 0.03034965882608643, test error 0.045112136627690244\n",
            "Loss: 0.1914152209792963\n",
            "training error 0.03030281664338173, test error 0.04507550502159632\n",
            "Loss: 0.11005856774297662\n",
            "training error 0.030352046513037273, test error 0.04478670252966742\n",
            "Loss: 0.0\n",
            "training error 0.03018899091405131, test error 0.044967861811178435\n",
            "Loss: 0.4044934573850689\n",
            "training error 0.03014921729455472, test error 0.04496369620277825\n",
            "Loss: 0.39519246364161376\n",
            "training error 0.03012378423032408, test error 0.04496865150125086\n",
            "Loss: 0.40625668179727903\n",
            "training error 0.03002676498130314, test error 0.04483956943108355\n",
            "Loss: 0.1180415132842283\n",
            "training error 0.03001334775502909, test error 0.04478013312968574\n",
            "Loss: 0.0\n",
            "training error 0.029945076150400894, test error 0.04468389050912795\n",
            "Loss: 0.0\n",
            "training error 0.029899138575897124, test error 0.04459945840110842\n",
            "Loss: 0.0\n",
            "training error 0.029897146431224545, test error 0.044521117512143805\n",
            "Loss: 0.0\n",
            "training error 0.029823572934296768, test error 0.04444678526630293\n",
            "Loss: 0.0\n",
            "training error 0.02979450612339477, test error 0.044543453372462266\n",
            "Loss: 0.2174917838942747\n",
            "training error 0.02970603577676107, test error 0.044454895597986974\n",
            "Loss: 0.01824728523212027\n",
            "training error 0.029691460330715486, test error 0.044449886858398145\n",
            "Loss: 0.006978214682207273\n",
            "training error 0.02965655384480597, test error 0.04432737214871763\n",
            "Loss: 0.0\n",
            "training error 0.029570199847627945, test error 0.04424882993972351\n",
            "Loss: 0.0\n",
            "training error 0.0295585682412944, test error 0.04421697484919828\n",
            "Loss: 0.0\n",
            "training error 0.029486523880737078, test error 0.044083329142431064\n",
            "Loss: 0.0\n",
            "training error 0.029442612992908637, test error 0.044149298730821816\n",
            "Loss: 0.1496474737141762\n",
            "training error 0.029398487597791262, test error 0.04411501153065374\n",
            "Loss: 0.07186931849070266\n",
            "training error 0.029367456861425365, test error 0.044126852207864664\n",
            "Loss: 0.09872908031283156\n",
            "training error 0.029318316177023854, test error 0.043973561788194594\n",
            "Loss: 0.0\n",
            "training error 0.029282259527246654, test error 0.043929155983452775\n",
            "Loss: 0.0\n",
            "training error 0.02923649575556789, test error 0.043877588500916324\n",
            "Loss: 0.0\n",
            "training error 0.02920276750978768, test error 0.043957383397973206\n",
            "Loss: 0.18185798213412596\n",
            "training error 0.02915615262449025, test error 0.04395868225686337\n",
            "Loss: 0.1848181696342488\n",
            "training error 0.02911056144476594, test error 0.04383978024545341\n",
            "Loss: 0.0\n",
            "training error 0.029094098625497635, test error 0.043762976068691464\n",
            "Loss: 0.0\n",
            "training error 0.02906971580210345, test error 0.04367246089163105\n",
            "Loss: 0.0\n",
            "training error 0.029013353911707243, test error 0.04383389295840342\n",
            "Loss: 0.369642707272555\n",
            "training error 0.028970019172996898, test error 0.04372781330519972\n",
            "Loss: 0.1267444344526858\n",
            "training error 0.028959917512935435, test error 0.04386221470275213\n",
            "Loss: 0.43449305866214427\n",
            "training error 0.02888014446047136, test error 0.04370849907112758\n",
            "Loss: 0.0825192323967272\n",
            "training error 0.028852628576447947, test error 0.04374615609055991\n",
            "Loss: 0.16874523996190938\n",
            "training error 0.02881742107894417, test error 0.043631634036092735\n",
            "Loss: 0.0\n",
            "training error 0.028773904465399392, test error 0.043690211362910195\n",
            "Loss: 0.13425425866242513\n",
            "training error 0.028737024594597368, test error 0.0436136959317728\n",
            "Loss: 0.0\n",
            "training error 0.028722881720040568, test error 0.04356997301228969\n",
            "Loss: 0.0\n",
            "training error 0.028735065939462776, test error 0.043730138692354925\n",
            "Loss: 0.3676056444195108\n",
            "training error 0.02867049932730187, test error 0.04375677620221836\n",
            "Loss: 0.42874295532837614\n",
            "training error 0.02857258446816793, test error 0.043551415268796294\n",
            "Loss: 0.0\n",
            "training error 0.02854949015842794, test error 0.04349489687427309\n",
            "Loss: 0.0\n",
            "training error 0.02855572008820427, test error 0.043497876320679925\n",
            "Loss: 0.0068501056927461335\n",
            "training error 0.02851426241231388, test error 0.043247894603738964\n",
            "Loss: 0.0\n",
            "training error 0.02847164414802513, test error 0.04325618578494629\n",
            "Loss: 0.01917129442552401\n",
            "training error 0.028463614992999896, test error 0.04317385262159828\n",
            "Loss: 0.0\n",
            "training error 0.028408972277363737, test error 0.043331083589090054\n",
            "Loss: 0.36418099832284945\n",
            "training error 0.02838703025684659, test error 0.04339086385558452\n",
            "Loss: 0.5026450520602355\n",
            "training error 0.028325503350268476, test error 0.043226384936583734\n",
            "Loss: 0.12167622715044679\n",
            "training error 0.028302299576727977, test error 0.04325520277076646\n",
            "Loss: 0.18842457697991755\n",
            "training error 0.028254267620652085, test error 0.04321492438495805\n",
            "Loss: 0.09513110567118677\n",
            "training error 0.028242936796525137, test error 0.043161502884017364\n",
            "Loss: 0.0\n",
            "training error 0.028226577860995913, test error 0.043293437483213036\n",
            "Loss: 0.3056765644843473\n",
            "training error 0.028210606067941558, test error 0.043352819755055866\n",
            "Loss: 0.4432581311003103\n",
            "training error 0.028143817465703404, test error 0.043261971368002514\n",
            "Loss: 0.2327733680987043\n",
            "training error 0.028128341500863737, test error 0.04321082758522684\n",
            "Loss: 0.11427938768031609\n",
            "training error 0.02811420737107878, test error 0.04297672076894101\n",
            "Loss: 0.0\n",
            "training error 0.028090808005398323, test error 0.04300205738007781\n",
            "Loss: 0.05895426799318315\n",
            "training error 0.028021016294049766, test error 0.043151672022151914\n",
            "Loss: 0.4070837655378856\n",
            "training error 0.027988796184585794, test error 0.043173482971966534\n",
            "Loss: 0.45783437988065057\n",
            "training error 0.02798994331654263, test error 0.04302540030792252\n",
            "Loss: 0.1132695517725324\n",
            "training error 0.027941770355415854, test error 0.042832815368759096\n",
            "Loss: 0.0\n",
            "training error 0.0279591529425026, test error 0.04300864482617024\n",
            "Loss: 0.41050175174661696\n",
            "training error 0.0278741070180064, test error 0.04282778175997717\n",
            "Loss: 0.0\n",
            "training error 0.027864055081198855, test error 0.042979886071010366\n",
            "Loss: 0.3551533719062361\n",
            "training error 0.027884352530221722, test error 0.04265959355261102\n",
            "Loss: 0.0\n",
            "training error 0.02781386841023536, test error 0.04272020640357377\n",
            "Loss: 0.14208492372997927\n",
            "training error 0.02779001974121206, test error 0.04273210301384681\n",
            "Loss: 0.1699722271061077\n",
            "training error 0.027733147806565796, test error 0.04278036852086028\n",
            "Loss: 0.28311326525019\n",
            "training error 0.027721611857319903, test error 0.042671316875944804\n",
            "Loss: 0.02748109477255234\n",
            "training error 0.027723427279780815, test error 0.042730352680751614\n",
            "Loss: 0.1658692037309839\n",
            "training error 0.02766065329194196, test error 0.042758826259033024\n",
            "Loss: 0.2326152177226426\n",
            "training error 0.0276420135919569, test error 0.04295401309519252\n",
            "Loss: 0.6901602150015673\n",
            "training error 0.027632909744652597, test error 0.04286610252336134\n",
            "Loss: 0.48408565003235093\n",
            "training error 0.02756464288200668, test error 0.043006343105144236\n",
            "Loss: 0.8128290113818659\n",
            "training error 0.027534180079412638, test error 0.04291984229026489\n",
            "Loss: 0.6100591121031451\n",
            "training error 0.027548779854263482, test error 0.042826543002449056\n",
            "Loss: 0.39135264997811614\n",
            "training error 0.027505343114030738, test error 0.04244656204644346\n",
            "Loss: 0.0\n",
            "training error 0.027482084882770482, test error 0.04253686643964295\n",
            "Loss: 0.21274842730649723\n",
            "training error 0.02747813490203259, test error 0.042463154049076905\n",
            "Loss: 0.039089155478100324\n",
            "training error 0.027436067352608624, test error 0.04252746656353526\n",
            "Loss: 0.19060322719017808\n",
            "training error 0.02744065449886676, test error 0.04253786327242745\n",
            "Loss: 0.21509686905643033\n",
            "training error 0.02740922253272958, test error 0.04271183272387374\n",
            "Loss: 0.6249520918561791\n",
            "training error 0.027385804535703655, test error 0.04271555998043886\n",
            "Loss: 0.6337331482843656\n",
            "training error 0.02732451184539554, test error 0.04268867604144536\n",
            "Loss: 0.5703971849050804\n",
            "training error 0.02732821987452143, test error 0.042700589024458564\n",
            "Loss: 0.5984630221339327\n",
            "training error 0.027291292246043687, test error 0.04269942501468739\n",
            "Loss: 0.5957207275520959\n",
            "training error 0.027249819250967568, test error 0.04257395596474218\n",
            "Loss: 0.300127765728897\n",
            "training error 0.02722620658730345, test error 0.04265198526506145\n",
            "Loss: 0.48395725993832794\n",
            "training error 0.02720860699106159, test error 0.04255913508428406\n",
            "Loss: 0.26521120301197243\n",
            "training error 0.027197921972108498, test error 0.04264037574608135\n",
            "Loss: 0.4566063546579402\n",
            "training error 0.027153184573751, test error 0.04263440011974009\n",
            "Loss: 0.4425283562214055\n",
            "training error 0.027168327848976007, test error 0.042781344494315685\n",
            "Loss: 0.7887151084366106\n",
            "training error 0.02711680256737591, test error 0.04279339802493908\n",
            "Loss: 0.817112062258718\n",
            "training error 0.027110578322627144, test error 0.042800664980301675\n",
            "Loss: 0.834232307131888\n",
            "training error 0.027064891545809575, test error 0.04295863307117495\n",
            "Loss: 1.2063898700940712\n",
            "training error 0.027036353349300227, test error 0.042961267974743166\n",
            "Loss: 1.212597448378827\n",
            "training error 0.027043842472955184, test error 0.04282827158552151\n",
            "Loss: 0.8992708023335183\n",
            "training error 0.027060212556681815, test error 0.04268123102004299\n",
            "Loss: 0.5528574336427061\n",
            "training error 0.02696655710219762, test error 0.04270792750317754\n",
            "Loss: 0.6157517691258496\n",
            "training error 0.02699131151879373, test error 0.04278871914976439\n",
            "Loss: 0.8060890843092361\n",
            "training error 0.026926859632614503, test error 0.04266063920255286\n",
            "Loss: 0.5043451007296307\n",
            "training error 0.0268884735604882, test error 0.042780432327540925\n",
            "Loss: 0.7865661316272421\n",
            "training error 0.02689650579544978, test error 0.042635439635789155\n",
            "Loss: 0.4449773556195913\n",
            "training error 0.02695582699006467, test error 0.042582147356578046\n",
            "Loss: 0.31942589363593843\n",
            "training error 0.026875424583381165, test error 0.042494125491803536\n",
            "Loss: 0.11205488281484932\n",
            "training error 0.026817732826231167, test error 0.04261828438370912\n",
            "Loss: 0.40456123885315165\n",
            "training error 0.026781059076389606, test error 0.042687857387990545\n",
            "Loss: 0.5684685164444314\n",
            "training error 0.026781764184564606, test error 0.04241284054336477\n",
            "Loss: 0.0\n",
            "training error 0.02678796108571095, test error 0.04232497218828053\n",
            "Loss: 0.0\n",
            "training error 0.026740877809734647, test error 0.042294517749986556\n",
            "Loss: 0.0\n",
            "training error 0.0267216182769953, test error 0.042447556268130675\n",
            "Loss: 0.36184008303101756\n",
            "training error 0.026766582281840666, test error 0.04226421291424575\n",
            "Loss: 0.0\n",
            "training error 0.02666549438293118, test error 0.042603163702994704\n",
            "Loss: 0.801980601026897\n",
            "training error 0.02663688903717191, test error 0.04263575646193899\n",
            "Loss: 0.8790972836690614\n",
            "training error 0.026626073684542076, test error 0.0426920581688587\n",
            "Loss: 1.0123109484637682\n",
            "training error 0.026618733979660535, test error 0.042795110990058366\n",
            "Loss: 1.256140926816296\n",
            "training error 0.026587142927848497, test error 0.04267271444255863\n",
            "Loss: 0.9665423774523507\n",
            "training error 0.026590881747447996, test error 0.04268199733788177\n",
            "Loss: 0.9885063386456672\n",
            "training error 0.02657099466230282, test error 0.04270425748549943\n",
            "Loss: 1.041175360692348\n",
            "training error 0.026594717081845635, test error 0.04282690952208515\n",
            "Loss: 1.3313784145965624\n",
            "training error 0.026532300330241034, test error 0.04259226546856322\n",
            "Loss: 0.7761946377259799\n",
            "training error 0.026486967915974456, test error 0.04271895381575539\n",
            "Loss: 1.0759478768297637\n",
            "training error 0.026522197839219493, test error 0.04265077531997812\n",
            "Loss: 0.9146329224602123\n",
            "training error 0.026439611371901, test error 0.04275869131633549\n",
            "Loss: 1.1699695037336477\n",
            "training error 0.026488935867235684, test error 0.042625876122644416\n",
            "Loss: 0.8557197294374763\n",
            "training error 0.026404797522551024, test error 0.042568412661849936\n",
            "Loss: 0.7197572760231141\n",
            "training error 0.02645602507176656, test error 0.04238428775631583\n",
            "Loss: 0.28410523653594755\n",
            "training error 0.02637888616603488, test error 0.04249651851463187\n",
            "Loss: 0.5496508378316722\n",
            "training error 0.026347884005059108, test error 0.042592954985702466\n",
            "Loss: 0.7778260821365235\n",
            "training error 0.026355800053506465, test error 0.04273339556238282\n",
            "Loss: 1.1101180308007663\n",
            "training error 0.026300045183937942, test error 0.04268751491706245\n",
            "Loss: 1.0015613059577877\n",
            "training error 0.026377354847935006, test error 0.04272372988732838\n",
            "Loss: 1.087248386749784\n",
            "training error 0.02627170967918277, test error 0.04273763068212597\n",
            "Loss: 1.1201386119287005\n",
            "training error 0.026288031147191908, test error 0.04292081409396216\n",
            "Loss: 1.553563013343351\n",
            "training error 0.026272546522779435, test error 0.04284128626216473\n",
            "Loss: 1.3653947586575343\n",
            "training error 0.026245670970685486, test error 0.042734304977068496\n",
            "Loss: 1.1122697677502291\n",
            "training error 0.02627167064614501, test error 0.042545249577135756\n",
            "Loss: 0.664951843443129\n",
            "training error 0.026190851006661554, test error 0.04285182980012934\n",
            "Loss: 1.3903414860129137\n",
            "training error 0.02620025043078261, test error 0.04273331047864789\n",
            "Loss: 1.1099167169016821\n",
            "training error 0.02614358325922852, test error 0.04281619609802669\n",
            "Loss: 1.3060297251977993\n",
            "training error 0.026125652003466494, test error 0.042848336709377495\n",
            "Loss: 1.3820765959061676\n",
            "training error 0.026095842425569664, test error 0.042747464988153155\n",
            "Loss: 1.1434072483212265\n",
            "training error 0.026134596554203044, test error 0.04290825998590181\n",
            "Loss: 1.523859140504591\n",
            "training error 0.02609449753158138, test error 0.04289123308026494\n",
            "Loss: 1.4835723246317656\n",
            "training error 0.02609665033173335, test error 0.043156908425547647\n",
            "Loss: 2.112178246672136\n",
            "training error 0.02602530043973503, test error 0.042962059511650705\n",
            "Loss: 1.6511524746027684\n",
            "training error 0.026051653940341975, test error 0.043113997038379724\n",
            "Loss: 2.0106469884064504\n",
            "training error 0.026009774354979243, test error 0.04316506965130711\n",
            "Loss: 2.131488261449954\n",
            "training error 0.025966963682530732, test error 0.043078232873819386\n",
            "Loss: 1.9260265445503189\n",
            "training error 0.025962257180045654, test error 0.04308055987612464\n",
            "Loss: 1.931532390145918\n",
            "training error 0.02595223743715425, test error 0.043017454213794194\n",
            "Loss: 1.7822201044575658\n",
            "training error 0.026023985244160514, test error 0.04314803890866628\n",
            "Loss: 2.091192366964978\n",
            "training error 0.02593086900045382, test error 0.04293151035506126\n",
            "Loss: 1.5788710940138673\n",
            "training error 0.025916503304256704, test error 0.04296136303145478\n",
            "Loss: 1.649504555126935\n",
            "training error 0.025902015026562808, test error 0.04304948236584066\n",
            "Loss: 1.8580008888092303\n",
            "training error 0.02593429399334202, test error 0.04313843100010799\n",
            "Loss: 2.068459402369638\n",
            "training error 0.025830063718028075, test error 0.04289475549854625\n",
            "Loss: 1.4919066056662933\n",
            "training error 0.025886155691277196, test error 0.04280692041303183\n",
            "Loss: 1.2840828241312163\n",
            "training error 0.0258230944242052, test error 0.04291638004879326\n",
            "Loss: 1.5430717611392808\n",
            "training error 0.02586490881688823, test error 0.04282069870945024\n",
            "Loss: 1.3166832098200798\n",
            "training error 0.025780742945468414, test error 0.042875205337136735\n",
            "Loss: 1.4456495951567616\n",
            "training error 0.02590044322052306, test error 0.042939718098892854\n",
            "Loss: 1.5982911737116856\n",
            "training error 0.025738427274930732, test error 0.04297647612087171\n",
            "Loss: 1.685263151761851\n",
            "training error 0.025735051904675454, test error 0.04279525253118103\n",
            "Loss: 1.2564758227315398\n",
            "training error 0.02574951251294255, test error 0.04250300555555108\n",
            "Loss: 0.5649996175010719\n",
            "training error 0.02570152808606944, test error 0.04255205397991802\n",
            "Loss: 0.6810515228481862\n",
            "training error 0.025787574649270215, test error 0.04268908199644529\n",
            "Loss: 1.005269122275143\n",
            "training error 0.025764339365521718, test error 0.04267128842363782\n",
            "Loss: 0.9631683197745211\n",
            "training error 0.025649400954278735, test error 0.04293255353182207\n",
            "Loss: 1.5813393211234983\n",
            "training error 0.025642509199179004, test error 0.042998396419180865\n",
            "Loss: 1.7371280672487055\n",
            "training error 0.02561888578267931, test error 0.04294796864237391\n",
            "Loss: 1.6178125202887417\n",
            "training error 0.025617320122252776, test error 0.04320944096032688\n",
            "Loss: 2.2364737940323343\n",
            "training error 0.02561026457405149, test error 0.04328302721589566\n",
            "Loss: 2.410583875575978\n",
            "training error 0.025636783746341082, test error 0.043423282051047014\n",
            "Loss: 2.74243634715976\n",
            "training error 0.025588371482717594, test error 0.043261733027902566\n",
            "Loss: 2.360200379646926\n",
            "training error 0.02556192396120833, test error 0.04322158944493936\n",
            "Loss: 2.265217934227537\n",
            "training error 0.025523012380811394, test error 0.04312103006422243\n",
            "Loss: 2.0272876055095645\n",
            "training error 0.0255061893606386, test error 0.04319988820685657\n",
            "Loss: 2.2138713301234514\n",
            "training error 0.025515956463688563, test error 0.04307331985355923\n",
            "Loss: 1.9144020047295207\n",
            "training error 0.025492319795398934, test error 0.04306318468256396\n",
            "Loss: 1.8904215013759496\n",
            "training error 0.025476380144717307, test error 0.043159178291404164\n",
            "Loss: 2.117548903547073\n",
            "training error 0.02545405107617541, test error 0.04325601004663041\n",
            "Loss: 2.346659417027408\n",
            "training error 0.025452434839870926, test error 0.04320333044153817\n",
            "Loss: 2.222015891311857\n",
            "training error 0.025465953520132246, test error 0.043068888363956156\n",
            "Loss: 1.9039167991678818\n",
            "training error 0.025418556556373643, test error 0.04316953694397605\n",
            "Loss: 2.142058179498596\n",
            "training error 0.025435537101552247, test error 0.043302034128896195\n",
            "Loss: 2.4555555234311077\n",
            "training error 0.025407499540715354, test error 0.04335014542600722\n",
            "Loss: 2.5693901220042337\n",
            "training error 0.02558210552465068, test error 0.04301446593435823\n",
            "Loss: 1.7751496322308213\n",
            "training error 0.02540288198091337, test error 0.043391421442851116\n",
            "Loss: 2.6670519829447015\n",
            "training error 0.02533182229527174, test error 0.04336991434300227\n",
            "Loss: 2.616164723095604\n",
            "training error 0.025325961424427494, test error 0.04338889159843714\n",
            "Loss: 2.661066199134865\n",
            "training error 0.02529300327304968, test error 0.043324322939975235\n",
            "Loss: 2.5082923651753575\n",
            "training error 0.025282028915277963, test error 0.043336864714467155\n",
            "Loss: 2.5379670559530387\n",
            "training error 0.025277854409831317, test error 0.043376967346316554\n",
            "Loss: 2.6328526082541437\n",
            "training error 0.025336132021801003, test error 0.04315717646813068\n",
            "Loss: 2.1128124536395676\n",
            "training error 0.025270760359432272, test error 0.043559554645834095\n",
            "Loss: 3.0648665674115394\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9606481481481481 0.04226421291424575\n",
            "training error 0.12501383352764542, test error 0.25001460086332317\n",
            "training error 0.12515798777427142, test error 0.25002361293112246\n",
            "training error 0.12506932510220842, test error 0.2502708691126986\n",
            "training error 0.1250551476936374, test error 0.25025563751038665\n",
            "training error 0.12506207038983436, test error 0.25031209721935027\n",
            "training error 0.12498515962924352, test error 0.250276309456083\n",
            "training error 0.12499037707322684, test error 0.25035402976456145\n",
            "training error 0.12497883996819248, test error 0.25036091258796134\n",
            "training error 0.12506435265037696, test error 0.25052536816884513\n",
            "training error 0.1251156383468343, test error 0.25042375106435244\n",
            "training error 0.12500565937119862, test error 0.2503988040595272\n",
            "training error 0.12513543444045094, test error 0.25034785984271124\n",
            "training error 0.12496779906121987, test error 0.2504912166010315\n",
            "training error 0.12498065628039211, test error 0.2504509840228963\n",
            "training error 0.125069189058122, test error 0.25048200050610625\n",
            "training error 0.12507265718575838, test error 0.2505569601661347\n",
            "training error 0.12509955777559584, test error 0.25044682971230847\n",
            "training error 0.1250864615942551, test error 0.2504051749008837\n",
            "training error 0.12498017571973209, test error 0.25053675893161775\n",
            "training error 0.12501033937546716, test error 0.2504614653216676\n",
            "training error 0.12502262570037212, test error 0.2506160505828118\n",
            "training error 0.12499032200451123, test error 0.2505821067728265\n",
            "training error 0.12500857752011282, test error 0.250553386458438\n",
            "training error 0.12501493371490746, test error 0.2505482633925922\n",
            "training error 0.12500815272901175, test error 0.25046107940499374\n",
            "training error 0.1250920551587502, test error 0.2506330135108981\n",
            "training error 0.12505720890058458, test error 0.2505622753465742\n",
            "training error 0.12496168317156553, test error 0.25046333002615184\n",
            "training error 0.12513588267443732, test error 0.2504007019030296\n",
            "training error 0.12497579733731273, test error 0.2504858759185734\n",
            "training error 0.12501695157802775, test error 0.25041727202381847\n",
            "training error 0.12503159535000702, test error 0.2504031962344725\n",
            "training error 0.12497183366279926, test error 0.25052323459825093\n",
            "training error 0.1249899068178511, test error 0.2505958993657804\n",
            "training error 0.1250042302886439, test error 0.2505640080694626\n",
            "training error 0.12496906331086326, test error 0.2504251209063965\n",
            "training error 0.1250043194248512, test error 0.250545359275007\n",
            "training error 0.12495960421694106, test error 0.25060194819885734\n",
            "training error 0.12513951612499746, test error 0.2507197753261264\n",
            "training error 0.1250778844647253, test error 0.2505040940993688\n",
            "training error 0.12494903251331535, test error 0.25057725461381614\n",
            "training error 0.12496367202067656, test error 0.25063342260711957\n",
            "training error 0.1249616840972229, test error 0.2507470097065577\n",
            "training error 0.12500471418668616, test error 0.2508332067616967\n",
            "training error 0.12495692789777718, test error 0.2508057392301362\n",
            "training error 0.12496099864007758, test error 0.25077862001290707\n",
            "training error 0.12496186833360216, test error 0.25064806430140246\n",
            "training error 0.12499319468830275, test error 0.25061496419776197\n",
            "training error 0.12497822472324566, test error 0.25071617025862336\n",
            "training error 0.12495285974946137, test error 0.2506157373390625\n",
            "Loss: 0.2404405477374416\n",
            "training error 0.12501595595459572, test error 0.2505052786975817\n",
            "Loss: 0.19625967146086332\n",
            "training error 0.12497164697220418, test error 0.2505542399039654\n",
            "Loss: 0.21584301027972597\n",
            "training error 0.12499788093107297, test error 0.25056619507734856\n",
            "Loss: 0.22062480035993648\n",
            "training error 0.12504344109524318, test error 0.2506250082328783\n",
            "Loss: 0.24414868869551132\n",
            "training error 0.12497351286227099, test error 0.2506399199772105\n",
            "Loss: 0.25011303808979424\n",
            "training error 0.12497889836878741, test error 0.25079194442165204\n",
            "Loss: 0.3109192645728065\n",
            "training error 0.12498317169228233, test error 0.25076178980262503\n",
            "Loss: 0.29885812137442347\n",
            "training error 0.12495221817987139, test error 0.2508210676743419\n",
            "Loss: 0.32256788532907255\n",
            "training error 0.12494578008825868, test error 0.25087612748472304\n",
            "Loss: 0.34459052328341677\n",
            "training error 0.12512490582740413, test error 0.2509759317216586\n",
            "Loss: 0.3845098866289698\n",
            "training error 0.1250040383185841, test error 0.25091806344734935\n",
            "Loss: 0.36136392870913525\n",
            "training error 0.12495474724912356, test error 0.2507757356720864\n",
            "Loss: 0.30443614338320213\n",
            "training error 0.1249611260624148, test error 0.25078870073460313\n",
            "Loss: 0.30962186552583315\n",
            "training error 0.12507155216221383, test error 0.2508066207278721\n",
            "Loss: 0.31678944422206534\n",
            "training error 0.12495225119838634, test error 0.25065875077612704\n",
            "Loss: 0.2576449177686291\n",
            "training error 0.12495343050547657, test error 0.25064954017159446\n",
            "Loss: 0.2539608911154678\n",
            "training error 0.12505564994041915, test error 0.25052802300480925\n",
            "Loss: 0.2053568630444813\n",
            "training error 0.12515920242658296, test error 0.25071839302851584\n",
            "Loss: 0.281500425480119\n",
            "training error 0.1251017312917917, test error 0.250671205596452\n",
            "Loss: 0.26262655495379317\n",
            "training error 0.1250883567704392, test error 0.25076195897536663\n",
            "Loss: 0.29892578651917745\n",
            "training error 0.12526885865569715, test error 0.25055182986200125\n",
            "Loss: 0.21487904979269334\n",
            "training error 0.12504478520850867, test error 0.25072645396344106\n",
            "Loss: 0.28472461114663616\n",
            "training error 0.12516239200155635, test error 0.25067431447695926\n",
            "Loss: 0.263870034533209\n",
            "training error 0.12494051307701762, test error 0.2507710731698202\n",
            "Loss: 0.3025712513928802\n",
            "training error 0.12492150861360068, test error 0.25080552232019926\n",
            "Loss: 0.3163501068117469\n",
            "training error 0.12506986109254453, test error 0.25082887888349265\n",
            "Loss: 0.32569218651938847\n",
            "training error 0.1249403755370626, test error 0.2507561938611236\n",
            "Loss: 0.2966198754951188\n",
            "training error 0.1250312340766107, test error 0.25064498633594684\n",
            "Loss: 0.2521394632341023\n",
            "training error 0.1250708168279509, test error 0.2506846113382885\n",
            "Loss: 0.26798853853005067\n",
            "training error 0.12507576415497684, test error 0.2507564099213172\n",
            "Loss: 0.2967062945254062\n",
            "training error 0.1249269972190195, test error 0.25072461278067365\n",
            "Loss: 0.28398818104973245\n",
            "training error 0.12501836515723705, test error 0.250700467403319\n",
            "Loss: 0.27433059414430083\n",
            "training error 0.1250028060732064, test error 0.2508268158536774\n",
            "Loss: 0.3248670227856909\n",
            "training error 0.12492594722547137, test error 0.25089152791965263\n",
            "Loss: 0.35075033750082696\n",
            "training error 0.12493346359715236, test error 0.25083800831474506\n",
            "Loss: 0.3293437457566828\n",
            "training error 0.12492356353359166, test error 0.2508070195486307\n",
            "Loss: 0.31694896320904054\n",
            "training error 0.12500484846719215, test error 0.2508425250654225\n",
            "Loss: 0.331150340516273\n",
            "training error 0.12497967646919497, test error 0.25089733810324616\n",
            "Loss: 0.35307427521225065\n",
            "training error 0.12491389316520703, test error 0.25079327506050053\n",
            "Loss: 0.311451489028447\n",
            "training error 0.1249510355089905, test error 0.2508293497210336\n",
            "Loss: 0.3258805105369911\n",
            "training error 0.12497075190917992, test error 0.2509304040875843\n",
            "Loss: 0.36629989652556816\n",
            "training error 0.12492075977986884, test error 0.25087291134543394\n",
            "Loss: 0.34330414269685416\n",
            "training error 0.12494933593092607, test error 0.25084730841642083\n",
            "Loss: 0.3330635691764616\n",
            "training error 0.12498899187570954, test error 0.2506796292554844\n",
            "Loss: 0.26599582178994385\n",
            "training error 0.12497686293345879, test error 0.25075735442912583\n",
            "Loss: 0.29708407558513006\n",
            "training error 0.12492447887684087, test error 0.25075223410060105\n",
            "Loss: 0.2950360637861804\n",
            "training error 0.12493955551113954, test error 0.2507178519001553\n",
            "Loss: 0.2812839867766703\n",
            "training error 0.12498064239713859, test error 0.25065935422415186\n",
            "Loss: 0.25788628288199167\n",
            "training error 0.1250130436998623, test error 0.2506158874535708\n",
            "Loss: 0.2405005900340651\n",
            "training error 0.12489730299276168, test error 0.2506972723539247\n",
            "Loss: 0.27305264902297566\n",
            "training error 0.12492331307816533, test error 0.2506275722484636\n",
            "Loss: 0.24517423503418456\n",
            "training error 0.12489897788305661, test error 0.2506156755933028\n",
            "Loss: 0.2404158508759302\n",
            "training error 0.12525272148312847, test error 0.25082178365625357\n",
            "Loss: 0.32285426136837714\n",
            "training error 0.1250138859177135, test error 0.2508249443319747\n",
            "Loss: 0.324118457823408\n",
            "training error 0.12489445495898951, test error 0.2506559343459812\n",
            "Loss: 0.25651841150216015\n",
            "training error 0.1249574372087409, test error 0.2507303799274108\n",
            "Loss: 0.2862949050239427\n",
            "training error 0.1251058453684909, test error 0.250695904516337\n",
            "Loss: 0.2725055459406045\n",
            "training error 0.12488618844630318, test error 0.25079256800898136\n",
            "Loss: 0.3111686849375239\n",
            "training error 0.12496065652570909, test error 0.2507315613331843\n",
            "Loss: 0.2867674397356801\n",
            "training error 0.12494290366663302, test error 0.25062279417722405\n",
            "Loss: 0.2432631181541911\n",
            "training error 0.1250741902687733, test error 0.25055774290202615\n",
            "Loss: 0.2172441276739212\n",
            "training error 0.12497752535022684, test error 0.2506530532372671\n",
            "Loss: 0.25536603531925994\n",
            "training error 0.12492343304042151, test error 0.25064922210472906\n",
            "Loss: 0.2538336717993639\n",
            "training error 0.12504429384244461, test error 0.250577321457916\n",
            "Loss: 0.2250750926744649\n",
            "training error 0.12487891107379039, test error 0.2506640182233074\n",
            "Loss: 0.25975177359311985\n",
            "training error 0.1250267588689827, test error 0.2507695447208032\n",
            "Loss: 0.3019599074906676\n",
            "training error 0.12494740174942134, test error 0.2506851181169619\n",
            "Loss: 0.26819123816104184\n",
            "training error 0.1249181263285281, test error 0.2506939418367525\n",
            "Loss: 0.2717205199550321\n",
            "training error 0.12485707463406456, test error 0.2506951033087872\n",
            "Loss: 0.2721850816369109\n",
            "training error 0.1248906147636412, test error 0.2507865729476766\n",
            "Loss: 0.3087708004603451\n",
            "training error 0.1248628116167011, test error 0.25076327428641065\n",
            "Loss: 0.29945188021109903\n",
            "training error 0.12484437479636724, test error 0.25078234919174897\n",
            "Loss: 0.3070813967563035\n",
            "training error 0.12487060319955205, test error 0.25074649527839205\n",
            "Loss: 0.29274066896156636\n",
            "training error 0.12489470006148272, test error 0.25086394578276777\n",
            "Loss: 0.33971812706607984\n",
            "training error 0.12484830263449376, test error 0.2508286442708526\n",
            "Loss: 0.32559834694392276\n",
            "training error 0.12505703044916122, test error 0.25095317103601494\n",
            "Loss: 0.3754061440615075\n",
            "training error 0.12502835239581017, test error 0.2509119170147931\n",
            "Loss: 0.35890549926740345\n",
            "training error 0.12484598639288318, test error 0.2507361949460285\n",
            "Loss: 0.28862077663207586\n",
            "training error 0.12490226165152774, test error 0.2506608987392203\n",
            "Loss: 0.25850405282947797\n",
            "training error 0.12490002872007211, test error 0.25062615847321423\n",
            "Loss: 0.24460875796026293\n",
            "training error 0.12498461028715317, test error 0.25063515720225216\n",
            "Loss: 0.24820803936496105\n",
            "training error 0.12486013798698102, test error 0.2507130667548603\n",
            "Loss: 0.27937004043976366\n",
            "training error 0.12488091463333952, test error 0.25072146952662433\n",
            "Loss: 0.2827309528564692\n",
            "training error 0.12505434427185436, test error 0.2505273004481622\n",
            "Loss: 0.20506785726457366\n",
            "training error 0.12491178340549534, test error 0.25073732146145605\n",
            "Loss: 0.2890713564876801\n",
            "training error 0.12483309088937354, test error 0.25073125017399495\n",
            "Loss: 0.28664298332863325\n",
            "training error 0.12480457115332586, test error 0.25074780419419174\n",
            "Loss: 0.2932642047051459\n",
            "training error 0.12492546132877382, test error 0.2508850363206976\n",
            "Loss: 0.34815384956268236\n",
            "training error 0.12477096480968411, test error 0.25081500013503455\n",
            "Loss: 0.320141011343944\n",
            "training error 0.124827068745998, test error 0.25073849679800786\n",
            "Loss: 0.28954146365252065\n",
            "training error 0.12481761191560115, test error 0.25071000706757374\n",
            "Loss: 0.27814623699946583\n",
            "training error 0.12475901582286823, test error 0.25079156445740497\n",
            "Loss: 0.3107672877499468\n",
            "training error 0.1248358784596202, test error 0.2507573777396799\n",
            "Loss: 0.2970933992622271\n",
            "training error 0.1249466794105823, test error 0.2509181620113252\n",
            "Loss: 0.3614033519970361\n",
            "training error 0.12473879175109226, test error 0.2507703343530329\n",
            "Loss: 0.30227574193673057\n",
            "training error 0.12471802754296375, test error 0.25070121381203675\n",
            "Loss: 0.2746291401952705\n",
            "training error 0.12469592048681434, test error 0.2507540341329337\n",
            "Loss: 0.29575603467044687\n",
            "training error 0.12470035507223355, test error 0.2508186804598283\n",
            "Loss: 0.321613055288994\n",
            "training error 0.12470553859119533, test error 0.25067162518017366\n",
            "Loss: 0.26279437864098476\n",
            "training error 0.12467328839230966, test error 0.25063504650535035\n",
            "Loss: 0.24816376319012345\n",
            "training error 0.12466685963538883, test error 0.250599815750234\n",
            "Loss: 0.23407228413461212\n",
            "training error 0.12466353007490925, test error 0.2505990551602271\n",
            "Loss: 0.2337680658992758\n",
            "training error 0.12474106234830738, test error 0.25075755921484316\n",
            "Loss: 0.29716598508826575\n",
            "training error 0.12480586041273498, test error 0.25081297413961173\n",
            "Loss: 0.3193306605021151\n",
            "training error 0.12467430353352364, test error 0.25070137414763216\n",
            "Loss: 0.27469327068798677\n",
            "training error 0.12464390116019293, test error 0.2506664649439628\n",
            "Loss: 0.2607304046998271\n",
            "training error 0.12458865258513595, test error 0.25047360790385087\n",
            "Loss: 0.18359209379881225\n",
            "training error 0.12455691223479594, test error 0.25045348775404175\n",
            "Loss: 0.17554450388219678\n",
            "training error 0.12459600445799475, test error 0.25042230064164483\n",
            "Loss: 0.1630703874549111\n",
            "training error 0.12453746880493438, test error 0.25037945456874755\n",
            "Loss: 0.14593295918099436\n",
            "training error 0.12451582845598064, test error 0.2503461246937451\n",
            "Loss: 0.13260178776646203\n",
            "training error 0.12459670357510548, test error 0.2502881625965783\n",
            "Loss: 0.1094183028953255\n",
            "training error 0.12459661553840906, test error 0.2503255785666917\n",
            "Loss: 0.12438381690296207\n",
            "training error 0.12446713107471942, test error 0.25046313541947157\n",
            "Loss: 0.17940334468449226\n",
            "training error 0.12468689809466185, test error 0.2504655969050391\n",
            "Loss: 0.18038788141117035\n",
            "training error 0.12445858084545768, test error 0.25030500376089104\n",
            "Loss: 0.11615437521053096\n",
            "training error 0.12439810038264001, test error 0.25024033525960654\n",
            "Loss: 0.09028848535401757\n",
            "training error 0.1243831454757557, test error 0.2502429688731388\n",
            "Loss: 0.09134186924566468\n",
            "training error 0.12444529205967719, test error 0.2503204642731118\n",
            "Loss: 0.12233821894098718\n",
            "training error 0.12438610399264907, test error 0.2500962847864548\n",
            "Loss: 0.03267166111482478\n",
            "training error 0.12429903933447903, test error 0.25005002781384383\n",
            "Loss: 0.014169952634102145\n",
            "training error 0.12425131979530324, test error 0.24994905499829237\n",
            "Loss: 0.0\n",
            "training error 0.12424993659500196, test error 0.24983572509916677\n",
            "Loss: 0.0\n",
            "training error 0.12423671433571626, test error 0.24972318594712847\n",
            "Loss: 0.0\n",
            "training error 0.12422704426018229, test error 0.24958546655886701\n",
            "Loss: 0.0\n",
            "training error 0.124179930920489, test error 0.24960613454794425\n",
            "Loss: 0.008280926514747655\n",
            "training error 0.12408509949462412, test error 0.2495172743744441\n",
            "Loss: 0.0\n",
            "training error 0.12405071473229189, test error 0.249433166007401\n",
            "Loss: 0.0\n",
            "training error 0.1241538027682985, test error 0.24931619407840902\n",
            "Loss: 0.0\n",
            "training error 0.12399509911490333, test error 0.24922656140938676\n",
            "Loss: 0.0\n",
            "training error 0.1239564184376785, test error 0.24909633733613915\n",
            "Loss: 0.0\n",
            "training error 0.12393512388563555, test error 0.24903948152874827\n",
            "Loss: 0.0\n",
            "training error 0.123829994563576, test error 0.2488590243736826\n",
            "Loss: 0.0\n",
            "training error 0.12383171118881471, test error 0.24884070424640492\n",
            "Loss: 0.0\n",
            "training error 0.12372429372491939, test error 0.2487661315036328\n",
            "Loss: 0.0\n",
            "training error 0.12366011012295061, test error 0.24862787091968866\n",
            "Loss: 0.0\n",
            "training error 0.1236286829439901, test error 0.24849699137458262\n",
            "Loss: 0.0\n",
            "training error 0.12355646313116102, test error 0.2484861183738587\n",
            "Loss: 0.0\n",
            "training error 0.12362493340356813, test error 0.2484944698729169\n",
            "Loss: 0.0033609519569433033\n",
            "training error 0.12342178355774742, test error 0.24821503959938893\n",
            "Loss: 0.0\n",
            "training error 0.12346607810443971, test error 0.2481289726340861\n",
            "Loss: 0.0\n",
            "training error 0.12334265365220443, test error 0.24796898629137953\n",
            "Loss: 0.0\n",
            "training error 0.12338882626192432, test error 0.24765114852039413\n",
            "Loss: 0.0\n",
            "training error 0.1231615252450862, test error 0.2476645995365939\n",
            "Loss: 0.005431437035574582\n",
            "training error 0.12310097071605099, test error 0.2472902657558208\n",
            "Loss: 0.0\n",
            "training error 0.12298621686985559, test error 0.24717740685954961\n",
            "Loss: 0.0\n",
            "training error 0.12286811681853857, test error 0.2468658134601717\n",
            "Loss: 0.0\n",
            "training error 0.12274149154053692, test error 0.24681898058560647\n",
            "Loss: 0.0\n",
            "training error 0.1227174339233558, test error 0.24671882529567785\n",
            "Loss: 0.0\n",
            "training error 0.12248143713176692, test error 0.24642239116396097\n",
            "Loss: 0.0\n",
            "training error 0.1223532321710389, test error 0.24609492619673676\n",
            "Loss: 0.0\n",
            "training error 0.12232687363310604, test error 0.24589148110680464\n",
            "Loss: 0.0\n",
            "training error 0.12226295968600605, test error 0.24570614979312017\n",
            "Loss: 0.0\n",
            "training error 0.12205183452747026, test error 0.245218993873331\n",
            "Loss: 0.0\n",
            "training error 0.1220731097046715, test error 0.2450380114706572\n",
            "Loss: 0.0\n",
            "training error 0.12183063856849993, test error 0.2445977456709869\n",
            "Loss: 0.0\n",
            "training error 0.12154313726057951, test error 0.24427881106784724\n",
            "Loss: 0.0\n",
            "training error 0.12137341230816137, test error 0.24401210479494156\n",
            "Loss: 0.0\n",
            "training error 0.12114653465412366, test error 0.2435478436115081\n",
            "Loss: 0.0\n",
            "training error 0.1209863746994749, test error 0.2432049726694265\n",
            "Loss: 0.0\n",
            "training error 0.12076370775338831, test error 0.24281504002963966\n",
            "Loss: 0.0\n",
            "training error 0.12059693888685986, test error 0.24244473252522342\n",
            "Loss: 0.0\n",
            "training error 0.12035960593074121, test error 0.24201068577837143\n",
            "Loss: 0.0\n",
            "training error 0.12026179600233931, test error 0.2415657213958202\n",
            "Loss: 0.0\n",
            "training error 0.11993526280729612, test error 0.24124889679509373\n",
            "Loss: 0.0\n",
            "training error 0.11979975319465928, test error 0.24065268730459596\n",
            "Loss: 0.0\n",
            "training error 0.11940745419786768, test error 0.24019713398065268\n",
            "Loss: 0.0\n",
            "training error 0.11923249644375565, test error 0.23974764355282643\n",
            "Loss: 0.0\n",
            "training error 0.11898161254487895, test error 0.2390199038030772\n",
            "Loss: 0.0\n",
            "training error 0.11857504895312598, test error 0.23846417776809117\n",
            "Loss: 0.0\n",
            "training error 0.11874524325698241, test error 0.2379168255928088\n",
            "Loss: 0.0\n",
            "training error 0.1181063495358139, test error 0.23715182323553702\n",
            "Loss: 0.0\n",
            "training error 0.11763179903103292, test error 0.23647467233798075\n",
            "Loss: 0.0\n",
            "training error 0.11731507762514234, test error 0.23582802574088138\n",
            "Loss: 0.0\n",
            "training error 0.1170203011312043, test error 0.23512846447978167\n",
            "Loss: 0.0\n",
            "training error 0.1165707776423658, test error 0.23428692084992414\n",
            "Loss: 0.0\n",
            "training error 0.11622544216394001, test error 0.2334836820074413\n",
            "Loss: 0.0\n",
            "training error 0.11578463839990542, test error 0.23265765918309786\n",
            "Loss: 0.0\n",
            "training error 0.11568408243459341, test error 0.23161463193793547\n",
            "Loss: 0.0\n",
            "training error 0.11498159343640933, test error 0.23070066455649624\n",
            "Loss: 0.0\n",
            "training error 0.1145110152701325, test error 0.22977715940210178\n",
            "Loss: 0.0\n",
            "training error 0.11413435001372707, test error 0.22887301782669345\n",
            "Loss: 0.0\n",
            "training error 0.1135824790018971, test error 0.22773227291391324\n",
            "Loss: 0.0\n",
            "training error 0.11308492668111614, test error 0.2266629866870312\n",
            "Loss: 0.0\n",
            "training error 0.11253688143397937, test error 0.22554921824975524\n",
            "Loss: 0.0\n",
            "training error 0.1119988608371998, test error 0.2244647369826413\n",
            "Loss: 0.0\n",
            "training error 0.11156270462542342, test error 0.22335534065884294\n",
            "Loss: 0.0\n",
            "training error 0.11088387837949185, test error 0.22219958253432226\n",
            "Loss: 0.0\n",
            "training error 0.11036038605655168, test error 0.22097753811428295\n",
            "Loss: 0.0\n",
            "training error 0.10966403262478844, test error 0.2196627129198635\n",
            "Loss: 0.0\n",
            "training error 0.10906729498380624, test error 0.21840474274991215\n",
            "Loss: 0.0\n",
            "training error 0.108434945312257, test error 0.21711036729613079\n",
            "Loss: 0.0\n",
            "training error 0.10780096152591088, test error 0.21564165548184497\n",
            "Loss: 0.0\n",
            "training error 0.10717954291941098, test error 0.21417024863775194\n",
            "Loss: 0.0\n",
            "training error 0.10649793965320893, test error 0.21294979310611106\n",
            "Loss: 0.0\n",
            "training error 0.10578236312338708, test error 0.21140242775479914\n",
            "Loss: 0.0\n",
            "training error 0.10489941859364668, test error 0.20971512181644786\n",
            "Loss: 0.0\n",
            "training error 0.10420702073017162, test error 0.20826360548835865\n",
            "Loss: 0.0\n",
            "training error 0.10352180199106692, test error 0.2064970219796036\n",
            "Loss: 0.0\n",
            "training error 0.10265800399839171, test error 0.20487998999722334\n",
            "Loss: 0.0\n",
            "training error 0.10174754653122908, test error 0.20318856866196586\n",
            "Loss: 0.0\n",
            "training error 0.10096134888669688, test error 0.20151723895563994\n",
            "Loss: 0.0\n",
            "training error 0.10010244244429056, test error 0.19958667712029854\n",
            "Loss: 0.0\n",
            "training error 0.09923733334247105, test error 0.1976772522069534\n",
            "Loss: 0.0\n",
            "training error 0.09850792998883177, test error 0.1958281901674415\n",
            "Loss: 0.0\n",
            "training error 0.09754217529055038, test error 0.19387171823256888\n",
            "Loss: 0.0\n",
            "training error 0.09659307182827843, test error 0.1920872892634332\n",
            "Loss: 0.0\n",
            "training error 0.0957080343427879, test error 0.19017009158135692\n",
            "Loss: 0.0\n",
            "training error 0.0948565983098791, test error 0.18831803059269803\n",
            "Loss: 0.0\n",
            "training error 0.09393260354939066, test error 0.18628925538958224\n",
            "Loss: 0.0\n",
            "training error 0.09293547236584355, test error 0.18444558057752342\n",
            "Loss: 0.0\n",
            "training error 0.09204938072892957, test error 0.18243121299270484\n",
            "Loss: 0.0\n",
            "training error 0.09108309679815162, test error 0.1806057119463259\n",
            "Loss: 0.0\n",
            "training error 0.09027318036465702, test error 0.178719017840964\n",
            "Loss: 0.0\n",
            "training error 0.08922818133852785, test error 0.17646476909085856\n",
            "Loss: 0.0\n",
            "training error 0.08824521251343745, test error 0.17443216249851043\n",
            "Loss: 0.0\n",
            "training error 0.08734860991391936, test error 0.17222540868179345\n",
            "Loss: 0.0\n",
            "training error 0.0863066073045601, test error 0.17017457609868483\n",
            "Loss: 0.0\n",
            "training error 0.08535786322138801, test error 0.16825130740830274\n",
            "Loss: 0.0\n",
            "training error 0.08447337721969109, test error 0.16603603200401537\n",
            "Loss: 0.0\n",
            "training error 0.08352192668345165, test error 0.16420971480789545\n",
            "Loss: 0.0\n",
            "training error 0.08250610456781399, test error 0.16218978448602725\n",
            "Loss: 0.0\n",
            "training error 0.08162702512456585, test error 0.15998175792356836\n",
            "Loss: 0.0\n",
            "training error 0.08073585239797403, test error 0.15826315620730882\n",
            "Loss: 0.0\n",
            "training error 0.07970401672115798, test error 0.15599950283516384\n",
            "Loss: 0.0\n",
            "training error 0.07864603026868958, test error 0.1537723700424238\n",
            "Loss: 0.0\n",
            "training error 0.07770576975148803, test error 0.15184567895219628\n",
            "Loss: 0.0\n",
            "training error 0.07675153835476671, test error 0.14974750855003693\n",
            "Loss: 0.0\n",
            "training error 0.0758291777182761, test error 0.1478283569456096\n",
            "Loss: 0.0\n",
            "training error 0.07496344926801198, test error 0.1458099086315828\n",
            "Loss: 0.0\n",
            "training error 0.07414631734229932, test error 0.14389276228791661\n",
            "Loss: 0.0\n",
            "training error 0.07313128019750927, test error 0.1418388362017429\n",
            "Loss: 0.0\n",
            "training error 0.0723218759173212, test error 0.13990570834772256\n",
            "Loss: 0.0\n",
            "training error 0.0713539317260817, test error 0.13791813489957863\n",
            "Loss: 0.0\n",
            "training error 0.07057348218211451, test error 0.13612820624156421\n",
            "Loss: 0.0\n",
            "training error 0.06971997169639002, test error 0.13397761880750705\n",
            "Loss: 0.0\n",
            "training error 0.06881978272852643, test error 0.1321028793977546\n",
            "Loss: 0.0\n",
            "training error 0.06806433090583494, test error 0.13048620859303584\n",
            "Loss: 0.0\n",
            "training error 0.06711592968778662, test error 0.12860274929180807\n",
            "Loss: 0.0\n",
            "training error 0.06634263967978944, test error 0.12657620219688007\n",
            "Loss: 0.0\n",
            "training error 0.06550367375157215, test error 0.12477458771086236\n",
            "Loss: 0.0\n",
            "training error 0.06470677349639808, test error 0.12306018942622861\n",
            "Loss: 0.0\n",
            "training error 0.06396761987033683, test error 0.12142432101855913\n",
            "Loss: 0.0\n",
            "training error 0.06315787599777732, test error 0.11976083632433608\n",
            "Loss: 0.0\n",
            "training error 0.06245842448174776, test error 0.11821807665355885\n",
            "Loss: 0.0\n",
            "training error 0.06169041995634943, test error 0.11651056370707484\n",
            "Loss: 0.0\n",
            "training error 0.06093239917903456, test error 0.11482249307213531\n",
            "Loss: 0.0\n",
            "training error 0.06023849092147184, test error 0.11329287188405536\n",
            "Loss: 0.0\n",
            "training error 0.05953206761319229, test error 0.11167889925083246\n",
            "Loss: 0.0\n",
            "training error 0.058859502804237705, test error 0.11020613381352143\n",
            "Loss: 0.0\n",
            "training error 0.058152946927819485, test error 0.1084336543181767\n",
            "Loss: 0.0\n",
            "training error 0.05750860800094733, test error 0.10682310227239462\n",
            "Loss: 0.0\n",
            "training error 0.05683406464911342, test error 0.10529078920774791\n",
            "Loss: 0.0\n",
            "training error 0.05630004717567173, test error 0.10377642954513261\n",
            "Loss: 0.0\n",
            "training error 0.055583955569815886, test error 0.10256416459579158\n",
            "Loss: 0.0\n",
            "training error 0.05502945444955814, test error 0.10122282922883719\n",
            "Loss: 0.0\n",
            "training error 0.05438586651133988, test error 0.09984142696604048\n",
            "Loss: 0.0\n",
            "training error 0.05381153806121765, test error 0.09852098631605326\n",
            "Loss: 0.0\n",
            "training error 0.053218377733948115, test error 0.09721247754780521\n",
            "Loss: 0.0\n",
            "training error 0.05265693016871164, test error 0.09592224902733297\n",
            "Loss: 0.0\n",
            "training error 0.0521919571781704, test error 0.09470345476977626\n",
            "Loss: 0.0\n",
            "training error 0.05158136706178815, test error 0.0934628369661328\n",
            "Loss: 0.0\n",
            "training error 0.05111921065735035, test error 0.09243258810491378\n",
            "Loss: 0.0\n",
            "training error 0.05057410121000329, test error 0.09099941564301005\n",
            "Loss: 0.0\n",
            "training error 0.050088649912842245, test error 0.08994269256519215\n",
            "Loss: 0.0\n",
            "training error 0.049637114653490993, test error 0.08873524795309179\n",
            "Loss: 0.0\n",
            "training error 0.049119349178521975, test error 0.08786060299206937\n",
            "Loss: 0.0\n",
            "training error 0.048656940636674466, test error 0.08683212381152364\n",
            "Loss: 0.0\n",
            "training error 0.04819806909476486, test error 0.08571643172521337\n",
            "Loss: 0.0\n",
            "training error 0.04774230168310901, test error 0.08469637997705004\n",
            "Loss: 0.0\n",
            "training error 0.04731981892487786, test error 0.08357588176698824\n",
            "Loss: 0.0\n",
            "training error 0.046916244482770944, test error 0.08250969264020458\n",
            "Loss: 0.0\n",
            "training error 0.04656415450143287, test error 0.08143262289197092\n",
            "Loss: 0.0\n",
            "training error 0.04608591781204411, test error 0.08064003889571703\n",
            "Loss: 0.0\n",
            "training error 0.04573562569393538, test error 0.07984907533756343\n",
            "Loss: 0.0\n",
            "training error 0.04532062855387985, test error 0.07888036274846816\n",
            "Loss: 0.0\n",
            "training error 0.04496079887101463, test error 0.07806922830925932\n",
            "Loss: 0.0\n",
            "training error 0.04459376377476567, test error 0.07711836851751687\n",
            "Loss: 0.0\n",
            "training error 0.04421644586318656, test error 0.0762538692394283\n",
            "Loss: 0.0\n",
            "training error 0.04397968905776837, test error 0.0752890955402006\n",
            "Loss: 0.0\n",
            "training error 0.04359344920299568, test error 0.07449132326432643\n",
            "Loss: 0.0\n",
            "training error 0.043207775956327424, test error 0.07390586356899738\n",
            "Loss: 0.0\n",
            "training error 0.04288806756917206, test error 0.07292627754915769\n",
            "Loss: 0.0\n",
            "training error 0.042616249694860706, test error 0.07227364121013405\n",
            "Loss: 0.0\n",
            "training error 0.04224113421295124, test error 0.07142990512160637\n",
            "Loss: 0.0\n",
            "training error 0.0419726345606009, test error 0.07067588328287484\n",
            "Loss: 0.0\n",
            "training error 0.04170622478537532, test error 0.06988708113749754\n",
            "Loss: 0.0\n",
            "training error 0.041397999193910184, test error 0.06944101405219852\n",
            "Loss: 0.0\n",
            "training error 0.041149620846504094, test error 0.06883056798860544\n",
            "Loss: 0.0\n",
            "training error 0.04085293045407284, test error 0.0681382200139467\n",
            "Loss: 0.0\n",
            "training error 0.04057598299342355, test error 0.06752708046634585\n",
            "Loss: 0.0\n",
            "training error 0.0404539817674324, test error 0.06708459663117414\n",
            "Loss: 0.0\n",
            "training error 0.04003981244943654, test error 0.06626371627567237\n",
            "Loss: 0.0\n",
            "training error 0.03983332180306678, test error 0.06567805616184934\n",
            "Loss: 0.0\n",
            "training error 0.03960210649280102, test error 0.06516927505597178\n",
            "Loss: 0.0\n",
            "training error 0.03936264125135396, test error 0.064694219671107\n",
            "Loss: 0.0\n",
            "training error 0.03911947686944144, test error 0.06415955217260352\n",
            "Loss: 0.0\n",
            "training error 0.03891950846991, test error 0.06373865166691046\n",
            "Loss: 0.0\n",
            "training error 0.038694413887119694, test error 0.06323796926423846\n",
            "Loss: 0.0\n",
            "training error 0.03848726629726739, test error 0.062807857910111\n",
            "Loss: 0.0\n",
            "training error 0.03831637231292424, test error 0.062250629412321234\n",
            "Loss: 0.0\n",
            "training error 0.038082854093130815, test error 0.06150693324339391\n",
            "Loss: 0.0\n",
            "training error 0.03787562689508884, test error 0.06108543992642887\n",
            "Loss: 0.0\n",
            "training error 0.03766048541932611, test error 0.060695310396947505\n",
            "Loss: 0.0\n",
            "training error 0.03755195006278998, test error 0.06013480057300673\n",
            "Loss: 0.0\n",
            "training error 0.03729854863898421, test error 0.059837131897867275\n",
            "Loss: 0.0\n",
            "training error 0.03717841846689464, test error 0.05945456983575135\n",
            "Loss: 0.0\n",
            "training error 0.03694306422274823, test error 0.058989456203945675\n",
            "Loss: 0.0\n",
            "training error 0.03679729870074512, test error 0.058701832120340795\n",
            "Loss: 0.0\n",
            "training error 0.036635424588597656, test error 0.05818914517508288\n",
            "Loss: 0.0\n",
            "training error 0.036469715180780266, test error 0.05785235049365852\n",
            "Loss: 0.0\n",
            "training error 0.0362879550460491, test error 0.05742805140519475\n",
            "Loss: 0.0\n",
            "training error 0.0361452695990307, test error 0.05708074167805635\n",
            "Loss: 0.0\n",
            "training error 0.03601230697818423, test error 0.05679565735434967\n",
            "Loss: 0.0\n",
            "training error 0.03586405794581517, test error 0.056504499581067055\n",
            "Loss: 0.0\n",
            "training error 0.03569575899370069, test error 0.05612397716787819\n",
            "Loss: 0.0\n",
            "training error 0.03558115289168185, test error 0.055704571901131275\n",
            "Loss: 0.0\n",
            "training error 0.03540025791414826, test error 0.05535808327094033\n",
            "Loss: 0.0\n",
            "training error 0.03529718551519522, test error 0.05512354872112674\n",
            "Loss: 0.0\n",
            "training error 0.03519787232591543, test error 0.05466558656476142\n",
            "Loss: 0.0\n",
            "training error 0.035010142013559314, test error 0.0544587910737733\n",
            "Loss: 0.0\n",
            "training error 0.03493007182306116, test error 0.054227317365567296\n",
            "Loss: 0.0\n",
            "training error 0.034822654906898814, test error 0.05379891559638051\n",
            "Loss: 0.0\n",
            "training error 0.034613749251715516, test error 0.053694779094084516\n",
            "Loss: 0.0\n",
            "training error 0.0344900496916011, test error 0.053485749891977144\n",
            "Loss: 0.0\n",
            "training error 0.03440058091528721, test error 0.05317380545655289\n",
            "Loss: 0.0\n",
            "training error 0.0342844686761446, test error 0.05291087289266315\n",
            "Loss: 0.0\n",
            "training error 0.03414580204718926, test error 0.052746622465552\n",
            "Loss: 0.0\n",
            "training error 0.03403334572013356, test error 0.05240189138377159\n",
            "Loss: 0.0\n",
            "training error 0.033932509295315834, test error 0.052176642759758154\n",
            "Loss: 0.0\n",
            "training error 0.03384782525004304, test error 0.051920087095489384\n",
            "Loss: 0.0\n",
            "training error 0.03371797547582807, test error 0.051843335256883866\n",
            "Loss: 0.0\n",
            "training error 0.03361792927096513, test error 0.051674185264259015\n",
            "Loss: 0.0\n",
            "training error 0.03352552883929265, test error 0.05139956496911009\n",
            "Loss: 0.0\n",
            "training error 0.033414822541427996, test error 0.05103668643661269\n",
            "Loss: 0.0\n",
            "training error 0.033304852949674425, test error 0.05090328119717471\n",
            "Loss: 0.0\n",
            "training error 0.03323227544376451, test error 0.05052658553984269\n",
            "Loss: 0.0\n",
            "training error 0.03313456336956967, test error 0.050317972096830275\n",
            "Loss: 0.0\n",
            "training error 0.033046700244863815, test error 0.05024180104433297\n",
            "Loss: 0.0\n",
            "training error 0.03297430235824518, test error 0.050090380415337914\n",
            "Loss: 0.0\n",
            "training error 0.032857870515257674, test error 0.04989778709985656\n",
            "Loss: 0.0\n",
            "training error 0.03275758625121401, test error 0.049668879330106164\n",
            "Loss: 0.0\n",
            "training error 0.032666338111171964, test error 0.04942307707640363\n",
            "Loss: 0.0\n",
            "training error 0.03262050115041958, test error 0.04921670999219975\n",
            "Loss: 0.0\n",
            "training error 0.03253484025176162, test error 0.04928436659758982\n",
            "Loss: 0.13746673721342084\n",
            "training error 0.0324600719638381, test error 0.049090755019797344\n",
            "Loss: 0.0\n",
            "training error 0.03232782377777954, test error 0.04887649724992636\n",
            "Loss: 0.0\n",
            "training error 0.032279578905850266, test error 0.048660219718458485\n",
            "Loss: 0.0\n",
            "training error 0.03218395168469992, test error 0.04864038851069356\n",
            "Loss: 0.0\n",
            "training error 0.03211359624869756, test error 0.048560645766486596\n",
            "Loss: 0.0\n",
            "training error 0.03201893179339036, test error 0.048344090278904066\n",
            "Loss: 0.0\n",
            "training error 0.032002646848951, test error 0.04825096135069714\n",
            "Loss: 0.0\n",
            "training error 0.031880673497866586, test error 0.0482481167171251\n",
            "Loss: 0.0\n",
            "training error 0.031793476048083774, test error 0.04801554198517403\n",
            "Loss: 0.0\n",
            "training error 0.03175412040527024, test error 0.047765465487834106\n",
            "Loss: 0.0\n",
            "training error 0.03167188674098269, test error 0.04779085138524692\n",
            "Loss: 0.05314696958051979\n",
            "training error 0.03162282940825544, test error 0.04784202105407385\n",
            "Loss: 0.16027388293586853\n",
            "training error 0.031519241769596466, test error 0.04763265068522789\n",
            "Loss: 0.0\n",
            "training error 0.031443354160124504, test error 0.04737622748762233\n",
            "Loss: 0.0\n",
            "training error 0.031392293669544935, test error 0.04733563932604137\n",
            "Loss: 0.0\n",
            "training error 0.0313319213690442, test error 0.04723629732209211\n",
            "Loss: 0.0\n",
            "training error 0.03129378162606491, test error 0.047003384824610986\n",
            "Loss: 0.0\n",
            "training error 0.031184077624404734, test error 0.046980598725103406\n",
            "Loss: 0.0\n",
            "training error 0.03111823763668202, test error 0.046896508764808914\n",
            "Loss: 0.0\n",
            "training error 0.03105460021152657, test error 0.04677720829788721\n",
            "Loss: 0.0\n",
            "training error 0.030995666062125928, test error 0.046669273458646014\n",
            "Loss: 0.0\n",
            "training error 0.030978021918827515, test error 0.0465769286917258\n",
            "Loss: 0.0\n",
            "training error 0.030882351864931993, test error 0.0464422612810739\n",
            "Loss: 0.0\n",
            "training error 0.030819024483787174, test error 0.04643883243689263\n",
            "Loss: 0.0\n",
            "training error 0.03080977879829908, test error 0.04631117174131961\n",
            "Loss: 0.0\n",
            "training error 0.030745970213489435, test error 0.04634268486585117\n",
            "Loss: 0.0680464849984519\n",
            "training error 0.030692948131830287, test error 0.046253027778150026\n",
            "Loss: 0.0\n",
            "training error 0.030578392044475933, test error 0.04609004320473047\n",
            "Loss: 0.0\n",
            "training error 0.030532178407884743, test error 0.04608039308487189\n",
            "Loss: 0.0\n",
            "training error 0.030514788282458332, test error 0.04588381184541053\n",
            "Loss: 0.0\n",
            "training error 0.0304159025984084, test error 0.04593732035715489\n",
            "Loss: 0.1166174072996462\n",
            "training error 0.030384408815174475, test error 0.04577686982920252\n",
            "Loss: 0.0\n",
            "training error 0.03034995412927041, test error 0.045770099486241105\n",
            "Loss: 0.0\n",
            "training error 0.030265254025471006, test error 0.04558645087096868\n",
            "Loss: 0.0\n",
            "training error 0.030240420715490546, test error 0.04550173301957255\n",
            "Loss: 0.0\n",
            "training error 0.03018367251014357, test error 0.04562575096650498\n",
            "Loss: 0.2725565351084258\n",
            "training error 0.030110990759975916, test error 0.04539364298059282\n",
            "Loss: 0.0\n",
            "training error 0.03004261096587086, test error 0.045392834567991945\n",
            "Loss: 0.0\n",
            "training error 0.030014841821803986, test error 0.04535124670086875\n",
            "Loss: 0.0\n",
            "training error 0.02997551750266214, test error 0.04537142463710001\n",
            "Loss: 0.044492572308652\n",
            "training error 0.029932306472458835, test error 0.045229765305160645\n",
            "Loss: 0.0\n",
            "training error 0.02985566210961667, test error 0.04503623277344654\n",
            "Loss: 0.0\n",
            "training error 0.029811407916845, test error 0.04497093922766821\n",
            "Loss: 0.0\n",
            "training error 0.029771592885878686, test error 0.045038862230323475\n",
            "Loss: 0.15103754518310808\n",
            "training error 0.029707142757505377, test error 0.044994149916663256\n",
            "Loss: 0.051612640059706116\n",
            "training error 0.029658155282398277, test error 0.044974370215042664\n",
            "Loss: 0.0076293433790342036\n",
            "training error 0.029683787270741505, test error 0.045055643662756245\n",
            "Loss: 0.1883537158501758\n",
            "training error 0.029568102340753497, test error 0.044936527219078426\n",
            "Loss: 0.0\n",
            "training error 0.02953194069226557, test error 0.044821426083477875\n",
            "Loss: 0.0\n",
            "training error 0.02948630515897775, test error 0.04469903171082008\n",
            "Loss: 0.0\n",
            "training error 0.029441658551900264, test error 0.044879245775862796\n",
            "Loss: 0.40317218996737125\n",
            "training error 0.029399112037430047, test error 0.04502560810753056\n",
            "Loss: 0.7306117922716293\n",
            "training error 0.02934561264794003, test error 0.04485328208571201\n",
            "Loss: 0.34508661370977745\n",
            "training error 0.029302725255871093, test error 0.044750335587098906\n",
            "Loss: 0.11477625871347819\n",
            "training error 0.02927497827133512, test error 0.04465943704695833\n",
            "Loss: 0.0\n",
            "training error 0.029223140238631692, test error 0.0446178148775695\n",
            "Loss: 0.0\n",
            "training error 0.029166676303608596, test error 0.04463146743080611\n",
            "Loss: 0.030598883594068482\n",
            "training error 0.029181746331941424, test error 0.04461684472267718\n",
            "Loss: 0.0\n",
            "training error 0.029076390682729446, test error 0.04445986603143734\n",
            "Loss: 0.0\n",
            "training error 0.02921107408491193, test error 0.04468995199368903\n",
            "Loss: 0.5175138451586792\n",
            "training error 0.029013111494451224, test error 0.044460550999224226\n",
            "Loss: 0.0015406429394237975\n",
            "training error 0.028987126278246657, test error 0.04460101968658704\n",
            "Loss: 0.3174855611348226\n",
            "training error 0.028904307367094144, test error 0.04455053282828057\n",
            "Loss: 0.2039295322642687\n",
            "training error 0.028963417297055916, test error 0.04470508681055554\n",
            "Loss: 0.5515553711853416\n",
            "training error 0.02885863880666847, test error 0.044375869279310864\n",
            "Loss: 0.0\n",
            "training error 0.02878200329337721, test error 0.044563542113319074\n",
            "Loss: 0.42291641168077376\n",
            "training error 0.028756801307965398, test error 0.044535862161033155\n",
            "Loss: 0.360540276327348\n",
            "training error 0.0287084562753751, test error 0.04444762968266531\n",
            "Loss: 0.16171041721519952\n",
            "training error 0.028660893873540375, test error 0.04432926271693327\n",
            "Loss: 0.0\n",
            "training error 0.028626880957709665, test error 0.044399090345461634\n",
            "Loss: 0.15752039228409398\n",
            "training error 0.028686078216542413, test error 0.04411305398438635\n",
            "Loss: 0.0\n",
            "training error 0.028593420313057963, test error 0.04434517193355116\n",
            "Loss: 0.5261887994582493\n",
            "training error 0.028590068802013913, test error 0.044052423334592396\n",
            "Loss: 0.0\n",
            "training error 0.02851050454963284, test error 0.04407009539366914\n",
            "Loss: 0.04011597487503238\n",
            "training error 0.028471673673516877, test error 0.04391210571424668\n",
            "Loss: 0.0\n",
            "training error 0.028432861850214344, test error 0.04394874426962262\n",
            "Loss: 0.08343611580450272\n",
            "training error 0.028409029688531765, test error 0.04401849695773117\n",
            "Loss: 0.24228226306617362\n",
            "training error 0.028351309193926033, test error 0.04394171284301523\n",
            "Loss: 0.06742361425620125\n",
            "training error 0.028322047685171818, test error 0.043918939145404914\n",
            "Loss: 0.015561611193737335\n",
            "training error 0.028344165219522367, test error 0.04388205540238523\n",
            "Loss: 0.0\n",
            "training error 0.028276319944004202, test error 0.04405706820386428\n",
            "Loss: 0.3988254421408266\n",
            "training error 0.02821356689870423, test error 0.044073246219267344\n",
            "Loss: 0.43569248324617504\n",
            "training error 0.028186677463606095, test error 0.04401086505586151\n",
            "Loss: 0.2935360531659992\n",
            "training error 0.028164803344721476, test error 0.04384956827862207\n",
            "Loss: 0.0\n",
            "training error 0.028154948814475764, test error 0.0439297964536207\n",
            "Loss: 0.1829622916441398\n",
            "training error 0.028094537977922735, test error 0.04400198867499625\n",
            "Loss: 0.34759839687747984\n",
            "training error 0.028062383617196655, test error 0.044041632212422716\n",
            "Loss: 0.43800644188847215\n",
            "training error 0.028038531415716918, test error 0.04384271295834401\n",
            "Loss: 0.0\n",
            "training error 0.027970104796922867, test error 0.04398065795391875\n",
            "Loss: 0.3146360849197505\n",
            "training error 0.028012518002613382, test error 0.04383202043616473\n",
            "Loss: 0.0\n",
            "training error 0.027941804641510148, test error 0.04377073964840296\n",
            "Loss: 0.0\n",
            "training error 0.02795989955042301, test error 0.04399614135556332\n",
            "Loss: 0.5149597858545185\n",
            "training error 0.027861695203696805, test error 0.04354342059667656\n",
            "Loss: 0.0\n",
            "training error 0.027808889997576115, test error 0.043566288859211225\n",
            "Loss: 0.05251829603944991\n",
            "training error 0.027816069041643545, test error 0.04349425973009116\n",
            "Loss: 0.0\n",
            "training error 0.027809587537393973, test error 0.04368901517788659\n",
            "Loss: 0.44777276128851184\n",
            "training error 0.027734380315922576, test error 0.043520719286321514\n",
            "Loss: 0.060834593793646086\n",
            "training error 0.027716352291144625, test error 0.043561395059170734\n",
            "Loss: 0.15435445848759333\n",
            "training error 0.02774183752581418, test error 0.043455171273548907\n",
            "Loss: 0.0\n",
            "training error 0.027639181892644932, test error 0.043659082978856206\n",
            "Loss: 0.4692461203838816\n",
            "training error 0.027617581287476975, test error 0.04370541578250877\n",
            "Loss: 0.5758681915774444\n",
            "training error 0.02759568234662858, test error 0.043622436513281314\n",
            "Loss: 0.38491446433261967\n",
            "training error 0.027570515155310282, test error 0.04349913493284038\n",
            "Loss: 0.1011701438586643\n",
            "training error 0.02751225828879571, test error 0.04349487172416734\n",
            "Loss: 0.09135955389181216\n",
            "training error 0.027491982769749384, test error 0.04343092671791465\n",
            "Loss: 0.0\n",
            "training error 0.027489464300706503, test error 0.04358886617996992\n",
            "Loss: 0.3636566704668498\n",
            "training error 0.027444639634755796, test error 0.043329517308674535\n",
            "Loss: 0.0\n",
            "training error 0.02749982200256957, test error 0.04285550173728741\n",
            "Loss: 0.0\n",
            "training error 0.027425336976419353, test error 0.043018293797700315\n",
            "Loss: 0.3798626869680577\n",
            "training error 0.027377003481025224, test error 0.04322042223524872\n",
            "Loss: 0.8515137687532981\n",
            "training error 0.027334092214760804, test error 0.043228845387681514\n",
            "Loss: 0.8711685437327921\n",
            "training error 0.02733345112346912, test error 0.04340724070416888\n",
            "Loss: 1.2874402224100523\n",
            "training error 0.027269929525874308, test error 0.04330456035778193\n",
            "Loss: 1.0478435726813728\n",
            "training error 0.027236633766477714, test error 0.04330501407624136\n",
            "Loss: 1.048902289627951\n",
            "training error 0.027216725743933082, test error 0.043230256268854385\n",
            "Loss: 0.8744607258696746\n",
            "training error 0.02722895294986562, test error 0.04317090768090135\n",
            "Loss: 0.7359753843215744\n",
            "training error 0.027198029663050325, test error 0.043439299686768774\n",
            "Loss: 1.3622473797183954\n",
            "training error 0.02716689719308039, test error 0.04338212766053464\n",
            "Loss: 1.2288408766639947\n",
            "training error 0.027126460409729478, test error 0.043437957726733256\n",
            "Loss: 1.3591160197269803\n",
            "training error 0.027073235224367536, test error 0.043325525202305336\n",
            "Loss: 1.096763416513613\n",
            "training error 0.027060637414910105, test error 0.04332675711418006\n",
            "Loss: 1.09963798763002\n",
            "training error 0.027075896896181374, test error 0.04323781722964407\n",
            "Loss: 0.8921036433089391\n",
            "training error 0.027047490283844962, test error 0.04352664142615757\n",
            "Loss: 1.5660525758965038\n",
            "training error 0.026995151246340953, test error 0.043507827253757014\n",
            "Loss: 1.5221511591871906\n",
            "training error 0.026979067159614224, test error 0.04323776576579045\n",
            "Loss: 0.8919835563853562\n",
            "training error 0.026953926992951058, test error 0.043496388636488685\n",
            "Loss: 1.4954600301497756\n",
            "training error 0.026911397188169577, test error 0.043507849183153736\n",
            "Loss: 1.5222023297390086\n",
            "training error 0.026876246619915466, test error 0.043456860291343755\n",
            "Loss: 1.403223692824307\n",
            "training error 0.026844636103645233, test error 0.043387984294523826\n",
            "Loss: 1.2425068792815352\n",
            "training error 0.026868019310722815, test error 0.04332120362121471\n",
            "Loss: 1.0866793411546949\n",
            "training error 0.026817104518994647, test error 0.043323562582786614\n",
            "Loss: 1.092183795603452\n",
            "training error 0.026807669358447687, test error 0.04323889439611567\n",
            "Loss: 0.8946171279910153\n",
            "training error 0.02678104121399267, test error 0.0435174155470206\n",
            "Loss: 1.5445247002143336\n",
            "training error 0.026825825975919766, test error 0.043456137318741386\n",
            "Loss: 1.4015366921521277\n",
            "training error 0.02672789180134794, test error 0.04327772379867542\n",
            "Loss: 0.9852225368315981\n",
            "training error 0.0267567575603468, test error 0.04341683812465167\n",
            "Loss: 1.3098350610975418\n",
            "training error 0.026677430952021994, test error 0.04335003518662125\n",
            "Loss: 1.153955569964915\n",
            "training error 0.02665112013107412, test error 0.04348421686763861\n",
            "Loss: 1.4670581485787926\n",
            "training error 0.026624280299200054, test error 0.04344993075323068\n",
            "Loss: 1.3870541513834933\n",
            "training error 0.026603315228017392, test error 0.043337212775179774\n",
            "Loss: 1.1240354642102934\n",
            "training error 0.026629094721048373, test error 0.04321850439175067\n",
            "Loss: 0.8470386292255716\n",
            "training error 0.026544919161466805, test error 0.0434413579655913\n",
            "Loss: 1.3670502142182572\n",
            "training error 0.026516713079826518, test error 0.04330637897987219\n",
            "Loss: 1.052087186725159\n",
            "training error 0.026517898378995392, test error 0.043307595438411124\n",
            "Loss: 1.0549256986772448\n",
            "training error 0.026481596372965273, test error 0.04322797029840701\n",
            "Loss: 0.8691265905668599\n",
            "training error 0.026459205635218083, test error 0.04304033722698299\n",
            "Loss: 0.43129932494703205\n",
            "training error 0.026490683226048382, test error 0.04299346485377321\n",
            "Loss: 0.3219262659238886\n",
            "training error 0.02642076201840693, test error 0.043090771833991674\n",
            "Loss: 0.5489845811315286\n",
            "training error 0.026441980749274268, test error 0.04320674741371956\n",
            "Loss: 0.8196046299617787\n",
            "training error 0.02641650906198657, test error 0.042942638363333833\n",
            "Loss: 0.20332658005171034\n",
            "training error 0.026371277721351807, test error 0.04291840737673661\n",
            "Loss: 0.14678544620672085\n",
            "training error 0.026345408437151184, test error 0.042822137783791916\n",
            "Loss: 0.0\n",
            "training error 0.02637499466484401, test error 0.042922558516074516\n",
            "Loss: 0.23450658346302333\n",
            "training error 0.02630453765849195, test error 0.0428867326521338\n",
            "Loss: 0.15084456705085625\n",
            "training error 0.026287304043320376, test error 0.04311346386563262\n",
            "Loss: 0.6803165299957836\n",
            "training error 0.026253588042322747, test error 0.04307678376613971\n",
            "Loss: 0.5946596679350602\n",
            "training error 0.026233211820801055, test error 0.04314296819209361\n",
            "Loss: 0.7492162346531162\n",
            "training error 0.026233529481147936, test error 0.04311804953380151\n",
            "Loss: 0.6910251690460667\n",
            "training error 0.026206809945959975, test error 0.0431075751647613\n",
            "Loss: 0.6665649959153086\n",
            "training error 0.026200071378792822, test error 0.04316100797753559\n",
            "Loss: 0.7913434762519822\n",
            "training error 0.026192396367899995, test error 0.04297513929538189\n",
            "Loss: 0.35729536055035815\n",
            "training error 0.026210074296507662, test error 0.04287410381666701\n",
            "Loss: 0.12135319618433726\n",
            "training error 0.026105624859427995, test error 0.04306594320595795\n",
            "Loss: 0.5693443503381301\n",
            "training error 0.026112237257525338, test error 0.043080340473431344\n",
            "Loss: 0.6029654356423997\n",
            "training error 0.026107420187280985, test error 0.04317017114614213\n",
            "Loss: 0.8127416807339971\n",
            "training error 0.026050576550636327, test error 0.04304012434933159\n",
            "Loss: 0.5090511049221424\n",
            "training error 0.026059621250196393, test error 0.04297338844967454\n",
            "Loss: 0.35320671435481454\n",
            "training error 0.026023785872731964, test error 0.04314744618770161\n",
            "Loss: 0.7596734323544796\n",
            "training error 0.02599174801207386, test error 0.04316739141658746\n",
            "Loss: 0.806250343078907\n",
            "training error 0.02595946177199158, test error 0.0431233036192559\n",
            "Loss: 0.7032947233614584\n",
            "training error 0.02594528670699402, test error 0.04309807274163581\n",
            "Loss: 0.6443745504651988\n",
            "training error 0.025967281679144145, test error 0.04310973289063436\n",
            "Loss: 0.671603805243226\n",
            "training error 0.02590885194656581, test error 0.04312843601714877\n",
            "Loss: 0.715280107927696\n",
            "training error 0.02599147022422111, test error 0.04294480793747522\n",
            "Loss: 0.28646433838184215\n",
            "training error 0.02585999358221583, test error 0.043165156734802174\n",
            "Loss: 0.8010318231708835\n",
            "training error 0.025868442425913223, test error 0.04326166496192299\n",
            "Loss: 1.0264017652510393\n",
            "training error 0.025879234418773024, test error 0.04282615296870631\n",
            "Loss: 0.009376423322593475\n",
            "training error 0.025911916618471236, test error 0.04298636842790394\n",
            "Loss: 0.3835180881001765\n",
            "training error 0.02581168619976386, test error 0.04287702440196427\n",
            "Loss: 0.12817346590558998\n",
            "training error 0.0258161455992273, test error 0.04281677864704502\n",
            "Loss: 0.0\n",
            "training error 0.025772502788467492, test error 0.042992770773629195\n",
            "Loss: 0.4110354214990064\n",
            "training error 0.02579321096533699, test error 0.042997278471636834\n",
            "Loss: 0.42156329900420175\n",
            "training error 0.02579851664613477, test error 0.043040530413974744\n",
            "Loss: 0.5225796381698622\n",
            "training error 0.02577624462238452, test error 0.04321064259501235\n",
            "Loss: 0.919882252735782\n",
            "training error 0.025693981135555547, test error 0.04309756821237442\n",
            "Loss: 0.6557932992672155\n",
            "training error 0.025722263682831592, test error 0.04296359030660759\n",
            "Loss: 0.3428834774628742\n",
            "training error 0.025674384985264448, test error 0.04310227067651156\n",
            "Loss: 0.6667760594975203\n",
            "training error 0.02567588155317575, test error 0.04324791901235096\n",
            "Loss: 1.0069425560012224\n",
            "training error 0.025694104075092268, test error 0.04303873678312725\n",
            "Loss: 0.5183905541141121\n",
            "training error 0.02562796240473801, test error 0.043193169774095645\n",
            "Loss: 0.8790739026711014\n",
            "training error 0.025624779712427877, test error 0.0430869303990909\n",
            "Loss: 0.6309483351674805\n",
            "training error 0.02562673567008328, test error 0.0429865294086359\n",
            "Loss: 0.3964585075168614\n",
            "training error 0.025658591861237028, test error 0.04291741525439178\n",
            "Loss: 0.23504011867951924\n",
            "training error 0.02555909130089237, test error 0.04310950091998961\n",
            "Loss: 0.6836625318256795\n",
            "training error 0.02560638225423356, test error 0.04302205988592374\n",
            "Loss: 0.47944111015667534\n",
            "training error 0.025525256305464348, test error 0.04309594640772597\n",
            "Loss: 0.6520055209716435\n",
            "training error 0.025512514508580848, test error 0.04295622251788708\n",
            "Loss: 0.3256757636802776\n",
            "training error 0.025481436058120418, test error 0.04285020124908444\n",
            "Loss: 0.07805959041180532\n",
            "training error 0.025504157409415005, test error 0.04295310412348197\n",
            "Loss: 0.3183926506025392\n",
            "training error 0.025516393522113812, test error 0.04285541761659723\n",
            "Loss: 0.0902425889409475\n",
            "training error 0.02554484392639631, test error 0.04306349872354081\n",
            "Loss: 0.5762228833924965\n",
            "training error 0.025451622826843663, test error 0.04312054144862901\n",
            "Loss: 0.7094480509335366\n",
            "training error 0.025424185249725905, test error 0.04302390454342815\n",
            "Loss: 0.48374936865416274\n",
            "training error 0.025389876852654796, test error 0.04321152809300877\n",
            "Loss: 0.9219503625385261\n",
            "training error 0.02541840431162267, test error 0.043000150113380424\n",
            "Loss: 0.42827011309516383\n",
            "training error 0.025389926086554427, test error 0.04331854792784175\n",
            "Loss: 1.1718987197355535\n",
            "training error 0.025339218172315603, test error 0.04307803659715956\n",
            "Loss: 0.6101765671541637\n",
            "training error 0.02532147077919637, test error 0.04312573701039958\n",
            "Loss: 0.7215824569648754\n",
            "training error 0.025382342009167704, test error 0.04311949487297832\n",
            "Loss: 0.7070037389517392\n",
            "training error 0.02529116275767121, test error 0.0431200445223404\n",
            "Loss: 0.7082874631819447\n",
            "training error 0.025312433221118816, test error 0.04293870875926967\n",
            "Loss: 0.284771802264161\n",
            "training error 0.025318056127229642, test error 0.04301692117070208\n",
            "Loss: 0.4674394711169372\n",
            "training error 0.025243905471722786, test error 0.04314233325683653\n",
            "Loss: 0.7603435383945589\n",
            "training error 0.025248743524109944, test error 0.04304578842969851\n",
            "Loss: 0.5348599074706195\n",
            "training error 0.0252173910420239, test error 0.04310965401352704\n",
            "Loss: 0.6840200868362789\n",
            "training error 0.025232123118995926, test error 0.04295878933872941\n",
            "Loss: 0.3316706584935769\n",
            "training error 0.025189338506948687, test error 0.043115545776320284\n",
            "Loss: 0.6977804933391063\n",
            "training error 0.025243678096551158, test error 0.04293716868669274\n",
            "Loss: 0.2811749119197904\n",
            "training error 0.025171188309170957, test error 0.042907914099195636\n",
            "Loss: 0.21284985706626625\n",
            "training error 0.025163171273286357, test error 0.04297498860460689\n",
            "Loss: 0.369504578721469\n",
            "training error 0.02514641859077904, test error 0.04304375603513096\n",
            "Loss: 0.5301131828646\n",
            "training error 0.025113963305848933, test error 0.042948905082994294\n",
            "Loss: 0.30858565292462004\n",
            "training error 0.025112902690798602, test error 0.043071566076132495\n",
            "Loss: 0.5950644516902637\n",
            "training error 0.025120646034408986, test error 0.04316846078017752\n",
            "Loss: 0.8213652316806597\n",
            "training error 0.02507493211703828, test error 0.04286743815064366\n",
            "Loss: 0.11831694302890661\n",
            "training error 0.025175455974521702, test error 0.042980811186911314\n",
            "Loss: 0.3831034119088583\n",
            "training error 0.025110542247187853, test error 0.04244510415612425\n",
            "Loss: 0.0\n",
            "training error 0.025091267065811346, test error 0.04281918747904111\n",
            "Loss: 0.8813344444647431\n",
            "training error 0.025203209439002172, test error 0.04248120415961994\n",
            "Loss: 0.08505104231315563\n",
            "training error 0.025003336982420594, test error 0.04282875474535165\n",
            "Loss: 0.9038747739108732\n",
            "training error 0.025001133217827706, test error 0.04292867324654013\n",
            "Loss: 1.1392811963358174\n",
            "training error 0.025022405462726497, test error 0.042983356060351044\n",
            "Loss: 1.2681130484377245\n",
            "training error 0.02498671711402459, test error 0.04311049319336532\n",
            "Loss: 1.5676461407506226\n",
            "training error 0.024957877174831875, test error 0.043020968774039554\n",
            "Loss: 1.3567280122511294\n",
            "training error 0.024953585554090843, test error 0.04298881942152297\n",
            "Loss: 1.2809846417122461\n",
            "training error 0.02496464358724694, test error 0.0431963189772781\n",
            "Loss: 1.7698503421989198\n",
            "training error 0.02492116618220695, test error 0.04272879359108418\n",
            "Loss: 0.6683678615004629\n",
            "training error 0.024950415651015835, test error 0.0427357942782325\n",
            "Loss: 0.6848613706754492\n",
            "training error 0.024886730367453248, test error 0.04280021919158465\n",
            "Loss: 0.8366454565742032\n",
            "training error 0.02488362516729937, test error 0.042893521920802355\n",
            "Loss: 1.056465223948333\n",
            "training error 0.024867638984792006, test error 0.04294184565444801\n",
            "Loss: 1.170315182869186\n",
            "training error 0.02497689494743859, test error 0.04271495142332885\n",
            "Loss: 0.6357559312660355\n",
            "training error 0.02483982461205392, test error 0.042968208745110086\n",
            "Loss: 1.2324262111873319\n",
            "training error 0.02481817540548423, test error 0.043019938088003314\n",
            "Loss: 1.3542997321073225\n",
            "training error 0.02482331354775969, test error 0.04298194301668206\n",
            "Loss: 1.2647839397052163\n",
            "training error 0.02484208981362856, test error 0.043078600475901944\n",
            "Loss: 1.4925073983739656\n",
            "training error 0.024864518064556022, test error 0.04302294476830967\n",
            "Loss: 1.361383423774809\n",
            "training error 0.02482188271682336, test error 0.04317192351527754\n",
            "Loss: 1.7123750161617046\n",
            "training error 0.024787687055664386, test error 0.04317266345597009\n",
            "Loss: 1.7141183048336561\n",
            "training error 0.024795893619968003, test error 0.04295521192711889\n",
            "Loss: 1.2018059117450353\n",
            "training error 0.024745288821115024, test error 0.04294772852826172\n",
            "Loss: 1.1841751413512513\n",
            "training error 0.02475977028640949, test error 0.04291250969641927\n",
            "Loss: 1.1012001256394122\n",
            "training error 0.024744132420879278, test error 0.04291189943721668\n",
            "Loss: 1.09976236452487\n",
            "training error 0.024694003578644395, test error 0.04280308298724247\n",
            "Loss: 0.8433925142496523\n",
            "training error 0.024714497688896515, test error 0.04270821969976386\n",
            "Loss: 0.6198960960769329\n",
            "training error 0.024671230922431092, test error 0.04291098731611998\n",
            "Loss: 1.0976134215199274\n",
            "training error 0.024659201411124362, test error 0.0428287754163542\n",
            "Loss: 0.9039234744688374\n",
            "training error 0.024732030867546176, test error 0.04317883954202571\n",
            "Loss: 1.7286690667611238\n",
            "training error 0.024662614474357882, test error 0.04313422242513479\n",
            "Loss: 1.6235518388075576\n",
            "training error 0.024702703196373396, test error 0.04291257647831699\n",
            "Loss: 1.1013574627435352\n",
            "training error 0.024610907001689466, test error 0.04302496931264645\n",
            "Loss: 1.3661532184944036\n",
            "training error 0.02462971770551922, test error 0.04286142757411319\n",
            "Loss: 0.9808514462765539\n",
            "training error 0.02457215153738899, test error 0.0428748623717033\n",
            "Loss: 1.0125036187879077\n",
            "training error 0.02463334255084343, test error 0.04315956727546596\n",
            "Loss: 1.6832639088685708\n",
            "training error 0.02463443608253196, test error 0.04312780852922884\n",
            "Loss: 1.6084408006007633\n",
            "training error 0.02453029646869883, test error 0.04302037566569807\n",
            "Loss: 1.355330658296472\n",
            "training error 0.02455313839775525, test error 0.04306138586416387\n",
            "Loss: 1.4519500429844179\n",
            "training error 0.024526766262453724, test error 0.04298789871163378\n",
            "Loss: 1.278815463646854\n",
            "training error 0.024523675856535098, test error 0.04295571487814012\n",
            "Loss: 1.2029908564665348\n",
            "training error 0.024609697769454736, test error 0.04319658675271011\n",
            "Loss: 1.77048121691894\n",
            "training error 0.024511444601651382, test error 0.042849466350219786\n",
            "Loss: 0.9526709902941688\n",
            "training error 0.02453963916607506, test error 0.04318350725613442\n",
            "Loss: 1.7396661280276948\n",
            "training error 0.024562215866828746, test error 0.04283670644780953\n",
            "Loss: 0.9226088602465543\n",
            "training error 0.024432617132642078, test error 0.04310729590951976\n",
            "Loss: 1.5601133901328001\n",
            "training error 0.024478180532467725, test error 0.04317080947176624\n",
            "Loss: 1.7097503471134257\n",
            "training error 0.024432198880791265, test error 0.043268300005535956\n",
            "Loss: 1.939436516361881\n",
            "training error 0.024479245646874207, test error 0.04309524717200768\n",
            "Loss: 1.531726753436713\n",
            "training error 0.024409438941828773, test error 0.0433082371240385\n",
            "Loss: 2.033527741478558\n",
            "training error 0.02443917884749451, test error 0.0431640265971124\n",
            "Loss: 1.693770000760897\n",
            "training error 0.024387280389642857, test error 0.04336465994584772\n",
            "Loss: 2.1664590251471605\n",
            "training error 0.024404661172052873, test error 0.043148946804469034\n",
            "Loss: 1.65824224569191\n",
            "training error 0.024413859859359045, test error 0.04308663126935189\n",
            "Loss: 1.5114278218471\n",
            "training error 0.02433225866358548, test error 0.04332410755758128\n",
            "Loss: 2.0709182341120513\n",
            "training error 0.02432629355947614, test error 0.043319444297616104\n",
            "Loss: 2.059931666737813\n",
            "training error 0.024421223551264925, test error 0.043259609841587\n",
            "Loss: 1.9189626263297344\n",
            "training error 0.024350591820463178, test error 0.04308368589711656\n",
            "Loss: 1.5044885710338685\n",
            "training error 0.02430129935583225, test error 0.04302556666361362\n",
            "Loss: 1.3675605680086855\n",
            "training error 0.0243242900923237, test error 0.04282894188394204\n",
            "Loss: 0.9043156694961363\n",
            "training error 0.024314276855813322, test error 0.04287711182026342\n",
            "Loss: 1.0178032843320084\n",
            "training error 0.02430273171017614, test error 0.0432510297088633\n",
            "Loss: 1.8987479681393582\n",
            "training error 0.024313342705725383, test error 0.0427433686664329\n",
            "Loss: 0.702706510535478\n",
            "training error 0.024259557361228482, test error 0.042834333409864166\n",
            "Loss: 0.9170180200482658\n",
            "training error 0.024237136181327695, test error 0.042942036154211445\n",
            "Loss: 1.1707639973254524\n",
            "training error 0.02427560623057955, test error 0.043056894504696824\n",
            "Loss: 1.4413684704889684\n",
            "training error 0.02423672518369344, test error 0.04299899852545654\n",
            "Loss: 1.3049664510067505\n",
            "training error 0.02419680105701374, test error 0.04321629352664766\n",
            "Loss: 1.816910067382027\n",
            "training error 0.02433723468381526, test error 0.043336637298068414\n",
            "Loss: 2.100438106276936\n",
            "training error 0.02422815264007701, test error 0.04289058353286621\n",
            "Loss: 1.0495424280345\n",
            "training error 0.02429420391626049, test error 0.042853622842981554\n",
            "Loss: 0.9624636220815219\n",
            "training error 0.024246459811932203, test error 0.04279573648579877\n",
            "Loss: 0.8260842720159145\n",
            "training error 0.02416886684366811, test error 0.042952674463578124\n",
            "Loss: 1.1958276874216134\n",
            "training error 0.024224883195685137, test error 0.04282946072862717\n",
            "Loss: 0.9055380594406248\n",
            "training error 0.02420039814425678, test error 0.043194057379515545\n",
            "Loss: 1.7645220533243355\n",
            "training error 0.024252287670270968, test error 0.04287349546349131\n",
            "Loss: 1.009283204468825\n",
            "training error 0.024116595565799568, test error 0.04322872713924546\n",
            "Loss: 1.8462034637465763\n",
            "training error 0.024176280399131393, test error 0.04321764459605438\n",
            "Loss: 1.8200931657241792\n",
            "training error 0.024223019772406186, test error 0.04290818457152135\n",
            "Loss: 1.0910101991828514\n",
            "training error 0.02425377200137023, test error 0.04332426745611072\n",
            "Loss: 2.0712949525407476\n",
            "training error 0.02406571058044774, test error 0.04329086870496024\n",
            "Loss: 1.9926080184066608\n",
            "training error 0.024098261521661495, test error 0.04338125008715576\n",
            "Loss: 2.2055451379931146\n",
            "training error 0.02402816539785708, test error 0.04339509427580175\n",
            "Loss: 2.2381618294142758\n",
            "training error 0.02407181783869708, test error 0.043522969795251894\n",
            "Loss: 2.5394345485947545\n",
            "training error 0.024019076545905214, test error 0.04365485116633986\n",
            "Loss: 2.850144991436099\n",
            "training error 0.0240188131706446, test error 0.0436225339560865\n",
            "Loss: 2.774006150700803\n",
            "training error 0.02404225859201471, test error 0.04366669992135583\n",
            "Loss: 2.878060472506383\n",
            "training error 0.0239990103376474, test error 0.04348221526536623\n",
            "Loss: 2.4434175150736204\n",
            "training error 0.023977857835974687, test error 0.043607998198767844\n",
            "Loss: 2.7397601343282396\n",
            "training error 0.023981795858670607, test error 0.04346320449080716\n",
            "Loss: 2.398628428235372\n",
            "training error 0.02398922212141603, test error 0.043451943925862144\n",
            "Loss: 2.372098713751458\n",
            "training error 0.02394708539969369, test error 0.04357663569558474\n",
            "Loss: 2.6658705684839745\n",
            "training error 0.023973879147864516, test error 0.04339577051197376\n",
            "Loss: 2.2397550312344805\n",
            "training error 0.023973610356675872, test error 0.04337164826480829\n",
            "Loss: 2.1829233950656945\n",
            "training error 0.023962543424636898, test error 0.043579868152311\n",
            "Loss: 2.6734861858573655\n",
            "training error 0.024098390809134708, test error 0.04347753460635554\n",
            "Loss: 2.4323899558209128\n",
            "training error 0.023947309611065237, test error 0.04306680573109739\n",
            "Loss: 1.4647191645150937\n",
            "training error 0.02392606015953297, test error 0.04323403117209501\n",
            "Loss: 1.8586996819913049\n",
            "training error 0.02391294052913483, test error 0.04321170711588881\n",
            "Loss: 1.8061045555331612\n",
            "training error 0.023888153979882133, test error 0.04330380901922515\n",
            "Loss: 2.0230951959556043\n",
            "training error 0.0238600579245718, test error 0.04330178834345119\n",
            "Loss: 2.0183345155093235\n",
            "training error 0.02386595992877495, test error 0.04302480699352591\n",
            "Loss: 1.365770797191046\n",
            "training error 0.023877785077117342, test error 0.043149484225696436\n",
            "Loss: 1.6595084016787665\n",
            "training error 0.023861472936893245, test error 0.04314868265051883\n",
            "Loss: 1.6576199031262284\n",
            "training error 0.02384498948375929, test error 0.042979866388654205\n",
            "Loss: 1.259891436625904\n",
            "training error 0.02383789909865609, test error 0.04314073161287779\n",
            "Loss: 1.6388873830886164\n",
            "training error 0.02382918095674129, test error 0.04337636870885209\n",
            "Loss: 2.1940446872327213\n",
            "training error 0.023842425004467812, test error 0.043419450406706486\n",
            "Loss: 2.295544492006285\n",
            "training error 0.023776306910437937, test error 0.043145786108063515\n",
            "Loss: 1.650795694509255\n",
            "training error 0.023790923715400362, test error 0.043285136858192665\n",
            "Loss: 1.9791038772777103\n",
            "training error 0.023780535230238584, test error 0.043416583779601226\n",
            "Loss: 2.288790763485027\n",
            "training error 0.02385168832646867, test error 0.04307021113025891\n",
            "Loss: 1.4727422315548022\n",
            "training error 0.023762376189057386, test error 0.04308976550071763\n",
            "Loss: 1.5188120218109225\n",
            "training error 0.02374041303797533, test error 0.04308472046174072\n",
            "Loss: 1.506925989070007\n",
            "training error 0.02387024397229028, test error 0.043268598742799154\n",
            "Loss: 1.9401403366708037\n",
            "training error 0.02383969519641274, test error 0.0428940989735493\n",
            "Loss: 1.0578247511739747\n",
            "training error 0.023785871393831905, test error 0.04320320632511559\n",
            "Loss: 1.7860768257343507\n",
            "training error 0.023796878423199382, test error 0.043587886335671504\n",
            "Loss: 2.692376900157445\n",
            "training error 0.02367621366687671, test error 0.043450460745809286\n",
            "Loss: 2.3686043647980437\n",
            "training error 0.023701826957702753, test error 0.04345883172909517\n",
            "Loss: 2.388326270191632\n",
            "training error 0.023712496265133397, test error 0.04332492188280716\n",
            "Loss: 2.0728367715784346\n",
            "training error 0.023677104334729, test error 0.0433662355171917\n",
            "Loss: 2.1701710465340973\n",
            "training error 0.023695846247917405, test error 0.043626938642701396\n",
            "Loss: 2.7843835233153147\n",
            "training error 0.023647049806186014, test error 0.04349950797260051\n",
            "Loss: 2.4841588622279698\n",
            "training error 0.023676031515721615, test error 0.04332515594520563\n",
            "Loss: 2.073388218919958\n",
            "training error 0.023658579765236387, test error 0.04313454154245124\n",
            "Loss: 1.6243036742024763\n",
            "training error 0.02378815987526888, test error 0.04288547466195484\n",
            "Loss: 1.0375060082566723\n",
            "training error 0.023633051900768916, test error 0.043175524632532276\n",
            "Loss: 1.7208591919608551\n",
            "training error 0.02363745674968707, test error 0.04334323075057136\n",
            "Loss: 2.115972177011427\n",
            "training error 0.023604717716308916, test error 0.043320289409081884\n",
            "Loss: 2.0619227361027948\n",
            "training error 0.023606471927488767, test error 0.04337592440603438\n",
            "Loss: 2.1929979167593316\n",
            "training error 0.023582511977118758, test error 0.04330689223978352\n",
            "Loss: 2.030359215256916\n",
            "training error 0.023591410097413838, test error 0.043348959567582473\n",
            "Loss: 2.1294691800816556\n",
            "training error 0.023553108881373113, test error 0.0434799203602646\n",
            "Loss: 2.4380107546303265\n",
            "training error 0.02358660547208442, test error 0.04333061736216472\n",
            "Loss: 2.086255231659506\n",
            "training error 0.023598248393551208, test error 0.04331114396693175\n",
            "Loss: 2.040376217765849\n",
            "training error 0.023529945499014916, test error 0.04310318208722056\n",
            "Loss: 1.55042128928633\n",
            "training error 0.02355245215253044, test error 0.043113941257542754\n",
            "Loss: 1.57576972589899\n",
            "training error 0.023575666084658376, test error 0.043190397958302595\n",
            "Loss: 1.7559005143136464\n",
            "training error 0.023609091516501896, test error 0.04321008827124456\n",
            "Loss: 1.802290582929178\n",
            "training error 0.02352994661630984, test error 0.0433432431532836\n",
            "Loss: 2.116001397607037\n",
            "training error 0.023513141212177416, test error 0.04309557146400661\n",
            "Loss: 1.5324907803024201\n",
            "training error 0.023563730360252503, test error 0.04262968221456423\n",
            "Loss: 0.4348630121416397\n",
            "training error 0.02350629794756506, test error 0.04282369991605078\n",
            "Loss: 0.8919656753201721\n",
            "training error 0.023562873425470074, test error 0.04311541433334701\n",
            "Loss: 1.5792402694011054\n",
            "training error 0.02350120237111172, test error 0.0430953475578713\n",
            "Loss: 1.5319632609576805\n",
            "training error 0.02347828347597449, test error 0.043211117101957484\n",
            "Loss: 1.8047144919603353\n",
            "training error 0.023480227457240134, test error 0.0432955899930771\n",
            "Loss: 2.003731299196576\n",
            "training error 0.02347779141460936, test error 0.04326115285299565\n",
            "Loss: 1.9225979370194546\n",
            "training error 0.023476282019983506, test error 0.04357435645188611\n",
            "Loss: 2.6605007060606534\n",
            "training error 0.023454233164729454, test error 0.043407641047756\n",
            "Loss: 2.2677218274486544\n",
            "training error 0.023445006982954583, test error 0.043625071993319484\n",
            "Loss: 2.7799857266341155\n",
            "training error 0.02342115167185773, test error 0.043395109511914026\n",
            "Loss: 2.238197725455948\n",
            "training error 0.02339535659128115, test error 0.04346759451017089\n",
            "Loss: 2.4089712450360734\n",
            "training error 0.02341625582768531, test error 0.04340179859791168\n",
            "Loss: 2.2539571072047737\n",
            "training error 0.02338610648512163, test error 0.043763011784024926\n",
            "Loss: 3.104969711119243\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hU1b3/8fd3ZnIhEFEuIho0qNQLhYCkaLxGU8/BWrlIeyrForaKelot+msVqPV4emy1Hp/W4qn1YA9VlIKXitqKWgVGrE5V8EIRRSnGBioYuQQQIcnM+v2xd+IQcyUzmdnk83qeebIva/b+zkyS76y19l7LnHOIiIiEMh2AiIhkByUEEREBlBBERMSnhCAiIoASgoiI+JQQREQEUEKQLGZmp5nZmkzHIdJdKCFIs8ys0sy+nMkYnHMvOOeOyWQM2cg868xsdaZjkf2LEoJkjJmFMx1DZ2XoNZwOHAwcaWZf6soTm1mkK88nXUsJQTrEzEJmNt3M/m5mm83sITPrk7T/YTPbaGY1ZrbMzIYm7bvXzH5jZovM7BPgTL8m8gMzW+k/50Ezy/fLl5vZ+qTnt1jW33+dmX1oZv80s0vNzJnZ0S28jj5m9ju/7FYze8zffrGZ/aVJ2cbjNPMafuC/3nBS+QlmtrI979c+ugh4HFjkLyfHOtTMnjWzLWa2ycxm+tvDZjbTj2OHma0ws0FmVuy/vkjSMaJmdmnS+/Gimf3SzDYDN5nZUWa2xH89H5vZPDM7MOn5g8zsUTOr9sv8j5nl+jENSyp3sJntMrP+nXw/JEWUEKSjrgLGA2cAhwJbgV8n7X8KGIL3DfY1YF6T538T+ClQCDT84/03YAwwGBgOXNzK+Zsta2ZjgGuBLwNHA+VtvI77gQJgqB/rL9so39Jr+BXwCXBWk/2/95fber86xMwKgK/hva/zgAvMLNffVwg8Bzztn+toYLH/1GuBScBXgAOAbwO72nnaE4F1wAC8123ALf45jgMGATf5MYSBPwEfAMXAYcAC51wtsAC4MOm4k4DFzrnq9r8DklbOOT30+NwDqAS+3Mz2t4GKpPWBQB0QaabsgYADevvr9wJzmznPhUnrtwF3+8vlwPp2lp0D3JK072j/3Ec3E9dAIAEc1My+i4G/NNnWeJwWXsPNwBx/uRAvQRzR0fernZ/LhUA1EAHygRpggr9vEvB6C89bA4xrZnux//oiSduiwKVJ78c/2ohpfMN5gbKG+JopdyLwD8D89eXAv2X6d12Pzx6qIUhHHQEsNLNtZrYN7x9eHBjgN0vc6jdLbMf7Bw7QL+n5Vc0cc2PS8i6gVyvnb6nsoU2O3dx5GgwCtjjntrZSpjVNj/174HwzywPOB15zzn3g72vx/Wp6UDN7ysx2+o/JLZz7IuAh51y9c2438Ac+azYaBPy9hee1tq8te71eMxtgZgvMbIP/OT/AZ5/xIOAD51x904M4517G+8zKzexYvKT9xD7GJGmgDiLpqCrg2865F5vuMLNvAePwmm0qgd54TSSWVCxdw+t+CBQlrQ9qpWwV0MfMDnTObWuy7xO8piQAzOyQZp6/12twzq02sw+Ac9i7uajhXM2+X587qHPntLbfzIrwmqZGm9lEf3MBkG9m/fxzXdDC06uAo4BVTbZ/knSc7f5y09fc9DP7mb9tmHNui5mNB/4n6TyHm1mkuaQA3IdXy9kIPOInNckSqiFIa3LMLD/pEQHuBn5qZkcAmFl/Mxvnly8E9gCb8f7B/KwLY30IuMTMjvPb2X/cUkHn3Id4fR13mdlBZpZjZqf7u98EhprZCL/D+qZ2nv/3wPfxrgB6OGl7a+9XR30LeBc4BhjhP74ArMdrLvoTMNDMpplZnpkVmtmJ/nN/C/yXmQ0xz3Az6+u89vsNwIV+De/beImjNYXATqDGzA4Dfpi07xW85HyrmfX0f29OSdr/ADABLynM3cf3QdJECUFaswj4NOlxE14n6hPAn81sB/BXvLZh8P7AP8D7B7Pa39clnHNPAbOApcDapHPvaeEp38Jry38H+AiY5h/nXeAneJ2z7/FZx3db5uN1HC9xzn2ctL2196ujLgLucs5tTH7gJZ2LnHM7gLOB8/C+gb8HnOk/9xd4SfPPeDWB/wN6+Psuw/unvhmvk/2lNuL4T+AEvP6LJ4FHG3Y45+L++Y/G6y9YD3wjaX8V3sUGDnih42+BpFND547IfsXMjsNrHslroelCMsTM5gD/dM7dkOlYZG9KCLLfMLMJeLWaAry26oRzbnxmo5JkZlYMvAGMdM69n9lopCk1Gcn+5HK85p+/413Jc2Vmw5FkZvZfeLW2/1YyyE6qIYiICKAagoiI+AJ3H0K/fv1ccXFxpsMQEQmUFStWfOyca3XcqMAlhOLiYpYvX57pMEREAsW/ebJVajISERFACUFERHxKCCIiAighiIiITwlBREQAJQQREfEF7rJT2f/NXjGbO/56B5s+2URdvK7ZMvFEHIcjEmr5V1hlHD1yenBA3gGMOGQE1518HWWDylp8jogSgmTU9c9dz5zX57Cn3hulujZey554SyNWS0d9Wv8pWz7dQuW2Sh575zEKIgWEQ2Gg5cTSt6AvM06dwdRRUzMRsmRQ4MYyKi0tdboxLfhiVTEuXHgh67auy3Qo0oK8cB5hC7erNpIbyWXqqKn8/Ms/78IIpSPMbIVzrrTVMkoI0tVmr5jN5X+6PNNhSBpEQhF6RHp8bnt7m7lCoRBH9zmakw47iSklU9TElUJKCJJ1YlUxTplzCq6dUyv3iPRo9p9ItrbZZ1OZeldPbby2xTJBkB/OJyeUQ32inrxIHlNLpzL+mPHMfdObfVNJo/2UECTr3PLCLcxcMvNz2/PCeeSGcwHvW+awAcO4teJW/bF3Uqwqxtw35/LX9X+lclslcRdv3NdcYtldv5u6RPMd+dkqL5xHxCLU+xPjRSyCw2HY535ma5mcUA4Jl8CwxtdlZuRGcjmk1yF8/8Tvd7pPRwlBsk5zzUWTh03mgfMfyFBE0lRyR397aiNBTCJBVJhbSMWRFft8tZgSgmSd0+acxl+qPpu3fvwx41l4wcIMRiSpMHvFbH72ws/Y8umWZve3J7HoCrP2yQnl8PzFz3c4KbQnIeiyU+ky/3r/v+6VDCKhCNedcl0GI5JUmTpqakouU41Vxbjtxdt4fePr7Kjd0Xgfyn5fC2n4Xm5N1pO3+eoSdUQro2lpTlVCkC5x58t38ud1f95r24CeA9RHIHspG1TWYo2xoRayfc92+hX0ozZeu1fSyMaO/bbK7KmLU7vHQbyZMqE45O7aOzkAJHLou7O8xWN2hhKCdInfvvbbz22bPHxyBiKRoEpVLSSb3HILzPyPVgoUxaBiOgxYCfX5sP4keOk6Nvcog3NSH48SgnSJo/ocxcqPVjaujz50tG5iylLXXw9z5sCePRCPg3MQaeU/RVeUyZY4Ul2mtq2rgteXwX3P77UpJwfKy9t43j5SQpC0i1XFeG/Le43rhjH+2PEZjEhact118N//nekopDmFhVBR4X1GZWlqaVVCkLSKVcWomFvBp/WfAhCyEHnhPMqLyzMbmDRr/vxMRyAtKSuDhWm+IE8JQdIqWhlld/3uxvWzis/i6/1/wvQLy1i50qtWN5Vt1fruVOaTT1ouL5k1cWL6z6GEIGnVtCZwQv7X+ffzyppNBJJd8vIgHM6OJJbpRJnOMqEQDBzo9Sfs2OGVLy6GDRugoAAOOgi+8x2Y2gX96UoIknbJ4xbd/vb3SAwc5nWWSVY74wx45plMRyFdSTOmSVpFK6N7rSdcHZTMzUww0iFd0UQh2UU1BEmrvjvLP38XZpKCAq9ZIlk2Vuu7U5m+fWHGjK5popDsooQgafXx6ydBIgTmvMSQyIE3pzTuv+EG75+PiGSeEoKk1Uc126EgAf84CTaN8JKB338QiaTvBhsR6Tj1IUjazJ4Nv/rjs95K0csw4j4AevSA00+HZcvSd4ONiHScEoKkzR/+AIz8nbcSchCqhcFRfvxjeP55JQORbKOEIGlTcm4Mjn7a6ztwgIuQs6FczUQiWUoJQdJi2TKYu3IuWKLx6qJDdpzD8w+UqWYgkqWUECTlYjE480zYtDG+16Wm1ZWHZC4oEWlTWhOCmY0xszVmttbMpjez/wgzW2xmK80samZF6YxHukY0CokEsGaCtyFhEM/FvT6FaDSDgYlIq9KWEMwsDPwabxqH44FJZnZ8k2K3A3Odc8OBnwC3pCse6TqNfQRbhng/V38Nmxslr7pM/QciWSydNYTRwFrn3DrnXC2wABjXpMzxwBJ/eWkz+yXI8rcC8EU3hZ9eUcbixbqySCSbpTMhHAZUJa2v97clexM431+eABSaWd+mBzKzqWa23MyWV1dXpyVYSZ2lS/2FopcAKDxsPTNmKBmIZLtMdyr/ADjDzF4HzgA2AJ8bGNk5N9s5V+qcK+3fv39XxygdVFaGNxfsv1wHwCsHXUOsKpbZoESkTelMCBuAQUnrRf62Rs65fzrnznfOjQR+5G/blsaYpAt84QtAcRTCdQDEXS1zl0UzGZKItEM6E8KrwBAzG2xmucAFwBPJBcysn5k1xDADmJPGeKSLbNsG7OoL+APaWYKN73+uJVBEskzaEoJzrh74HvAM8DbwkHPuLTP7iZmN9YuVA2vM7F1gAPDTdMUjXecvfwEOXeGtNNyHMPD1TIUjIu2U1tFOnXOLgEVNtt2YtPwI8Eg6Y5CuFYvB1VcDk975LBkYHDIgk1GJSHtkulNZ9jPRKNQdEoPDX2zclhPKYUrJlJafJCJZQQlBUqq8HGxwFEIJAAzjOyO/Q9kgXXMqku2UECSlysrg3KHljeu54VzVDkQCQglBUq5nT2iYSNk1TqgsItlOCUFS7o2t0cbleCJOtDLaYlkRyR5KCJJSsRisefZU7wqjhBGxXMqLyzMdloi0gxKCpFQ0Cm7TF72Vv5/DJaHF6lAWCQglBEmp8nKw/B0A5KydwJSzlAxEgkIJQVKqrAwOP+0FAC763oca4VQkQJQQJKViVTH+MeI7AMytvFmjnIoEiBKCpFS0MooLeaOc1ifqdYWRSIAoIUhKlReXYwlviKycUI6uMBIJECUESamyQWUcvPZ6AO6fcL+uMBIJECUESbnwLm9o0zOKz8hwJCLSEUoIknJ1Ie+y08LcwgxHIiIdoYQgKVcf3oG5EPmR/EyHIiIdoIQgKRcP7yCSKMTM2i4sIllDCUFSbk/PvwNO9yCIBIwSgqRUrCrGnqJnqAttp2JuhZKCSIAoIUhKRSujYHEwqI3X6sY0kQBRQpCUKi8uBxcC582WphvTRIJDCUFSqmxQGVY9nAMSg1k8RUNfiwSJEoKkVCwGri6PnruHKBmIBIwSgqRMLAZnnQWEa9m4IZeY+pNFAkUJQVImGoXdu4FwLa4+l2g0wwGJSIcoIUjKlJf7C+FaLJH72bqIBIISgqRMWRn07g2Eaznk4FzNliYSMEoIklIFBUC4ltxQXqZDEZEOSmtCMLMxZrbGzNaa2fRm9h9uZkvN7HUzW2lmX0lnPJJ+kQgQrmXrx+pUFgmatCUEMwsDvwbOAY4HJpnZ8U2K3QA85JwbCVwA3JWueKRr1NcD4Vq2b82logIlBZEASWcNYTSw1jm3zjlXCywAxjUp44AD/OXewD/TGI90gfx8IFwL8Vxqa9GVRiIBEknjsQ8DqpLW1wMnNilzE/BnM7sK6Al8OY3xSJrFYlBb5yCyB0vkkpuLrjQSCZBMdypPAu51zhUBXwHuN7PPxWRmU81suZktr66u7vIgpW2xGFRUwIZ/1gNQOjKXxYvRlUYiAZLOhLABGJS0XuRvS/Yd4CEA51wMyAf6NT2Qc262c67UOVfav3//NIUrnRGNwp49eM1FwLYDXoQidSCIBEk6E8KrwBAzG2xmuXidxk80KfMPoALAzI7DSwiqAgRQeTmEw8CgvwDwnntG8yGIBEzaEoJzrh74HvAM8Dbe1URvmdlPzGysX+z/AZeZ2ZvAfOBi55xLV0ySPmVlMHUqMHipv8Wxp17zIYgESTo7lXHOLQIWNdl2Y9LyauCUdMYgXWfHDiBU560kQoRDmg9BJEjSmhCkeyk8LgZH3OmtuBDXHHeHhsAWCZBMX2Uk+5Gag6Jg3lVGobDjwIGbMxuQiHSIEoKkzPEF5eDCAORF1FwkEjRKCJIyQ3qUwerzyQ3lafpMkQBSQpCUiceB3X0ozOmtZCASQEoIkjKJBBDZTV44P9OhiMg+UEKQlEkkgJxPyYv0yHQoIrIPlBAkZeJxILKbfNUQRAJJCUFSRk1GIsHWZkIws/OaG4FUpKlEAui5ic27P9IYRiIB1J5/9N8A3jOz28zs2HQHJMH17qcxGLCSqp3va2A7kQBqMyE45y4ERgJ/B+41s5g/P0Fh2qOTQFmzOwqWAKA2roHtRIKmXU1BzrntwCN402AOBCYAr/kznYkAcHROOWCAkRvWncoiQdOePoSxZrYQiAI5wGjn3DlACd7w1SIAHBEqg0/6M7z/CbpTWSSA2jPa6UTgl865ZckbnXO7zOw76QlLgiiRAMwx6pDRSgYiAdSehHAT8GHDipn1AAY45yqdc4vTFZgEj3fZ6af0iOiyU5Egak8fwsNAImk97m8T2UvjjWlKCCKB1J6EEHHO1Tas+Mu56QtJgqouXg/hegpyNXSFSBC1JyFUJ82BjJmNAz5OX0gSVHWJPQBqMhIJqPb0IVwBzDOz/8G7prAKmJLWqCSQdsc/BaBHjhKCSBC1mRCcc38HTjKzXv76zrRHJYFUm9gNoCYjkYBqTw0BMzsXGArkmxkAzrmfpDEuCaCGhKAagkgwtefGtLvxxjO6Cq/J6OvAEWmOSwJoT0JNRiJB1p5O5ZOdc1OArc65/wTKgC+kNywJog2JVwD4oKYys4GIyD5pT0LY7f/cZWaHAnV44xmJNIpVxXgq/F0Ablhyg0Y6FQmg9iSEP5rZgcB/A68BlcDv0xmUBE+0MkqcOgDqEnUa6VQkgFrtVPYnxlnsnNsG/MHM/gTkO+dquiQ6CYzy4nLC5BBnDzmhHI10KhJArdYQnHMJ4NdJ63uUDKQ5ZYPKOGXPzQDcde5dGtxOJIDa02S02MwmWsP1piItOCB+NAAjDxmZ4UhEZF+0JyFcjjeY3R4z225mO8xse3sObmZjzGyNma01s+nN7P+lmb3hP941s20djF+ySCIRByASatftLSKSZdpzp/I+TZVpZmG85qazgfXAq2b2hHNuddKxr0kqfxXeVJ0SUHFXDyghiARVm3+5ZnZ6c9ubTpjTjNHAWufcOv84C4BxwOoWyk8C/qOteCR71SshiARae/5yf5i0nI/3j34FcFYbzzsMbyC8BuuBE5sraGZHAIOBJS3snwpMBTj88MPbEbJkQkMNIRwKZzgSEdkX7WkyOi953cwGAXekOI4LgEecc/EWYpgNzAYoLS11KT63pEjCqQ9BJMja06nc1HrguHaU2wAMSlov8rc15wJg/j7EIlmkPqEmI5Ega08fwp1Aw7fyEDAC747ltrwKDDGzwXiJ4ALgm80c/1jgIEBjHQRcAr/JyNRkJBJE7fkqtzxpuR6Y75x7sa0nOefqzex7wDNAGJjjnHvLzH4CLHfOPeEXvQBY4JxTU1DA6SojkWBrz1/uI8DuhvZ9MwubWYFzbldbT3TOLQIWNdl2Y5P1m9ofrmSz7Tvi0A9eWx7h7GavTRORbNauO5WB5CmwegDPpSccCapYDN79u1dDGHtehJgaAEUCpz0JIT952kx/uSB9IUkQRaPg/D6E2t1hotGMhiMi+6A9CeETMzuhYcXMRgGfpi8kCaLycrCIlxByIxHKyzMajojsg/b0IUwDHjazf+JNoXkI3pSaIo3KyqDo8DhVwOJnw5RpsFORwGnPjWmv+peGHuNvWuOcq0tvWBJEuXn1kAhz8skaGFckiNpsMjKz7wI9nXOrnHOrgF5m9u/pD02CJu7qMXQPgkhQtacP4TJ/xjQAnHNbgcvSF5IEVcLFMad7EESCqj0JIZw8OY4/rHVu+kKSoIq7eiUEkQBrz1/v08CDZva//vrlwFPpC0mCKk49ISUEkcBqz1/v9XhDT1/hr6/Eu9JIZC/qQxAJtjabjJxzCeBloBJvLoSzgLfTG5YEkfoQRIKtxb9eM/sC3ixmk4CPgQcBnHNndk1oEjQJ6gm1q9IpItmotRrCO3i1ga865051zt0JNDuBjQhAbd6H1Id2EKvSQEYiQdRaQjgf+BBYamb3mFkF3p3KIp8Tq4rxyYBnqQtvo2JuhZKCSAC1mBCcc4855y4AjgWW4g1hcbCZ/cbM/qWrApRgmLssChYHg9p4LdHKaKZDEpEOak+n8ifOud/7cysXAa/jXXkkAnhDX8/5j3JwIXAQsVzKi8szHZaIdFCH5lR2zm11zs12zlWkKyAJnmgU6taVwT9LYXsRl4QWUzZIo9uJBE2HEoJIcxqHuq4rgG2DmXKWkoFIECkhSKeVlcFRRwGRPfTKz9PQ1yIBpYQgKdGnDxDeQ8TyMh2KiOwjJQRJifx8ILIHiyshiASVEoKkRF4eEN6Dq1dCEAkqJQRJCS8h1EJcI6OLBJUSgqREbi4Q2UOiTjUEkaBSQpCUiESA8B5278wjplErRAJJCUFSYudOILKH2l15VFSgpCASQEoIkhJbtwLhPRDPo7bWu3tZRIJFCUFSIie/HkIJKPor4eLYZ3cvi0hgpDUhmNkYM1tjZmvNbHoLZf7NzFab2Vtm9vt0xiPp8fvfw7L3l3krxUtw36qAIrUZiQRN2hKCmYWBXwPnAMcDk8zs+CZlhgAzgFOcc0PxhtiWgLnvPuDI57yVkKMeDX8tEkTprCGMBtY659Y552qBBcC4JmUuA37tnNsK4Jz7KI3xSJoceijw0TBvJREiN6Thr0WCKJ0J4TCgKml9vb8t2ReAL5jZi2b2VzMb09yBzGyqmS03s+XV1dVpClf21UEHATVHAPCVoiksvVjDX4sEUaY7lSPAEKAcmATcY2YHNi3kz8FQ6pwr7d+/fxeHKG355BMgfxsAN55zhZKBSEClMyFsAAYlrRf525KtB55wztU5594H3sVLEBIglZXAoa8A8P629zMai4jsu3QmhFeBIWY22MxygQuAJ5qUeQyvdoCZ9cNrQlqXxpgkxWIxeO6dGJz+MwAuWngJsSpdYSQSRGlLCM65euB7wDPA28BDzrm3zOwnZjbWL/YMsNnMVgNLgR865zanKyZJvWgUEodHweoBqEvU6QojkYCKpPPgzrlFwKIm225MWnbAtf5DAqi8HOzucpyLAHW6wkgkwDLdqSwBV1YGpw8ugzemALD4omfVqSwSUEoI0mkHHwx8MgDiEU45/JRMhyMi+0gJQTrNObzJcRI5mQ5FRDpBCUE6rb4ezZYmsh9QQpBOi8dRQhDZDyghSKd5CaFOCUEk4JQQpNNUQxDZPyghSKd91oegTmWRIFNCkE5LriFoLmWR4FJCkE7buhUIeX0IFRUoKYgElBKCdNrWrTTWEGprvfGNRCR4lBCk03r2BCLejWm5ud74RiISPEoI0mkFBVDYu5ajinNZvNgb30hEgkcJQTqtvh5c/hbCB1VBkToQRIJKCUE6bUfvGDvz1/DulnepmFuhCXJEAkoJQTptR78o4ACojddqghyRgFJCkE7L31gOgGHkhjVBjkhQKSFIp0U2ngSEOOOIM1g8ZbEmyBEJKCUE6bR62wmW4NwvnKtkIBJgSgjSaXWRrQC8VPWSOpRFAkwJQTpt18HPA/D4O4/rKiORAFNCkE779OgHAEiQ0FVGIgGmhCCdEquKUTdoccNVp0RCEV1lJBJQSgjSKXOXRcHiYN5lp5eMuEQdyyIBpYQg+ywWgzk3loMLgYPcUD5TSqZkOiwR2UeRTAcgwRWNQu26Mvj4WAjXcsmAuaodZKG6ujrWr1/P7t27Mx2KdIH8/HyKiorIyen4DIbdKiHMng133AGbNkFdXfNl4nFwDiKtvDPZUibTccTj/kKoHj4cxcgvKhlko/Xr11NYWEhxcTFmlulwJI2cc2zevJn169czePDgDj+/2ySE2bPh8sszHcV+qqAaDnyff78lxrBhZRr+Osvs3r1byaCbMDP69u1LdXX1Pj0/rX0IZjbGzNaY2Vozm97M/ovNrNrM3vAfl6Yrlj/8IV1H7uaKXoIeW+GwV4lPrmDuEt2DkI2UDLqPznzWaUsIZhYGfg2cAxwPTDKz45sp+qBzboT/+G264pk4MV1H7uaOXAwGmINQLRRHMx2RiOyjdDYZjQbWOufWAZjZAmAcsDqN52zR1KkwaxasXetN+ag+hNSUcdtHsxPAhcjLyWXK6eUtH0S6pc2bN1NRUQHAxo0bCYfD9O/fH4BXXnmF3NzcFp+7fPly5s6dy6xZs1o9x8knn8xLL72UspinTZvGww8/TFVVFaFQ97kYM50J4TCgKml9PXBiM+UmmtnpwLvANc65qmbKpMShh8IBB0AKf2+6vfXbhzLolzDhuHH88OQf6iqj/UQs5l1FVl7e+SlR+/btyxtvvAHATTfdRK9evfjBD37QuL++vp5IC99GSktLKS0tbfMcqUwGiUSChQsXMmjQIJ5//nnOPPPMlB07WWuvO1Mynfr+CBQ754YDzwL3NVfIzKaa2XIzW76vnSXgTfWYZe9/4O2q2wXA147/mpJBAEyb5v2Tb+0xciSceirMnOn9HDmy9fLTpnU8josvvpgrrriCE088keuuu45XXnmFsrIyRo4cycknn8yaNWsAiEajfPWrXwW8ZPLtb3+b8vJyjjzyyL1qDb169WosX15ezte+9jWOPfZYJk+ejHPebfSLFi3i2GOPZdSoUVx99dWNx20qGo0ydOhQrrzySubPn9+4fdOmTUyYMIGSkhJKSkoak9DcuXMZPnw4JSUlfOtb32p8fY888kiz8Z122mmMHTuW44/3WtDHjx/PqFGjGDp0KLNnz258ztNPP80JJ5xASUkJFRUVJBIJhgwZ0thhnEgkOProo/e5A7k56fz3uAEYlLRe5G9r5JzbnLT6W+C25g7knJsNzAYoLS11+xLMk0/CX/4CQ4fuy7OlJbUT3v0AABJbSURBVJ/UfgJAQU5BhiORVKmpgUTCW04kvPXevVN/nvXr1/PSSy8RDofZvn07L7zwApFIhOeee46ZM2fyh2auBHnnnXdYunQpO3bs4JhjjuHKK6/83PX2r7/+Om+99RaHHnoop5xyCi+++CKlpaVcfvnlLFu2jMGDBzNp0qQW45o/fz6TJk1i3LhxzJw5k7q6OnJycrj66qs544wzWLhwIfF4nJ07d/LWW29x880389JLL9GvXz+2bNnS5ut+7bXXWLVqVeNloXPmzKFPnz58+umnfOlLX2LixIkkEgkuu+yyxni3bNlCKBTiwgsvZN68eUybNo3nnnuOkpKSxua3VEhnQngVGGJmg/ESwQXAN5MLmNlA59yH/upY4O10BBKLwYQJXr/BypXeui6NTI1P6ryE0DOnZ4Yjkfa44462y8RiUFEBtbWQmwvz5qXn7+XrX/864XAYgJqaGi666CLee+89zIy6Fjr5zj33XPLy8sjLy+Pggw9m06ZNFBUV7VVm9OjRjdtGjBhBZWUlvXr14sgjj2z8Jzxp0qS9vo03qK2tZdGiRfziF7+gsLCQE088kWeeeYavfvWrLFmyhLlz5wIQDofp3bs3c+fO5etf/zr9+vUDoE+fPm2+7tGjR+91j8CsWbNYuHAhAFVVVbz33ntUV1dz+umnN5ZrOO63v/1txo0bx7Rp05gzZw6XXHJJm+friLQlBOdcvZl9D3gGCANznHNvmdlPgOXOuSeAq81sLFAPbAEuTkcs0ajXXATeN55oVAkhVRqajFRD2H+UlcHixanrQ2hJz56ffYn48Y9/zJlnnsnChQuprKykvLy82efk5eU1LofDYeob/rA7WKYlzzzzDNu2bWPYsGEA7Nq1ix49erTYvNSSSCRCwq9mJRIJamtrG/clv+5oNMpzzz1HLBajoKCA8vLyVu8oHzRoEAMGDGDJkiW88sorzJs3r0NxtSWtfQjOuUXOuS84545yzv3U33ajnwxwzs1wzg11zpU45850zr2TjjjKyz/rOwiFvHVJjYVve99snl77dIYjkVQqK4MZM7rui1NNTQ2HHXYYAPfee2/Kj3/MMcewbt06KisrAXjwwQebLTd//nx++9vfUllZSWVlJe+//z7PPvssu3btoqKigt/85jcAxONxampqOOuss3j44YfZvNlr/W5oMiouLmbFihUAPPHEEy3WeGpqajjooIMoKCjgnXfe4a9//SsAJ510EsuWLeP999/f67gAl156KRdeeOFeNaxUyXSncpcoK4NrrvGWjzlGtYNUmb1iNnevuBuAm1+4mdkrPl8FF2mP6667jhkzZjBy5MgOfaNvrx49enDXXXcxZswYRo0aRWFhIb2bdIzs2rWLp59+mnPPPbdxW8+ePTn11FP54x//yK9+9SuWLl3KsGHDGDVqFKtXr2bo0KH86Ec/4owzzqCkpIRrr70WgMsuu4znn3+ekpISYrHYXrWCZGPGjKG+vp7jjjuO6dOnc9JJJwHQv39/Zs+ezfnnn09JSQnf+MY3Gp8zduxYdu7cmfLmIgBr6IEPitLSUrd8+fIOP+/ee+GSS+BLX4JXXkl9XN3R6HtG8+o/X/1s/dDRvHzZyxmMSJrz9ttvc9xxx2U6jIzbuXMnvXr1wjnHd7/7XYYMGcI1Dd8UA2T58uVcc801vPDCCy2Wae4zN7MVzrlWr+HtFjUEgIYLEQKW/7Jaj5wee60fWnhohiIRads999zDiBEjGDp0KDU1NVwewMHNbr31ViZOnMgtt9ySluN3mxrCT++LccPS6YQOW0nPXvEWy8UTcRyOSKjl/vZsKZPpOHbX76Yu4bWNRkIRll28TPciZCHVELqffa0hdIvbtGJVMW6oPAWKHQlgR22bT5EOurbsWiUDkYDrFgnBm/TdeYOwSVq88eEbmQ5BRDqpW/QhlBeXEyLsTQQfrBaywJh4vIaTFQm6blFDKBtUxqwRL/C9hdMJHbqSnoXqQ0hVmb4FfZlx6gymjpra4nNFJBi6RUIAGNm/DO57nuOGwqpVmY5GpPvozPDX4N3Nm5uby8knn9ximfHjx7Nx48bGG7tk33SbhKDLTkXaL1YVI1oZpby4vNMXC7Q1/HVbotEovXr1ajEhbNu2jRUrVtCrVy/WrVvHkUce2al4W5KNw1Wn2v796pI0fAlRQpDubNrT03hjY+sXANTsqWHlppUkXIKQhRg+YDi981oe7nTEISO4Y0w7Rs1LsmLFCq699lp27txJv379uPfeexk4cCCzZs3i7rvvJhKJcPzxx3Prrbdy9913Ew6HeeCBB7jzzjs57bTT9jrWo48+ynnnnceAAQNYsGABM2fOBGDt2rVcccUVVFdXEw6HefjhhznqqKP4+c9/zgMPPEAoFOKcc87h1ltvpby8nNtvv53S0lI+/vhjSktLqays5N577+XRRx9l586dxONxnnzyScaNG8fWrVupq6vj5ptvZty4cYA3DPbtt9+OmTF8+HDuuusuhg8fzrvvvktOTg7bt2+npKSkcT0bdZuE0PD+NwzrKyLNq9ldQ8L5A7O5BDW7a1pNCB3lnOOqq67i8ccfp3///jz44IP86Ec/Ys6cOdx66628//775OXlsW3bNg488ECuuOKKVmsV8+fP58Ybb2TAgAFMnDixMSFMnjyZ6dOnM2HCBHbv3k0ikeCpp57i8ccf5+WXX6agoKDdw1WvXLmSPn36UF9fz8KFCznggAP4+OOPOemkkxg7diyrV6/+3DDYhYWFlJeX8+STTzJ+/HgWLFjA+eefn7XJALpRQmiYBU81BOnO2vNNPlYVo2JuBbXxWnLDucw7f15K7zHZs2cPq1at4uyzzwa8geIGDhwIwPDhw5k8eTLjx49n/PjxbR5r06ZNvPfee5x66qmYGTk5OaxatYojjjiCDRs2MGHCBADy8/MBeO6557jkkksoKPBG523PcNVnn312YznnHDNnzmTZsmWEQiE2bNjApk2bWLJkSbPDYF966aXcdtttjB8/nt/97nfcc889HXmruly3SQjm34OghCDSurJBZSyesjhlfQhNOecYOnQosVjsc/uefPJJli1bxh//+Ed++tOf8re//a3VYz300ENs3bq1cd6A7du3M3/+fKZPn96hmJKHq246/HTywHTz5s2jurqaFStWkJOTQ3FxcavDVZ9yyilUVlYSjUaJx+N88Ytf7FBcXa1b3IcAnyWEzZu9CUBEpGVlg8qYcdqMtNx9npeXR3V1dWNCqKur46233iKRSFBVVcWZZ57Jz3/+c2pqati5cyeFhYXs2LGj2WPNnz+fp59+unG46hUrVrBgwQIKCwspKiriscceA7xaya5duzj77LP53e9+x65d3jwezQ1XnTz1ZVM1NTUcfPDB5OTksHTpUj744AOAFofBBpgyZQrf/OY30zI6aap1m4SwcqX3c8sWbzYoJQWRzAiFQjzyyCNcf/31lJSUMGLECF566SXi8TgXXnghw4YNY+TIkVx99dUceOCBnHfeeSxcuJARI0bsNcJnZWUlH3zwQeOQ0QCDBw+md+/evPzyy9x///3MmjWL4cOHc/LJJ7Nx40bGjBnD2LFjKS0tZcSIEdx+++0A/OAHP+A3v/kNI0eO5OOPP24x9smTJ7N8+XKGDRvG3LlzOfbYYwFaHAa74Tlbt25tddrObNFtBrf72c/ghhu8JqNwGP7rv7wJQET2dxrcLrMeeeQRHn/8ce6///4uO6cGt2vDmWdCfv5n88Rq1jQRSberrrqKp556ikWLFmU6lHbpNgmhq+aJFRFpcOedd2Y6hA7pNgkBvCSgRCDdkXMOMw332x10phug23Qqi3RX+fn5bN68uVP/KCQYnHNs3ry58b6LjupWNQSR7qioqIj169dTXV2d6VCkC+Tn51NUVLRPz1VCENnP5eTkNN64JdIaNRmJiAighCAiIj4lBBERAQJ4p7KZVQMf7OPT+wEt35eeWdkcG2R3fIpt3yi2fRPU2I5wzvVv7cmBSwidYWbL27p1O1OyOTbI7vgU275RbPtmf45NTUYiIgIoIYiIiK+7JYTZmQ6gFdkcG2R3fIpt3yi2fbPfxtat+hBERKRl3a2GICIiLVBCEBERoBslBDMbY2ZrzGytmXVsBu7UnH+OmX1kZquStvUxs2fN7D3/50H+djOzWX6sK83shDTHNsjMlprZajN7y8y+ny3xmVm+mb1iZm/6sf2nv32wmb3sx/CgmeX62/P89bX+/uJ0xZYUY9jMXjezP2VTbGZWaWZ/M7M3zGy5vy3jn6l/vgPN7BEze8fM3jazsiyK7Rj/PWt4bDezaVkU3zX+38IqM5vv/42k5nfOObffP4Aw8HfgSCAXeBM4votjOB04AViVtO02YLq/PB34ub/8FeApwICTgJfTHNtA4AR/uRB4Fzg+G+Lzz9HLX84BXvbP+RBwgb/9buBKf/nfgbv95QuAB7vgs70W+D3wJ389K2IDKoF+TbZl/DP1z3cfcKm/nAscmC2xNYkzDGwEjsiG+IDDgPeBHkm/axen6neuS97UTD+AMuCZpPUZwIwMxFHM3glhDTDQXx4IrPGX/xeY1Fy5LorzceDsbIsPKABeA07Euxsz0vTzBZ4ByvzliF/O0hhTEbAYOAv4k/9PIVtiq+TzCSHjnynQ2/+nZtkWWzOx/gvwYrbEh5cQqoA+/u/Qn4B/TdXvXHdpMmp4Exus97dl2gDn3If+8kZggL+csXj9KuVIvG/iWRGf3yTzBvAR8CxebW+bc66+mfM3xubvrwH6pis24A7gOiDhr/fNotgc8GczW2FmU/1t2fCZDgaqgd/5TW2/NbOeWRJbUxcA8/3ljMfnnNsA3A78A/gQ73doBSn6nesuCSHrOS+FZ/QaYDPrBfwBmOac2568L5PxOefizrkReN/GRwPHZiKOpszsq8BHzrkVmY6lBac6504AzgG+a2anJ+/M4GcawWs+/Y1zbiTwCV4TTDbE1shvhx8LPNx0X6bi8/stxuEl1UOBnsCYVB2/uySEDcCgpPUif1umbTKzgQD+z4/87V0er5nl4CWDec65R7MtPgDn3DZgKV6V+EAza5jgKfn8jbH5+3sDm9MU0inAWDOrBBbgNRv9Kktia/g2iXPuI2AhXjLNhs90PbDeOfeyv/4IXoLIhtiSnQO85pzb5K9nQ3xfBt53zlU75+qAR/F+D1PyO9ddEsKrwBC/Jz4Xrxr4RIZjAi+Gi/zli/Da7hu2T/GvXjgJqEmqqqacmRnwf8DbzrlfZFN8ZtbfzA70l3vg9W28jZcYvtZCbA0xfw1Y4n+bSznn3AznXJFzrhjvd2qJc25yNsRmZj3NrLBhGa8tfBVZ8Jk65zYCVWZ2jL+pAlidDbE1MYnPmosa4sh0fP8ATjKzAv/vtuG9S83vXFd0zGTDA+9KgHfx2p9/lIHzz8dr86vD+4b0Hby2vMXAe8BzQB+/rAG/9mP9G1Ca5thOxav+rgTe8B9fyYb4gOHA635sq4Ab/e1HAq8Aa/Gq9Hn+9nx/fa2//8gu+nzL+ewqo4zH5sfwpv94q+F3Phs+U/98I4Dl/uf6GHBQtsTmn7Mn3jfp3knbsiI+4D+Bd/y/h/uBvFT9zmnoChERAbpPk5GIiLRBCUFERAAlBBER8SkhiIgIoIQgIiI+JQQRn5nFm4xymbJRcc2s2JJGuhXJRpG2i4h0G586b4gMkW5JNQSRNpg3r8Bt5s0t8IqZHe1vLzazJf4Y+IvN7HB/+wAzW2jeHA5vmtnJ/qHCZnaPP5b9n/07rzGzq82bi2KlmS3I0MsUUUIQSdKjSZPRN5L21TjnhgH/gzfCKcCdwH3OueHAPGCWv30W8LxzrgRvjJ63/O1DgF8754YC24CJ/vbpwEj/OFek68WJtEV3Kov4zGync65XM9srgbOcc+v8QQA3Ouf6mtnHeOPe1/nbP3TO9TOzaqDIObcn6RjFwLPOuSH++vVAjnPuZjN7GtiJN4TDY865nWl+qSLNUg1BpH1cC8sdsSdpOc5nfXjn4o2FcwLwatKolSJdSglBpH2+kfQz5i+/hDfKKcBk4AV/eTFwJTRO7tO7pYOaWQgY5JxbClyPNzzx52opIl1B30REPtPDn5mtwdPOuYZLTw8ys5V43/In+duuwpv164d4M4Bd4m//PjDbzL6DVxO4Em+k2+aEgQf8pGHALOfN+yDS5dSHINIGvw+h1Dn3caZjEUknNRmJiAigGoKIiPhUQxAREUAJQUREfEoIIiICKCGIiIhPCUFERAD4/2mRunGJipQwAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9560185185185185 0.04244510415612425\n",
            "training error 0.1250216070346282, test error 0.250009364866003\n",
            "training error 0.12504596295723666, test error 0.2500681663583951\n",
            "training error 0.12500263800931047, test error 0.25021095840832475\n",
            "training error 0.1250127029199794, test error 0.2503144131033662\n",
            "training error 0.12499887347530493, test error 0.25036562413023866\n",
            "training error 0.12510858639521938, test error 0.25038018088785297\n",
            "training error 0.1250871853889031, test error 0.25039564999021857\n",
            "training error 0.12510019260244754, test error 0.25026149424096\n",
            "training error 0.12502982455563097, test error 0.2503254470556655\n",
            "training error 0.12497650588545582, test error 0.25038950404762517\n",
            "training error 0.12517285928149166, test error 0.2503426997624394\n",
            "training error 0.12500694743799315, test error 0.2503050125205502\n",
            "training error 0.12505691600115315, test error 0.250371141322745\n",
            "training error 0.12506458730181103, test error 0.25039144577259065\n",
            "training error 0.12525497551909232, test error 0.2506722496993189\n",
            "training error 0.12496060057960984, test error 0.2505131662593165\n",
            "training error 0.12541153483255207, test error 0.2507091489281738\n",
            "training error 0.12497778370668718, test error 0.2505740904824519\n",
            "training error 0.12498740607193186, test error 0.2504751354355945\n",
            "training error 0.12510717084001877, test error 0.25038934557821996\n",
            "training error 0.12513839389699455, test error 0.25054355639316467\n",
            "training error 0.1250300352719987, test error 0.2506702319359106\n",
            "training error 0.1250683842576318, test error 0.25058795046378113\n",
            "training error 0.12495579801055512, test error 0.2506167013287193\n",
            "training error 0.12498879743313777, test error 0.2506345013886557\n",
            "training error 0.12498797041148162, test error 0.25056870450387764\n",
            "training error 0.12496854304310887, test error 0.25056019308501926\n",
            "training error 0.12502325176264903, test error 0.25071730509267803\n",
            "training error 0.12500696832630853, test error 0.25059794365313703\n",
            "training error 0.12505623269266986, test error 0.2506459367125805\n",
            "training error 0.12509442223338527, test error 0.2506528287273879\n",
            "training error 0.12504490993982523, test error 0.25068307866514783\n",
            "training error 0.12506554646187895, test error 0.2509137756644543\n",
            "training error 0.12503525141322253, test error 0.25078266834728896\n",
            "training error 0.12506746689088766, test error 0.2507759221971223\n",
            "training error 0.12507697620571448, test error 0.2508041946436748\n",
            "training error 0.1250278006396824, test error 0.2507571760374379\n",
            "training error 0.12503565450510726, test error 0.25059498976222416\n",
            "training error 0.125035717781605, test error 0.25053910097888094\n",
            "training error 0.12528367496165124, test error 0.25067791218110164\n",
            "training error 0.12512355131395744, test error 0.2507030081115409\n",
            "training error 0.12507258755338146, test error 0.2505835610899044\n",
            "training error 0.12505594343829388, test error 0.2506780997613114\n",
            "training error 0.12495640284663717, test error 0.25060008592116867\n",
            "training error 0.12507067525964694, test error 0.2506513056144796\n",
            "training error 0.12494887095583015, test error 0.2506390870360847\n",
            "training error 0.12496561439665757, test error 0.2506044362623726\n",
            "training error 0.1249874160435491, test error 0.25059756131575495\n",
            "training error 0.1250193038367675, test error 0.2507176545681898\n",
            "training error 0.12494476955966421, test error 0.25062704291129967\n",
            "Loss: 0.2470619633099469\n",
            "training error 0.12506880341499918, test error 0.2506360947247918\n",
            "Loss: 0.2506825530814494\n",
            "training error 0.12497350178664095, test error 0.2507387006699086\n",
            "Loss: 0.2917233937602681\n",
            "training error 0.12509684500642973, test error 0.25084430035752797\n",
            "Loss: 0.33396168658421654\n",
            "training error 0.12503432318959246, test error 0.25085770631009613\n",
            "Loss: 0.3393238667470655\n",
            "training error 0.1251169732939043, test error 0.2505828786675667\n",
            "Loss: 0.2293969275395158\n",
            "training error 0.12495552514964493, test error 0.2507299400129708\n",
            "Loss: 0.28821926224802397\n",
            "training error 0.1250320423012953, test error 0.25077047025796717\n",
            "Loss: 0.3044307529728396\n",
            "training error 0.12496663774432785, test error 0.2507777244837734\n",
            "Loss: 0.3073323346036272\n",
            "training error 0.12498135197699352, test error 0.2507414484505164\n",
            "Loss: 0.29282246483277596\n",
            "training error 0.12509178380191574, test error 0.2506653828847452\n",
            "Loss: 0.2623973782317268\n",
            "training error 0.12503620043535027, test error 0.2508573473226875\n",
            "Loss: 0.33918027716242083\n",
            "training error 0.12498198359628326, test error 0.2507120200005985\n",
            "Loss: 0.2810515257986923\n",
            "training error 0.12493924004638338, test error 0.25071755439100935\n",
            "Loss: 0.2832651990400059\n",
            "training error 0.1249354955841465, test error 0.25067559528911787\n",
            "Loss: 0.26648218696605497\n",
            "training error 0.12496129451479164, test error 0.2506413350022649\n",
            "Loss: 0.252778585554414\n",
            "training error 0.12498972293608514, test error 0.25062281696541133\n",
            "Loss: 0.24537164827289626\n",
            "training error 0.1251298276673987, test error 0.2505929910582566\n",
            "Loss: 0.23344173229926657\n",
            "training error 0.12499473017454102, test error 0.2506438528970175\n",
            "Loss: 0.2537857057292925\n",
            "training error 0.12499753771340762, test error 0.2508164834976886\n",
            "Loss: 0.3228353594347144\n",
            "training error 0.12495147854655872, test error 0.2507497291393317\n",
            "Loss: 0.29613461628748006\n",
            "training error 0.12494533631857355, test error 0.25066513808592905\n",
            "Loss: 0.26229946237315005\n",
            "training error 0.12494683324611472, test error 0.2506784254603745\n",
            "Loss: 0.267614213063605\n",
            "training error 0.12513809834815182, test error 0.2505929330624102\n",
            "Loss: 0.2334185348296769\n",
            "training error 0.1249383006889449, test error 0.2505935352883521\n",
            "Loss: 0.23365941618314068\n",
            "training error 0.12499994723198318, test error 0.25056144677824854\n",
            "Loss: 0.22082449293105988\n",
            "training error 0.12493660982073933, test error 0.25063608349785577\n",
            "Loss: 0.2506780624752514\n",
            "training error 0.12494933439719545, test error 0.2506632231034846\n",
            "Loss: 0.26153349808797355\n",
            "training error 0.12492935648861427, test error 0.250690728993318\n",
            "Loss: 0.27253544189442813\n",
            "training error 0.12494319087984017, test error 0.25067530104015723\n",
            "Loss: 0.26636449179060495\n",
            "training error 0.12503693425106752, test error 0.2508400168387432\n",
            "Loss: 0.3322483432512069\n",
            "training error 0.12493449728244417, test error 0.2507494720332202\n",
            "Loss: 0.2960317776951493\n",
            "training error 0.12494616386190191, test error 0.25076497152981314\n",
            "Loss: 0.3022313440999014\n",
            "training error 0.12492257263678028, test error 0.250760114455261\n",
            "Loss: 0.30028858705366535\n",
            "training error 0.12501409295456448, test error 0.25090451394119917\n",
            "Loss: 0.3580462178590649\n",
            "training error 0.12492661334832117, test error 0.2509146484243321\n",
            "Loss: 0.362099859264986\n",
            "training error 0.1249728075568297, test error 0.2508196698334925\n",
            "Loss: 0.3241098460146974\n",
            "training error 0.12494282833173909, test error 0.25076235501328137\n",
            "Loss: 0.3011847766910325\n",
            "training error 0.12505974513701484, test error 0.2507169007709006\n",
            "Loss: 0.2830037607898461\n",
            "training error 0.12517622485981328, test error 0.25091006029361507\n",
            "Loss: 0.36026467572316534\n",
            "training error 0.12491685744112771, test error 0.2507518263690418\n",
            "Loss: 0.29697347674826347\n",
            "training error 0.12495221985345328, test error 0.25066198055224315\n",
            "Loss: 0.26103649620881075\n",
            "training error 0.12491633165718576, test error 0.2506510695941198\n",
            "Loss: 0.2566722764408169\n",
            "training error 0.12491025802313786, test error 0.2506695912165817\n",
            "Loss: 0.2640806479119684\n",
            "training error 0.12495161565341652, test error 0.2507259696815358\n",
            "Loss: 0.2866311891624118\n",
            "training error 0.12499734957442261, test error 0.2507730907617144\n",
            "Loss: 0.30547891520813497\n",
            "training error 0.12494543118965447, test error 0.2507037484520051\n",
            "Loss: 0.27774303029579617\n",
            "training error 0.12493575652394297, test error 0.2506772548286196\n",
            "Loss: 0.2671459779014951\n",
            "training error 0.12491907895816137, test error 0.25082127253503905\n",
            "Loss: 0.3247509026196749\n",
            "training error 0.12494529051843885, test error 0.2507989500079126\n",
            "Loss: 0.31582222623252054\n",
            "training error 0.12494608555052845, test error 0.2507074437894138\n",
            "Loss: 0.2792211098912034\n",
            "training error 0.12491328784716814, test error 0.25074976481621547\n",
            "Loss: 0.2961488865064199\n",
            "training error 0.12498973241122838, test error 0.2508260036460193\n",
            "Loss: 0.3266432761244831\n",
            "training error 0.1251305299133727, test error 0.25057402991903627\n",
            "Loss: 0.22585756071014096\n",
            "training error 0.12500412363240465, test error 0.2507602547369683\n",
            "Loss: 0.3003446976347135\n",
            "training error 0.1248956860649746, test error 0.25075779458041453\n",
            "Loss: 0.2993606718743047\n",
            "training error 0.12492464190969314, test error 0.2508266253250144\n",
            "Loss: 0.3268919384077362\n",
            "training error 0.12498982916703733, test error 0.2508922270388589\n",
            "Loss: 0.35313164102035444\n",
            "training error 0.12495994211573037, test error 0.2507389816702252\n",
            "Loss: 0.29183578967661994\n",
            "training error 0.12498508954363541, test error 0.2508964344600283\n",
            "Loss: 0.35481454644739063\n",
            "training error 0.12495942548382405, test error 0.2508514827983291\n",
            "Loss: 0.33683455528852324\n",
            "training error 0.12493144480570752, test error 0.25086296471324243\n",
            "Loss: 0.3414271492177656\n",
            "training error 0.12497152800104801, test error 0.2508043182215705\n",
            "Loss: 0.3179694312625436\n",
            "training error 0.12489424854215295, test error 0.2508334533333543\n",
            "Loss: 0.32962303943813165\n",
            "training error 0.12493093561509618, test error 0.2508696239388712\n",
            "Loss: 0.34409073969259296\n",
            "training error 0.12497505939361547, test error 0.2510234874215102\n",
            "Loss: 0.4056338273771276\n",
            "training error 0.12493206999665998, test error 0.25097457681667484\n",
            "Loss: 0.3860703182815506\n",
            "training error 0.1249919183395767, test error 0.25098210055043596\n",
            "Loss: 0.389079699056194\n",
            "training error 0.12492471498191644, test error 0.2510585260911882\n",
            "Loss: 0.41964877025608693\n",
            "training error 0.12531363844736515, test error 0.2508133333285891\n",
            "Loss: 0.32157533899461743\n",
            "training error 0.12496260790895539, test error 0.25093352990012807\n",
            "Loss: 0.3696521666779917\n",
            "training error 0.1248605319214082, test error 0.2509105957728426\n",
            "Loss: 0.3604788593909669\n",
            "training error 0.12484891166704512, test error 0.25085875516327616\n",
            "Loss: 0.33974339230389283\n",
            "training error 0.12485992012166656, test error 0.25091945617794276\n",
            "Loss: 0.36402288867360966\n",
            "training error 0.12489384525159912, test error 0.2509022733658926\n",
            "Loss: 0.357150021307473\n",
            "training error 0.12486196156357565, test error 0.2508971496151137\n",
            "Loss: 0.3551005977662225\n",
            "training error 0.12485914596387805, test error 0.2509980355737517\n",
            "Loss: 0.3954534696244627\n",
            "training error 0.12486005376166809, test error 0.25091660350076234\n",
            "Loss: 0.36288186054374005\n",
            "training error 0.12488812519741453, test error 0.2508535948196215\n",
            "Loss: 0.33767933216060086\n",
            "training error 0.12492482956874533, test error 0.25099404848223045\n",
            "Loss: 0.3938586927554555\n",
            "training error 0.12482533473875987, test error 0.25089518633795355\n",
            "Loss: 0.35431531631837654\n",
            "training error 0.12482527169638911, test error 0.25082874729864846\n",
            "Loss: 0.3277406960673801\n",
            "training error 0.12481735287585129, test error 0.2508729253590793\n",
            "Loss: 0.34541125830991337\n",
            "training error 0.12482448062351789, test error 0.25087931097859417\n",
            "Loss: 0.34796541043868867\n",
            "training error 0.12491207189234041, test error 0.2508521473936621\n",
            "Loss: 0.33710038346395077\n",
            "training error 0.12488538923134324, test error 0.25092790291868683\n",
            "Loss: 0.36740145841183125\n",
            "training error 0.12494515436586426, test error 0.2509042753106165\n",
            "Loss: 0.35795076920144275\n",
            "training error 0.1250810601400819, test error 0.2507198142308626\n",
            "Loss: 0.28416910112161187\n",
            "training error 0.12486518012046506, test error 0.2508759259471571\n",
            "Loss: 0.34661144858254556\n",
            "training error 0.12484558190094987, test error 0.25084734670315517\n",
            "Loss: 0.33518017919100807\n",
            "training error 0.12482975187219121, test error 0.2507870868420132\n",
            "Loss: 0.31107713762121847\n",
            "training error 0.1248980469419922, test error 0.2506406785077434\n",
            "Loss: 0.25251599758224774\n",
            "training error 0.1248250322715327, test error 0.25063780452232565\n",
            "Loss: 0.25136644647669737\n",
            "training error 0.12476997875560962, test error 0.250615175716975\n",
            "Loss: 0.24231526338891474\n",
            "training error 0.12487133638256454, test error 0.2506049570340888\n",
            "Loss: 0.23822794334324993\n",
            "training error 0.12479376931223536, test error 0.25059605057628626\n",
            "Loss: 0.23466549366968348\n",
            "training error 0.12487438223180033, test error 0.2506323506372407\n",
            "Loss: 0.2491849741595109\n",
            "training error 0.12484089490899083, test error 0.2507222307287604\n",
            "Loss: 0.285135664073799\n",
            "training error 0.12473357368390978, test error 0.250647092917753\n",
            "Loss: 0.2550816654775412\n",
            "training error 0.12472408978097232, test error 0.2506121220812954\n",
            "Loss: 0.24109385487038892\n",
            "training error 0.12480937446049943, test error 0.25050404648438396\n",
            "Loss: 0.19786523542670853\n",
            "training error 0.12472092552857834, test error 0.25059822220089933\n",
            "Loss: 0.23553411097698707\n",
            "training error 0.124721466209305, test error 0.2505512503368494\n",
            "Loss: 0.21674606914696692\n",
            "training error 0.12469310429072246, test error 0.25052173910237774\n",
            "Loss: 0.20494201753176178\n",
            "training error 0.12472979758732691, test error 0.25049563439168543\n",
            "Loss: 0.19450052438758636\n",
            "training error 0.12468350420828753, test error 0.2506096837188376\n",
            "Loss: 0.24011854642178587\n",
            "training error 0.1247020168794213, test error 0.2506157243665689\n",
            "Loss: 0.2425347150059487\n",
            "training error 0.1247671432098069, test error 0.2506454303032408\n",
            "Loss: 0.25441664458398794\n",
            "training error 0.1246556938813192, test error 0.2506159310436795\n",
            "Loss: 0.242617382753485\n",
            "training error 0.12468046868305047, test error 0.25054491421419084\n",
            "Loss: 0.21421171501909697\n",
            "training error 0.12462843617410115, test error 0.2503967698041179\n",
            "Loss: 0.1549561706708502\n",
            "training error 0.12461183710871224, test error 0.25039956891667503\n",
            "Loss: 0.15607577375398307\n",
            "training error 0.12462277363498836, test error 0.25044339364970347\n",
            "Loss: 0.17360501032954723\n",
            "training error 0.12466997784828623, test error 0.25046304185142226\n",
            "Loss: 0.1814639966236431\n",
            "training error 0.12456425068681341, test error 0.25038190259579074\n",
            "Loss: 0.14900951009870766\n",
            "training error 0.12457568596645384, test error 0.25039433005097284\n",
            "Loss: 0.15398030596820433\n",
            "training error 0.1246025701883669, test error 0.25036730948465463\n",
            "Loss: 0.14317248429613905\n",
            "training error 0.12458578119948414, test error 0.250440603922959\n",
            "Loss: 0.17248916143086657\n",
            "training error 0.12463440578520853, test error 0.25026033759811145\n",
            "Loss: 0.10038533246263803\n",
            "training error 0.12454793804177129, test error 0.25024624208002616\n",
            "Loss: 0.09474733642482747\n",
            "training error 0.12455452964038659, test error 0.25020188610513316\n",
            "Loss: 0.07700561106314119\n",
            "training error 0.12447139528107863, test error 0.2501993533557231\n",
            "Loss: 0.07599254924786081\n",
            "training error 0.1244348453872602, test error 0.25025115968455763\n",
            "Loss: 0.09671430455584495\n",
            "training error 0.1245479367793682, test error 0.25015539686202976\n",
            "Loss: 0.05841061038054374\n",
            "training error 0.1244635325994922, test error 0.2502132349878483\n",
            "Loss: 0.08154499410635285\n",
            "training error 0.12438779870969209, test error 0.25015347736942906\n",
            "Loss: 0.057642842100458225\n",
            "training error 0.12434395615115398, test error 0.25003362089850995\n",
            "Loss: 0.009702049569204618\n",
            "training error 0.12449313486167592, test error 0.250117642118653\n",
            "Loss: 0.04330927871762924\n",
            "training error 0.12429660913606892, test error 0.25009252439623253\n",
            "Loss: 0.033262566093905654\n",
            "training error 0.12426833951257818, test error 0.2500778928223547\n",
            "Loss: 0.027410155770946076\n",
            "training error 0.12428527506768826, test error 0.2500388420403135\n",
            "Loss: 0.01179042806109809\n",
            "training error 0.1244565973412129, test error 0.24989538982993068\n",
            "Loss: 0.0\n",
            "training error 0.12413567006574454, test error 0.24993404715573883\n",
            "Loss: 0.01546940335093172\n",
            "training error 0.12423228362993935, test error 0.24973768034686494\n",
            "Loss: 0.0\n",
            "training error 0.12415713000920368, test error 0.24973246641784033\n",
            "Loss: 0.0\n",
            "training error 0.12422062103767129, test error 0.2495784908615904\n",
            "Loss: 0.0\n",
            "training error 0.12429613529240438, test error 0.24963733937604202\n",
            "Loss: 0.023579161108178148\n",
            "training error 0.12396844292203309, test error 0.2494735839827869\n",
            "Loss: 0.0\n",
            "training error 0.12397322668021936, test error 0.24937214129565877\n",
            "Loss: 0.0\n",
            "training error 0.12393080231757496, test error 0.2492829501981859\n",
            "Loss: 0.0\n",
            "training error 0.12381058547676613, test error 0.24924082625893573\n",
            "Loss: 0.0\n",
            "training error 0.12382135371196894, test error 0.2490301179675355\n",
            "Loss: 0.0\n",
            "training error 0.12371295139664627, test error 0.24889600195403577\n",
            "Loss: 0.0\n",
            "training error 0.12364441234111224, test error 0.24876812223259465\n",
            "Loss: 0.0\n",
            "training error 0.1235899411815343, test error 0.24868151928982077\n",
            "Loss: 0.0\n",
            "training error 0.12355686767775989, test error 0.24846479618540568\n",
            "Loss: 0.0\n",
            "training error 0.12354494073504198, test error 0.2485072585319172\n",
            "Loss: 0.017089884427679003\n",
            "training error 0.12346832423140965, test error 0.2483075584107506\n",
            "Loss: 0.0\n",
            "training error 0.12330336064541893, test error 0.2482442260610104\n",
            "Loss: 0.0\n",
            "training error 0.12325453557998253, test error 0.24801889051993165\n",
            "Loss: 0.0\n",
            "training error 0.12337970541364579, test error 0.2478589461677846\n",
            "Loss: 0.0\n",
            "training error 0.12306383570404135, test error 0.24769284171655712\n",
            "Loss: 0.0\n",
            "training error 0.12298011275400622, test error 0.2474676806990945\n",
            "Loss: 0.0\n",
            "training error 0.12297493000672485, test error 0.24740514494861987\n",
            "Loss: 0.0\n",
            "training error 0.12279822508306946, test error 0.2471730640017852\n",
            "Loss: 0.0\n",
            "training error 0.12267396937729673, test error 0.24691846463410697\n",
            "Loss: 0.0\n",
            "training error 0.12259412490589751, test error 0.24678909391136672\n",
            "Loss: 0.0\n",
            "training error 0.1224885977820302, test error 0.24660731772855288\n",
            "Loss: 0.0\n",
            "training error 0.12247434722494555, test error 0.24647260560126913\n",
            "Loss: 0.0\n",
            "training error 0.12231020748826994, test error 0.24616976250283015\n",
            "Loss: 0.0\n",
            "training error 0.1223091266932591, test error 0.2457852328605626\n",
            "Loss: 0.0\n",
            "training error 0.12198361954955148, test error 0.2455677175831434\n",
            "Loss: 0.0\n",
            "training error 0.12185616647367159, test error 0.2453985286770062\n",
            "Loss: 0.0\n",
            "training error 0.12168969701615782, test error 0.2449352867038944\n",
            "Loss: 0.0\n",
            "training error 0.12150419867663943, test error 0.24454877154478968\n",
            "Loss: 0.0\n",
            "training error 0.12158820649930979, test error 0.2444473325364754\n",
            "Loss: 0.0\n",
            "training error 0.12113750982481256, test error 0.24394274582707284\n",
            "Loss: 0.0\n",
            "training error 0.1210204343369043, test error 0.24347988965174008\n",
            "Loss: 0.0\n",
            "training error 0.12091208043979226, test error 0.24302253744572397\n",
            "Loss: 0.0\n",
            "training error 0.12056547698856483, test error 0.24268368104812407\n",
            "Loss: 0.0\n",
            "training error 0.12040070763229593, test error 0.24225459634480367\n",
            "Loss: 0.0\n",
            "training error 0.12025451232379583, test error 0.2417923385360257\n",
            "Loss: 0.0\n",
            "training error 0.1200982011562777, test error 0.24142107293112292\n",
            "Loss: 0.0\n",
            "training error 0.11966137336630113, test error 0.24085836141638028\n",
            "Loss: 0.0\n",
            "training error 0.11943534679685616, test error 0.24031474458151705\n",
            "Loss: 0.0\n",
            "training error 0.11922166392177261, test error 0.2398541754993237\n",
            "Loss: 0.0\n",
            "training error 0.11893679192766327, test error 0.23923055339612373\n",
            "Loss: 0.0\n",
            "training error 0.11859019096533299, test error 0.23862376352018314\n",
            "Loss: 0.0\n",
            "training error 0.11855700211265506, test error 0.23815421778284596\n",
            "Loss: 0.0\n",
            "training error 0.11817989314176289, test error 0.23717991250596904\n",
            "Loss: 0.0\n",
            "training error 0.11776227518332724, test error 0.23661797398832599\n",
            "Loss: 0.0\n",
            "training error 0.1172778728826816, test error 0.2358176475633358\n",
            "Loss: 0.0\n",
            "training error 0.11693628380697725, test error 0.23518215511557905\n",
            "Loss: 0.0\n",
            "training error 0.11663051145502645, test error 0.23449389476048435\n",
            "Loss: 0.0\n",
            "training error 0.11641288234157607, test error 0.23359902431654753\n",
            "Loss: 0.0\n",
            "training error 0.11594621897642639, test error 0.23287430621789076\n",
            "Loss: 0.0\n",
            "training error 0.11539390732466959, test error 0.2320455189880014\n",
            "Loss: 0.0\n",
            "training error 0.11492862498222614, test error 0.23102090498552352\n",
            "Loss: 0.0\n",
            "training error 0.1146714114217024, test error 0.2300452771870728\n",
            "Loss: 0.0\n",
            "training error 0.11406625206675568, test error 0.2291490042041505\n",
            "Loss: 0.0\n",
            "training error 0.1135369966334277, test error 0.22810117397081475\n",
            "Loss: 0.0\n",
            "training error 0.11324975227992394, test error 0.2268635944872507\n",
            "Loss: 0.0\n",
            "training error 0.11250675467478191, test error 0.2259043415899223\n",
            "Loss: 0.0\n",
            "training error 0.1120659895202908, test error 0.22473834318621816\n",
            "Loss: 0.0\n",
            "training error 0.11142722567170939, test error 0.2235734266770604\n",
            "Loss: 0.0\n",
            "training error 0.11085561011551033, test error 0.22233718686163895\n",
            "Loss: 0.0\n",
            "training error 0.11030790077042978, test error 0.2210946354336856\n",
            "Loss: 0.0\n",
            "training error 0.10962840876802574, test error 0.21987163861941625\n",
            "Loss: 0.0\n",
            "training error 0.10916671903455535, test error 0.21852795178718556\n",
            "Loss: 0.0\n",
            "training error 0.10838578941186126, test error 0.21726522916175225\n",
            "Loss: 0.0\n",
            "training error 0.10775173768619935, test error 0.21597287740626606\n",
            "Loss: 0.0\n",
            "training error 0.1071756094449151, test error 0.2145488017600945\n",
            "Loss: 0.0\n",
            "training error 0.10633673173624253, test error 0.2129344907477595\n",
            "Loss: 0.0\n",
            "training error 0.10562829497253642, test error 0.21150586659199005\n",
            "Loss: 0.0\n",
            "training error 0.10496114242459245, test error 0.2098147742755973\n",
            "Loss: 0.0\n",
            "training error 0.10416725821935997, test error 0.20824330425259793\n",
            "Loss: 0.0\n",
            "training error 0.10348870986983903, test error 0.20662459268228228\n",
            "Loss: 0.0\n",
            "training error 0.1026026417750573, test error 0.20489132338684377\n",
            "Loss: 0.0\n",
            "training error 0.10191770429593414, test error 0.20307264447863035\n",
            "Loss: 0.0\n",
            "training error 0.10119481629211241, test error 0.20146535201043486\n",
            "Loss: 0.0\n",
            "training error 0.10015931373099853, test error 0.1995965352549561\n",
            "Loss: 0.0\n",
            "training error 0.09936447612415104, test error 0.1977665622956286\n",
            "Loss: 0.0\n",
            "training error 0.09855273614164027, test error 0.19612498754952662\n",
            "Loss: 0.0\n",
            "training error 0.09779171540684496, test error 0.19411447950386287\n",
            "Loss: 0.0\n",
            "training error 0.09671045994362276, test error 0.19234209298768257\n",
            "Loss: 0.0\n",
            "training error 0.09578065621580654, test error 0.19045417973761333\n",
            "Loss: 0.0\n",
            "training error 0.09494038616711153, test error 0.18858884278916035\n",
            "Loss: 0.0\n",
            "training error 0.09395802176872077, test error 0.18661287134095206\n",
            "Loss: 0.0\n",
            "training error 0.09322729046501506, test error 0.184432621226118\n",
            "Loss: 0.0\n",
            "training error 0.09208686623211995, test error 0.18261548448877774\n",
            "Loss: 0.0\n",
            "training error 0.09123022171589557, test error 0.18053947157726088\n",
            "Loss: 0.0\n",
            "training error 0.09031544476589762, test error 0.17859720260822848\n",
            "Loss: 0.0\n",
            "training error 0.08930816193736589, test error 0.17668525331048382\n",
            "Loss: 0.0\n",
            "training error 0.08837806065554023, test error 0.17460020504004056\n",
            "Loss: 0.0\n",
            "training error 0.08741627274815772, test error 0.1726161342742848\n",
            "Loss: 0.0\n",
            "training error 0.08640000943258463, test error 0.1704973820180211\n",
            "Loss: 0.0\n",
            "training error 0.08540281695582119, test error 0.16834746824072427\n",
            "Loss: 0.0\n",
            "training error 0.0844854241607599, test error 0.16630401300055506\n",
            "Loss: 0.0\n",
            "training error 0.08357230015549781, test error 0.16433469557889455\n",
            "Loss: 0.0\n",
            "training error 0.08255443744544083, test error 0.16218482043455876\n",
            "Loss: 0.0\n",
            "training error 0.08174906128329781, test error 0.16024842970264896\n",
            "Loss: 0.0\n",
            "training error 0.08075109677149278, test error 0.15829748991514317\n",
            "Loss: 0.0\n",
            "training error 0.07975388366773903, test error 0.15612829127442104\n",
            "Loss: 0.0\n",
            "training error 0.07879890801192783, test error 0.15406770196192063\n",
            "Loss: 0.0\n",
            "training error 0.0778706633609402, test error 0.15211862182390148\n",
            "Loss: 0.0\n",
            "training error 0.07698696487314159, test error 0.14996435764440258\n",
            "Loss: 0.0\n",
            "training error 0.07602195533620854, test error 0.14807531398910428\n",
            "Loss: 0.0\n",
            "training error 0.07527569330264204, test error 0.14607016686466148\n",
            "Loss: 0.0\n",
            "training error 0.07428948688335582, test error 0.14405201474540055\n",
            "Loss: 0.0\n",
            "training error 0.07359536545816975, test error 0.1423396607571501\n",
            "Loss: 0.0\n",
            "training error 0.07244086863755898, test error 0.14015678155302635\n",
            "Loss: 0.0\n",
            "training error 0.07157910179166739, test error 0.1381894318764772\n",
            "Loss: 0.0\n",
            "training error 0.07069500644671194, test error 0.1364572965038355\n",
            "Loss: 0.0\n",
            "training error 0.06991437133581326, test error 0.1344537323104299\n",
            "Loss: 0.0\n",
            "training error 0.06900457831504367, test error 0.1326933587299899\n",
            "Loss: 0.0\n",
            "training error 0.06818451605788306, test error 0.13084828940527626\n",
            "Loss: 0.0\n",
            "training error 0.06736168026784253, test error 0.1291333738383923\n",
            "Loss: 0.0\n",
            "training error 0.0665751452236569, test error 0.12737432839136306\n",
            "Loss: 0.0\n",
            "training error 0.06576643406722482, test error 0.12561343483785586\n",
            "Loss: 0.0\n",
            "training error 0.06502710268532198, test error 0.1238727826655834\n",
            "Loss: 0.0\n",
            "training error 0.06421619415292769, test error 0.1222083980209173\n",
            "Loss: 0.0\n",
            "training error 0.06354861215086756, test error 0.12061860876307705\n",
            "Loss: 0.0\n",
            "training error 0.06271952795332104, test error 0.11883911333877427\n",
            "Loss: 0.0\n",
            "training error 0.06199520975428302, test error 0.11728061978362098\n",
            "Loss: 0.0\n",
            "training error 0.06133520538903974, test error 0.11554617412172348\n",
            "Loss: 0.0\n",
            "training error 0.06068598882540364, test error 0.11386466696049442\n",
            "Loss: 0.0\n",
            "training error 0.05989293052807336, test error 0.11235623629930243\n",
            "Loss: 0.0\n",
            "training error 0.059180789847026, test error 0.11076263849176239\n",
            "Loss: 0.0\n",
            "training error 0.058509268015325716, test error 0.10910444170355298\n",
            "Loss: 0.0\n",
            "training error 0.05789293559296949, test error 0.10777482084569152\n",
            "Loss: 0.0\n",
            "training error 0.05719092065897674, test error 0.10636384831479674\n",
            "Loss: 0.0\n",
            "training error 0.05657492735636952, test error 0.10484303291554334\n",
            "Loss: 0.0\n",
            "training error 0.05597570031311673, test error 0.1036360560223332\n",
            "Loss: 0.0\n",
            "training error 0.05533769771017779, test error 0.10206677479978953\n",
            "Loss: 0.0\n",
            "training error 0.05475103513579929, test error 0.10072163194394375\n",
            "Loss: 0.0\n",
            "training error 0.05417851436184571, test error 0.0995066396787509\n",
            "Loss: 0.0\n",
            "training error 0.05364454852284185, test error 0.0982245838198435\n",
            "Loss: 0.0\n",
            "training error 0.053065488822867185, test error 0.09707403945958701\n",
            "Loss: 0.0\n",
            "training error 0.05250182475058328, test error 0.09576585454358476\n",
            "Loss: 0.0\n",
            "training error 0.051972277310959096, test error 0.09456127665581936\n",
            "Loss: 0.0\n",
            "training error 0.05150614993930683, test error 0.09339303192033728\n",
            "Loss: 0.0\n",
            "training error 0.050944955214803175, test error 0.09207231677328467\n",
            "Loss: 0.0\n",
            "training error 0.05044256617344194, test error 0.09095116090403074\n",
            "Loss: 0.0\n",
            "training error 0.04996645253257893, test error 0.08980206320588416\n",
            "Loss: 0.0\n",
            "training error 0.0495112672638493, test error 0.08877686599023125\n",
            "Loss: 0.0\n",
            "training error 0.04904489719111888, test error 0.08760010155460732\n",
            "Loss: 0.0\n",
            "training error 0.04863013888881894, test error 0.086607246468687\n",
            "Loss: 0.0\n",
            "training error 0.048101756306678276, test error 0.08568820215682389\n",
            "Loss: 0.0\n",
            "training error 0.047682592078085874, test error 0.08458813040943662\n",
            "Loss: 0.0\n",
            "training error 0.0473871834312677, test error 0.08377033083156774\n",
            "Loss: 0.0\n",
            "training error 0.04686557820343642, test error 0.0826581542513616\n",
            "Loss: 0.0\n",
            "training error 0.04646945790085842, test error 0.08169899625184746\n",
            "Loss: 0.0\n",
            "training error 0.0460653906946263, test error 0.08080250925208357\n",
            "Loss: 0.0\n",
            "training error 0.045689577695085484, test error 0.07986988901467622\n",
            "Loss: 0.0\n",
            "training error 0.04536488414660271, test error 0.07880943472209176\n",
            "Loss: 0.0\n",
            "training error 0.0449728194388804, test error 0.07799652256860425\n",
            "Loss: 0.0\n",
            "training error 0.044579825122015256, test error 0.07711833787364798\n",
            "Loss: 0.0\n",
            "training error 0.044262473599992594, test error 0.07616740382502184\n",
            "Loss: 0.0\n",
            "training error 0.0438892251319405, test error 0.07536211089113179\n",
            "Loss: 0.0\n",
            "training error 0.043588530154540436, test error 0.07465659636928085\n",
            "Loss: 0.0\n",
            "training error 0.043255329427141435, test error 0.07394971446402804\n",
            "Loss: 0.0\n",
            "training error 0.04293521972064069, test error 0.07321351222761277\n",
            "Loss: 0.0\n",
            "training error 0.04260899112790104, test error 0.07241811454930941\n",
            "Loss: 0.0\n",
            "training error 0.04232836727802055, test error 0.07179250507299946\n",
            "Loss: 0.0\n",
            "training error 0.042022291808546275, test error 0.07095296323837857\n",
            "Loss: 0.0\n",
            "training error 0.041743070628760315, test error 0.07020814635378392\n",
            "Loss: 0.0\n",
            "training error 0.04147169341640947, test error 0.06968740118343261\n",
            "Loss: 0.0\n",
            "training error 0.04120810149008489, test error 0.06889160453367761\n",
            "Loss: 0.0\n",
            "training error 0.04089664647101149, test error 0.06831818963149572\n",
            "Loss: 0.0\n",
            "training error 0.04064809384243553, test error 0.06771639118584062\n",
            "Loss: 0.0\n",
            "training error 0.0404131494820888, test error 0.06709339001940896\n",
            "Loss: 0.0\n",
            "training error 0.04014557962704486, test error 0.06654730777009926\n",
            "Loss: 0.0\n",
            "training error 0.03991605993120664, test error 0.06594821548616397\n",
            "Loss: 0.0\n",
            "training error 0.03971917588434607, test error 0.06531230186355137\n",
            "Loss: 0.0\n",
            "training error 0.03943914065323403, test error 0.06495033584742851\n",
            "Loss: 0.0\n",
            "training error 0.039227271738696, test error 0.06446230606706\n",
            "Loss: 0.0\n",
            "training error 0.03902530087370595, test error 0.06382988441676621\n",
            "Loss: 0.0\n",
            "training error 0.03882284103807355, test error 0.06346316448031238\n",
            "Loss: 0.0\n",
            "training error 0.03857321415771895, test error 0.06277980456875083\n",
            "Loss: 0.0\n",
            "training error 0.03840763749697015, test error 0.06219804998710875\n",
            "Loss: 0.0\n",
            "training error 0.03820041037248621, test error 0.06192251922906561\n",
            "Loss: 0.0\n",
            "training error 0.038011937670854326, test error 0.061487557160052934\n",
            "Loss: 0.0\n",
            "training error 0.037798100046481, test error 0.06084847759279027\n",
            "Loss: 0.0\n",
            "training error 0.03764481231350338, test error 0.060324212312640804\n",
            "Loss: 0.0\n",
            "training error 0.03744878381847846, test error 0.059909483427166184\n",
            "Loss: 0.0\n",
            "training error 0.0372563247938168, test error 0.05955100523609903\n",
            "Loss: 0.0\n",
            "training error 0.037135761935456264, test error 0.0592945732003765\n",
            "Loss: 0.0\n",
            "training error 0.03690691842212607, test error 0.05878508328904852\n",
            "Loss: 0.0\n",
            "training error 0.03679516065680216, test error 0.058442268368372484\n",
            "Loss: 0.0\n",
            "training error 0.036601336783668406, test error 0.05801195502670303\n",
            "Loss: 0.0\n",
            "training error 0.03647516774733686, test error 0.057687857553715534\n",
            "Loss: 0.0\n",
            "training error 0.03626439584410476, test error 0.0573167152347387\n",
            "Loss: 0.0\n",
            "training error 0.03611391613645898, test error 0.05702941756728085\n",
            "Loss: 0.0\n",
            "training error 0.03597905099259319, test error 0.05666772990595937\n",
            "Loss: 0.0\n",
            "training error 0.03584632518990037, test error 0.05641423739990681\n",
            "Loss: 0.0\n",
            "training error 0.03571382903391662, test error 0.05595378690083425\n",
            "Loss: 0.0\n",
            "training error 0.03554749905093334, test error 0.055673705679732\n",
            "Loss: 0.0\n",
            "training error 0.03542805387863203, test error 0.05538138037448495\n",
            "Loss: 0.0\n",
            "training error 0.03528796501000879, test error 0.055216954904535986\n",
            "Loss: 0.0\n",
            "training error 0.03513889559930023, test error 0.05488648138455719\n",
            "Loss: 0.0\n",
            "training error 0.03503449881331441, test error 0.054540836852254165\n",
            "Loss: 0.0\n",
            "training error 0.03491156319139361, test error 0.0543631060769331\n",
            "Loss: 0.0\n",
            "training error 0.03476467427154053, test error 0.05393061658608111\n",
            "Loss: 0.0\n",
            "training error 0.03464481564001876, test error 0.05366994862343875\n",
            "Loss: 0.0\n",
            "training error 0.034554798685516364, test error 0.05318676499804547\n",
            "Loss: 0.0\n",
            "training error 0.03443084566332574, test error 0.052981235304121875\n",
            "Loss: 0.0\n",
            "training error 0.034307953252222835, test error 0.05267044959803437\n",
            "Loss: 0.0\n",
            "training error 0.03419787169601994, test error 0.052597989012683234\n",
            "Loss: 0.0\n",
            "training error 0.03409277217988778, test error 0.05234240355722766\n",
            "Loss: 0.0\n",
            "training error 0.0339898210908209, test error 0.052027131240231624\n",
            "Loss: 0.0\n",
            "training error 0.033873671771797904, test error 0.05189856287242666\n",
            "Loss: 0.0\n",
            "training error 0.03375323299499126, test error 0.051656315446290756\n",
            "Loss: 0.0\n",
            "training error 0.03366289159900627, test error 0.05139229625160641\n",
            "Loss: 0.0\n",
            "training error 0.033569462442156056, test error 0.0512878579426035\n",
            "Loss: 0.0\n",
            "training error 0.03346578916754229, test error 0.051138857612117686\n",
            "Loss: 0.0\n",
            "training error 0.03341318541654804, test error 0.050590558398878174\n",
            "Loss: 0.0\n",
            "training error 0.03328670934391831, test error 0.0506075919174675\n",
            "Loss: 0.033669362680321946\n",
            "training error 0.033166787028032364, test error 0.05034173851627827\n",
            "Loss: 0.0\n",
            "training error 0.033093752524830135, test error 0.05021428150517022\n",
            "Loss: 0.0\n",
            "training error 0.03302577757622311, test error 0.05003292614787656\n",
            "Loss: 0.0\n",
            "training error 0.032894547986379495, test error 0.04990160205843264\n",
            "Loss: 0.0\n",
            "training error 0.03285431370843163, test error 0.04960899038466045\n",
            "Loss: 0.0\n",
            "training error 0.032794037385565335, test error 0.04941394732879253\n",
            "Loss: 0.0\n",
            "training error 0.03267013802837885, test error 0.04934187626508799\n",
            "Loss: 0.0\n",
            "training error 0.03256882288744642, test error 0.04909963245184363\n",
            "Loss: 0.0\n",
            "training error 0.03248558541251062, test error 0.048917117213220895\n",
            "Loss: 0.0\n",
            "training error 0.03241151857676885, test error 0.04863793295117437\n",
            "Loss: 0.0\n",
            "training error 0.03233641387821623, test error 0.048531680936582576\n",
            "Loss: 0.0\n",
            "training error 0.032265602913730075, test error 0.04835980952478113\n",
            "Loss: 0.0\n",
            "training error 0.03217626214322287, test error 0.04810774079900358\n",
            "Loss: 0.0\n",
            "training error 0.03214527437429608, test error 0.04806142565225879\n",
            "Loss: 0.0\n",
            "training error 0.03205991996841745, test error 0.04777501920974016\n",
            "Loss: 0.0\n",
            "training error 0.03196919386085074, test error 0.0478143165319873\n",
            "Loss: 0.0822549585477228\n",
            "training error 0.03188447450725876, test error 0.047746365628489426\n",
            "Loss: 0.0\n",
            "training error 0.031803400215917876, test error 0.04760272926834277\n",
            "Loss: 0.0\n",
            "training error 0.031780504394483836, test error 0.04754171622492156\n",
            "Loss: 0.0\n",
            "training error 0.031671304329767884, test error 0.04741641037275299\n",
            "Loss: 0.0\n",
            "training error 0.031622421702642595, test error 0.04731725395491044\n",
            "Loss: 0.0\n",
            "training error 0.03153037488484305, test error 0.0471994276685581\n",
            "Loss: 0.0\n",
            "training error 0.031476423588699724, test error 0.04699536985488065\n",
            "Loss: 0.0\n",
            "training error 0.03142551618514286, test error 0.04685684128698676\n",
            "Loss: 0.0\n",
            "training error 0.031378366851369174, test error 0.0466000135973072\n",
            "Loss: 0.0\n",
            "training error 0.03129656370499257, test error 0.04658996334105962\n",
            "Loss: 0.0\n",
            "training error 0.03125929170000912, test error 0.046425748600092374\n",
            "Loss: 0.0\n",
            "training error 0.031158533962237906, test error 0.046426911236229246\n",
            "Loss: 0.0025042916311113217\n",
            "training error 0.031098595357071317, test error 0.04639171710058978\n",
            "Loss: 0.0\n",
            "training error 0.03104798968499619, test error 0.04621936774567386\n",
            "Loss: 0.0\n",
            "training error 0.030980144505417476, test error 0.046067126276585976\n",
            "Loss: 0.0\n",
            "training error 0.030935674605568737, test error 0.04599103898013549\n",
            "Loss: 0.0\n",
            "training error 0.03090030791946337, test error 0.0457911147424349\n",
            "Loss: 0.0\n",
            "training error 0.03083662298554314, test error 0.04570948600784925\n",
            "Loss: 0.0\n",
            "training error 0.030802866358499673, test error 0.04569688416771039\n",
            "Loss: 0.0\n",
            "training error 0.030753831939630445, test error 0.04574878814955919\n",
            "Loss: 0.11358319674119599\n",
            "training error 0.03064502859537329, test error 0.045689866817350974\n",
            "Loss: 0.0\n",
            "training error 0.030613216762147552, test error 0.0455023198935315\n",
            "Loss: 0.0\n",
            "training error 0.030555564975176436, test error 0.045591249784529705\n",
            "Loss: 0.19544034503360397\n",
            "training error 0.03049237080280135, test error 0.04536524311341153\n",
            "Loss: 0.0\n",
            "training error 0.030579772302355538, test error 0.04538540124412022\n",
            "Loss: 0.0444351872165516\n",
            "training error 0.030373121790299305, test error 0.04528252147049297\n",
            "Loss: 0.0\n",
            "training error 0.030365020891513862, test error 0.04521343829801996\n",
            "Loss: 0.0\n",
            "training error 0.030266646893793686, test error 0.045350781921435594\n",
            "Loss: 0.30376726164984547\n",
            "training error 0.030201982988111352, test error 0.045288018881370656\n",
            "Loss: 0.16495224906167838\n",
            "training error 0.030151973305813512, test error 0.04523981221201293\n",
            "Loss: 0.05833202469391896\n",
            "training error 0.03010282649346668, test error 0.045228501058945506\n",
            "Loss: 0.03331478757766515\n",
            "training error 0.030073044573137083, test error 0.04522016382725761\n",
            "Loss: 0.014875066995179154\n",
            "training error 0.03001439149609116, test error 0.04530672418924893\n",
            "Loss: 0.20632337362640474\n",
            "training error 0.02997903685098542, test error 0.04514140110208359\n",
            "Loss: 0.0\n",
            "training error 0.029920614537404026, test error 0.04512426462299111\n",
            "Loss: 0.0\n",
            "training error 0.029861639594638896, test error 0.045119874854143614\n",
            "Loss: 0.0\n",
            "training error 0.029810639932342547, test error 0.04498913528713152\n",
            "Loss: 0.0\n",
            "training error 0.02980488374552855, test error 0.044950224124391355\n",
            "Loss: 0.0\n",
            "training error 0.029738367350170453, test error 0.044962478893593553\n",
            "Loss: 0.027262976861441857\n",
            "training error 0.029677220839995484, test error 0.044735261125592736\n",
            "Loss: 0.0\n",
            "training error 0.029629445299038618, test error 0.044634267854845096\n",
            "Loss: 0.0\n",
            "training error 0.02972261783393777, test error 0.044402992525447015\n",
            "Loss: 0.0\n",
            "training error 0.029538033525292818, test error 0.04462326052229445\n",
            "Loss: 0.4960656575594635\n",
            "training error 0.029500290593578483, test error 0.04456965783543172\n",
            "Loss: 0.37534702168822154\n",
            "training error 0.029521653795633068, test error 0.044663826338251715\n",
            "Loss: 0.5874239504358059\n",
            "training error 0.029389651705870976, test error 0.0445579186371628\n",
            "Loss: 0.34890916783816284\n",
            "training error 0.02935346765308873, test error 0.044529743669180624\n",
            "Loss: 0.28545630941645417\n",
            "training error 0.02936049011780645, test error 0.044450317255541084\n",
            "Loss: 0.10658004652941422\n",
            "training error 0.029274972887550797, test error 0.04457191611323945\n",
            "Loss: 0.38043289018330206\n",
            "training error 0.029241215558478573, test error 0.04467281320155174\n",
            "Loss: 0.6076632694296258\n",
            "training error 0.029190064216120293, test error 0.044497521260147756\n",
            "Loss: 0.21288820713281087\n",
            "training error 0.02913895505406991, test error 0.044537384393926994\n",
            "Loss: 0.3026639891510863\n",
            "training error 0.029098615060152957, test error 0.044466074152966835\n",
            "Loss: 0.1420661625084696\n",
            "training error 0.029075615191242958, test error 0.044534376391712455\n",
            "Loss: 0.29588966597273725\n",
            "training error 0.02903672677396419, test error 0.044469374909472247\n",
            "Loss: 0.14949979776068822\n",
            "training error 0.028990141503903322, test error 0.04416912921207444\n",
            "Loss: 0.0\n",
            "training error 0.029013302330940148, test error 0.04395854717606089\n",
            "Loss: 0.0\n",
            "training error 0.02890464868973623, test error 0.04416834234749543\n",
            "Loss: 0.47725683606938407\n",
            "training error 0.028891111070604807, test error 0.04400659616172325\n",
            "Loss: 0.10930521764040968\n",
            "training error 0.028963121545260644, test error 0.04422691478383244\n",
            "Loss: 0.6105015406826286\n",
            "training error 0.028801698761008084, test error 0.04401261778440098\n",
            "Loss: 0.12300362913162388\n",
            "training error 0.028770433125565906, test error 0.04383193032322542\n",
            "Loss: 0.0\n",
            "training error 0.028718857273534425, test error 0.043919195212727385\n",
            "Loss: 0.1990897705359096\n",
            "training error 0.02870480599955188, test error 0.0439462591763607\n",
            "Loss: 0.26083462966881577\n",
            "training error 0.028652923525627334, test error 0.04390591009934789\n",
            "Loss: 0.168780556952286\n",
            "training error 0.028607197313651796, test error 0.043891358648170696\n",
            "Loss: 0.13558226732668377\n",
            "training error 0.028597165483019566, test error 0.043926294287639084\n",
            "Loss: 0.21528589710242407\n",
            "training error 0.028567051874612694, test error 0.04397731492275409\n",
            "Loss: 0.33168650902797303\n",
            "training error 0.028503724662305985, test error 0.04391925892355977\n",
            "Loss: 0.19923512309492075\n",
            "training error 0.02848277564835022, test error 0.043781016714457104\n",
            "Loss: 0.0\n",
            "training error 0.02845913332483887, test error 0.0439324510584957\n",
            "Loss: 0.34589042329979947\n",
            "training error 0.028427920757326246, test error 0.04381995640396446\n",
            "Loss: 0.08894194888464657\n",
            "training error 0.028437758923555, test error 0.04379289989563529\n",
            "Loss: 0.027142314340689033\n",
            "training error 0.028344299466487314, test error 0.04372561640992565\n",
            "Loss: 0.0\n",
            "training error 0.02831750092727838, test error 0.04364946310239414\n",
            "Loss: 0.0\n",
            "training error 0.02828704029649071, test error 0.04395690610230653\n",
            "Loss: 0.7043454330496024\n",
            "training error 0.028256019465790903, test error 0.04373534924843904\n",
            "Loss: 0.1967633504298183\n",
            "training error 0.02820103548277291, test error 0.043667277273594374\n",
            "Loss: 0.04081189076357017\n",
            "training error 0.028191957046591807, test error 0.04351057183288958\n",
            "Loss: 0.0\n",
            "training error 0.028145816556907156, test error 0.04354238722695461\n",
            "Loss: 0.07312106627148651\n",
            "training error 0.028099396562456927, test error 0.04343081551438298\n",
            "Loss: 0.0\n",
            "training error 0.02820411056775849, test error 0.043566100591078225\n",
            "Loss: 0.3114955938380737\n",
            "training error 0.028035186667202793, test error 0.04331408074167331\n",
            "Loss: 0.0\n",
            "training error 0.028001321464249492, test error 0.04330783685533699\n",
            "Loss: 0.0\n",
            "training error 0.028039520623401728, test error 0.04319847535054652\n",
            "Loss: 0.0\n",
            "training error 0.027982043556130404, test error 0.04353269371649756\n",
            "Loss: 0.7736809302618441\n",
            "training error 0.027923319039751877, test error 0.043382194509631586\n",
            "Loss: 0.42529084092486347\n",
            "training error 0.027874995876729487, test error 0.04343087065090192\n",
            "Loss: 0.5379710706675533\n",
            "training error 0.027891125618497627, test error 0.04361796454150698\n",
            "Loss: 0.9710740658238404\n",
            "training error 0.02787040553808935, test error 0.043378231653997026\n",
            "Loss: 0.41611724023087326\n",
            "training error 0.027797396269113095, test error 0.043610289059418564\n",
            "Loss: 0.9533061190939218\n",
            "training error 0.027804711326320054, test error 0.043577702534441494\n",
            "Loss: 0.8778716860204483\n",
            "training error 0.027745740149890886, test error 0.04350307697204339\n",
            "Loss: 0.7051212317682243\n",
            "training error 0.027821796493008612, test error 0.043713105579774\n",
            "Loss: 1.1913157236485006\n",
            "training error 0.027644446713976898, test error 0.04345854907384053\n",
            "Loss: 0.6020437554417502\n",
            "training error 0.02762701244370508, test error 0.043460389085056166\n",
            "Loss: 0.6063031909905758\n",
            "training error 0.027614759216109848, test error 0.04351110514185255\n",
            "Loss: 0.7237056140734222\n",
            "training error 0.02756843086370034, test error 0.043497378004670094\n",
            "Loss: 0.691928712062273\n",
            "training error 0.027690792638028115, test error 0.04325328673884658\n",
            "Loss: 0.1268826917044663\n",
            "training error 0.02751334504816589, test error 0.04359823930354789\n",
            "Loss: 0.9254121812340976\n",
            "training error 0.027491923780102016, test error 0.043503656566790665\n",
            "Loss: 0.706462933628238\n",
            "training error 0.02750921731624792, test error 0.0434326949956013\n",
            "Loss: 0.5421942398525115\n",
            "training error 0.027439969509132793, test error 0.043549865366158506\n",
            "Loss: 0.8134315222019461\n",
            "training error 0.027403453047713865, test error 0.04352680439568374\n",
            "Loss: 0.7600477620399726\n",
            "training error 0.027394777542474893, test error 0.043509959788473544\n",
            "Loss: 0.7210542395290398\n",
            "training error 0.02734328092626474, test error 0.04356790236894238\n",
            "Loss: 0.8551853170697177\n",
            "training error 0.027330690551342463, test error 0.0434574626318568\n",
            "Loss: 0.5995287546809225\n",
            "training error 0.027321848598467282, test error 0.04325219125392154\n",
            "Loss: 0.1243467574703061\n",
            "training error 0.027275598996902736, test error 0.043334017673684215\n",
            "Loss: 0.31376645133374126\n",
            "training error 0.027251047209886515, test error 0.04348110636004465\n",
            "Loss: 0.6542615386414319\n",
            "training error 0.02724867811083062, test error 0.04333106168094757\n",
            "Loss: 0.3069236340522252\n",
            "training error 0.0272786196794903, test error 0.0436562668960126\n",
            "Loss: 1.0597400527476708\n",
            "training error 0.027162816805988352, test error 0.04343512362769707\n",
            "Loss: 0.5478162718248525\n",
            "training error 0.02712284245768094, test error 0.043493075493209604\n",
            "Loss: 0.6819688432808446\n",
            "training error 0.027091032520979854, test error 0.04356567775174615\n",
            "Loss: 0.8500355584771357\n",
            "training error 0.02706820516594061, test error 0.04363283135648326\n",
            "Loss: 1.00548920398702\n",
            "training error 0.02706356019399576, test error 0.04355497349670441\n",
            "Loss: 0.8252563157958193\n",
            "training error 0.027133448548650296, test error 0.043867126961201\n",
            "Loss: 1.5478592825985515\n",
            "training error 0.026983419879376924, test error 0.0435856637366478\n",
            "Loss: 0.8963010452552433\n",
            "training error 0.027026230610211484, test error 0.04328171250142987\n",
            "Loss: 0.19268539041690236\n",
            "training error 0.026956858202793782, test error 0.04334038301991248\n",
            "Loss: 0.32850156912809236\n",
            "training error 0.026971196220829666, test error 0.04324083381903433\n",
            "Loss: 0.09805547104169143\n",
            "training error 0.026963241102426358, test error 0.043549065984978784\n",
            "Loss: 0.8115810374956345\n",
            "training error 0.026868102265188272, test error 0.043322277148758276\n",
            "Loss: 0.28658835110990655\n",
            "training error 0.026874713220820117, test error 0.04351410132458118\n",
            "Loss: 0.7306414670273043\n",
            "training error 0.026841703925871412, test error 0.04361257946055047\n",
            "Loss: 0.9586081606898889\n",
            "training error 0.026795917133432646, test error 0.04350763550438204\n",
            "Loss: 0.7156737623880138\n",
            "training error 0.02678957230813944, test error 0.043419618688742306\n",
            "Loss: 0.5119239426883748\n",
            "training error 0.02691335731848943, test error 0.04321962374769384\n",
            "Loss: 0.04895635083346228\n",
            "training error 0.026796849997556934, test error 0.043486194185112334\n",
            "Loss: 0.6660393271545662\n",
            "training error 0.026729938905151917, test error 0.04347542385892995\n",
            "Loss: 0.6411071366200893\n",
            "training error 0.0266880789643419, test error 0.04362555335774969\n",
            "Loss: 0.9886413900896285\n",
            "training error 0.02667131922696318, test error 0.04347211534338283\n",
            "Loss: 0.633448265513481\n",
            "training error 0.02662071961434401, test error 0.0435880472996524\n",
            "Loss: 0.9018187469455263\n",
            "training error 0.02665548116187569, test error 0.04367816911206448\n",
            "Loss: 1.110441416335517\n",
            "training error 0.02670479350021791, test error 0.043376601912419675\n",
            "Loss: 0.41234455713470286\n",
            "training error 0.026567195513336715, test error 0.04346787518246187\n",
            "Loss: 0.6236327317786561\n",
            "training error 0.02652928823192194, test error 0.043525667471244804\n",
            "Loss: 0.7574158996195735\n",
            "training error 0.026705686648700162, test error 0.04374277039961663\n",
            "Loss: 1.2599867116912522\n",
            "training error 0.026548203178773388, test error 0.043582362437666756\n",
            "Loss: 0.888658879752291\n",
            "training error 0.026478754659674284, test error 0.043455858007470136\n",
            "Loss: 0.5958142152819201\n",
            "training error 0.026460810429919816, test error 0.043539348737184916\n",
            "Loss: 0.7890866144517394\n",
            "training error 0.026458468669107747, test error 0.04350582311734232\n",
            "Loss: 0.7114782739477299\n",
            "training error 0.02644667068799711, test error 0.043436981681612766\n",
            "Loss: 0.5521174743571722\n",
            "training error 0.026455349260998327, test error 0.043448909802715145\n",
            "Loss: 0.5797298403158901\n",
            "training error 0.026367340886796032, test error 0.04365311721899038\n",
            "Loss: 1.0524488763886453\n",
            "training error 0.026355673289000764, test error 0.0435942617586234\n",
            "Loss: 0.9162045763540405\n",
            "training error 0.026331394660148593, test error 0.043483927373946574\n",
            "Loss: 0.6607918938890123\n",
            "training error 0.026324832828545596, test error 0.04354721115884042\n",
            "Loss: 0.8072873069338238\n",
            "training error 0.026333374687588097, test error 0.043457872462054827\n",
            "Loss: 0.6004774691776849\n",
            "training error 0.026247467846532493, test error 0.04358436526557097\n",
            "Loss: 0.8932952190859211\n",
            "training error 0.026230537799018995, test error 0.04352207365794941\n",
            "Loss: 0.7490965937500205\n",
            "training error 0.026215221999812188, test error 0.04356762107050801\n",
            "Loss: 0.8545341403046081\n",
            "training error 0.026193140450433, test error 0.043648551986874125\n",
            "Loss: 1.0418808364770493\n",
            "training error 0.026180303955555848, test error 0.04349501642498209\n",
            "Loss: 0.6864618994749216\n",
            "training error 0.026183582404818596, test error 0.04366273202504116\n",
            "Loss: 1.0747061573985972\n",
            "training error 0.0261119872468539, test error 0.043553618443325295\n",
            "Loss: 0.8221195074521903\n",
            "training error 0.026148972931834626, test error 0.043474227414895115\n",
            "Loss: 0.638337492494645\n",
            "training error 0.02610282937841373, test error 0.043624474747292134\n",
            "Loss: 0.9861445185014528\n",
            "training error 0.026077662385201615, test error 0.0436333653210987\n",
            "Loss: 1.0067252768138912\n",
            "training error 0.026091456200924464, test error 0.04370131647387894\n",
            "Loss: 1.1640251635085974\n",
            "training error 0.026030481232795104, test error 0.04344182311060738\n",
            "Loss: 0.5633248814596925\n",
            "training error 0.026057620061784935, test error 0.04341833399839762\n",
            "Loss: 0.5089500174878525\n",
            "training error 0.02600172999303416, test error 0.04346156416406118\n",
            "Loss: 0.6090233772829912\n",
            "training error 0.025988945859271948, test error 0.04348498295278964\n",
            "Loss: 0.663235449673083\n",
            "training error 0.02595429333047211, test error 0.04366803927014077\n",
            "Loss: 1.0869918805787249\n",
            "training error 0.025967336264669842, test error 0.04345294684253891\n",
            "Loss: 0.5890751697309016\n",
            "training error 0.025921359273800373, test error 0.043514332292059546\n",
            "Loss: 0.7311761328378186\n",
            "training error 0.02597808187486877, test error 0.043695010216485905\n",
            "Loss: 1.149426830252942\n",
            "training error 0.025932742746661512, test error 0.0436749514237102\n",
            "Loss: 1.1029928007809886\n",
            "training error 0.025881864067490806, test error 0.04359550077832988\n",
            "Loss: 0.9190727787533826\n",
            "training error 0.025885745069133385, test error 0.04354283336887678\n",
            "Loss: 0.7971531762078765\n",
            "training error 0.02586588797823048, test error 0.043425478781624734\n",
            "Loss: 0.5254894512737529\n",
            "training error 0.025905691293635708, test error 0.04322324168424791\n",
            "Loss: 0.057331499550428155\n",
            "training error 0.025872399977849122, test error 0.04333989662487239\n",
            "Loss: 0.3273756149453577\n",
            "training error 0.025786100075555648, test error 0.04312755642224284\n",
            "Loss: 0.0\n",
            "training error 0.025807534218853154, test error 0.04330592720390775\n",
            "Loss: 0.41358888947604644\n",
            "training error 0.025786223847105928, test error 0.0432765896244049\n",
            "Loss: 0.3455637521007304\n",
            "training error 0.02571075392578578, test error 0.04320045045273316\n",
            "Loss: 0.16901961654549602\n",
            "training error 0.02573181246563819, test error 0.04312335820450543\n",
            "Loss: 0.0\n",
            "training error 0.025705735262135657, test error 0.04314670364270506\n",
            "Loss: 0.05413641045513007\n",
            "training error 0.025687706529419495, test error 0.04298246733693404\n",
            "Loss: 0.0\n",
            "training error 0.02565191310349818, test error 0.04299875064964675\n",
            "Loss: 0.03788361562648834\n",
            "training error 0.025684321828116783, test error 0.042968742605702055\n",
            "Loss: 0.0\n",
            "training error 0.025707647107109717, test error 0.04332913849712165\n",
            "Loss: 0.8387396734568897\n",
            "training error 0.025614042116959723, test error 0.04304691227682955\n",
            "Loss: 0.18192217502106534\n",
            "training error 0.025613499867368245, test error 0.0432695301242144\n",
            "Loss: 0.7000147090001851\n",
            "training error 0.025599791023454846, test error 0.04332296597578188\n",
            "Loss: 0.8243745304122863\n",
            "training error 0.025590367244246234, test error 0.043279565420663704\n",
            "Loss: 0.7233695847557842\n",
            "training error 0.02552951974911184, test error 0.04329102751363854\n",
            "Loss: 0.7500450057240204\n",
            "training error 0.025537915647659223, test error 0.04328746658048918\n",
            "Loss: 0.741757741695781\n",
            "training error 0.025562358595943344, test error 0.04316241709191469\n",
            "Loss: 0.4507334272959129\n",
            "training error 0.025497828278176538, test error 0.04333684361162486\n",
            "Loss: 0.8566715793865365\n",
            "training error 0.025505867049699482, test error 0.04321258200413329\n",
            "Loss: 0.5674808794588149\n",
            "training error 0.025449262026973198, test error 0.04351240754651385\n",
            "Loss: 1.2652568072579529\n",
            "training error 0.02542665223226541, test error 0.04335701861667747\n",
            "Loss: 0.9036243264979538\n",
            "training error 0.025477484706724587, test error 0.043329379324225495\n",
            "Loss: 0.8393001439040981\n",
            "training error 0.02539504250359043, test error 0.04349280949252819\n",
            "Loss: 1.2196467828606838\n",
            "training error 0.025382035962077113, test error 0.043591398359271435\n",
            "Loss: 1.4490900031288056\n",
            "training error 0.025366413446008915, test error 0.043508103404383186\n",
            "Loss: 1.2552398929391861\n",
            "training error 0.025410356936761726, test error 0.0433277710398369\n",
            "Loss: 0.8355572268647249\n",
            "training error 0.025361098921976417, test error 0.043211580150984294\n",
            "Loss: 0.5651492935472069\n",
            "training error 0.025424007450084017, test error 0.04309120441255414\n",
            "Loss: 0.2850020722641178\n",
            "training error 0.025349664659326906, test error 0.043524894071855474\n",
            "Loss: 1.294316362144654\n",
            "training error 0.025294832621048476, test error 0.043428231801483065\n",
            "Loss: 1.0693568578384216\n",
            "training error 0.025279728420379635, test error 0.043389654374062135\n",
            "Loss: 0.9795766476634604\n",
            "training error 0.025250533621692762, test error 0.04336805574316148\n",
            "Loss: 0.929310734371902\n",
            "training error 0.025283863171009616, test error 0.04354671003852261\n",
            "Loss: 1.3450880751252337\n",
            "training error 0.0252736116307268, test error 0.04353802630746826\n",
            "Loss: 1.324878661193729\n",
            "training error 0.025242624540848517, test error 0.043376716937602436\n",
            "Loss: 0.9494676994486806\n",
            "training error 0.02519858394874645, test error 0.04330026880079812\n",
            "Loss: 0.7715520049964564\n",
            "training error 0.025401122099382365, test error 0.0429634950535699\n",
            "Loss: 0.0\n",
            "training error 0.025155056892354814, test error 0.04332636738855559\n",
            "Loss: 0.8446061814413097\n",
            "training error 0.02515866103671427, test error 0.043335165121427426\n",
            "Loss: 0.8650834095180082\n",
            "training error 0.0251342827811997, test error 0.04339368713678839\n",
            "Loss: 1.0012967582877064\n",
            "training error 0.025162895394202214, test error 0.04348501321863235\n",
            "Loss: 1.2138634541072113\n",
            "training error 0.025140379100159295, test error 0.043309330462499525\n",
            "Loss: 0.8049517584600929\n",
            "training error 0.025138726967120016, test error 0.0436924797296711\n",
            "Loss: 1.6967536630626778\n",
            "training error 0.025074240520461352, test error 0.04358844281902204\n",
            "Loss: 1.454601783846754\n",
            "training error 0.02505862380712034, test error 0.04365708285635851\n",
            "Loss: 1.6143654093406345\n",
            "training error 0.02504109538284163, test error 0.04355483323645481\n",
            "Loss: 1.3763735518899978\n",
            "training error 0.025086456156849776, test error 0.04362667092206644\n",
            "Loss: 1.543579887226687\n",
            "training error 0.02503408756072202, test error 0.043655736241684215\n",
            "Loss: 1.61123108641692\n",
            "training error 0.025015496796419505, test error 0.04352192656945809\n",
            "Loss: 1.2997813962571936\n",
            "training error 0.025022995035405483, test error 0.04333463038320107\n",
            "Loss: 0.8638387756126686\n",
            "training error 0.025048749750409473, test error 0.0430089499379253\n",
            "Loss: 0.10579885155692637\n",
            "training error 0.025033700724176933, test error 0.043101987286464805\n",
            "Loss: 0.32234861880351406\n",
            "training error 0.024947622218747623, test error 0.04300780521780949\n",
            "Loss: 0.10313444980287745\n",
            "training error 0.024953110862446275, test error 0.043061316810006474\n",
            "Loss: 0.22768575115827971\n",
            "training error 0.02497182648124416, test error 0.04315029245889865\n",
            "Loss: 0.4347816794137316\n",
            "training error 0.024961348458803677, test error 0.04296181846542305\n",
            "Loss: 0.0\n",
            "training error 0.025023229092192286, test error 0.043026332334731125\n",
            "Loss: 0.1501655926412715\n",
            "training error 0.02488614980731509, test error 0.043211009349451\n",
            "Loss: 0.580028716029557\n",
            "training error 0.024919318023725347, test error 0.04330163043784024\n",
            "Loss: 0.7909627305247247\n",
            "training error 0.02487806333442853, test error 0.04338688159295363\n",
            "Loss: 0.9893974294237218\n",
            "training error 0.0249581202851721, test error 0.04349109550031555\n",
            "Loss: 1.231970744717148\n",
            "training error 0.024832245515045696, test error 0.04322367757261934\n",
            "Loss: 0.6095158830556446\n",
            "training error 0.02483109448666676, test error 0.04294681635378608\n",
            "Loss: 0.0\n",
            "training error 0.02492085954257312, test error 0.042826575474770624\n",
            "Loss: 0.0\n",
            "training error 0.024815375356982687, test error 0.0427091128981698\n",
            "Loss: 0.0\n",
            "training error 0.024858457238823, test error 0.04285651016256489\n",
            "Loss: 0.34511900246330107\n",
            "training error 0.02479002676174211, test error 0.042763142105837704\n",
            "Loss: 0.1265051039498699\n",
            "training error 0.024766853112079133, test error 0.04290587442789924\n",
            "Loss: 0.46070151398032166\n",
            "training error 0.024798590876476123, test error 0.04319423973995903\n",
            "Loss: 1.1358860179229158\n",
            "training error 0.024769563678717525, test error 0.0431848635604054\n",
            "Loss: 1.113932437252707\n",
            "training error 0.02479213733612242, test error 0.0433272901819857\n",
            "Loss: 1.4474130738556967\n",
            "training error 0.024756427199206465, test error 0.04341283759357887\n",
            "Loss: 1.647715552151463\n",
            "training error 0.02471526758229983, test error 0.04294292436640555\n",
            "Loss: 0.547451006049271\n",
            "training error 0.02468158385513777, test error 0.042983387327488125\n",
            "Loss: 0.6421918197464516\n",
            "training error 0.024784458662606975, test error 0.04329291201389392\n",
            "Loss: 1.36691932027726\n",
            "training error 0.024658291935884695, test error 0.04304213224956358\n",
            "Loss: 0.7797383949130099\n",
            "training error 0.024654195161738782, test error 0.04313546768656737\n",
            "Loss: 0.9982759169316235\n",
            "training error 0.024659425309468644, test error 0.043288055828903126\n",
            "Loss: 1.3555489483326078\n",
            "training error 0.024628811047673612, test error 0.04314114473087542\n",
            "Loss: 1.011568265853935\n",
            "training error 0.02474003471201814, test error 0.04306761637227147\n",
            "Loss: 0.8394074467349188\n",
            "training error 0.024604868658611768, test error 0.04330292669288566\n",
            "Loss: 1.3903678967335908\n",
            "training error 0.024575530917815537, test error 0.043284897390894805\n",
            "Loss: 1.3481537162755686\n",
            "training error 0.02464920657459278, test error 0.043127259265843\n",
            "Loss: 0.9790565509290028\n",
            "training error 0.02454195249784917, test error 0.04342193627502356\n",
            "Loss: 1.6690193930118058\n",
            "training error 0.024552591759816986, test error 0.04341982379245149\n",
            "Loss: 1.664073182639525\n",
            "training error 0.024547893453627716, test error 0.04341907070814364\n",
            "Loss: 1.6623098954702487\n",
            "training error 0.02452033957054224, test error 0.043474839462956924\n",
            "Loss: 1.792888011073468\n",
            "training error 0.02458000818689293, test error 0.04352107763752408\n",
            "Loss: 1.901151029032655\n",
            "training error 0.024516752919952655, test error 0.04359091175618519\n",
            "Loss: 2.064662078366819\n",
            "training error 0.02446336122653109, test error 0.043359951489463026\n",
            "Loss: 1.523886934493346\n",
            "training error 0.02458036848251409, test error 0.04347954674609905\n",
            "Loss: 1.8039097411509664\n",
            "training error 0.024481476298128337, test error 0.04323920326480662\n",
            "Loss: 1.2411645446738673\n",
            "training error 0.024453228671350004, test error 0.04332216350348777\n",
            "Loss: 1.4354093628206277\n",
            "training error 0.024472348824356574, test error 0.04332351647197378\n",
            "Loss: 1.4385772312079714\n",
            "training error 0.02440671274433192, test error 0.04325156398825732\n",
            "Loss: 1.2701061981335693\n",
            "training error 0.024427162288772085, test error 0.043327269824269585\n",
            "Loss: 1.447365407878265\n",
            "training error 0.024437344875597736, test error 0.043143442099409034\n",
            "Loss: 1.0169473720392785\n",
            "training error 0.02440176941389966, test error 0.04312448804636187\n",
            "Loss: 0.9725679603377335\n",
            "training error 0.0244143663180597, test error 0.042881690722512696\n",
            "Loss: 0.40407728616225214\n",
            "training error 0.024414601538854667, test error 0.04319140457769633\n",
            "Loss: 1.1292477103807874\n",
            "training error 0.02435411768144399, test error 0.04317102085030388\n",
            "Loss: 1.0815208296069967\n",
            "training error 0.02441476425136648, test error 0.043117194143014105\n",
            "Loss: 0.9554898642293974\n",
            "training error 0.024336305159055303, test error 0.043136416645545\n",
            "Loss: 1.0004978291026756\n",
            "training error 0.02433430646755066, test error 0.04331549256913236\n",
            "Loss: 1.4197898991916258\n",
            "training error 0.024307587129711768, test error 0.043452052717824804\n",
            "Loss: 1.7395346548788604\n",
            "training error 0.024314310280469948, test error 0.043327791783800876\n",
            "Loss: 1.4485875347169497\n",
            "training error 0.024304465279597776, test error 0.04326263128643812\n",
            "Loss: 1.2960193989232538\n",
            "training error 0.024285229709152357, test error 0.043342291242769185\n",
            "Loss: 1.4825368677384043\n",
            "training error 0.024256514986877444, test error 0.04350091966088047\n",
            "Loss: 1.8539527257298571\n",
            "training error 0.024280710621896868, test error 0.043329701402694626\n",
            "Loss: 1.4530587558783337\n",
            "training error 0.02423269785229893, test error 0.04343218660093057\n",
            "Loss: 1.6930197180277773\n",
            "training error 0.02420925425207057, test error 0.043620223035003444\n",
            "Loss: 2.13329211263642\n",
            "training error 0.024238612969315045, test error 0.043560231954533436\n",
            "Loss: 1.992827756439075\n",
            "training error 0.024272689171915152, test error 0.04353130809226447\n",
            "Loss: 1.925104827288271\n",
            "training error 0.024188213876950398, test error 0.04384397647930461\n",
            "Loss: 2.657193053483997\n",
            "training error 0.02417856048497441, test error 0.043797908803483186\n",
            "Loss: 2.5493292448133253\n",
            "training error 0.02415046178251065, test error 0.04378206776965494\n",
            "Loss: 2.5122387206762076\n",
            "training error 0.024142765920496568, test error 0.0437411071139432\n",
            "Loss: 2.416332594483883\n",
            "training error 0.024155237209469837, test error 0.04360173306955688\n",
            "Loss: 2.0899993252384563\n",
            "training error 0.02415355847648579, test error 0.04344787742852454\n",
            "Loss: 1.7297585461822917\n",
            "training error 0.024199381974563054, test error 0.0432763904409381\n",
            "Loss: 1.3282353677559255\n",
            "training error 0.02417886505768871, test error 0.04338677108960691\n",
            "Loss: 1.5866829007964434\n",
            "training error 0.024135943060177426, test error 0.04367925683568405\n",
            "Loss: 2.271515074141983\n",
            "training error 0.02425345610251445, test error 0.043562660180356635\n",
            "Loss: 1.9985132545878859\n",
            "training error 0.0240771056670074, test error 0.04362670522893239\n",
            "Loss: 2.1484696555285066\n",
            "training error 0.02412351083977871, test error 0.043534740610285165\n",
            "Loss: 1.9331417959532082\n",
            "training error 0.024158676613121453, test error 0.04390048285645209\n",
            "Loss: 2.789498253271705\n",
            "training error 0.02401864271029234, test error 0.043721030925717795\n",
            "Loss: 2.369325792274557\n",
            "training error 0.02404084690220466, test error 0.043690568118884006\n",
            "Loss: 2.2979995465002068\n",
            "training error 0.024011569838413797, test error 0.043602327807143695\n",
            "Loss: 2.0913918561211053\n",
            "training error 0.024042522480252952, test error 0.04379105876626168\n",
            "Loss: 2.5332904260304545\n",
            "training error 0.02401072552410406, test error 0.04377256852230542\n",
            "Loss: 2.4899969865241234\n",
            "training error 0.024011471144794412, test error 0.04393452422809892\n",
            "Loss: 2.8692034246902587\n",
            "training error 0.023997377797101718, test error 0.0437280489289148\n",
            "Loss: 2.385757890065321\n",
            "training error 0.023953946107140342, test error 0.04385312775026869\n",
            "Loss: 2.678620028532408\n",
            "training error 0.023980677836104813, test error 0.04372440133905026\n",
            "Loss: 2.3772173477383696\n",
            "training error 0.023950451007283137, test error 0.04379246799156632\n",
            "Loss: 2.536590015296092\n",
            "training error 0.023940126106557946, test error 0.043744216791406686\n",
            "Loss: 2.423613657593071\n",
            "training error 0.02400587033993573, test error 0.043834888473740646\n",
            "Loss: 2.6359142093515198\n",
            "training error 0.023979379328052178, test error 0.043542331404907035\n",
            "Loss: 1.9509150394293018\n",
            "training error 0.023890649554134225, test error 0.043786615716782704\n",
            "Loss: 2.522887378115213\n",
            "training error 0.023885045575560485, test error 0.04386046718440581\n",
            "Loss: 2.69580473137232\n",
            "training error 0.023883979535037687, test error 0.043894067573217704\n",
            "Loss: 2.7744773764633246\n",
            "training error 0.023967167542728783, test error 0.04384549683172738\n",
            "Loss: 2.6607528380817147\n",
            "training error 0.023897506278191545, test error 0.04368348376040753\n",
            "Loss: 2.2814120830860984\n",
            "training error 0.02385629646045452, test error 0.043695196992303266\n",
            "Loss: 2.3088376864313753\n",
            "training error 0.02389430122004996, test error 0.04365549582952194\n",
            "Loss: 2.2158805630277945\n",
            "training error 0.023952728835263157, test error 0.04360616388208589\n",
            "Loss: 2.100373721306026\n",
            "training error 0.023866275588717487, test error 0.04341556155001097\n",
            "Loss: 1.654093480062535\n",
            "training error 0.023818920305455717, test error 0.04354082314087349\n",
            "Loss: 1.9473835588360577\n",
            "training error 0.023808923936894273, test error 0.043653908371528674\n",
            "Loss: 2.212163656059829\n",
            "training error 0.023802095581225098, test error 0.043648837657074585\n",
            "Loss: 2.2002909803941373\n",
            "training error 0.023808558408982296, test error 0.043588826547304234\n",
            "Loss: 2.0597797271788565\n",
            "training error 0.02381949320096265, test error 0.04356233581669777\n",
            "Loss: 1.9977537828104408\n",
            "training error 0.02375750413068174, test error 0.043684955443219216\n",
            "Loss: 2.2848579116501178\n",
            "training error 0.023775134159548315, test error 0.043866436368586434\n",
            "Loss: 2.7097811026326024\n",
            "training error 0.02373775572139761, test error 0.04400760004074018\n",
            "Loss: 3.040304643335312\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9583333333333333 0.0427091128981698\n",
            "training error 0.12518026245133249, test error 0.24997815351255787\n",
            "training error 0.12515857963980676, test error 0.25009455526198066\n",
            "training error 0.12510679975802538, test error 0.24994307933701843\n",
            "training error 0.1251161138805924, test error 0.2501280692831848\n",
            "training error 0.12500374225567615, test error 0.2500707026333541\n",
            "training error 0.1251969872451516, test error 0.25005458734581343\n",
            "training error 0.12500503944659816, test error 0.2502209902401037\n",
            "training error 0.12501094534854407, test error 0.2501420119491789\n",
            "training error 0.12499068481410203, test error 0.2502628524485217\n",
            "training error 0.1249811833686222, test error 0.2503569675903951\n",
            "training error 0.12500993126447132, test error 0.25045684092407255\n",
            "training error 0.1249697094821362, test error 0.250446067648558\n",
            "training error 0.12504481941887458, test error 0.25040587876763437\n",
            "training error 0.12506751967491092, test error 0.2505593337314507\n",
            "training error 0.12497513542237237, test error 0.2505727150396696\n",
            "training error 0.12504032527327824, test error 0.2505176726591563\n",
            "training error 0.12496916788111041, test error 0.25049234079035654\n",
            "training error 0.12497982009023534, test error 0.25056460778400774\n",
            "training error 0.1250363028983098, test error 0.2506214294373027\n",
            "training error 0.12500259982201622, test error 0.25059133940077116\n",
            "training error 0.12508134386748798, test error 0.25046759667640955\n",
            "training error 0.12500227544009387, test error 0.25044170930622284\n",
            "training error 0.12500997563085192, test error 0.2505463113748622\n",
            "training error 0.12497151041439993, test error 0.2504571022889342\n",
            "training error 0.12522968650517924, test error 0.250626272393155\n",
            "training error 0.12497265542934151, test error 0.25046465345243635\n",
            "training error 0.12524522261000762, test error 0.25048877225475785\n",
            "training error 0.12504168741308513, test error 0.2504504657410837\n",
            "training error 0.12513182380026447, test error 0.2504234713126557\n",
            "training error 0.12500094192180736, test error 0.25052675039963745\n",
            "training error 0.12506069016338744, test error 0.25043702634067416\n",
            "training error 0.12500118521480766, test error 0.2504553940304284\n",
            "training error 0.12509201003740678, test error 0.2504236349474849\n",
            "training error 0.12497522658651283, test error 0.2504909570436588\n",
            "training error 0.12498043727665488, test error 0.25053534810172035\n",
            "training error 0.12499239825968289, test error 0.2504342886962992\n",
            "training error 0.12501397475624526, test error 0.25057116290612474\n",
            "training error 0.12496387143345587, test error 0.250569713191922\n",
            "training error 0.1250229874334213, test error 0.25049087891867694\n",
            "training error 0.12506172059001433, test error 0.25043742905538896\n",
            "training error 0.12496594669212166, test error 0.2504457477471543\n",
            "training error 0.1250460272008879, test error 0.2504250012953429\n",
            "training error 0.1250450515307826, test error 0.25048845738571485\n",
            "training error 0.1249703541790905, test error 0.2504829089215872\n",
            "training error 0.12495482763365254, test error 0.2504759230935738\n",
            "training error 0.12502516687787749, test error 0.25049939247275277\n",
            "training error 0.12496144641658623, test error 0.2504403559694127\n",
            "training error 0.12497904593373148, test error 0.2505230529953518\n",
            "training error 0.12497228418953901, test error 0.25045978620945925\n",
            "training error 0.12497354772345126, test error 0.250420219739713\n",
            "Loss: 0.19089962561085194\n",
            "training error 0.12509109692646403, test error 0.2504750006781673\n",
            "Loss: 0.212816991196485\n",
            "training error 0.1251957585791416, test error 0.25046224424344277\n",
            "Loss: 0.2077132552745331\n",
            "training error 0.12504911889653866, test error 0.2505564256620045\n",
            "Loss: 0.2453944020426535\n",
            "training error 0.12502207842487592, test error 0.2506013855870513\n",
            "Loss: 0.26338246763184525\n",
            "training error 0.12496914786681095, test error 0.25053060314279396\n",
            "Loss: 0.23506304208700257\n",
            "training error 0.12497740014347222, test error 0.250459248776032\n",
            "Loss: 0.20651479544171458\n",
            "training error 0.1250476126316769, test error 0.25054335678106693\n",
            "Loss: 0.24016565917357457\n",
            "training error 0.12498612288113754, test error 0.2505523863372064\n",
            "Loss: 0.24377830416595359\n",
            "training error 0.12495979979584354, test error 0.25050105573081616\n",
            "Loss: 0.2232413857098159\n",
            "training error 0.12500395193017594, test error 0.2504780761257477\n",
            "Loss: 0.21404745038284823\n",
            "training error 0.12500392137006874, test error 0.2505755777950812\n",
            "Loss: 0.2530569999139365\n",
            "training error 0.12509244598681385, test error 0.25058992030408944\n",
            "Loss: 0.2587953100309015\n",
            "training error 0.12498039534913663, test error 0.2506483913483144\n",
            "Loss: 0.2821890540705585\n",
            "training error 0.12496207863033218, test error 0.25059361064169594\n",
            "Loss: 0.2602717812403732\n",
            "training error 0.1250250636542565, test error 0.2505377520230881\n",
            "Loss: 0.23792324542333265\n",
            "training error 0.12495903706976226, test error 0.25046131753422496\n",
            "Loss: 0.20734248716995918\n",
            "training error 0.12499520243314947, test error 0.2505210462500392\n",
            "Loss: 0.2312394144114105\n",
            "training error 0.12499522438389367, test error 0.2505760977496878\n",
            "Loss: 0.25326502912121907\n",
            "training error 0.12497443142599714, test error 0.25060141258553015\n",
            "Loss: 0.2633932694827923\n",
            "training error 0.125009293129098, test error 0.25069227197848704\n",
            "Loss: 0.29974530339302863\n",
            "training error 0.12497937096317964, test error 0.25067568602336315\n",
            "Loss: 0.2931094104657639\n",
            "training error 0.12520088431385212, test error 0.25087469854695066\n",
            "Loss: 0.3727325487080435\n",
            "training error 0.12513653004685124, test error 0.25088178163926145\n",
            "Loss: 0.3755664308581652\n",
            "training error 0.1251244582043383, test error 0.2507429219752856\n",
            "Loss: 0.3200099160131753\n",
            "training error 0.12500789708258891, test error 0.2506606457354046\n",
            "Loss: 0.28709192520535165\n",
            "training error 0.12494589629641559, test error 0.2507495266385137\n",
            "Loss: 0.32265238294830745\n",
            "training error 0.12493980447745824, test error 0.25075849342304435\n",
            "Loss: 0.3262399135790517\n",
            "training error 0.12521240951080864, test error 0.2505932210208674\n",
            "Loss: 0.26011589741692376\n",
            "training error 0.12497560520906596, test error 0.2507551771835119\n",
            "Loss: 0.3249131156772167\n",
            "training error 0.125106234302441, test error 0.25056935957682114\n",
            "Loss: 0.25056914616876647\n",
            "training error 0.12498777186647385, test error 0.2506829153147137\n",
            "Loss: 0.29600178554962664\n",
            "training error 0.12493422712594615, test error 0.2507623874602442\n",
            "Loss: 0.3277978831816464\n",
            "training error 0.12495353846367718, test error 0.2507911086322185\n",
            "Loss: 0.3392889682920952\n",
            "training error 0.1251685953385908, test error 0.25072689875635407\n",
            "Loss: 0.313599168824652\n",
            "training error 0.12495767769851938, test error 0.2507678271614057\n",
            "Loss: 0.3299742591693189\n",
            "training error 0.1249788672936785, test error 0.25093464553134975\n",
            "Loss: 0.3967168032663615\n",
            "training error 0.12498417823025774, test error 0.2509638037295755\n",
            "Loss: 0.40838273868777364\n",
            "training error 0.12505050690804087, test error 0.25106782316139065\n",
            "Loss: 0.4499999869392868\n",
            "training error 0.1249217851993755, test error 0.2509124000503252\n",
            "Loss: 0.3878165844311221\n",
            "training error 0.12492347413436415, test error 0.2509610709160307\n",
            "Loss: 0.40728936432747886\n",
            "training error 0.12512087473533803, test error 0.25086295780006906\n",
            "Loss: 0.3680351804461468\n",
            "training error 0.1250019434531449, test error 0.25087936633646957\n",
            "Loss: 0.37460008972229897\n",
            "training error 0.12497375473384684, test error 0.2510657468646393\n",
            "Loss: 0.4491692791009694\n",
            "training error 0.12493770937470723, test error 0.2508786564520041\n",
            "Loss: 0.3743160712700311\n",
            "training error 0.12502387979101742, test error 0.2508117038515249\n",
            "Loss: 0.34752893211147384\n",
            "training error 0.12501607343412563, test error 0.250765187305441\n",
            "Loss: 0.3289180763089039\n",
            "training error 0.12494411258603297, test error 0.2507365374708569\n",
            "Loss: 0.3174555326529216\n",
            "training error 0.12495706952884297, test error 0.2508402071101637\n",
            "Loss: 0.3589328320371621\n",
            "training error 0.1249013596494739, test error 0.2508472487228481\n",
            "Loss: 0.3617501185581906\n",
            "training error 0.12496475621558882, test error 0.25094874380393306\n",
            "Loss: 0.4023573965649341\n",
            "training error 0.12495184083431145, test error 0.250869959919547\n",
            "Loss: 0.3708366660870155\n",
            "training error 0.12492691039820679, test error 0.25086331666988143\n",
            "Loss: 0.3681787610618992\n",
            "training error 0.1248940977922742, test error 0.2508930048342878\n",
            "Loss: 0.3800567312321812\n",
            "training error 0.12496999674528177, test error 0.250820439489182\n",
            "Loss: 0.35102398293673787\n",
            "training error 0.12501408307145967, test error 0.25103524370747415\n",
            "Loss: 0.43696523758638417\n",
            "training error 0.12500219857529343, test error 0.2509563493538756\n",
            "Loss: 0.40540030936040417\n",
            "training error 0.12492163269859984, test error 0.25096836127393174\n",
            "Loss: 0.4102061715943073\n",
            "training error 0.12489120214127364, test error 0.25094194201524894\n",
            "Loss: 0.3996360614904937\n",
            "training error 0.12497809468349373, test error 0.25092909732569535\n",
            "Loss: 0.3944970155974614\n",
            "training error 0.12492544109811293, test error 0.25094764605525766\n",
            "Loss: 0.4019181970966734\n",
            "training error 0.12489296192020477, test error 0.2509078888193274\n",
            "Loss: 0.3860116810868064\n",
            "training error 0.1250724874280189, test error 0.2509734787235075\n",
            "Loss: 0.41225361759256174\n",
            "training error 0.12485999782453484, test error 0.25083460892326115\n",
            "Loss: 0.3566930473160257\n",
            "training error 0.1249181228581953, test error 0.2508664487351134\n",
            "Loss: 0.3694318724664081\n",
            "training error 0.12486604438536643, test error 0.25082818897441433\n",
            "Loss: 0.3541244829597501\n",
            "training error 0.12495932004547512, test error 0.2508027357159539\n",
            "Loss: 0.3439408609415162\n",
            "training error 0.12488613543406901, test error 0.2507971665771126\n",
            "Loss: 0.34171269809095683\n",
            "training error 0.12496513967152785, test error 0.2506891006103355\n",
            "Loss: 0.2984764672404383\n",
            "training error 0.12493140877911238, test error 0.2506330677733141\n",
            "Loss: 0.2760582281877477\n",
            "training error 0.12486006705987772, test error 0.25074294834357497\n",
            "Loss: 0.32002046573091913\n",
            "training error 0.12502681198125024, test error 0.2508390545804516\n",
            "Loss: 0.35847171516401044\n",
            "training error 0.12497533813740978, test error 0.2506481097639503\n",
            "Loss: 0.2820763946743243\n",
            "training error 0.12497881134408073, test error 0.2506893455713147\n",
            "Loss: 0.29857447394654457\n",
            "training error 0.12484066469059055, test error 0.25070342032675513\n",
            "Loss: 0.3042056582456931\n",
            "training error 0.12482835430909352, test error 0.25073877305934467\n",
            "Loss: 0.31834997169628565\n",
            "training error 0.12482111141399184, test error 0.25073477185158466\n",
            "Loss: 0.316749124107063\n",
            "training error 0.12492388775197419, test error 0.25079896823547376\n",
            "Loss: 0.34243352555534123\n",
            "training error 0.12480643388969566, test error 0.2507017518479572\n",
            "Loss: 0.30353811473842196\n",
            "training error 0.12490209927378716, test error 0.2507602512017646\n",
            "Loss: 0.3269431851899096\n",
            "training error 0.12490557667822398, test error 0.2506433233570235\n",
            "Loss: 0.2801613958916205\n",
            "training error 0.12486160104754469, test error 0.2506452708869549\n",
            "Loss: 0.28094058527206034\n",
            "training error 0.12482404851905728, test error 0.25061633207267553\n",
            "Loss: 0.2693624234137326\n",
            "training error 0.12484882767519326, test error 0.25068179125921974\n",
            "Loss: 0.29555206095754905\n",
            "training error 0.12479758379260951, test error 0.2506828040350773\n",
            "Loss: 0.29595726355817753\n",
            "training error 0.12481780447742871, test error 0.25058475433704225\n",
            "Loss: 0.2567284526244551\n",
            "training error 0.12476429865196434, test error 0.25062649346585275\n",
            "Loss: 0.2734279063245637\n",
            "training error 0.12486954641960676, test error 0.25050226941904163\n",
            "Loss: 0.22372697155947918\n",
            "training error 0.12475596178391865, test error 0.2506365515184802\n",
            "Loss: 0.2774520436017669\n",
            "training error 0.12490571083148629, test error 0.2507876179345006\n",
            "Loss: 0.3378923712240134\n",
            "training error 0.12480427155577101, test error 0.2505762936082753\n",
            "Loss: 0.2533433903977311\n",
            "training error 0.12475591634256734, test error 0.250645848391467\n",
            "Loss: 0.28117163968399694\n",
            "training error 0.12491902687239075, test error 0.2504806396053704\n",
            "Loss: 0.21507307574903933\n",
            "training error 0.12480332698894367, test error 0.2505563573449728\n",
            "Loss: 0.2453670690067078\n",
            "training error 0.12477274510520396, test error 0.25050940771832875\n",
            "Loss: 0.22658294152912983\n",
            "training error 0.12469479961346348, test error 0.2504287016386918\n",
            "Loss: 0.19429315785077783\n",
            "training error 0.12486190926939143, test error 0.25055940819689426\n",
            "Loss: 0.24658768768899275\n",
            "training error 0.12468169160464906, test error 0.2505194703628828\n",
            "Loss: 0.23060891599531175\n",
            "training error 0.12466111336285861, test error 0.25054562002927716\n",
            "Loss: 0.24107116462555034\n",
            "training error 0.12468671424857418, test error 0.25042445643820044\n",
            "Loss: 0.19259469094279336\n",
            "training error 0.12468289548671427, test error 0.2504629805239615\n",
            "Loss: 0.20800783455261485\n",
            "training error 0.12466452250825313, test error 0.25036757521477176\n",
            "Loss: 0.1698370200444499\n",
            "training error 0.12460457403694589, test error 0.2503972104774129\n",
            "Loss: 0.1816938246896438\n",
            "training error 0.12463536932495484, test error 0.25039078122572866\n",
            "Loss: 0.179121538350957\n",
            "training error 0.12478507202857145, test error 0.2504684171649507\n",
            "Loss: 0.21018298619259745\n",
            "training error 0.12456966707462934, test error 0.25030101375818564\n",
            "Loss: 0.1432063740739009\n",
            "training error 0.1246812054289548, test error 0.25035460869861026\n",
            "Loss: 0.16464923241059637\n",
            "training error 0.12452835054692128, test error 0.25035277112106996\n",
            "Loss: 0.16391403400255466\n",
            "training error 0.12460414565873038, test error 0.25031494808934884\n",
            "Loss: 0.1487813758703771\n",
            "training error 0.12455469370631053, test error 0.25030719708171045\n",
            "Loss: 0.1456802667462842\n",
            "training error 0.1245091133950166, test error 0.25016550956664924\n",
            "Loss: 0.08899235386745641\n",
            "training error 0.12446091647056257, test error 0.2501578366319207\n",
            "Loss: 0.0859224810192405\n",
            "training error 0.1244356083709825, test error 0.25009483659273196\n",
            "Loss: 0.06071672643070958\n",
            "training error 0.12441936936002022, test error 0.25001791542086926\n",
            "Loss: 0.02994125064368358\n",
            "training error 0.12439504382756705, test error 0.25006514045239864\n",
            "Loss: 0.04883556516306342\n",
            "training error 0.12435390451758746, test error 0.2500136219103981\n",
            "Loss: 0.028223455343034942\n",
            "training error 0.12434101470691054, test error 0.2499959310821119\n",
            "Loss: 0.021145512503761843\n",
            "training error 0.12430015293947069, test error 0.2499829956928998\n",
            "Loss: 0.015970178485136266\n",
            "training error 0.12428858824533144, test error 0.24990269276181987\n",
            "Loss: 0.0\n",
            "training error 0.12431153475179445, test error 0.24984638170558202\n",
            "Loss: 0.0\n",
            "training error 0.12421608006371962, test error 0.24975152237402048\n",
            "Loss: 0.0\n",
            "training error 0.12416929240442226, test error 0.24971777796326133\n",
            "Loss: 0.0\n",
            "training error 0.12427727976616816, test error 0.24971857623453297\n",
            "Loss: 0.00031966937963368025\n",
            "training error 0.12418402991778787, test error 0.24953972949318798\n",
            "Loss: 0.0\n",
            "training error 0.12413014637858162, test error 0.24956077626426046\n",
            "Loss: 0.00843423655032538\n",
            "training error 0.12405955000683705, test error 0.24930976711098957\n",
            "Loss: 0.0\n",
            "training error 0.12395721922894046, test error 0.249249066757962\n",
            "Loss: 0.0\n",
            "training error 0.12394750589875958, test error 0.24924342078146733\n",
            "Loss: 0.0\n",
            "training error 0.12392377773392328, test error 0.24923343933202505\n",
            "Loss: 0.0\n",
            "training error 0.12383406154578362, test error 0.24906760974528375\n",
            "Loss: 0.0\n",
            "training error 0.12379159782546634, test error 0.24903719923725184\n",
            "Loss: 0.0\n",
            "training error 0.1237160846642686, test error 0.248845303254439\n",
            "Loss: 0.0\n",
            "training error 0.12371042781165856, test error 0.2486801195340648\n",
            "Loss: 0.0\n",
            "training error 0.12361200714784804, test error 0.2485906685001095\n",
            "Loss: 0.0\n",
            "training error 0.12356037320121269, test error 0.24854408072137543\n",
            "Loss: 0.0\n",
            "training error 0.12357010236884748, test error 0.24834085201415496\n",
            "Loss: 0.0\n",
            "training error 0.1235677804372283, test error 0.2483752923357174\n",
            "Loss: 0.013868165983610403\n",
            "training error 0.12338792938934069, test error 0.24796455388437175\n",
            "Loss: 0.0\n",
            "training error 0.1232281224485954, test error 0.2478153295149617\n",
            "Loss: 0.0\n",
            "training error 0.12323240573480349, test error 0.2476311629694823\n",
            "Loss: 0.0\n",
            "training error 0.12310091189035055, test error 0.24754195120811096\n",
            "Loss: 0.0\n",
            "training error 0.1230339852871088, test error 0.24739195855753782\n",
            "Loss: 0.0\n",
            "training error 0.12297724825129532, test error 0.24709475071495063\n",
            "Loss: 0.0\n",
            "training error 0.12277552553349957, test error 0.24698742314768488\n",
            "Loss: 0.0\n",
            "training error 0.1226485854074255, test error 0.2467711836013998\n",
            "Loss: 0.0\n",
            "training error 0.12253664028769623, test error 0.24648099690608946\n",
            "Loss: 0.0\n",
            "training error 0.12241429414929815, test error 0.24629051682143097\n",
            "Loss: 0.0\n",
            "training error 0.12229417891074026, test error 0.24603760346458964\n",
            "Loss: 0.0\n",
            "training error 0.1221875870417151, test error 0.24569289657913296\n",
            "Loss: 0.0\n",
            "training error 0.12208667010852013, test error 0.2453934111000818\n",
            "Loss: 0.0\n",
            "training error 0.12186793863617601, test error 0.24509023427966092\n",
            "Loss: 0.0\n",
            "training error 0.12176493279612775, test error 0.244758849558222\n",
            "Loss: 0.0\n",
            "training error 0.12188752344393504, test error 0.24450984594099845\n",
            "Loss: 0.0\n",
            "training error 0.12140270574172819, test error 0.2440960391596598\n",
            "Loss: 0.0\n",
            "training error 0.12138738504650548, test error 0.24379179754950173\n",
            "Loss: 0.0\n",
            "training error 0.1210836386312136, test error 0.24335098046412942\n",
            "Loss: 0.0\n",
            "training error 0.12084201960429024, test error 0.24299948079459563\n",
            "Loss: 0.0\n",
            "training error 0.12064913580497194, test error 0.24262270486892662\n",
            "Loss: 0.0\n",
            "training error 0.12049856217728641, test error 0.24216186561176084\n",
            "Loss: 0.0\n",
            "training error 0.12023668422436232, test error 0.24175250187512473\n",
            "Loss: 0.0\n",
            "training error 0.12004194437501883, test error 0.24120095088654664\n",
            "Loss: 0.0\n",
            "training error 0.11976865334654815, test error 0.2407067457308618\n",
            "Loss: 0.0\n",
            "training error 0.1195405983494542, test error 0.24016509540892098\n",
            "Loss: 0.0\n",
            "training error 0.11924435002089838, test error 0.23965134327537277\n",
            "Loss: 0.0\n",
            "training error 0.11910798164143743, test error 0.23909619455819464\n",
            "Loss: 0.0\n",
            "training error 0.11872542700497718, test error 0.2385024115381479\n",
            "Loss: 0.0\n",
            "training error 0.11839224544098116, test error 0.23790816122020123\n",
            "Loss: 0.0\n",
            "training error 0.11808904676297553, test error 0.23728343815155398\n",
            "Loss: 0.0\n",
            "training error 0.11780342996092336, test error 0.2366892681394609\n",
            "Loss: 0.0\n",
            "training error 0.1174816217435216, test error 0.23603089650325199\n",
            "Loss: 0.0\n",
            "training error 0.11712122039440738, test error 0.2351589833954455\n",
            "Loss: 0.0\n",
            "training error 0.11682000740636339, test error 0.23438693927201176\n",
            "Loss: 0.0\n",
            "training error 0.11635894256219347, test error 0.23352349834675407\n",
            "Loss: 0.0\n",
            "training error 0.11607017734214266, test error 0.23284299886254511\n",
            "Loss: 0.0\n",
            "training error 0.11554866905058583, test error 0.2318307561016351\n",
            "Loss: 0.0\n",
            "training error 0.11514885303432651, test error 0.2310056070335251\n",
            "Loss: 0.0\n",
            "training error 0.11468618284621192, test error 0.2300300373140238\n",
            "Loss: 0.0\n",
            "training error 0.11420880901188105, test error 0.2290240543351158\n",
            "Loss: 0.0\n",
            "training error 0.11377341770239408, test error 0.22807598922198244\n",
            "Loss: 0.0\n",
            "training error 0.11332599305689656, test error 0.2269793201365497\n",
            "Loss: 0.0\n",
            "training error 0.11272683585437891, test error 0.2259831225050223\n",
            "Loss: 0.0\n",
            "training error 0.11237950335124808, test error 0.2248559914314817\n",
            "Loss: 0.0\n",
            "training error 0.11172842016652118, test error 0.223830991059772\n",
            "Loss: 0.0\n",
            "training error 0.11109803081283963, test error 0.2228027732199512\n",
            "Loss: 0.0\n",
            "training error 0.11059864896918531, test error 0.22148071412650808\n",
            "Loss: 0.0\n",
            "training error 0.1099379695983824, test error 0.2203995520125989\n",
            "Loss: 0.0\n",
            "training error 0.10931801373411171, test error 0.2191806607782423\n",
            "Loss: 0.0\n",
            "training error 0.10871685819132773, test error 0.21770876726519586\n",
            "Loss: 0.0\n",
            "training error 0.10809938012371972, test error 0.21630364917559392\n",
            "Loss: 0.0\n",
            "training error 0.1073473750698981, test error 0.2148201555821426\n",
            "Loss: 0.0\n",
            "training error 0.10679497966186811, test error 0.21326721794441533\n",
            "Loss: 0.0\n",
            "training error 0.10592616616809224, test error 0.21188683829413799\n",
            "Loss: 0.0\n",
            "training error 0.1053022439201303, test error 0.2104979301300591\n",
            "Loss: 0.0\n",
            "training error 0.10447192402344348, test error 0.20877553024253778\n",
            "Loss: 0.0\n",
            "training error 0.1036646655795161, test error 0.20711523645389238\n",
            "Loss: 0.0\n",
            "training error 0.10291012647814611, test error 0.205524374453309\n",
            "Loss: 0.0\n",
            "training error 0.10214817399019537, test error 0.20379274480362633\n",
            "Loss: 0.0\n",
            "training error 0.10129272193791726, test error 0.20206639493772688\n",
            "Loss: 0.0\n",
            "training error 0.10067395767601081, test error 0.20056783969932213\n",
            "Loss: 0.0\n",
            "training error 0.09967850731417117, test error 0.1985826374678607\n",
            "Loss: 0.0\n",
            "training error 0.09887970944423136, test error 0.19689712864385983\n",
            "Loss: 0.0\n",
            "training error 0.09795805244331769, test error 0.19509429166552747\n",
            "Loss: 0.0\n",
            "training error 0.09709829711973866, test error 0.1930373375098641\n",
            "Loss: 0.0\n",
            "training error 0.09625716103418745, test error 0.19105603700943177\n",
            "Loss: 0.0\n",
            "training error 0.09526635709043442, test error 0.18918646487232804\n",
            "Loss: 0.0\n",
            "training error 0.0943335015170278, test error 0.18747261288302403\n",
            "Loss: 0.0\n",
            "training error 0.09350658356057456, test error 0.1853515332387263\n",
            "Loss: 0.0\n",
            "training error 0.09248513105598254, test error 0.1832590337378005\n",
            "Loss: 0.0\n",
            "training error 0.0918314221716878, test error 0.18138200033141444\n",
            "Loss: 0.0\n",
            "training error 0.09069419495315123, test error 0.17908260356540578\n",
            "Loss: 0.0\n",
            "training error 0.0896282284377077, test error 0.1770639402427393\n",
            "Loss: 0.0\n",
            "training error 0.08862291281710755, test error 0.1751235066401451\n",
            "Loss: 0.0\n",
            "training error 0.08768311750196794, test error 0.17318044300480728\n",
            "Loss: 0.0\n",
            "training error 0.08674451087880015, test error 0.17117675208002758\n",
            "Loss: 0.0\n",
            "training error 0.08587064325039927, test error 0.1690986655168892\n",
            "Loss: 0.0\n",
            "training error 0.0848347285750347, test error 0.1671279363445796\n",
            "Loss: 0.0\n",
            "training error 0.08386688402485219, test error 0.16495166898019883\n",
            "Loss: 0.0\n",
            "training error 0.0828695472416751, test error 0.162904039263015\n",
            "Loss: 0.0\n",
            "training error 0.0819468076286268, test error 0.16078529397314745\n",
            "Loss: 0.0\n",
            "training error 0.08120224216504801, test error 0.15842435848167813\n",
            "Loss: 0.0\n",
            "training error 0.08015427936390047, test error 0.15662578503421215\n",
            "Loss: 0.0\n",
            "training error 0.07904919227533377, test error 0.1544911233053979\n",
            "Loss: 0.0\n",
            "training error 0.0781034242999791, test error 0.15241532562523494\n",
            "Loss: 0.0\n",
            "training error 0.07718034516094704, test error 0.15056245915689612\n",
            "Loss: 0.0\n",
            "training error 0.07625647429378699, test error 0.14867792436526361\n",
            "Loss: 0.0\n",
            "training error 0.07541980331161921, test error 0.14654914737502797\n",
            "Loss: 0.0\n",
            "training error 0.07438838728893703, test error 0.14457105267606982\n",
            "Loss: 0.0\n",
            "training error 0.07360611743416078, test error 0.14258436041584283\n",
            "Loss: 0.0\n",
            "training error 0.07270554619719119, test error 0.14069795015519615\n",
            "Loss: 0.0\n",
            "training error 0.07176483998315636, test error 0.13875610855898018\n",
            "Loss: 0.0\n",
            "training error 0.07083982357085093, test error 0.13703135580305884\n",
            "Loss: 0.0\n",
            "training error 0.06999459529856443, test error 0.13497133416046225\n",
            "Loss: 0.0\n",
            "training error 0.06921020860130538, test error 0.13310799723416114\n",
            "Loss: 0.0\n",
            "training error 0.0683249178699989, test error 0.13132099362156108\n",
            "Loss: 0.0\n",
            "training error 0.06750674826992793, test error 0.12930762110398084\n",
            "Loss: 0.0\n",
            "training error 0.06661256781724124, test error 0.12722345974748384\n",
            "Loss: 0.0\n",
            "training error 0.06581117889290888, test error 0.12540161331161265\n",
            "Loss: 0.0\n",
            "training error 0.06496974905020948, test error 0.12373812171741932\n",
            "Loss: 0.0\n",
            "training error 0.06431662641005276, test error 0.12193722838417391\n",
            "Loss: 0.0\n",
            "training error 0.06345951054427987, test error 0.12038232547247435\n",
            "Loss: 0.0\n",
            "training error 0.0627479077932819, test error 0.11848158088980947\n",
            "Loss: 0.0\n",
            "training error 0.06202790849809455, test error 0.11699981823081697\n",
            "Loss: 0.0\n",
            "training error 0.06123541736492133, test error 0.11540209101140386\n",
            "Loss: 0.0\n",
            "training error 0.06059518456173863, test error 0.11366604012679393\n",
            "Loss: 0.0\n",
            "training error 0.05978053102087441, test error 0.11223576795759681\n",
            "Loss: 0.0\n",
            "training error 0.059218445049995565, test error 0.1107284019092202\n",
            "Loss: 0.0\n",
            "training error 0.05841991392056623, test error 0.1093282662044228\n",
            "Loss: 0.0\n",
            "training error 0.057802106950741686, test error 0.1078214865035825\n",
            "Loss: 0.0\n",
            "training error 0.05717053007152193, test error 0.10641627305668505\n",
            "Loss: 0.0\n",
            "training error 0.05654503221525321, test error 0.10472390887413074\n",
            "Loss: 0.0\n",
            "training error 0.05588338836174902, test error 0.10319211461363081\n",
            "Loss: 0.0\n",
            "training error 0.05536977169889362, test error 0.10199990082364718\n",
            "Loss: 0.0\n",
            "training error 0.05466965895105076, test error 0.10050638687263823\n",
            "Loss: 0.0\n",
            "training error 0.05407827118349338, test error 0.0992649043626826\n",
            "Loss: 0.0\n",
            "training error 0.05350070060328001, test error 0.09786540083071697\n",
            "Loss: 0.0\n",
            "training error 0.052920660768940336, test error 0.0964357506513669\n",
            "Loss: 0.0\n",
            "training error 0.0523871265744033, test error 0.09512802803874414\n",
            "Loss: 0.0\n",
            "training error 0.05184620736764375, test error 0.09386087795542179\n",
            "Loss: 0.0\n",
            "training error 0.051381113305809446, test error 0.09263691491698282\n",
            "Loss: 0.0\n",
            "training error 0.05089743140925695, test error 0.09174693366866665\n",
            "Loss: 0.0\n",
            "training error 0.05029313932987073, test error 0.09030259761378107\n",
            "Loss: 0.0\n",
            "training error 0.04984349855108046, test error 0.08937765096444901\n",
            "Loss: 0.0\n",
            "training error 0.049357129373222924, test error 0.0882155227377227\n",
            "Loss: 0.0\n",
            "training error 0.048893034170422525, test error 0.08698333157390259\n",
            "Loss: 0.0\n",
            "training error 0.048503795264834774, test error 0.08607373276954264\n",
            "Loss: 0.0\n",
            "training error 0.047979845244680075, test error 0.08508358515953705\n",
            "Loss: 0.0\n",
            "training error 0.0476299432794394, test error 0.0838716965471664\n",
            "Loss: 0.0\n",
            "training error 0.0471168678371988, test error 0.08301744238274529\n",
            "Loss: 0.0\n",
            "training error 0.04670926908720839, test error 0.08215944513954979\n",
            "Loss: 0.0\n",
            "training error 0.04633255703899359, test error 0.08115720919750828\n",
            "Loss: 0.0\n",
            "training error 0.04605574321673251, test error 0.08028096645788844\n",
            "Loss: 0.0\n",
            "training error 0.045591558272895544, test error 0.07921063802100496\n",
            "Loss: 0.0\n",
            "training error 0.04514447124487406, test error 0.07847841913645925\n",
            "Loss: 0.0\n",
            "training error 0.044834819655265645, test error 0.0776409048123649\n",
            "Loss: 0.0\n",
            "training error 0.04442712515468729, test error 0.07680412992710552\n",
            "Loss: 0.0\n",
            "training error 0.0441101007045085, test error 0.07606796235009855\n",
            "Loss: 0.0\n",
            "training error 0.04376938883505696, test error 0.07516268573017065\n",
            "Loss: 0.0\n",
            "training error 0.043424418118566906, test error 0.0742817920194451\n",
            "Loss: 0.0\n",
            "training error 0.04308502120768606, test error 0.0736348079627512\n",
            "Loss: 0.0\n",
            "training error 0.04282727721805931, test error 0.07300113631325815\n",
            "Loss: 0.0\n",
            "training error 0.04244848792942452, test error 0.07217427225774366\n",
            "Loss: 0.0\n",
            "training error 0.042190848185332246, test error 0.07123175203803056\n",
            "Loss: 0.0\n",
            "training error 0.04185271508724286, test error 0.07066111532878872\n",
            "Loss: 0.0\n",
            "training error 0.04160314666183914, test error 0.06980030472716939\n",
            "Loss: 0.0\n",
            "training error 0.04139318789097602, test error 0.0692957722224404\n",
            "Loss: 0.0\n",
            "training error 0.0410007231794571, test error 0.06840187346525686\n",
            "Loss: 0.0\n",
            "training error 0.04077718582738429, test error 0.06774599489861012\n",
            "Loss: 0.0\n",
            "training error 0.04049665319176198, test error 0.0669386598359519\n",
            "Loss: 0.0\n",
            "training error 0.04021076538407846, test error 0.0663600202681086\n",
            "Loss: 0.0\n",
            "training error 0.040006278383716065, test error 0.06577275701090526\n",
            "Loss: 0.0\n",
            "training error 0.03973182457306531, test error 0.06528090500147316\n",
            "Loss: 0.0\n",
            "training error 0.03956342509626465, test error 0.06481247550280254\n",
            "Loss: 0.0\n",
            "training error 0.03929248772336294, test error 0.06421018408118634\n",
            "Loss: 0.0\n",
            "training error 0.039029439026340984, test error 0.06366245490623339\n",
            "Loss: 0.0\n",
            "training error 0.03883178167699292, test error 0.06319231310429577\n",
            "Loss: 0.0\n",
            "training error 0.0386125072595438, test error 0.06270483956270827\n",
            "Loss: 0.0\n",
            "training error 0.038454627384891665, test error 0.06237706638989972\n",
            "Loss: 0.0\n",
            "training error 0.03823384106132195, test error 0.061778057467888566\n",
            "Loss: 0.0\n",
            "training error 0.03798059775279343, test error 0.06129685573816738\n",
            "Loss: 0.0\n",
            "training error 0.03781045382978122, test error 0.06085241556890642\n",
            "Loss: 0.0\n",
            "training error 0.03764836285906551, test error 0.06058685872747324\n",
            "Loss: 0.0\n",
            "training error 0.03742846101246966, test error 0.06002703475938003\n",
            "Loss: 0.0\n",
            "training error 0.03727128836604298, test error 0.059503334004913325\n",
            "Loss: 0.0\n",
            "training error 0.03707879038812797, test error 0.0592150938227219\n",
            "Loss: 0.0\n",
            "training error 0.03692492082799852, test error 0.05887769111195346\n",
            "Loss: 0.0\n",
            "training error 0.03673396285903305, test error 0.05842989044607986\n",
            "Loss: 0.0\n",
            "training error 0.03662874243354003, test error 0.05793939404748587\n",
            "Loss: 0.0\n",
            "training error 0.0364942964348557, test error 0.05733435137539709\n",
            "Loss: 0.0\n",
            "training error 0.03624103753280032, test error 0.05707520591508998\n",
            "Loss: 0.0\n",
            "training error 0.0361037996239753, test error 0.056744909320547314\n",
            "Loss: 0.0\n",
            "training error 0.03596632925539232, test error 0.05633460928700021\n",
            "Loss: 0.0\n",
            "training error 0.035802945139480555, test error 0.05611229223848294\n",
            "Loss: 0.0\n",
            "training error 0.0357193005579476, test error 0.055738138258907374\n",
            "Loss: 0.0\n",
            "training error 0.03551389362426707, test error 0.055507847220981815\n",
            "Loss: 0.0\n",
            "training error 0.03539141256340546, test error 0.05532396254519114\n",
            "Loss: 0.0\n",
            "training error 0.035258225034155204, test error 0.05513005643031125\n",
            "Loss: 0.0\n",
            "training error 0.03514382153717526, test error 0.054598442097851856\n",
            "Loss: 0.0\n",
            "training error 0.03495879091724969, test error 0.054463158819323614\n",
            "Loss: 0.0\n",
            "training error 0.03484612059350577, test error 0.05411543400902029\n",
            "Loss: 0.0\n",
            "training error 0.03470997600863468, test error 0.05386171395113757\n",
            "Loss: 0.0\n",
            "training error 0.03461964900146384, test error 0.053501075289318537\n",
            "Loss: 0.0\n",
            "training error 0.03449755531860959, test error 0.05336313538380879\n",
            "Loss: 0.0\n",
            "training error 0.034435762818846714, test error 0.05296594294924364\n",
            "Loss: 0.0\n",
            "training error 0.034232924234062295, test error 0.052828095637917856\n",
            "Loss: 0.0\n",
            "training error 0.034224135284553196, test error 0.052430991393909776\n",
            "Loss: 0.0\n",
            "training error 0.034009740472353514, test error 0.05234724600293064\n",
            "Loss: 0.0\n",
            "training error 0.03390142565315393, test error 0.05218737129337259\n",
            "Loss: 0.0\n",
            "training error 0.033805136353061024, test error 0.05194210619941007\n",
            "Loss: 0.0\n",
            "training error 0.03370632889368002, test error 0.0515669482157447\n",
            "Loss: 0.0\n",
            "training error 0.03359688554867979, test error 0.05124602754899586\n",
            "Loss: 0.0\n",
            "training error 0.03351441906243563, test error 0.05107714403938975\n",
            "Loss: 0.0\n",
            "training error 0.03338533774456467, test error 0.05087335747209831\n",
            "Loss: 0.0\n",
            "training error 0.03332691725967671, test error 0.05072923607357818\n",
            "Loss: 0.0\n",
            "training error 0.033209570944291784, test error 0.05060056396180985\n",
            "Loss: 0.0\n",
            "training error 0.03309388095187045, test error 0.05034473759058679\n",
            "Loss: 0.0\n",
            "training error 0.03300052740756044, test error 0.050213700554436635\n",
            "Loss: 0.0\n",
            "training error 0.032959449457071514, test error 0.04991487523558519\n",
            "Loss: 0.0\n",
            "training error 0.03286288769789838, test error 0.04969718921439272\n",
            "Loss: 0.0\n",
            "training error 0.032750462563064506, test error 0.04947150457355703\n",
            "Loss: 0.0\n",
            "training error 0.0326779923130941, test error 0.0494471848449887\n",
            "Loss: 0.0\n",
            "training error 0.03259029021181926, test error 0.049346819661878843\n",
            "Loss: 0.0\n",
            "training error 0.03248617444549313, test error 0.049098892252906895\n",
            "Loss: 0.0\n",
            "training error 0.03243398973575797, test error 0.04899596774169844\n",
            "Loss: 0.0\n",
            "training error 0.03232157345200877, test error 0.04887569588834141\n",
            "Loss: 0.0\n",
            "training error 0.03224939583069181, test error 0.048685046731807825\n",
            "Loss: 0.0\n",
            "training error 0.032205648752947, test error 0.0484710195053463\n",
            "Loss: 0.0\n",
            "training error 0.03209886286292963, test error 0.04844275241174578\n",
            "Loss: 0.0\n",
            "training error 0.032078930815829625, test error 0.04820245427003596\n",
            "Loss: 0.0\n",
            "training error 0.031945296533232206, test error 0.04815223078404754\n",
            "Loss: 0.0\n",
            "training error 0.031857029785898085, test error 0.04811066706251352\n",
            "Loss: 0.0\n",
            "training error 0.03179283574694901, test error 0.048008019478772944\n",
            "Loss: 0.0\n",
            "training error 0.03178500265015929, test error 0.04789017416141044\n",
            "Loss: 0.0\n",
            "training error 0.031762844672491436, test error 0.04748021228109236\n",
            "Loss: 0.0\n",
            "training error 0.03159019016385983, test error 0.04765027511966044\n",
            "Loss: 0.35817623889564665\n",
            "training error 0.03152810174161998, test error 0.04750452857982036\n",
            "Loss: 0.05121354256809596\n",
            "training error 0.03146846994418475, test error 0.04737634911463726\n",
            "Loss: 0.0\n",
            "training error 0.03137458932498454, test error 0.04735561639568622\n",
            "Loss: 0.0\n",
            "training error 0.031343644570278946, test error 0.04725286470042615\n",
            "Loss: 0.0\n",
            "training error 0.031236189217162227, test error 0.047090826876933056\n",
            "Loss: 0.0\n",
            "training error 0.031206618272010784, test error 0.04691813688493994\n",
            "Loss: 0.0\n",
            "training error 0.031118890883559944, test error 0.04688817494383521\n",
            "Loss: 0.0\n",
            "training error 0.031058384155696204, test error 0.04683012687903877\n",
            "Loss: 0.0\n",
            "training error 0.030986632477632323, test error 0.046550225496278325\n",
            "Loss: 0.0\n",
            "training error 0.030912391011090647, test error 0.04640338837289802\n",
            "Loss: 0.0\n",
            "training error 0.030975247451388105, test error 0.0464889918528689\n",
            "Loss: 0.18447678708928272\n",
            "training error 0.030806868485039253, test error 0.046340763950905604\n",
            "Loss: 0.0\n",
            "training error 0.030760759331720133, test error 0.04617218818950603\n",
            "Loss: 0.0\n",
            "training error 0.030680611946179485, test error 0.04611824759063229\n",
            "Loss: 0.0\n",
            "training error 0.030621102146434084, test error 0.0460275089753698\n",
            "Loss: 0.0\n",
            "training error 0.03059388455154271, test error 0.045979708482161444\n",
            "Loss: 0.0\n",
            "training error 0.030559043705478916, test error 0.046099225447008424\n",
            "Loss: 0.2599341509382347\n",
            "training error 0.03051228282275568, test error 0.04619591177235268\n",
            "Loss: 0.47021457362026453\n",
            "training error 0.030427147943238593, test error 0.04605156672492941\n",
            "Loss: 0.15628251056842846\n",
            "training error 0.030371651626066313, test error 0.04592319746064333\n",
            "Loss: 0.0\n",
            "training error 0.030331344047101208, test error 0.045519027115496644\n",
            "Loss: 0.0\n",
            "training error 0.030268757912271974, test error 0.045589526679134434\n",
            "Loss: 0.15487932872315646\n",
            "training error 0.030193852981721062, test error 0.04552371549649303\n",
            "Loss: 0.010299826893245267\n",
            "training error 0.030145026645893337, test error 0.04520095297118379\n",
            "Loss: 0.0\n",
            "training error 0.03009703088309259, test error 0.04507067688119084\n",
            "Loss: 0.0\n",
            "training error 0.03011517873864317, test error 0.04521470851607497\n",
            "Loss: 0.31956838647841934\n",
            "training error 0.030048797099050076, test error 0.04504929197996079\n",
            "Loss: 0.0\n",
            "training error 0.02996381047333175, test error 0.04518422041181264\n",
            "Loss: 0.29951288005118926\n",
            "training error 0.029881055301423858, test error 0.045078329986236794\n",
            "Loss: 0.06445829667849345\n",
            "training error 0.029828763173478517, test error 0.04512408663933529\n",
            "Loss: 0.1660284903207021\n",
            "training error 0.029787998645892898, test error 0.045135020037631896\n",
            "Loss: 0.1902983463297092\n",
            "training error 0.029871808563532633, test error 0.04480728837438776\n",
            "Loss: 0.0\n",
            "training error 0.029683575146015648, test error 0.04504775270981668\n",
            "Loss: 0.5366634405985948\n",
            "training error 0.029655818498354056, test error 0.04496499033851829\n",
            "Loss: 0.3519560541420219\n",
            "training error 0.029609330114364847, test error 0.04485379331933935\n",
            "Loss: 0.1037887956151673\n",
            "training error 0.029601832233054338, test error 0.04502014412290124\n",
            "Loss: 0.4750471546837831\n",
            "training error 0.029489106004672683, test error 0.0448596990142978\n",
            "Loss: 0.1169690061851636\n",
            "training error 0.029452371570834836, test error 0.044968411111556666\n",
            "Loss: 0.35959046622648927\n",
            "training error 0.029412230395174394, test error 0.04477488050668887\n",
            "Loss: 0.0\n",
            "training error 0.029369086908454942, test error 0.04469195044598967\n",
            "Loss: 0.0\n",
            "training error 0.02936924351382108, test error 0.04446701191323679\n",
            "Loss: 0.0\n",
            "training error 0.029361318080605003, test error 0.04466967065111504\n",
            "Loss: 0.4557507445601905\n",
            "training error 0.029253205797727794, test error 0.04451999799270367\n",
            "Loss: 0.11915817408703067\n",
            "training error 0.029191831653638676, test error 0.04452204294105483\n",
            "Loss: 0.12375697275412989\n",
            "training error 0.029157366514230388, test error 0.04453297275251118\n",
            "Loss: 0.14833656779793536\n",
            "training error 0.029141656028424476, test error 0.04440257591200996\n",
            "Loss: 0.0\n",
            "training error 0.029079186317178377, test error 0.044376067701335\n",
            "Loss: 0.0\n",
            "training error 0.02906633988047225, test error 0.044571283637129755\n",
            "Loss: 0.4399126509104345\n",
            "training error 0.02899677545502295, test error 0.04448395034865927\n",
            "Loss: 0.2431099755173305\n",
            "training error 0.02897512573329627, test error 0.04452233105289483\n",
            "Loss: 0.3295996223555253\n",
            "training error 0.02890989933888398, test error 0.04424535159397972\n",
            "Loss: 0.0\n",
            "training error 0.02891976758559751, test error 0.04433449333317109\n",
            "Loss: 0.20147142237536553\n",
            "training error 0.02882066478777029, test error 0.04427119583707511\n",
            "Loss: 0.058411205164676794\n",
            "training error 0.028798889103054663, test error 0.04408677641734675\n",
            "Loss: 0.0\n",
            "training error 0.028748759949362206, test error 0.04397972523572415\n",
            "Loss: 0.0\n",
            "training error 0.028750185552143193, test error 0.043945930040177814\n",
            "Loss: 0.0\n",
            "training error 0.028714902538088886, test error 0.04399346963553199\n",
            "Loss: 0.10817747015641199\n",
            "training error 0.02862622101851323, test error 0.04397293726314753\n",
            "Loss: 0.06145557266628021\n",
            "training error 0.02878232517499554, test error 0.04371826003933969\n",
            "Loss: 0.0\n",
            "training error 0.028572761038030487, test error 0.04409334595306801\n",
            "Loss: 0.8579616695421954\n",
            "training error 0.02851825730118379, test error 0.04403432537400347\n",
            "Loss: 0.7229595468332128\n",
            "training error 0.0284819741364221, test error 0.04393976084990227\n",
            "Loss: 0.5066551376089912\n",
            "training error 0.028439184787513068, test error 0.044013921609621\n",
            "Loss: 0.676288511974743\n",
            "training error 0.0283899853003688, test error 0.044044436253068746\n",
            "Loss: 0.7460869061018061\n",
            "training error 0.028362716830131265, test error 0.043970869531395326\n",
            "Loss: 0.5778123187618167\n",
            "training error 0.028329809657761856, test error 0.043997182876522836\n",
            "Loss: 0.6380007734346105\n",
            "training error 0.028281787625577013, test error 0.04394275370845226\n",
            "Loss: 0.5135009236656662\n",
            "training error 0.02825500676527347, test error 0.04391423155720423\n",
            "Loss: 0.44826010387466475\n",
            "training error 0.028227662456278887, test error 0.04391646054876792\n",
            "Loss: 0.4533586406455292\n",
            "training error 0.02825061406801823, test error 0.04392071468048838\n",
            "Loss: 0.46308942983208645\n",
            "training error 0.028164494472535957, test error 0.043698815165579226\n",
            "Loss: 0.0\n",
            "training error 0.02813183591405822, test error 0.04364033772033889\n",
            "Loss: 0.0\n",
            "training error 0.0281453462124331, test error 0.04374884170685715\n",
            "Loss: 0.24863232547280845\n",
            "training error 0.02807421075172726, test error 0.04346643269218775\n",
            "Loss: 0.0\n",
            "training error 0.028031353362587233, test error 0.04360104276040543\n",
            "Loss: 0.3096874067649802\n",
            "training error 0.028119784038391306, test error 0.04366803773271388\n",
            "Loss: 0.46381777394481905\n",
            "training error 0.027954539715308875, test error 0.04350926579299151\n",
            "Loss: 0.09854294026632182\n",
            "training error 0.02799622673494699, test error 0.04337778443946088\n",
            "Loss: 0.0\n",
            "training error 0.02788541671203896, test error 0.0435804437678385\n",
            "Loss: 0.4671961258428281\n",
            "training error 0.02787505701978404, test error 0.04354740293834115\n",
            "Loss: 0.3910261924902736\n",
            "training error 0.02784627043341006, test error 0.04331926186330307\n",
            "Loss: 0.0\n",
            "training error 0.027888112195945516, test error 0.04341568249035403\n",
            "Loss: 0.22258141737323545\n",
            "training error 0.027762884331397966, test error 0.043367501234014415\n",
            "Loss: 0.11135778551252251\n",
            "training error 0.027725065899251826, test error 0.04339252138541252\n",
            "Loss: 0.16911535182806414\n",
            "training error 0.027725613240811076, test error 0.04311254541886014\n",
            "Loss: 0.0\n",
            "training error 0.02771947586288873, test error 0.043075513556918694\n",
            "Loss: 0.0\n",
            "training error 0.027703043824216712, test error 0.04329324146421433\n",
            "Loss: 0.505456323829856\n",
            "training error 0.027623228619785254, test error 0.04325780423751841\n",
            "Loss: 0.42318864140491463\n",
            "training error 0.027604560869792582, test error 0.043298478241208214\n",
            "Loss: 0.51761352536146\n",
            "training error 0.02761564373939513, test error 0.043113714571715575\n",
            "Loss: 0.08868382903062244\n",
            "training error 0.027555289869200725, test error 0.043256607718869425\n",
            "Loss: 0.4204109179370308\n",
            "training error 0.02755136693692592, test error 0.04322795819393656\n",
            "Loss: 0.35390091592624184\n",
            "training error 0.027467846330285192, test error 0.043343671100353844\n",
            "Loss: 0.6225289527443723\n",
            "training error 0.027463255311630866, test error 0.04344982019892747\n",
            "Loss: 0.8689545662971199\n",
            "training error 0.027398338243111244, test error 0.04344045815341094\n",
            "Loss: 0.8472205351887752\n",
            "training error 0.02739391484267669, test error 0.043463866820400196\n",
            "Loss: 0.9015638617247079\n",
            "training error 0.027374750038570317, test error 0.04352221473296753\n",
            "Loss: 1.0370188052629459\n",
            "training error 0.02738895124526639, test error 0.04355951616328074\n",
            "Loss: 1.1236142448365793\n",
            "training error 0.027335491170974505, test error 0.043181327021088484\n",
            "Loss: 0.24564643676265874\n",
            "training error 0.02731441537517643, test error 0.0429765333113771\n",
            "Loss: 0.0\n",
            "training error 0.027288005770148963, test error 0.043029528525584074\n",
            "Loss: 0.12331198010553379\n",
            "training error 0.02723440113781568, test error 0.04317487787250272\n",
            "Loss: 0.461518288803231\n",
            "training error 0.027189904588930138, test error 0.04328004101975271\n",
            "Loss: 0.7062172888087748\n",
            "training error 0.027225715910969447, test error 0.04309844888203482\n",
            "Loss: 0.2836793972524454\n",
            "training error 0.02714448747241036, test error 0.04319723865290431\n",
            "Loss: 0.5135484984983174\n",
            "training error 0.027097496755080507, test error 0.04327271193277503\n",
            "Loss: 0.6891635936570939\n",
            "training error 0.027128799756730673, test error 0.043238080899958266\n",
            "Loss: 0.6085823318651151\n",
            "training error 0.02712316554875982, test error 0.043523414467634025\n",
            "Loss: 1.2725110987771293\n",
            "training error 0.02704175329717856, test error 0.043464729316923766\n",
            "Loss: 1.1359594828404207\n",
            "training error 0.027055491024335163, test error 0.043451521075669734\n",
            "Loss: 1.1052258702469286\n",
            "training error 0.027003826074482977, test error 0.04315740387139095\n",
            "Loss: 0.4208588875779906\n",
            "training error 0.026988245389447673, test error 0.043175339432159815\n",
            "Loss: 0.4625922694655449\n",
            "training error 0.02695052282359443, test error 0.04325584843035693\n",
            "Loss: 0.6499247320767232\n",
            "training error 0.026910936283703543, test error 0.04328365514426717\n",
            "Loss: 0.714626818931352\n",
            "training error 0.026910780451998823, test error 0.04338787587293588\n",
            "Loss: 0.9571329510886573\n",
            "training error 0.0270373387416587, test error 0.043606798432747375\n",
            "Loss: 1.466533181733909\n",
            "training error 0.02681599321873126, test error 0.04328475013256129\n",
            "Loss: 0.7171746938057444\n",
            "training error 0.026814776077204335, test error 0.043464301487765845\n",
            "Loss: 1.1349639880320783\n",
            "training error 0.026779549920144564, test error 0.04334511202432166\n",
            "Loss: 0.8576278367408152\n",
            "training error 0.026829166967061738, test error 0.043456610674986365\n",
            "Loss: 1.117068610748495\n",
            "training error 0.026746379468140666, test error 0.04344324112791218\n",
            "Loss: 1.0859596635067081\n",
            "training error 0.02668808066478505, test error 0.043402345595985894\n",
            "Loss: 0.990801844168443\n",
            "training error 0.026668991378819524, test error 0.04329200131010998\n",
            "Loss: 0.7340471053056463\n",
            "training error 0.026735988341186945, test error 0.04348752438539859\n",
            "Loss: 1.1890002162789903\n",
            "training error 0.026694188454341204, test error 0.04342515799378491\n",
            "Loss: 1.0438829003665662\n",
            "training error 0.026719453401437665, test error 0.043255654923396454\n",
            "Loss: 0.6494744701650124\n",
            "training error 0.026590262090774613, test error 0.04339799171115409\n",
            "Loss: 0.9806710018311682\n",
            "training error 0.026735262023077154, test error 0.043062870353800416\n",
            "Loss: 0.2008934545692176\n",
            "training error 0.026528676354732533, test error 0.04337327527825785\n",
            "Loss: 0.9231595391983927\n",
            "training error 0.026538618984973533, test error 0.04350503673062289\n",
            "Loss: 1.229748838550182\n",
            "training error 0.026583636836350403, test error 0.0435677220847884\n",
            "Loss: 1.375608332873135\n",
            "training error 0.026498858069884522, test error 0.04303649904908887\n",
            "Loss: 0.13953135139426198\n",
            "training error 0.026481217532156234, test error 0.04304817114949888\n",
            "Loss: 0.16669059275382647\n",
            "training error 0.026438941275950183, test error 0.043174953437447455\n",
            "Loss: 0.4616941171889044\n",
            "training error 0.026535582690404107, test error 0.0429254254478621\n",
            "Loss: 0.0\n",
            "training error 0.026390303138284787, test error 0.043221562176210954\n",
            "Loss: 0.6898865305564605\n",
            "training error 0.026451854292478295, test error 0.04297202263116413\n",
            "Loss: 0.10855380655139246\n",
            "training error 0.026481509290070593, test error 0.04324716133462249\n",
            "Loss: 0.7495228839401458\n",
            "training error 0.026312125661446622, test error 0.04313092133438245\n",
            "Loss: 0.47872766402734523\n",
            "training error 0.026307529002285904, test error 0.04311820950273098\n",
            "Loss: 0.44911390593680434\n",
            "training error 0.026302679243116456, test error 0.04314861850995544\n",
            "Loss: 0.5199553871968909\n",
            "training error 0.0262505133777937, test error 0.04330632627005135\n",
            "Loss: 0.8873547978036855\n",
            "training error 0.026235147015945884, test error 0.04320097075611566\n",
            "Loss: 0.6419163127183047\n",
            "training error 0.026240852862806634, test error 0.043342612464858724\n",
            "Loss: 0.9718879024352312\n",
            "training error 0.026243276226432945, test error 0.043170198411376697\n",
            "Loss: 0.5702283925220542\n",
            "training error 0.026196514201860293, test error 0.04324098782569488\n",
            "Loss: 0.7351409439519063\n",
            "training error 0.026195207075595427, test error 0.0431018236253824\n",
            "Loss: 0.41094101148644313\n",
            "training error 0.026136356187689017, test error 0.043074136166075916\n",
            "Loss: 0.3464397071484937\n",
            "training error 0.026129626392397656, test error 0.04325258818199308\n",
            "Loss: 0.7621653850079069\n",
            "training error 0.026118249588274637, test error 0.04307247185095176\n",
            "Loss: 0.34256248262063416\n",
            "training error 0.026078831386640952, test error 0.042928097361691286\n",
            "Loss: 0.006224548274857966\n",
            "training error 0.02613011275388663, test error 0.04284927164028175\n",
            "Loss: 0.0\n",
            "training error 0.02609404880970742, test error 0.04285351896669612\n",
            "Loss: 0.009912248800914014\n",
            "training error 0.02603444141045463, test error 0.04314994610282239\n",
            "Loss: 0.7017026218433475\n",
            "training error 0.026016677384047556, test error 0.043113720629371584\n",
            "Loss: 0.6171609900627262\n",
            "training error 0.026001003460983593, test error 0.04301159385759469\n",
            "Loss: 0.37882141539213876\n",
            "training error 0.02601759471568846, test error 0.043006026381756275\n",
            "Loss: 0.3658282520890399\n",
            "training error 0.025959522363404444, test error 0.042952186175473125\n",
            "Loss: 0.24017802695770474\n",
            "training error 0.025921969897986015, test error 0.04301220236739922\n",
            "Loss: 0.38024153242386394\n",
            "training error 0.02599010130978418, test error 0.04292121363991854\n",
            "Loss: 0.16789550179694412\n",
            "training error 0.025894452639459106, test error 0.043037208913698\n",
            "Loss: 0.438600858829008\n",
            "training error 0.025887919759479516, test error 0.04321904072611447\n",
            "Loss: 0.8629530250523709\n",
            "training error 0.025920859677874244, test error 0.04304328559263371\n",
            "Loss: 0.4527823809484799\n",
            "training error 0.025912398259351738, test error 0.043148229405958125\n",
            "Loss: 0.6976962600113934\n",
            "training error 0.025824059049508966, test error 0.043084040727114914\n",
            "Loss: 0.5478951633158147\n",
            "training error 0.02580114742216387, test error 0.043213689056439936\n",
            "Loss: 0.8504635019644047\n",
            "training error 0.025780918116679704, test error 0.043280905887594666\n",
            "Loss: 1.0073315853218467\n",
            "training error 0.025766965454215907, test error 0.04330194166242755\n",
            "Loss: 1.056424076343565\n",
            "training error 0.025796634401537864, test error 0.04302251183167703\n",
            "Loss: 0.4043013679430274\n",
            "training error 0.02575168465208339, test error 0.04321554925329826\n",
            "Loss: 0.8548047586232066\n",
            "training error 0.02576326981654383, test error 0.043303714611985976\n",
            "Loss: 1.0605617185730942\n",
            "training error 0.02575396984984783, test error 0.04335619660425807\n",
            "Loss: 1.1830421955171921\n",
            "training error 0.025678803716080965, test error 0.043157844324800534\n",
            "Loss: 0.7201351918166621\n",
            "training error 0.02565172958172517, test error 0.04321442373061225\n",
            "Loss: 0.8521780565978743\n",
            "training error 0.025713605658664346, test error 0.04330424916176131\n",
            "Loss: 1.0618092305023863\n",
            "training error 0.02561141500923298, test error 0.04332217292405293\n",
            "Loss: 1.103639025048464\n",
            "training error 0.025673659265754847, test error 0.04293343108212365\n",
            "Loss: 0.1964081036158749\n",
            "training error 0.02558165729702576, test error 0.04298401225818648\n",
            "Loss: 0.31445252800530454\n",
            "training error 0.025680478954381292, test error 0.04315391019234967\n",
            "Loss: 0.7109538631726142\n",
            "training error 0.025572326319063907, test error 0.04263915665410005\n",
            "Loss: 0.0\n",
            "training error 0.025581170991598724, test error 0.04284351107283736\n",
            "Loss: 0.47926468244925413\n",
            "training error 0.02552835013450993, test error 0.04287166646878978\n",
            "Loss: 0.5452964667568727\n",
            "training error 0.02556285806533782, test error 0.04273438661209281\n",
            "Loss: 0.2233392155602365\n",
            "training error 0.025536844023000625, test error 0.042581879869812836\n",
            "Loss: 0.0\n",
            "training error 0.025498634399397143, test error 0.042670990039721864\n",
            "Loss: 0.20926781575043574\n",
            "training error 0.025485159322817478, test error 0.0428078439811931\n",
            "Loss: 0.5306579044211057\n",
            "training error 0.025509485092156302, test error 0.042836160023037154\n",
            "Loss: 0.5971557714260944\n",
            "training error 0.02543801236521528, test error 0.04267825908845546\n",
            "Loss: 0.22633857156444392\n",
            "training error 0.02547191403289111, test error 0.042901921927467696\n",
            "Loss: 0.751592129406542\n",
            "training error 0.02543589224410964, test error 0.04286749703659871\n",
            "Loss: 0.6707481390185288\n",
            "training error 0.02537380514883339, test error 0.04289181054090799\n",
            "Loss: 0.7278463798280344\n",
            "training error 0.02537189058143743, test error 0.042942146098573025\n",
            "Loss: 0.8460552466486693\n",
            "training error 0.02538822620325483, test error 0.04296818745570116\n",
            "Loss: 0.9072112059622439\n",
            "training error 0.025326732355897424, test error 0.04311506624867788\n",
            "Loss: 1.2521438238404903\n",
            "training error 0.025331959010824905, test error 0.0430232995977003\n",
            "Loss: 1.0366374834484438\n",
            "training error 0.0253242039027264, test error 0.04315973635560053\n",
            "Loss: 1.3570478512324957\n",
            "training error 0.02533152357655689, test error 0.04303349288913397\n",
            "Loss: 1.0605755798049898\n",
            "training error 0.025286968846905702, test error 0.04313300819405965\n",
            "Loss: 1.2942789889309747\n",
            "training error 0.025297831939492317, test error 0.04318316785485187\n",
            "Loss: 1.4120747765889563\n",
            "training error 0.02524844457318899, test error 0.043041297301566986\n",
            "Loss: 1.078903592698932\n",
            "training error 0.025222268656879668, test error 0.0431608567021871\n",
            "Loss: 1.3596788919239655\n",
            "training error 0.025228412390827955, test error 0.04316522595325881\n",
            "Loss: 1.369939714332613\n",
            "training error 0.025220812740540034, test error 0.043186637039393065\n",
            "Loss: 1.420221867679805\n",
            "training error 0.02523864140912717, test error 0.04328780719947457\n",
            "Loss: 1.657811566375167\n",
            "training error 0.02518339561335336, test error 0.04317264987066601\n",
            "Loss: 1.3873741663340322\n",
            "training error 0.025187836026088966, test error 0.04317483734885636\n",
            "Loss: 1.3925112767599668\n",
            "training error 0.025114036567980027, test error 0.043025514075038185\n",
            "Loss: 1.0418379991247173\n",
            "training error 0.02525368879262766, test error 0.04313863393602947\n",
            "Loss: 1.3074905756129684\n",
            "training error 0.025092167733338163, test error 0.04293016332300826\n",
            "Loss: 0.8179146957819627\n",
            "training error 0.025089203795468952, test error 0.04305982784461447\n",
            "Loss: 1.1224210304074944\n",
            "training error 0.025161333798873413, test error 0.04289015812268725\n",
            "Loss: 0.7239658131978244\n",
            "training error 0.025078457985494053, test error 0.04285915068759625\n",
            "Loss: 0.6511474332066225\n",
            "training error 0.025045537315855033, test error 0.042861211802746185\n",
            "Loss: 0.6559877905516709\n",
            "training error 0.025018017948334918, test error 0.042831332110666726\n",
            "Loss: 0.5858178211402443\n",
            "training error 0.025038769756161997, test error 0.042962955714227535\n",
            "Loss: 0.8949248966456658\n",
            "training error 0.02501632082364222, test error 0.04294896312827766\n",
            "Loss: 0.8620644734030458\n",
            "training error 0.024990926909772646, test error 0.043085142068312966\n",
            "Loss: 1.1818693773942623\n",
            "training error 0.025087434951413575, test error 0.043321675046272955\n",
            "Loss: 1.7373473851364007\n",
            "training error 0.024956059536318617, test error 0.043115850268535906\n",
            "Loss: 1.2539850292086596\n",
            "training error 0.02493875887022328, test error 0.043316450123152696\n",
            "Loss: 1.7250770881550803\n",
            "training error 0.02492403795153863, test error 0.043444568142179424\n",
            "Loss: 2.0259515902165903\n",
            "training error 0.02495564906509009, test error 0.04330129319127067\n",
            "Loss: 1.6894822954207855\n",
            "training error 0.02505511328636324, test error 0.04347205533702136\n",
            "Loss: 2.090502979037301\n",
            "training error 0.024899578737271902, test error 0.04320041137646382\n",
            "Loss: 1.4525697515986646\n",
            "training error 0.02490077539111615, test error 0.0432020642064239\n",
            "Loss: 1.4564512851644285\n",
            "training error 0.02486068348354925, test error 0.043277274317154404\n",
            "Loss: 1.6330759690920793\n",
            "training error 0.024859630580999006, test error 0.04321306520162805\n",
            "Loss: 1.4822862065859033\n",
            "training error 0.02482721983572074, test error 0.0433258142999481\n",
            "Loss: 1.7470680778061576\n",
            "training error 0.02486542365740718, test error 0.04336310993121956\n",
            "Loss: 1.834653763044769\n",
            "training error 0.02480084986843132, test error 0.043440263581249805\n",
            "Loss: 2.015842687221281\n",
            "training error 0.024790793201368688, test error 0.04337500049379348\n",
            "Loss: 1.8625777593790582\n",
            "training error 0.024870422340346518, test error 0.04323480555838905\n",
            "Loss: 1.5333416245887355\n",
            "training error 0.024779989889869826, test error 0.04329679384552128\n",
            "Loss: 1.6789159564917666\n",
            "training error 0.024741987684306035, test error 0.04336642695900112\n",
            "Loss: 1.8424435266524375\n",
            "training error 0.02474085757971028, test error 0.04348782001985361\n",
            "Loss: 2.127525024283883\n",
            "training error 0.024764165647009905, test error 0.04324184600517261\n",
            "Loss: 1.5498755277538523\n",
            "training error 0.024717786727238934, test error 0.043428151949348466\n",
            "Loss: 1.9873995279751977\n",
            "training error 0.02470968537933132, test error 0.04349367205393434\n",
            "Loss: 2.1412680391498906\n",
            "training error 0.024676902594532098, test error 0.04354374080349265\n",
            "Loss: 2.258850329343254\n",
            "training error 0.02470315981752616, test error 0.043512874328479445\n",
            "Loss: 2.186362982360035\n",
            "training error 0.024678598551856615, test error 0.043329706146158956\n",
            "Loss: 1.7562077546422916\n",
            "training error 0.024671429529776898, test error 0.0432957970983526\n",
            "Loss: 1.6765751787437555\n",
            "training error 0.02465647457682357, test error 0.043444210212906335\n",
            "Loss: 2.0251110231157776\n",
            "training error 0.024624364905354433, test error 0.04324792910553978\n",
            "Loss: 1.5641611825576662\n",
            "training error 0.024648748553403047, test error 0.0433509952247042\n",
            "Loss: 1.8062033833236457\n",
            "training error 0.024643002659344312, test error 0.043129260150759526\n",
            "Loss: 1.285477021259318\n",
            "training error 0.024615873578958104, test error 0.043176760984411984\n",
            "Loss: 1.3970287747227195\n",
            "training error 0.024650833744814877, test error 0.04315175930297344\n",
            "Loss: 1.3383144072148134\n",
            "training error 0.02455615442933278, test error 0.04359061749259724\n",
            "Loss: 2.368936331295024\n",
            "training error 0.0245588305245214, test error 0.04348500662705139\n",
            "Loss: 2.1209180054983845\n",
            "training error 0.02465972606252003, test error 0.04330582215786661\n",
            "Loss: 1.7001181964420242\n",
            "training error 0.024574441140048364, test error 0.04338013697256096\n",
            "Loss: 1.874640352160739\n",
            "training error 0.0245478662992032, test error 0.043637057960520295\n",
            "Loss: 2.4779979041167133\n",
            "training error 0.02455151984438848, test error 0.043448588857363704\n",
            "Loss: 2.0353939051086734\n",
            "training error 0.024529831402428478, test error 0.04353944368022623\n",
            "Loss: 2.2487588930807823\n",
            "training error 0.024477632675631824, test error 0.04344670169129657\n",
            "Loss: 2.030962052703611\n",
            "training error 0.02445620223075512, test error 0.04327138618350115\n",
            "Loss: 1.6192481773852352\n",
            "training error 0.02451626821639532, test error 0.04292864210030622\n",
            "Loss: 0.8143422309056181\n",
            "training error 0.02442888303740053, test error 0.043068106922738256\n",
            "Loss: 1.1418637561610323\n",
            "training error 0.024430378747666164, test error 0.043127642703641486\n",
            "Loss: 1.2816785813525078\n",
            "training error 0.024469434683212526, test error 0.043025941040791896\n",
            "Loss: 1.0428406926530798\n",
            "training error 0.02440422266678218, test error 0.043162402617068024\n",
            "Loss: 1.3633093443268463\n",
            "training error 0.02452383246109565, test error 0.04338649116064646\n",
            "Loss: 1.8895626339034077\n",
            "training error 0.024372576166676686, test error 0.04319165610946745\n",
            "Loss: 1.432008735919843\n",
            "training error 0.024390801467042558, test error 0.043106998953306525\n",
            "Loss: 1.2331984522504724\n",
            "training error 0.024372951857697354, test error 0.04300750989610467\n",
            "Loss: 0.9995566837188274\n",
            "training error 0.024381569026984216, test error 0.04301299865537327\n",
            "Loss: 1.0124465779306036\n",
            "training error 0.02435995468869537, test error 0.04327173700773421\n",
            "Loss: 1.620072058891009\n",
            "training error 0.02432321954580599, test error 0.04322118314430181\n",
            "Loss: 1.5013505191493248\n",
            "training error 0.024323733031486123, test error 0.04321870884957061\n",
            "Loss: 1.49553984395423\n",
            "training error 0.02432340162605197, test error 0.04317593306874964\n",
            "Loss: 1.3950844837123721\n",
            "training error 0.024373219620413993, test error 0.04288865507084765\n",
            "Loss: 0.7204360210792293\n",
            "training error 0.02431045347787037, test error 0.04309437114088543\n",
            "Loss: 1.2035430860249718\n",
            "training error 0.02426423959991214, test error 0.04320068675309787\n",
            "Loss: 1.4532164506990686\n",
            "training error 0.02428752885864067, test error 0.04280123108611606\n",
            "Loss: 0.5151280708457495\n",
            "training error 0.024269376822383587, test error 0.0428006054332001\n",
            "Loss: 0.5136587770572465\n",
            "training error 0.024249421130428963, test error 0.04291604234552834\n",
            "Loss: 0.7847527557194534\n",
            "training error 0.024231761389086587, test error 0.04300986433259504\n",
            "Loss: 1.005085881813339\n",
            "training error 0.02422983840709391, test error 0.042934614821087395\n",
            "Loss: 0.8283686684406266\n",
            "training error 0.02426120095489198, test error 0.04312528510501801\n",
            "Loss: 1.2761419572516308\n",
            "training error 0.024200365241624652, test error 0.04299410948740993\n",
            "Loss: 0.9680869394620872\n",
            "training error 0.02418449753722487, test error 0.04316672333327336\n",
            "Loss: 1.3734561866422634\n",
            "training error 0.024243964067808144, test error 0.0430699692217259\n",
            "Loss: 1.1462372103000495\n",
            "training error 0.0242058548558472, test error 0.04347768459036279\n",
            "Loss: 2.103722811883202\n",
            "training error 0.024234598064127554, test error 0.04312888540528494\n",
            "Loss: 1.2845969627092169\n",
            "training error 0.024163447377972987, test error 0.04344777090151105\n",
            "Loss: 2.033473003882258\n",
            "training error 0.024158562754457963, test error 0.043510373537750426\n",
            "Loss: 2.180490083519815\n",
            "training error 0.02412272721782771, test error 0.0433838014859732\n",
            "Loss: 1.8832461568444314\n",
            "training error 0.024125237197567567, test error 0.043285396076760206\n",
            "Loss: 1.6521492454026454\n",
            "training error 0.02411466300367009, test error 0.043192275688983185\n",
            "Loss: 1.4334637668335315\n",
            "training error 0.024096246013529485, test error 0.04347182179756098\n",
            "Loss: 2.089954531056404\n",
            "training error 0.02406747479267254, test error 0.04332455758959818\n",
            "Loss: 1.7441167981685268\n",
            "training error 0.024088806428827717, test error 0.04345120677026904\n",
            "Loss: 2.041541855629747\n",
            "training error 0.02405782438621523, test error 0.04352836032223044\n",
            "Loss: 2.2227305494997296\n",
            "training error 0.024080011740325126, test error 0.04377610715057416\n",
            "Loss: 2.8045433513327334\n",
            "training error 0.024030004077886673, test error 0.043678268104792736\n",
            "Loss: 2.5747764972611\n",
            "training error 0.024010353786592213, test error 0.04351594863034816\n",
            "Loss: 2.1935827243679373\n",
            "training error 0.024005542075012352, test error 0.04356715814293653\n",
            "Loss: 2.3138440015706863\n",
            "training error 0.024039767085679002, test error 0.04330828901164609\n",
            "Loss: 1.7059113971814588\n",
            "training error 0.024052931408207044, test error 0.04361287071212619\n",
            "Loss: 2.4211961648133995\n",
            "training error 0.02397832584857209, test error 0.04334523294840203\n",
            "Loss: 1.7926711571283782\n",
            "training error 0.02398355526412978, test error 0.043491832733406746\n",
            "Loss: 2.1369485480113637\n",
            "training error 0.02402462112489564, test error 0.04358875848505489\n",
            "Loss: 2.3645706068412764\n",
            "training error 0.023970060631850185, test error 0.04349308608319451\n",
            "Loss: 2.1398919356485413\n",
            "training error 0.023965070894582535, test error 0.04368545300186395\n",
            "Loss: 2.591649629901527\n",
            "training error 0.02395069819272506, test error 0.04343972559052703\n",
            "Loss: 2.0145792607957125\n",
            "training error 0.023934311095534386, test error 0.04348220939416343\n",
            "Loss: 2.114348936926236\n",
            "training error 0.023907973757063942, test error 0.04336095298913818\n",
            "Loss: 1.8295883641286759\n",
            "training error 0.02390114908533097, test error 0.04339322680387432\n",
            "Loss: 1.9053807312923832\n",
            "training error 0.02391911575700024, test error 0.04347864444137866\n",
            "Loss: 2.105976942087895\n",
            "training error 0.023913010420976762, test error 0.043526688637205074\n",
            "Loss: 2.2188047363827845\n",
            "training error 0.023940774580318746, test error 0.04345288559286259\n",
            "Loss: 2.045484430731359\n",
            "training error 0.023850022731166404, test error 0.043361631212557226\n",
            "Loss: 1.8311811153672641\n",
            "training error 0.023864311102039584, test error 0.043521792352606684\n",
            "Loss: 2.207306219611427\n",
            "training error 0.023935792983618566, test error 0.04371470299048488\n",
            "Loss: 2.660340793162419\n",
            "training error 0.02381859465121958, test error 0.04345006882863166\n",
            "Loss: 2.0388694944261987\n",
            "training error 0.02387710345853914, test error 0.04326748255614274\n",
            "Loss: 1.6100808335048233\n",
            "training error 0.023824433488625806, test error 0.043585191869961536\n",
            "Loss: 2.3561947082096024\n",
            "training error 0.023788116661242874, test error 0.04354595672491522\n",
            "Loss: 2.2640542363322025\n",
            "training error 0.02382000349840787, test error 0.04361193791169541\n",
            "Loss: 2.4190055606558625\n",
            "training error 0.023788814624477468, test error 0.04347075833957973\n",
            "Loss: 2.0874570885186294\n",
            "training error 0.023800258690465134, test error 0.043576908198883976\n",
            "Loss: 2.336741196286485\n",
            "training error 0.023770474496375458, test error 0.04339482032871102\n",
            "Loss: 1.9091229917129304\n",
            "training error 0.023762383326866424, test error 0.043576291925211334\n",
            "Loss: 2.335293928870108\n",
            "training error 0.02375222271667444, test error 0.043457268675265524\n",
            "Loss: 2.055777734869957\n",
            "training error 0.023751410309113697, test error 0.043510304029357814\n",
            "Loss: 2.180326848846237\n",
            "training error 0.02375993972957131, test error 0.04355810894757835\n",
            "Loss: 2.2925927196032037\n",
            "training error 0.023750554239576692, test error 0.04351187273148479\n",
            "Loss: 2.1840108151994553\n",
            "training error 0.023726622719830087, test error 0.043410778423585726\n",
            "Loss: 1.946599249039993\n",
            "training error 0.023772939341778324, test error 0.04346934620361617\n",
            "Loss: 2.084140804766288\n",
            "training error 0.023762671171287497, test error 0.043311209121844094\n",
            "Loss: 1.7127690328869072\n",
            "training error 0.023698382415072432, test error 0.04341135300528044\n",
            "Loss: 1.9479486063170182\n",
            "training error 0.023685706144747316, test error 0.04339098323903328\n",
            "Loss: 1.900111906036428\n",
            "training error 0.02385456392627893, test error 0.043870460883733656\n",
            "Loss: 3.0261252388585236\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9583333333333333 0.042581879869812836\n",
            "training error 0.12500345308231403, test error 0.25007664180882694\n",
            "training error 0.12506124697630064, test error 0.250231801884997\n",
            "training error 0.12503640025764207, test error 0.2501904641295485\n",
            "training error 0.1251396197536164, test error 0.25030457087798924\n",
            "training error 0.1249935513708149, test error 0.250262098643476\n",
            "training error 0.1250270333281673, test error 0.25030243820952003\n",
            "training error 0.12499470005562092, test error 0.2503366042483138\n",
            "training error 0.1252626125480991, test error 0.2505055282015285\n",
            "training error 0.12501019765757543, test error 0.25048336356923095\n",
            "training error 0.12510476783468844, test error 0.25040702826920774\n",
            "training error 0.1250058509281406, test error 0.25040256724871535\n",
            "training error 0.1251068095134156, test error 0.2503593244772688\n",
            "training error 0.12497002454153085, test error 0.25034481892761357\n",
            "training error 0.12499667235976805, test error 0.2503572854110455\n",
            "training error 0.12497144617898678, test error 0.25046897688546593\n",
            "training error 0.12508761274538946, test error 0.25056322026348915\n",
            "training error 0.12496367331650096, test error 0.25055157807081774\n",
            "training error 0.12502336073078132, test error 0.2505321163787379\n",
            "training error 0.1249740584639096, test error 0.25047304258133724\n",
            "training error 0.12531579111029756, test error 0.2503073134469108\n",
            "training error 0.12495298075272934, test error 0.2504539485863584\n",
            "training error 0.1249745375356186, test error 0.25045100929471836\n",
            "training error 0.12498557046700362, test error 0.2504645853210711\n",
            "training error 0.1250854342676248, test error 0.25058583804793777\n",
            "training error 0.12512574159437073, test error 0.25051757137813746\n",
            "training error 0.12496411907570561, test error 0.2505053619689289\n",
            "training error 0.12513057638084038, test error 0.25042830162457147\n",
            "training error 0.12496564382747688, test error 0.2505822752553411\n",
            "training error 0.12510906059071825, test error 0.2504468774156434\n",
            "training error 0.1249578185088989, test error 0.25048285955133875\n",
            "training error 0.12507878170888106, test error 0.25043695870898686\n",
            "training error 0.12506238067521908, test error 0.2505074297281541\n",
            "training error 0.1249785528886303, test error 0.25060544424478365\n",
            "training error 0.1251503601230265, test error 0.2507834423027552\n",
            "training error 0.12510705257493385, test error 0.2506707498088232\n",
            "training error 0.12512679350786798, test error 0.25074254267739954\n",
            "training error 0.12501358811549734, test error 0.25078421274931656\n",
            "training error 0.1250690136386762, test error 0.2505546390167549\n",
            "training error 0.12512234644363399, test error 0.25071012708269924\n",
            "training error 0.12494504616428243, test error 0.2506157826805912\n",
            "training error 0.1249906483538687, test error 0.2507012363156184\n",
            "training error 0.12498022632572768, test error 0.2506835616960321\n",
            "training error 0.12496348210252962, test error 0.25056637866032583\n",
            "training error 0.12506690385565358, test error 0.25067220235581433\n",
            "training error 0.12505388913418586, test error 0.25054706730900894\n",
            "training error 0.12497132583375377, test error 0.2506265011179069\n",
            "training error 0.1250975055793221, test error 0.2505239019227654\n",
            "training error 0.12504655627711636, test error 0.25070037888501556\n",
            "training error 0.12500567024204473, test error 0.2505339733093266\n",
            "training error 0.12499448945833849, test error 0.25062762940565547\n",
            "Loss: 0.22032749354086167\n",
            "training error 0.12511259543489578, test error 0.25043892696753295\n",
            "Loss: 0.14486965119395734\n",
            "training error 0.1250302780086839, test error 0.25058777718475406\n",
            "Loss: 0.2043914906366462\n",
            "training error 0.12495673742864065, test error 0.2505341992672675\n",
            "Loss: 0.18296689172208058\n",
            "training error 0.12520555502871666, test error 0.25048207257941757\n",
            "Loss: 0.16212260675692036\n",
            "training error 0.12497752860196752, test error 0.25049046658256396\n",
            "Loss: 0.1654791790004051\n",
            "training error 0.12498309263079455, test error 0.25065165800015454\n",
            "Loss: 0.22993598569160945\n",
            "training error 0.12505087654827526, test error 0.250621106240327\n",
            "Loss: 0.21771902707983237\n",
            "training error 0.12500458957558105, test error 0.2506157927153071\n",
            "Loss: 0.21559426845323415\n",
            "training error 0.12499446007830534, test error 0.25063274679694697\n",
            "Loss: 0.22237382271996786\n",
            "training error 0.12496955889325553, test error 0.25062579490523046\n",
            "Loss: 0.21959391826100738\n",
            "training error 0.12494302371459289, test error 0.25067063414531093\n",
            "Loss: 0.2375241174815912\n",
            "training error 0.12501115193673326, test error 0.25068002780976967\n",
            "Loss: 0.2412804317022177\n",
            "training error 0.12500823696678942, test error 0.2507408152367522\n",
            "Loss: 0.26558795060633855\n",
            "training error 0.1250292298568436, test error 0.2506748070881855\n",
            "Loss: 0.23919278307322767\n",
            "training error 0.1250770509003958, test error 0.25076549296698264\n",
            "Loss: 0.275456017472564\n",
            "training error 0.12496649805675106, test error 0.25091157505437606\n",
            "Loss: 0.3338709443272858\n",
            "training error 0.12494879671050226, test error 0.2509169431866438\n",
            "Loss: 0.33601753915875054\n",
            "training error 0.12494124654125312, test error 0.25099022376473257\n",
            "Loss: 0.3653207869785957\n",
            "training error 0.12499378310901874, test error 0.2510105041454547\n",
            "Loss: 0.3734304531095356\n",
            "training error 0.12495933550688491, test error 0.25096499074154927\n",
            "Loss: 0.35523067100422523\n",
            "training error 0.1251005600101109, test error 0.2508493592177388\n",
            "Loss: 0.30899223666902564\n",
            "training error 0.12499085386361407, test error 0.25108271436837637\n",
            "Loss: 0.40230569007662087\n",
            "training error 0.12510134246820562, test error 0.2509991174564787\n",
            "Loss: 0.36887717340547965\n",
            "training error 0.12493432446303933, test error 0.2510563173461521\n",
            "Loss: 0.3917501171797122\n",
            "training error 0.12498282929492208, test error 0.2511370265764593\n",
            "Loss: 0.4240239152135672\n",
            "training error 0.1249362815011405, test error 0.2510409604752519\n",
            "Loss: 0.38560925140787017\n",
            "training error 0.12499771041093942, test error 0.2510121491501034\n",
            "Loss: 0.37408825330900175\n",
            "training error 0.12494249904463336, test error 0.250959305240919\n",
            "Loss: 0.352957167733714\n",
            "training error 0.1250059599790408, test error 0.25085941472686296\n",
            "Loss: 0.31301320762073814\n",
            "training error 0.1249521599286888, test error 0.25096999251638896\n",
            "Loss: 0.3572307677759623\n",
            "training error 0.124956653163648, test error 0.2508726697435443\n",
            "Loss: 0.3183135893698896\n",
            "training error 0.12492453192551026, test error 0.2508252907838452\n",
            "Loss: 0.2993678136443245\n",
            "training error 0.12499667007407317, test error 0.2507978251509188\n",
            "Loss: 0.28838492746683375\n",
            "training error 0.12491341006208286, test error 0.2508552990663564\n",
            "Loss: 0.31136744795410287\n",
            "training error 0.12492139776324858, test error 0.25084224553370876\n",
            "Loss: 0.3061476351186254\n",
            "training error 0.1250297454552514, test error 0.2507595309149693\n",
            "Loss: 0.2730719275510607\n",
            "training error 0.12492703281638241, test error 0.25093400374677194\n",
            "Loss: 0.3428396717676696\n",
            "training error 0.12495571019686687, test error 0.25080879194163785\n",
            "Loss: 0.2927702993431147\n",
            "training error 0.1249089036775969, test error 0.2508267296894639\n",
            "Loss: 0.2999431994973767\n",
            "training error 0.12507742761715782, test error 0.25077744127957113\n",
            "Loss: 0.28023387777253994\n",
            "training error 0.12492140339316084, test error 0.2509319508246451\n",
            "Loss: 0.3420187545832398\n",
            "training error 0.12492042808807455, test error 0.2508221246601565\n",
            "Loss: 0.2981017523017826\n",
            "training error 0.12500106726142274, test error 0.2509197095417056\n",
            "Loss: 0.33712374205792095\n",
            "training error 0.1249341589966547, test error 0.2508195714024038\n",
            "Loss: 0.297080762202806\n",
            "training error 0.12497855453594849, test error 0.25075998514271597\n",
            "Loss: 0.27325356296628023\n",
            "training error 0.12495360166816509, test error 0.25082980104971375\n",
            "Loss: 0.3011713670813654\n",
            "training error 0.12491909655664392, test error 0.2508557896142705\n",
            "Loss: 0.31156360698381125\n",
            "training error 0.12493430684775439, test error 0.2507260122946517\n",
            "Loss: 0.25966858844863605\n",
            "training error 0.12508938868690683, test error 0.2508686910840048\n",
            "Loss: 0.31672261329522833\n",
            "training error 0.124919440889816, test error 0.2508150370273867\n",
            "Loss: 0.2952675680619077\n",
            "training error 0.12495943406157764, test error 0.2507845021692768\n",
            "Loss: 0.283057368065176\n",
            "training error 0.1249448601614368, test error 0.25082152784225265\n",
            "Loss: 0.29786309830373003\n",
            "training error 0.12504179032047544, test error 0.2506888895138801\n",
            "Loss: 0.244824026996171\n",
            "training error 0.12496353118696207, test error 0.250736905355422\n",
            "Loss: 0.2640244773839395\n",
            "training error 0.12490074400803747, test error 0.25086147821168675\n",
            "Loss: 0.31383834858906834\n",
            "training error 0.12495259386470756, test error 0.25093230642022746\n",
            "Loss: 0.34216094922396856\n",
            "training error 0.1249479345915664, test error 0.2508313280708202\n",
            "Loss: 0.3017819883274875\n",
            "training error 0.12493921199676748, test error 0.2508807628382272\n",
            "Loss: 0.32154983511614166\n",
            "training error 0.12498823527145325, test error 0.2507200352650183\n",
            "Loss: 0.25727850931522855\n",
            "training error 0.12490984461257972, test error 0.2508376929364731\n",
            "Loss: 0.3043271543241355\n",
            "training error 0.12498336808546134, test error 0.250720676079024\n",
            "Loss: 0.2575347563605668\n",
            "training error 0.12495790058779246, test error 0.25086472244746943\n",
            "Loss: 0.31513564519349835\n",
            "training error 0.1249485731230809, test error 0.2507553946420376\n",
            "Loss: 0.271417925441253\n",
            "training error 0.12487679307308819, test error 0.2508096332609857\n",
            "Loss: 0.293106723945491\n",
            "training error 0.12489364928567113, test error 0.2508709373309581\n",
            "Loss: 0.31762083671067476\n",
            "training error 0.12493698001110717, test error 0.25094104859694116\n",
            "Loss: 0.34565674821203096\n",
            "training error 0.12496177322476856, test error 0.2509892506006185\n",
            "Loss: 0.36493164063247097\n",
            "training error 0.1250653706334593, test error 0.25084894566129773\n",
            "Loss: 0.3088268647901904\n",
            "training error 0.12491905414533107, test error 0.2508773045299738\n",
            "Loss: 0.3201669357664194\n",
            "training error 0.12491711976617631, test error 0.25098374556299236\n",
            "Loss: 0.36273030044080556\n",
            "training error 0.12495131825186184, test error 0.25093984930630875\n",
            "Loss: 0.34517717897928346\n",
            "training error 0.12486602912147085, test error 0.250983468330679\n",
            "Loss: 0.3626194415011774\n",
            "training error 0.12496005178386103, test error 0.2509208836463755\n",
            "Loss: 0.3375932399931836\n",
            "training error 0.12498301287233167, test error 0.25098430525076165\n",
            "Loss: 0.36295410693678587\n",
            "training error 0.12487631058936616, test error 0.25109371395689944\n",
            "Loss: 0.40670417705386264\n",
            "training error 0.1248729828937074, test error 0.25100658521269087\n",
            "Loss: 0.3718633604232524\n",
            "training error 0.12486237323391963, test error 0.2510120766557558\n",
            "Loss: 0.374059264456994\n",
            "training error 0.12487425350434364, test error 0.25104030984981496\n",
            "Loss: 0.38534908099281395\n",
            "training error 0.12495239782208264, test error 0.25107451805140385\n",
            "Loss: 0.3990281680684715\n",
            "training error 0.12502873737510928, test error 0.2509044417824385\n",
            "Loss: 0.3310185100151575\n",
            "training error 0.1248521381544768, test error 0.25102126064560293\n",
            "Loss: 0.37773173453685693\n",
            "training error 0.12491487571670319, test error 0.2509260761584031\n",
            "Loss: 0.3396696082577533\n",
            "training error 0.12498077281004066, test error 0.2508975413007383\n",
            "Loss: 0.3282591632604026\n",
            "training error 0.12489180868227324, test error 0.25103003002555535\n",
            "Loss: 0.3812384114855627\n",
            "training error 0.1250454957264952, test error 0.2508747316618649\n",
            "Loss: 0.31913810392898423\n",
            "training error 0.12486490456736656, test error 0.2509546641093101\n",
            "Loss: 0.3511012840432981\n",
            "training error 0.12484232441826272, test error 0.25094219588311384\n",
            "Loss: 0.3461155220360723\n",
            "training error 0.12501750702728623, test error 0.25106647493886874\n",
            "Loss: 0.3958119090540668\n",
            "training error 0.12499746615425411, test error 0.2508996716021134\n",
            "Loss: 0.3291110226582683\n",
            "training error 0.12485744364606266, test error 0.25101411288495995\n",
            "Loss: 0.37487350651872475\n",
            "training error 0.12483592338555155, test error 0.25095339662649396\n",
            "Loss: 0.35059444629670367\n",
            "training error 0.12487849800459744, test error 0.2509187979287766\n",
            "Loss: 0.3367592086402915\n",
            "training error 0.12491162129983135, test error 0.25083786458369417\n",
            "Loss: 0.3043957921704532\n",
            "training error 0.12481768215843088, test error 0.2508255279049212\n",
            "Loss: 0.29946263300622\n",
            "training error 0.12489314367596678, test error 0.25076724934099953\n",
            "Loss: 0.27615835176661996\n",
            "training error 0.12494629082977263, test error 0.250786597581246\n",
            "Loss: 0.28389527597774133\n",
            "training error 0.12482121217324184, test error 0.25084347183790334\n",
            "Loss: 0.30663800646468786\n",
            "training error 0.12481047599510772, test error 0.250787247553683\n",
            "Loss: 0.28415518527287187\n",
            "training error 0.1248527240334209, test error 0.250755289302452\n",
            "Loss: 0.27137580252052373\n",
            "training error 0.12493511277514589, test error 0.25084918194547406\n",
            "Loss: 0.3089213494948062\n",
            "training error 0.12483188139756693, test error 0.25073269569637485\n",
            "Loss: 0.2623411298242839\n",
            "training error 0.12482791319961625, test error 0.2508453528162683\n",
            "Loss: 0.30739016722280965\n",
            "training error 0.12487414953863737, test error 0.2507832969552024\n",
            "Loss: 0.2825754301817973\n",
            "training error 0.1248925285970531, test error 0.25079715028281746\n",
            "Loss: 0.28811506295791656\n",
            "training error 0.12482325290345064, test error 0.2507609552856051\n",
            "Loss: 0.2736415011927784\n",
            "training error 0.12479880032396949, test error 0.2506634849803279\n",
            "Loss: 0.23466532789959693\n",
            "training error 0.12478073893230474, test error 0.2506894585784764\n",
            "Loss: 0.24505158307346342\n",
            "training error 0.1248873455569583, test error 0.2508355858606323\n",
            "Loss: 0.3034845822927901\n",
            "training error 0.12480694056518597, test error 0.2507358634836069\n",
            "Loss: 0.26360785638024353\n",
            "training error 0.12477150562782503, test error 0.25083004410402443\n",
            "Loss: 0.3012685590097819\n",
            "training error 0.12477794373690426, test error 0.250904209691974\n",
            "Loss: 0.3309257022811929\n",
            "training error 0.12480973714404266, test error 0.2509776856360366\n",
            "Loss: 0.36030707254077754\n",
            "training error 0.12476883220707057, test error 0.2509842962236217\n",
            "Loss: 0.3629504971874198\n",
            "training error 0.12474323496022018, test error 0.25101108599717004\n",
            "Loss: 0.37366312246684963\n",
            "training error 0.12481895562095484, test error 0.25095459638017614\n",
            "Loss: 0.35107420069258044\n",
            "training error 0.12476329359458431, test error 0.2508633684496496\n",
            "Loss: 0.3145942120512446\n",
            "training error 0.12493554441931268, test error 0.2510583790137146\n",
            "Loss: 0.3925745314662832\n",
            "training error 0.124710241003787, test error 0.25093407292701286\n",
            "Loss: 0.34286733538329006\n",
            "training error 0.12493051594032525, test error 0.2507521268383315\n",
            "Loss: 0.2701112045566001\n",
            "training error 0.12488284424618007, test error 0.2509510055657349\n",
            "Loss: 0.34963831511156673\n",
            "training error 0.12472339637475229, test error 0.2509541811467983\n",
            "Loss: 0.35090815824461163\n",
            "training error 0.12470456685496355, test error 0.25105414491785416\n",
            "Loss: 0.3908814121770243\n",
            "training error 0.12474209708731354, test error 0.2509859763048748\n",
            "Loss: 0.3636223237286629\n",
            "training error 0.12470195022734011, test error 0.2509375803202117\n",
            "Loss: 0.34426986269389115\n",
            "training error 0.12470736526999211, test error 0.25088915139956647\n",
            "Loss: 0.3249042313038908\n",
            "training error 0.12481930601684742, test error 0.25103760378298445\n",
            "Loss: 0.38426698599549614\n",
            "training error 0.12500636099823118, test error 0.2510553740404668\n",
            "Loss: 0.39137291054478496\n",
            "training error 0.12478248620291678, test error 0.2508038679199866\n",
            "Loss: 0.2908012943150462\n",
            "training error 0.12464146443928145, test error 0.25094745207718083\n",
            "Loss: 0.3482173553096546\n",
            "training error 0.12465206103565064, test error 0.2509121950572154\n",
            "Loss: 0.3341188694573116\n",
            "training error 0.12463847899354444, test error 0.25083660517290407\n",
            "Loss: 0.3038921822447138\n",
            "training error 0.12464083316901035, test error 0.250878365427737\n",
            "Loss: 0.3205911648169746\n",
            "training error 0.12475346132322268, test error 0.2506759372575331\n",
            "Loss: 0.23964471226558537\n",
            "training error 0.12474027894529251, test error 0.2507041605965346\n",
            "Loss: 0.2509305879864643\n",
            "training error 0.12459133424140041, test error 0.25078708163357183\n",
            "Loss: 0.2840888375684303\n",
            "training error 0.12463983156998529, test error 0.25073291606882\n",
            "Loss: 0.262429251787033\n",
            "training error 0.12458772491682016, test error 0.2507877893412737\n",
            "Loss: 0.28437183389180554\n",
            "training error 0.12458377254037592, test error 0.2506653058019279\n",
            "Loss: 0.23539343332632523\n",
            "training error 0.12456603762283504, test error 0.2505751771078855\n",
            "Loss: 0.1993530045239833\n",
            "training error 0.12456766826952956, test error 0.25060014815287174\n",
            "Loss: 0.20933836133523176\n",
            "training error 0.12457407311199922, test error 0.25048287655354984\n",
            "Loss: 0.16244409785119984\n",
            "training error 0.1245498303927836, test error 0.25054746376837594\n",
            "Loss: 0.18827106607939292\n",
            "training error 0.1244890998252377, test error 0.25053579409038335\n",
            "Loss: 0.1836046254601431\n",
            "training error 0.12448200524125877, test error 0.2504715648813917\n",
            "Loss: 0.1579208156780476\n",
            "training error 0.12450322531421765, test error 0.25044557150399044\n",
            "Loss: 0.14752665122779174\n",
            "training error 0.124500457114578, test error 0.25054639185592675\n",
            "Loss: 0.18784243250471455\n",
            "training error 0.12445855422708044, test error 0.25036647447439847\n",
            "Loss: 0.11589753584146933\n",
            "training error 0.12440524754620283, test error 0.250397084891942\n",
            "Loss: 0.12813795034885\n",
            "training error 0.12446455730866891, test error 0.25030674445665285\n",
            "Loss: 0.09201285100501888\n",
            "training error 0.12439508543046583, test error 0.250343173628244\n",
            "Loss: 0.10658005381438684\n",
            "training error 0.12438331399224607, test error 0.25026106003820875\n",
            "Loss: 0.0737446840488154\n",
            "training error 0.12438270013036146, test error 0.2503561826894839\n",
            "Loss: 0.11178208353848085\n",
            "training error 0.12432164523433585, test error 0.25033384583213547\n",
            "Loss: 0.1028500788590847\n",
            "training error 0.12441271236365722, test error 0.25033267934144726\n",
            "Loss: 0.10238362558310232\n",
            "training error 0.12430371944488453, test error 0.25025052824015104\n",
            "Loss: 0.06953325591161352\n",
            "training error 0.12426050131033652, test error 0.2502320430121908\n",
            "Loss: 0.06214143081890722\n",
            "training error 0.12425454461690427, test error 0.25029751778231774\n",
            "Loss: 0.08832331236263702\n",
            "training error 0.12416491959252424, test error 0.2501439129463666\n",
            "Loss: 0.026900208293390016\n",
            "training error 0.12421602448738435, test error 0.2501073032073138\n",
            "Loss: 0.012260800634988911\n",
            "training error 0.12415822455324824, test error 0.25018149712053545\n",
            "Loss: 0.041929270542850894\n",
            "training error 0.12407481130413854, test error 0.2501257221292989\n",
            "Loss: 0.019626111466042317\n",
            "training error 0.12413510479038066, test error 0.25009460196810207\n",
            "Loss: 0.007181861986471638\n",
            "training error 0.12398814290888958, test error 0.2498985960980683\n",
            "Loss: 0.0\n",
            "training error 0.12397791300645507, test error 0.2498475131522775\n",
            "Loss: 0.0\n",
            "training error 0.12391623477101367, test error 0.24981764036834186\n",
            "Loss: 0.0\n",
            "training error 0.12392986178131249, test error 0.24970707938251324\n",
            "Loss: 0.0\n",
            "training error 0.1238456615348584, test error 0.24946779491084198\n",
            "Loss: 0.0\n",
            "training error 0.12380847925294874, test error 0.24944005088470525\n",
            "Loss: 0.0\n",
            "training error 0.1237668478262202, test error 0.2493235047358981\n",
            "Loss: 0.0\n",
            "training error 0.12369826129018828, test error 0.249348312431871\n",
            "Loss: 0.00995000290855419\n",
            "training error 0.12371071989871894, test error 0.24927644218308678\n",
            "Loss: 0.0\n",
            "training error 0.12382032257170002, test error 0.24898102497022606\n",
            "Loss: 0.0\n",
            "training error 0.12350302944416917, test error 0.24886125812764104\n",
            "Loss: 0.0\n",
            "training error 0.12347740327988803, test error 0.24875674737785336\n",
            "Loss: 0.0\n",
            "training error 0.12341228650074752, test error 0.2487004279445429\n",
            "Loss: 0.0\n",
            "training error 0.1234726601537877, test error 0.24863535661329947\n",
            "Loss: 0.0\n",
            "training error 0.12328131319679866, test error 0.2484593502606217\n",
            "Loss: 0.0\n",
            "training error 0.12315788729398705, test error 0.24827822928136226\n",
            "Loss: 0.0\n",
            "training error 0.12309280279842715, test error 0.24806811580853427\n",
            "Loss: 0.0\n",
            "training error 0.12306240755329625, test error 0.24778913207680378\n",
            "Loss: 0.0\n",
            "training error 0.12291397772906541, test error 0.24764819421429246\n",
            "Loss: 0.0\n",
            "training error 0.12285911886358418, test error 0.24743579955428435\n",
            "Loss: 0.0\n",
            "training error 0.12282390284367413, test error 0.24725552760639366\n",
            "Loss: 0.0\n",
            "training error 0.12265642917810152, test error 0.2470263573476371\n",
            "Loss: 0.0\n",
            "training error 0.12260357833816185, test error 0.24679661886190635\n",
            "Loss: 0.0\n",
            "training error 0.12244182589422668, test error 0.24655473311097645\n",
            "Loss: 0.0\n",
            "training error 0.12237683039647801, test error 0.24636154823784087\n",
            "Loss: 0.0\n",
            "training error 0.12225633611661972, test error 0.2460290180916752\n",
            "Loss: 0.0\n",
            "training error 0.12203258808817816, test error 0.24591328782489075\n",
            "Loss: 0.0\n",
            "training error 0.12220610695193683, test error 0.24554695021416775\n",
            "Loss: 0.0\n",
            "training error 0.12176368880682643, test error 0.2454955972259122\n",
            "Loss: 0.0\n",
            "training error 0.12164725784253211, test error 0.24520512995203594\n",
            "Loss: 0.0\n",
            "training error 0.12154161846174104, test error 0.24479198454685885\n",
            "Loss: 0.0\n",
            "training error 0.12128949084660967, test error 0.244463889127983\n",
            "Loss: 0.0\n",
            "training error 0.12127729942774883, test error 0.2442527144211415\n",
            "Loss: 0.0\n",
            "training error 0.120951438054611, test error 0.24380012051691705\n",
            "Loss: 0.0\n",
            "training error 0.12088711600026461, test error 0.24327974878426595\n",
            "Loss: 0.0\n",
            "training error 0.12058193646817786, test error 0.24291157185857684\n",
            "Loss: 0.0\n",
            "training error 0.12038297315974174, test error 0.24252321931824347\n",
            "Loss: 0.0\n",
            "training error 0.12011142482675412, test error 0.2421006694866377\n",
            "Loss: 0.0\n",
            "training error 0.12016355315236518, test error 0.2415277926752924\n",
            "Loss: 0.0\n",
            "training error 0.11975084989603473, test error 0.24123259751311074\n",
            "Loss: 0.0\n",
            "training error 0.11939019262515108, test error 0.24061059153253372\n",
            "Loss: 0.0\n",
            "training error 0.1191526886197048, test error 0.24011427707947808\n",
            "Loss: 0.0\n",
            "training error 0.11885349948856769, test error 0.2395638810995934\n",
            "Loss: 0.0\n",
            "training error 0.11861607100742971, test error 0.23900125784970602\n",
            "Loss: 0.0\n",
            "training error 0.1183976336437429, test error 0.23825377290724195\n",
            "Loss: 0.0\n",
            "training error 0.118035469893394, test error 0.2377379746315367\n",
            "Loss: 0.0\n",
            "training error 0.1176995772161734, test error 0.23691634796229621\n",
            "Loss: 0.0\n",
            "training error 0.11735389949087965, test error 0.23629773456723183\n",
            "Loss: 0.0\n",
            "training error 0.1169317034323811, test error 0.23543081505483232\n",
            "Loss: 0.0\n",
            "training error 0.11657856692596943, test error 0.23468424168341603\n",
            "Loss: 0.0\n",
            "training error 0.11627509568434737, test error 0.23389864344365213\n",
            "Loss: 0.0\n",
            "training error 0.11582648707243001, test error 0.23295903962719525\n",
            "Loss: 0.0\n",
            "training error 0.1154683526118779, test error 0.2321049782370321\n",
            "Loss: 0.0\n",
            "training error 0.11492362066337004, test error 0.23112256943068776\n",
            "Loss: 0.0\n",
            "training error 0.11451184647620753, test error 0.23023615776248982\n",
            "Loss: 0.0\n",
            "training error 0.11402774596705141, test error 0.22921352098135783\n",
            "Loss: 0.0\n",
            "training error 0.11353444508763384, test error 0.22836293087433676\n",
            "Loss: 0.0\n",
            "training error 0.11308590480983545, test error 0.2272363895783223\n",
            "Loss: 0.0\n",
            "training error 0.1124577561450543, test error 0.226236191281614\n",
            "Loss: 0.0\n",
            "training error 0.1119555445772879, test error 0.2250079381588703\n",
            "Loss: 0.0\n",
            "training error 0.11143558626660618, test error 0.2238853201867109\n",
            "Loss: 0.0\n",
            "training error 0.11112347466915247, test error 0.22249807414027847\n",
            "Loss: 0.0\n",
            "training error 0.1101460811229114, test error 0.22141157681872076\n",
            "Loss: 0.0\n",
            "training error 0.10962267768586306, test error 0.21992275147589307\n",
            "Loss: 0.0\n",
            "training error 0.1088957939291892, test error 0.21853822762294722\n",
            "Loss: 0.0\n",
            "training error 0.1082195119980395, test error 0.2171335105450045\n",
            "Loss: 0.0\n",
            "training error 0.10757708987569115, test error 0.21571786126253883\n",
            "Loss: 0.0\n",
            "training error 0.10688544485557075, test error 0.21414500148896456\n",
            "Loss: 0.0\n",
            "training error 0.10614738956472776, test error 0.21275356230862277\n",
            "Loss: 0.0\n",
            "training error 0.1056166679425861, test error 0.2112856972153051\n",
            "Loss: 0.0\n",
            "training error 0.10463205622836246, test error 0.20943315360625767\n",
            "Loss: 0.0\n",
            "training error 0.1037862897593226, test error 0.20779569292254044\n",
            "Loss: 0.0\n",
            "training error 0.1029888596936368, test error 0.206154057204567\n",
            "Loss: 0.0\n",
            "training error 0.10216740242580585, test error 0.20444157560490298\n",
            "Loss: 0.0\n",
            "training error 0.10141582853890228, test error 0.202689395489033\n",
            "Loss: 0.0\n",
            "training error 0.10050762453548949, test error 0.20086877998031563\n",
            "Loss: 0.0\n",
            "training error 0.09990220100387132, test error 0.19896799705171714\n",
            "Loss: 0.0\n",
            "training error 0.098820905503445, test error 0.197045953226086\n",
            "Loss: 0.0\n",
            "training error 0.09807484891221231, test error 0.19533462218221181\n",
            "Loss: 0.0\n",
            "training error 0.09690703845342848, test error 0.19335014913214196\n",
            "Loss: 0.0\n",
            "training error 0.0962507266092903, test error 0.19156836842810102\n",
            "Loss: 0.0\n",
            "training error 0.09509767545431577, test error 0.18951332155076445\n",
            "Loss: 0.0\n",
            "training error 0.09419848812994149, test error 0.18757088689099152\n",
            "Loss: 0.0\n",
            "training error 0.0933415723503296, test error 0.18542782296180263\n",
            "Loss: 0.0\n",
            "training error 0.09223884847945765, test error 0.1835627395477319\n",
            "Loss: 0.0\n",
            "training error 0.09131428864687417, test error 0.1814450753928286\n",
            "Loss: 0.0\n",
            "training error 0.09034243861171824, test error 0.1792841297270941\n",
            "Loss: 0.0\n",
            "training error 0.0893347451465214, test error 0.17724325531340845\n",
            "Loss: 0.0\n",
            "training error 0.08842230699176855, test error 0.17505434826260255\n",
            "Loss: 0.0\n",
            "training error 0.08746734135710793, test error 0.17306175183689282\n",
            "Loss: 0.0\n",
            "training error 0.08644625719237324, test error 0.17087835990763042\n",
            "Loss: 0.0\n",
            "training error 0.08541833213878619, test error 0.16876293809431853\n",
            "Loss: 0.0\n",
            "training error 0.08445086193082427, test error 0.16658105288639927\n",
            "Loss: 0.0\n",
            "training error 0.0835174878814777, test error 0.16434834175955906\n",
            "Loss: 0.0\n",
            "training error 0.08259142143816599, test error 0.16251420876003173\n",
            "Loss: 0.0\n",
            "training error 0.08144357942706386, test error 0.1602719636541824\n",
            "Loss: 0.0\n",
            "training error 0.08053402262678047, test error 0.15819965293812024\n",
            "Loss: 0.0\n",
            "training error 0.07951922848605784, test error 0.15622196501036456\n",
            "Loss: 0.0\n",
            "training error 0.0785722958448512, test error 0.1541607840573304\n",
            "Loss: 0.0\n",
            "training error 0.07768025698721613, test error 0.15198237735740888\n",
            "Loss: 0.0\n",
            "training error 0.07672283684339606, test error 0.14991739490719694\n",
            "Loss: 0.0\n",
            "training error 0.07577877606709933, test error 0.1479413329292867\n",
            "Loss: 0.0\n",
            "training error 0.07481601519207448, test error 0.14598518971425492\n",
            "Loss: 0.0\n",
            "training error 0.07391514376712724, test error 0.14400112219596162\n",
            "Loss: 0.0\n",
            "training error 0.0731460606376385, test error 0.1418978829811615\n",
            "Loss: 0.0\n",
            "training error 0.07210422773174213, test error 0.13991047307603388\n",
            "Loss: 0.0\n",
            "training error 0.07126073649996345, test error 0.1381752300403236\n",
            "Loss: 0.0\n",
            "training error 0.07033107565495397, test error 0.1361691632390334\n",
            "Loss: 0.0\n",
            "training error 0.06954680578932669, test error 0.1341873200274375\n",
            "Loss: 0.0\n",
            "training error 0.0686937425859227, test error 0.1325194392291593\n",
            "Loss: 0.0\n",
            "training error 0.06783106722139917, test error 0.1305464760763192\n",
            "Loss: 0.0\n",
            "training error 0.06705708560408616, test error 0.128513338627339\n",
            "Loss: 0.0\n",
            "training error 0.06615018879633175, test error 0.1267757878868408\n",
            "Loss: 0.0\n",
            "training error 0.06538704687981053, test error 0.12509209588561032\n",
            "Loss: 0.0\n",
            "training error 0.06464787633781766, test error 0.1233804698332033\n",
            "Loss: 0.0\n",
            "training error 0.06383322429684445, test error 0.12157082000067472\n",
            "Loss: 0.0\n",
            "training error 0.0630764525692162, test error 0.11990196874862506\n",
            "Loss: 0.0\n",
            "training error 0.06233438091819131, test error 0.1183629636562928\n",
            "Loss: 0.0\n",
            "training error 0.06162815862054025, test error 0.1167569370761518\n",
            "Loss: 0.0\n",
            "training error 0.06084131392550987, test error 0.1150300697470038\n",
            "Loss: 0.0\n",
            "training error 0.06020289420410606, test error 0.11336557629301448\n",
            "Loss: 0.0\n",
            "training error 0.05945513258397363, test error 0.11180456398624256\n",
            "Loss: 0.0\n",
            "training error 0.05879745870120773, test error 0.11017342624490142\n",
            "Loss: 0.0\n",
            "training error 0.05808397492178312, test error 0.10869981403298844\n",
            "Loss: 0.0\n",
            "training error 0.057440611820279194, test error 0.10728942282954333\n",
            "Loss: 0.0\n",
            "training error 0.05680471219462899, test error 0.1058505820594602\n",
            "Loss: 0.0\n",
            "training error 0.056191671545352, test error 0.10450059776428258\n",
            "Loss: 0.0\n",
            "training error 0.05560633784402157, test error 0.10305816816761722\n",
            "Loss: 0.0\n",
            "training error 0.055053983691204664, test error 0.1015357426996435\n",
            "Loss: 0.0\n",
            "training error 0.05435918823904362, test error 0.10033361579423818\n",
            "Loss: 0.0\n",
            "training error 0.05379781877886902, test error 0.09905105396946302\n",
            "Loss: 0.0\n",
            "training error 0.0532309627166336, test error 0.0975647423246661\n",
            "Loss: 0.0\n",
            "training error 0.05265932570726207, test error 0.09629844095466474\n",
            "Loss: 0.0\n",
            "training error 0.05217477058930581, test error 0.09516062201632103\n",
            "Loss: 0.0\n",
            "training error 0.051589940690207545, test error 0.09405882005219636\n",
            "Loss: 0.0\n",
            "training error 0.05107434287861407, test error 0.09280024768260962\n",
            "Loss: 0.0\n",
            "training error 0.05059669548904794, test error 0.09147133769134617\n",
            "Loss: 0.0\n",
            "training error 0.05005839596367976, test error 0.09047416581402308\n",
            "Loss: 0.0\n",
            "training error 0.04956549339560141, test error 0.08912498935358473\n",
            "Loss: 0.0\n",
            "training error 0.04907069128470205, test error 0.08800331789515492\n",
            "Loss: 0.0\n",
            "training error 0.04861769574560747, test error 0.08690886015790358\n",
            "Loss: 0.0\n",
            "training error 0.04820381667134985, test error 0.08580386337249891\n",
            "Loss: 0.0\n",
            "training error 0.04778617918468457, test error 0.08478754985926162\n",
            "Loss: 0.0\n",
            "training error 0.04738015845605393, test error 0.08395450644051933\n",
            "Loss: 0.0\n",
            "training error 0.046871523206952495, test error 0.08277781307378677\n",
            "Loss: 0.0\n",
            "training error 0.04646381256191768, test error 0.08184675368690686\n",
            "Loss: 0.0\n",
            "training error 0.0460990688521251, test error 0.08098876153339475\n",
            "Loss: 0.0\n",
            "training error 0.04569251984022873, test error 0.0800727963110309\n",
            "Loss: 0.0\n",
            "training error 0.045295079043735385, test error 0.07916966526116632\n",
            "Loss: 0.0\n",
            "training error 0.04494295803229803, test error 0.07835169039951892\n",
            "Loss: 0.0\n",
            "training error 0.044554097108909514, test error 0.07742030495856857\n",
            "Loss: 0.0\n",
            "training error 0.044196517734006444, test error 0.07656416582510296\n",
            "Loss: 0.0\n",
            "training error 0.04389610620590339, test error 0.07562483222608711\n",
            "Loss: 0.0\n",
            "training error 0.04351208889157519, test error 0.07499668126093455\n",
            "Loss: 0.0\n",
            "training error 0.043223201984058934, test error 0.07407437485541167\n",
            "Loss: 0.0\n",
            "training error 0.04293573038680285, test error 0.07329484145128488\n",
            "Loss: 0.0\n",
            "training error 0.04254943744728505, test error 0.07238300057671115\n",
            "Loss: 0.0\n",
            "training error 0.04228313986102186, test error 0.07155462272042713\n",
            "Loss: 0.0\n",
            "training error 0.04194146242192598, test error 0.07102125617583158\n",
            "Loss: 0.0\n",
            "training error 0.04167912198552253, test error 0.07043740389877196\n",
            "Loss: 0.0\n",
            "training error 0.04139829163239552, test error 0.06957807429188825\n",
            "Loss: 0.0\n",
            "training error 0.04112692374737294, test error 0.06904736676427634\n",
            "Loss: 0.0\n",
            "training error 0.04081567067925746, test error 0.06824536751506391\n",
            "Loss: 0.0\n",
            "training error 0.040570375182822124, test error 0.0676739538636644\n",
            "Loss: 0.0\n",
            "training error 0.040431344555437884, test error 0.06686322300087527\n",
            "Loss: 0.0\n",
            "training error 0.04014158873999684, test error 0.06658115038984819\n",
            "Loss: 0.0\n",
            "training error 0.03979384836341376, test error 0.06568128558036501\n",
            "Loss: 0.0\n",
            "training error 0.03956019122128312, test error 0.06525847986042882\n",
            "Loss: 0.0\n",
            "training error 0.03933804195162827, test error 0.06473101032279521\n",
            "Loss: 0.0\n",
            "training error 0.03912448059702849, test error 0.06413596907325304\n",
            "Loss: 0.0\n",
            "training error 0.03886337452926472, test error 0.06354763526244173\n",
            "Loss: 0.0\n",
            "training error 0.038659441338321485, test error 0.06306238209704468\n",
            "Loss: 0.0\n",
            "training error 0.03844146676900439, test error 0.06260654686958493\n",
            "Loss: 0.0\n",
            "training error 0.03826201063218951, test error 0.062198431895035405\n",
            "Loss: 0.0\n",
            "training error 0.03806333694956327, test error 0.061561872327873884\n",
            "Loss: 0.0\n",
            "training error 0.03785470762878806, test error 0.06121514661437031\n",
            "Loss: 0.0\n",
            "training error 0.03766491536030047, test error 0.06078683400590842\n",
            "Loss: 0.0\n",
            "training error 0.037486267246841315, test error 0.060316094677983226\n",
            "Loss: 0.0\n",
            "training error 0.037258399914700424, test error 0.059887821064607466\n",
            "Loss: 0.0\n",
            "training error 0.037099900534405164, test error 0.05948795752373328\n",
            "Loss: 0.0\n",
            "training error 0.03695761536289497, test error 0.05917401021205113\n",
            "Loss: 0.0\n",
            "training error 0.03676283134300151, test error 0.05870420085009934\n",
            "Loss: 0.0\n",
            "training error 0.036611630793879496, test error 0.05809630468861427\n",
            "Loss: 0.0\n",
            "training error 0.03646053252957988, test error 0.05777846679889697\n",
            "Loss: 0.0\n",
            "training error 0.03632934823098708, test error 0.05730482423201843\n",
            "Loss: 0.0\n",
            "training error 0.03611258746342334, test error 0.05701046154384374\n",
            "Loss: 0.0\n",
            "training error 0.036011332336887046, test error 0.05650402967472331\n",
            "Loss: 0.0\n",
            "training error 0.03580525056100006, test error 0.0561838450964812\n",
            "Loss: 0.0\n",
            "training error 0.03567859967108948, test error 0.05592183796775075\n",
            "Loss: 0.0\n",
            "training error 0.035534028825612456, test error 0.055405527538438866\n",
            "Loss: 0.0\n",
            "training error 0.035415476924154615, test error 0.05513153538299381\n",
            "Loss: 0.0\n",
            "training error 0.03537094980735427, test error 0.05483876638780665\n",
            "Loss: 0.0\n",
            "training error 0.03518086327831675, test error 0.05459544030876107\n",
            "Loss: 0.0\n",
            "training error 0.03498366827814053, test error 0.05418170592078793\n",
            "Loss: 0.0\n",
            "training error 0.03484981816546087, test error 0.05395386143562372\n",
            "Loss: 0.0\n",
            "training error 0.034734672605800695, test error 0.053586350776392004\n",
            "Loss: 0.0\n",
            "training error 0.03463529697519851, test error 0.053253188302626965\n",
            "Loss: 0.0\n",
            "training error 0.034500725171504494, test error 0.05293403593484653\n",
            "Loss: 0.0\n",
            "training error 0.03442314367375555, test error 0.05275863891713893\n",
            "Loss: 0.0\n",
            "training error 0.034274763545805866, test error 0.05239611499914833\n",
            "Loss: 0.0\n",
            "training error 0.03420783537086234, test error 0.05228276633114288\n",
            "Loss: 0.0\n",
            "training error 0.03404739766743789, test error 0.052032117502840196\n",
            "Loss: 0.0\n",
            "training error 0.03395513143379623, test error 0.05174045243442464\n",
            "Loss: 0.0\n",
            "training error 0.03387131166548422, test error 0.051392015547083024\n",
            "Loss: 0.0\n",
            "training error 0.03372192871629429, test error 0.05118735587725145\n",
            "Loss: 0.0\n",
            "training error 0.03363437517221657, test error 0.051063400578270156\n",
            "Loss: 0.0\n",
            "training error 0.0335474875516967, test error 0.050796652858959276\n",
            "Loss: 0.0\n",
            "training error 0.0334535859405219, test error 0.05063657062219926\n",
            "Loss: 0.0\n",
            "training error 0.03336373456517088, test error 0.05036705690485457\n",
            "Loss: 0.0\n",
            "training error 0.0332373215152498, test error 0.050314374595491165\n",
            "Loss: 0.0\n",
            "training error 0.03319853292470068, test error 0.05005194569828977\n",
            "Loss: 0.0\n",
            "training error 0.03307030564251587, test error 0.04976995041668965\n",
            "Loss: 0.0\n",
            "training error 0.03298442342296319, test error 0.04957930581363286\n",
            "Loss: 0.0\n",
            "training error 0.03288521975751985, test error 0.04944390214437413\n",
            "Loss: 0.0\n",
            "training error 0.03280016874917367, test error 0.049251506244527905\n",
            "Loss: 0.0\n",
            "training error 0.03271865112342035, test error 0.04911177496296414\n",
            "Loss: 0.0\n",
            "training error 0.0326453738238474, test error 0.04883918698979007\n",
            "Loss: 0.0\n",
            "training error 0.032561286281325805, test error 0.04857821151799437\n",
            "Loss: 0.0\n",
            "training error 0.03250584403533934, test error 0.04848508629412711\n",
            "Loss: 0.0\n",
            "training error 0.03240941027026705, test error 0.0483916036423183\n",
            "Loss: 0.0\n",
            "training error 0.032326757092477426, test error 0.04820842975889921\n",
            "Loss: 0.0\n",
            "training error 0.03228464887787515, test error 0.04804612719013893\n",
            "Loss: 0.0\n",
            "training error 0.032203259033058054, test error 0.048088196897368624\n",
            "Loss: 0.0875610786759129\n",
            "training error 0.032107654058785545, test error 0.04782042161648312\n",
            "Loss: 0.0\n",
            "training error 0.03207163840885569, test error 0.04790664398557923\n",
            "Loss: 0.18030449373198643\n",
            "training error 0.031993289951689714, test error 0.04757334874317992\n",
            "Loss: 0.0\n",
            "training error 0.03188724160991636, test error 0.04750310914313827\n",
            "Loss: 0.0\n",
            "training error 0.03182122517779205, test error 0.047440065588087985\n",
            "Loss: 0.0\n",
            "training error 0.03177742643641579, test error 0.047177486064664646\n",
            "Loss: 0.0\n",
            "training error 0.031703977343872626, test error 0.047194948767643775\n",
            "Loss: 0.037014907821064646\n",
            "training error 0.03163005715273317, test error 0.04711874354637045\n",
            "Loss: 0.0\n",
            "training error 0.031545387846382084, test error 0.046922473883329985\n",
            "Loss: 0.0\n",
            "training error 0.03150777208166769, test error 0.046730828756910786\n",
            "Loss: 0.0\n",
            "training error 0.03142103724265571, test error 0.04659038324855203\n",
            "Loss: 0.0\n",
            "training error 0.031389805365284004, test error 0.04653999985300598\n",
            "Loss: 0.0\n",
            "training error 0.03130180143423927, test error 0.0465278763947772\n",
            "Loss: 0.0\n",
            "training error 0.03123336101546974, test error 0.04616870346517139\n",
            "Loss: 0.0\n",
            "training error 0.031173822182695102, test error 0.04603165725403123\n",
            "Loss: 0.0\n",
            "training error 0.03118022064227036, test error 0.045929105562871525\n",
            "Loss: 0.0\n",
            "training error 0.03112170379971683, test error 0.04567358308899178\n",
            "Loss: 0.0\n",
            "training error 0.031056435476417184, test error 0.04567926681717727\n",
            "Loss: 0.012444235378716861\n",
            "training error 0.030975188612405037, test error 0.04554929623196359\n",
            "Loss: 0.0\n",
            "training error 0.03091025038557916, test error 0.04553147352876323\n",
            "Loss: 0.0\n",
            "training error 0.0308517281348523, test error 0.04535288673719623\n",
            "Loss: 0.0\n",
            "training error 0.030831638084074586, test error 0.04538833331812206\n",
            "Loss: 0.0781572761425986\n",
            "training error 0.030761660241310537, test error 0.045328043700047296\n",
            "Loss: 0.0\n",
            "training error 0.030672394635809088, test error 0.045327574761263256\n",
            "Loss: 0.0\n",
            "training error 0.030614976136369494, test error 0.04525522473709118\n",
            "Loss: 0.0\n",
            "training error 0.030566641408169472, test error 0.045116232501214056\n",
            "Loss: 0.0\n",
            "training error 0.030536714125576264, test error 0.045128563047162625\n",
            "Loss: 0.027330619745868923\n",
            "training error 0.03051117372488477, test error 0.04501420796975643\n",
            "Loss: 0.0\n",
            "training error 0.030404525555115053, test error 0.044966480964830746\n",
            "Loss: 0.0\n",
            "training error 0.03036615581369131, test error 0.044878417600680724\n",
            "Loss: 0.0\n",
            "training error 0.030322311741295703, test error 0.04487538328448962\n",
            "Loss: 0.0\n",
            "training error 0.030299722585338883, test error 0.044925681931894175\n",
            "Loss: 0.11208516501282073\n",
            "training error 0.030276741890899402, test error 0.04477132388565901\n",
            "Loss: 0.0\n",
            "training error 0.030152103406825125, test error 0.044744912037308064\n",
            "Loss: 0.0\n",
            "training error 0.030169025016705497, test error 0.04457215830398323\n",
            "Loss: 0.0\n",
            "training error 0.03006548411656278, test error 0.04471781611414749\n",
            "Loss: 0.326791018668815\n",
            "training error 0.030029048067471475, test error 0.04457548064695745\n",
            "Loss: 0.007453852585648235\n",
            "training error 0.02998391526237607, test error 0.044664375417697216\n",
            "Loss: 0.20689398320150598\n",
            "training error 0.030043495536230765, test error 0.04437263122267701\n",
            "Loss: 0.0\n",
            "training error 0.029889725384505985, test error 0.04447111352087733\n",
            "Loss: 0.22194378716489105\n",
            "training error 0.02984287625256983, test error 0.04456150062392803\n",
            "Loss: 0.42564390717154676\n",
            "training error 0.029792445933762533, test error 0.04446201337833102\n",
            "Loss: 0.20143532892034077\n",
            "training error 0.02976257020684477, test error 0.04436500366575641\n",
            "Loss: 0.0\n",
            "training error 0.029715125030143716, test error 0.04443933810144856\n",
            "Loss: 0.16755196562630026\n",
            "training error 0.029662164382035346, test error 0.044288083693498445\n",
            "Loss: 0.0\n",
            "training error 0.029642973850577074, test error 0.0442359937053978\n",
            "Loss: 0.0\n",
            "training error 0.029584020601218404, test error 0.04419872834315213\n",
            "Loss: 0.0\n",
            "training error 0.029599625758506564, test error 0.044041972141665844\n",
            "Loss: 0.0\n",
            "training error 0.02953213668221507, test error 0.04401957097792564\n",
            "Loss: 0.0\n",
            "training error 0.02948633010913029, test error 0.0438354753365896\n",
            "Loss: 0.0\n",
            "training error 0.02946402011066552, test error 0.04399472249197088\n",
            "Loss: 0.3632837425817881\n",
            "training error 0.02940767118184354, test error 0.04389383014588773\n",
            "Loss: 0.13312233721671785\n",
            "training error 0.029388933036462495, test error 0.04386323710436618\n",
            "Loss: 0.06333173659784919\n",
            "training error 0.029302383809473302, test error 0.04377256571254463\n",
            "Loss: 0.0\n",
            "training error 0.029268251085217634, test error 0.043712846982145594\n",
            "Loss: 0.0\n",
            "training error 0.029241290586219753, test error 0.04390191600120031\n",
            "Loss: 0.4325250632427169\n",
            "training error 0.029200126867177382, test error 0.04363721683039574\n",
            "Loss: 0.0\n",
            "training error 0.02916667508166329, test error 0.043620597400377686\n",
            "Loss: 0.0\n",
            "training error 0.02911345435874437, test error 0.04352404181543716\n",
            "Loss: 0.0\n",
            "training error 0.029086337458041657, test error 0.04353746371511686\n",
            "Loss: 0.030837898135960273\n",
            "training error 0.029045678934414874, test error 0.043681687065857296\n",
            "Loss: 0.36220269038576003\n",
            "training error 0.029012281771586653, test error 0.04363175026297459\n",
            "Loss: 0.2474688540971437\n",
            "training error 0.028985154566275317, test error 0.04380704473493263\n",
            "Loss: 0.6502220558824545\n",
            "training error 0.028966854421862524, test error 0.04362986892635903\n",
            "Loss: 0.24314633133253327\n",
            "training error 0.028896138383871398, test error 0.04366297062488891\n",
            "Loss: 0.3192001561823421\n",
            "training error 0.028915237262608116, test error 0.043546382699404376\n",
            "Loss: 0.05132998461390148\n",
            "training error 0.028812540312947485, test error 0.043707142116740705\n",
            "Loss: 0.42068772491299367\n",
            "training error 0.028772874404448666, test error 0.04366838551713412\n",
            "Loss: 0.33164130828897687\n",
            "training error 0.02875754505471981, test error 0.04361975610803103\n",
            "Loss: 0.2199113147619558\n",
            "training error 0.0287025419484051, test error 0.043654548118147056\n",
            "Loss: 0.29984876694886253\n",
            "training error 0.02872331316655081, test error 0.04360071280370964\n",
            "Loss: 0.1761577856155938\n",
            "training error 0.028677051215704675, test error 0.04359145833665809\n",
            "Loss: 0.15489490040196952\n",
            "training error 0.028640281565430584, test error 0.04360955163354905\n",
            "Loss: 0.19646571077771569\n",
            "training error 0.028598836163410035, test error 0.04369295372812595\n",
            "Loss: 0.3880887565659741\n",
            "training error 0.02857677233232283, test error 0.04364811226760408\n",
            "Loss: 0.28506188072567884\n",
            "training error 0.02851403682708569, test error 0.04356343804383626\n",
            "Loss: 0.09051601541547605\n",
            "training error 0.02851056675199361, test error 0.04367756353712605\n",
            "Loss: 0.35272855021115657\n",
            "training error 0.028531856798331617, test error 0.0436869395801492\n",
            "Loss: 0.3742707660350231\n",
            "training error 0.02845903835369673, test error 0.04368001396929804\n",
            "Loss: 0.35835861596282825\n",
            "training error 0.02839666079402744, test error 0.04336928245816339\n",
            "Loss: 0.0\n",
            "training error 0.02836570641046362, test error 0.04320659995100387\n",
            "Loss: 0.0\n",
            "training error 0.02834948310753356, test error 0.04338715968753888\n",
            "Loss: 0.4178985079588804\n",
            "training error 0.028360340805385187, test error 0.04314689795224559\n",
            "Loss: 0.0\n",
            "training error 0.02828303977655245, test error 0.04341994555414767\n",
            "Loss: 0.6328325206699237\n",
            "training error 0.028253535075298538, test error 0.04330766392476783\n",
            "Loss: 0.3726014618714135\n",
            "training error 0.028204608344382074, test error 0.04323731499462956\n",
            "Loss: 0.2095562987727062\n",
            "training error 0.028181224852752217, test error 0.043282060575783717\n",
            "Loss: 0.3132615088290214\n",
            "training error 0.02819569602836274, test error 0.04334822130443016\n",
            "Loss: 0.4665998292794793\n",
            "training error 0.02809132071760426, test error 0.043267541167739926\n",
            "Loss: 0.27961040357491473\n",
            "training error 0.028072808514000343, test error 0.043223576708456564\n",
            "Loss: 0.17771557133918758\n",
            "training error 0.02803252557730144, test error 0.04325036995828667\n",
            "Loss: 0.23981331440234843\n",
            "training error 0.02807546806365363, test error 0.04339256062395165\n",
            "Loss: 0.5693634614890586\n",
            "training error 0.027993101190843337, test error 0.0432656890714258\n",
            "Loss: 0.27531786714234396\n",
            "training error 0.027950036873173052, test error 0.04315765275946609\n",
            "Loss: 0.024926026506943977\n",
            "training error 0.02792520860580323, test error 0.04330023709767677\n",
            "Loss: 0.3553885741702345\n",
            "training error 0.027893126412199445, test error 0.04331735852839511\n",
            "Loss: 0.39507029297489726\n",
            "training error 0.02786583958967576, test error 0.04309179047248733\n",
            "Loss: 0.0\n",
            "training error 0.027903179966957233, test error 0.04305522676002848\n",
            "Loss: 0.0\n",
            "training error 0.027824528854755642, test error 0.04297584699814356\n",
            "Loss: 0.0\n",
            "training error 0.027794840290495063, test error 0.04286089124823394\n",
            "Loss: 0.0\n",
            "training error 0.027762760148792788, test error 0.042862527349595385\n",
            "Loss: 0.0038172359785315635\n",
            "training error 0.02778339450173132, test error 0.04280946936234624\n",
            "Loss: 0.0\n",
            "training error 0.02770881222586595, test error 0.04301429755290566\n",
            "Loss: 0.4784646799186332\n",
            "training error 0.02767239117828948, test error 0.04304402576934499\n",
            "Loss: 0.5479077654839015\n",
            "training error 0.027694158542002918, test error 0.04308745234693773\n",
            "Loss: 0.6493492882114316\n",
            "training error 0.02771355539920003, test error 0.043227926094337184\n",
            "Loss: 0.9774863791210731\n",
            "training error 0.027630892819019176, test error 0.042988466456807935\n",
            "Loss: 0.4181250016126903\n",
            "training error 0.027584890685525807, test error 0.043105470498454326\n",
            "Loss: 0.6914384609691959\n",
            "training error 0.02755590164138445, test error 0.04316040891541232\n",
            "Loss: 0.8197708551247684\n",
            "training error 0.027535040846389712, test error 0.04310597593617737\n",
            "Loss: 0.6926191290096417\n",
            "training error 0.02751434495634477, test error 0.04316092205982872\n",
            "Loss: 0.8209695254751459\n",
            "training error 0.027464396181323538, test error 0.043154889765440206\n",
            "Loss: 0.8068784973022547\n",
            "training error 0.027478913952183628, test error 0.04314564804164471\n",
            "Loss: 0.7852904609795486\n",
            "training error 0.0274137999792697, test error 0.043156343968808186\n",
            "Loss: 0.8102754171651627\n",
            "training error 0.027422903716925982, test error 0.042972353416008614\n",
            "Loss: 0.3804860375252517\n",
            "training error 0.027510406561674545, test error 0.04272297988875111\n",
            "Loss: 0.0\n",
            "training error 0.027347003090786998, test error 0.04303649240938093\n",
            "Loss: 0.7338264359044944\n",
            "training error 0.027359260062274827, test error 0.042662460145319235\n",
            "Loss: 0.0\n",
            "training error 0.02736817029736887, test error 0.042823516689125485\n",
            "Loss: 0.37751349373114085\n",
            "training error 0.02727052031691699, test error 0.042863887922539805\n",
            "Loss: 0.47214290159183214\n",
            "training error 0.027293842578811497, test error 0.04273487140742091\n",
            "Loss: 0.1697306293519496\n",
            "training error 0.027214270917322265, test error 0.04287598506846018\n",
            "Loss: 0.5004983829193677\n",
            "training error 0.027248096311895238, test error 0.04278960631433215\n",
            "Loss: 0.29802821632840537\n",
            "training error 0.027233480787643243, test error 0.043083363633339825\n",
            "Loss: 0.9865898182779054\n",
            "training error 0.02718054324302522, test error 0.04288792162097543\n",
            "Loss: 0.5284774363415012\n",
            "training error 0.027146710366113627, test error 0.0430194961568059\n",
            "Loss: 0.8368856607671171\n",
            "training error 0.02715925451163992, test error 0.04289926933520852\n",
            "Loss: 0.5550762639628504\n",
            "training error 0.02709211574623745, test error 0.043044065389156776\n",
            "Loss: 0.8944754768892782\n",
            "training error 0.027039943748529725, test error 0.04293669386618441\n",
            "Loss: 0.6427986570185062\n",
            "training error 0.027027251053087953, test error 0.04289083596939121\n",
            "Loss: 0.5353086139291197\n",
            "training error 0.02704117849102682, test error 0.043033811660359\n",
            "Loss: 0.870440930445282\n",
            "training error 0.02696612058518558, test error 0.04293658020638521\n",
            "Loss: 0.6425322405980705\n",
            "training error 0.02696873580104048, test error 0.043049793106695815\n",
            "Loss: 0.9079011385120017\n",
            "training error 0.026922814284341394, test error 0.04305137073230886\n",
            "Loss: 0.9115990631222148\n",
            "training error 0.026911103216926694, test error 0.043083392164831986\n",
            "Loss: 0.9866566955561096\n",
            "training error 0.026880808442739504, test error 0.043013637324859647\n",
            "Loss: 0.8231526694527425\n",
            "training error 0.026882773787289805, test error 0.04316168831929635\n",
            "Loss: 1.17018140134586\n",
            "training error 0.02683890096232941, test error 0.04309136826111223\n",
            "Loss: 1.0053525144401387\n",
            "training error 0.026917254710288226, test error 0.04327934692439581\n",
            "Loss: 1.4459709472339544\n",
            "training error 0.02694646569554409, test error 0.04292378231562681\n",
            "Loss: 0.612534226618533\n",
            "training error 0.026790797796731894, test error 0.04320277168826194\n",
            "Loss: 1.2664800414750266\n",
            "training error 0.026776070586586573, test error 0.04309156119678106\n",
            "Loss: 1.0058047520002278\n",
            "training error 0.02672980200770796, test error 0.04312940273511573\n",
            "Loss: 1.094504602420887\n",
            "training error 0.026697681503534992, test error 0.043151995615522576\n",
            "Loss: 1.1474618869513398\n",
            "training error 0.02668646273699372, test error 0.04323881858235835\n",
            "Loss: 1.3509732797309049\n",
            "training error 0.02669739176742983, test error 0.04301070358401254\n",
            "Loss: 0.8162760363727095\n",
            "training error 0.026741156639400784, test error 0.043118657153056995\n",
            "Loss: 1.0693171612322416\n",
            "training error 0.026669184382267237, test error 0.0427253739881481\n",
            "Loss: 0.14746885813561317\n",
            "training error 0.026607309262068973, test error 0.042799043575646914\n",
            "Loss: 0.32014897842844725\n",
            "training error 0.026597807841580154, test error 0.042568576393864216\n",
            "Loss: 0.0\n",
            "training error 0.02659406841362754, test error 0.04265016946742423\n",
            "Loss: 0.1916744238874335\n",
            "training error 0.02653925115467649, test error 0.04259156196366688\n",
            "Loss: 0.053996566833691695\n",
            "training error 0.02654698336925881, test error 0.04285802275809993\n",
            "Loss: 0.6799531221284516\n",
            "training error 0.026514798784895367, test error 0.042810324073686574\n",
            "Loss: 0.5679017254079488\n",
            "training error 0.026500217214529956, test error 0.042966395140111184\n",
            "Loss: 0.934536176559364\n",
            "training error 0.02646077951517565, test error 0.0430271408655841\n",
            "Loss: 1.0772370386010532\n",
            "training error 0.02648871411376444, test error 0.04316672967063441\n",
            "Loss: 1.4051521743076556\n",
            "training error 0.026454498915152034, test error 0.04310084108963667\n",
            "Loss: 1.2503699697346882\n",
            "training error 0.026457411682217078, test error 0.043189797850786776\n",
            "Loss: 1.4593428053001567\n",
            "training error 0.026416150360418646, test error 0.042873997385267\n",
            "Loss: 0.7174799283323319\n",
            "training error 0.02638755504679735, test error 0.04306793905346674\n",
            "Loss: 1.1730781292336179\n",
            "training error 0.026375997203421125, test error 0.04292549053822103\n",
            "Loss: 0.8384451033891294\n",
            "training error 0.02633135981161741, test error 0.043182424201564464\n",
            "Loss: 1.4420209922470617\n",
            "training error 0.026391979471326477, test error 0.04291179148305738\n",
            "Loss: 0.8062639586947418\n",
            "training error 0.026283949423649266, test error 0.04318848750467783\n",
            "Loss: 1.4562646048529082\n",
            "training error 0.02627722952151371, test error 0.043307948647225536\n",
            "Loss: 1.7368968285909059\n",
            "training error 0.026254959565440564, test error 0.04314579149476615\n",
            "Loss: 1.3559652443184245\n",
            "training error 0.026248905507106127, test error 0.04329425377301145\n",
            "Loss: 1.7047255055770094\n",
            "training error 0.026239985427608338, test error 0.04334788961544878\n",
            "Loss: 1.8307241810813668\n",
            "training error 0.02617705213192511, test error 0.043247310023107374\n",
            "Loss: 1.594447563769097\n",
            "training error 0.026180132769534584, test error 0.04320256953987188\n",
            "Loss: 1.4893454273444906\n",
            "training error 0.02616193498259873, test error 0.04325889728061756\n",
            "Loss: 1.6216677775788835\n",
            "training error 0.026120176563955367, test error 0.043298971165368336\n",
            "Loss: 1.7158073710198174\n",
            "training error 0.026119495181203316, test error 0.043313905955005026\n",
            "Loss: 1.7508914421865418\n",
            "training error 0.026168275883704472, test error 0.043415754824879346\n",
            "Loss: 1.990149783672912\n",
            "training error 0.0261013480690275, test error 0.043294561291553235\n",
            "Loss: 1.7054479129672284\n",
            "training error 0.02607737060656265, test error 0.043255169405206684\n",
            "Loss: 1.6129104365384173\n",
            "training error 0.026038196515197988, test error 0.04330555225384695\n",
            "Loss: 1.7312673394662914\n",
            "training error 0.02607997634168616, test error 0.043416427497492444\n",
            "Loss: 1.9917299929965049\n",
            "training error 0.026033840179471777, test error 0.04330285271669404\n",
            "Loss: 1.7249257199394297\n",
            "training error 0.026034571914565933, test error 0.04311535544296192\n",
            "Loss: 1.2844663726563121\n",
            "training error 0.02596028431960323, test error 0.04304031363820942\n",
            "Loss: 1.1081818663148946\n",
            "training error 0.02597739980969272, test error 0.04300555624779531\n",
            "Loss: 1.0265315191373858\n",
            "training error 0.025940816341530126, test error 0.04306219321939743\n",
            "Loss: 1.1595802992471205\n",
            "training error 0.02592173696283474, test error 0.04298999188969698\n",
            "Loss: 0.9899684967935807\n",
            "training error 0.026017251475897347, test error 0.04271904243458181\n",
            "Loss: 0.35346740122434994\n",
            "training error 0.025910769703819343, test error 0.04293382478143992\n",
            "Loss: 0.8580234964783884\n",
            "training error 0.025884989663998728, test error 0.04281280623182891\n",
            "Loss: 0.5737326888852756\n",
            "training error 0.025871495160570284, test error 0.042843994872924646\n",
            "Loss: 0.6469995061900269\n",
            "training error 0.025839018156354348, test error 0.043077490597281096\n",
            "Loss: 1.1955161448392593\n",
            "training error 0.025895305653509275, test error 0.04323663592272088\n",
            "Loss: 1.5693724936334918\n",
            "training error 0.02580982324574939, test error 0.04319490388107335\n",
            "Loss: 1.4713376397981204\n",
            "training error 0.02579440057673283, test error 0.04318884857413693\n",
            "Loss: 1.4571128114167164\n",
            "training error 0.02580303243753145, test error 0.043223082449765166\n",
            "Loss: 1.5375333434812566\n",
            "training error 0.025759639301093852, test error 0.04310649905784749\n",
            "Loss: 1.2636613895803395\n",
            "training error 0.025736777574055944, test error 0.043145023149235365\n",
            "Loss: 1.3541602848955892\n",
            "training error 0.025772719256103787, test error 0.04326203534891399\n",
            "Loss: 1.629039572837887\n",
            "training error 0.025736417529804097, test error 0.04294535292915607\n",
            "Loss: 0.8851048524755489\n",
            "training error 0.02574719722429598, test error 0.042945689940339545\n",
            "Loss: 0.8858965425249332\n",
            "training error 0.025744319590703783, test error 0.04301388122938067\n",
            "Loss: 1.046088155253977\n",
            "training error 0.025646729095084152, test error 0.04321556298601175\n",
            "Loss: 1.5198689901238849\n",
            "training error 0.025649928417733732, test error 0.04299666900097763\n",
            "Loss: 1.0056540372703537\n",
            "training error 0.025676948244874015, test error 0.04296038471894741\n",
            "Loss: 0.920416791621137\n",
            "training error 0.025621393005816406, test error 0.04293146168011953\n",
            "Loss: 0.852472215414779\n",
            "training error 0.025681579591418556, test error 0.04290872875693756\n",
            "Loss: 0.7990691535608274\n",
            "training error 0.025589864889962927, test error 0.0430321814069024\n",
            "Loss: 1.0890780296449076\n",
            "training error 0.025565571490980824, test error 0.04304326878022619\n",
            "Loss: 1.1151239401804292\n",
            "training error 0.025584646036263193, test error 0.04306587543233672\n",
            "Loss: 1.1682303722615917\n",
            "training error 0.02555066064665174, test error 0.043045877522384486\n",
            "Loss: 1.1212522685843718\n",
            "training error 0.02556771695410158, test error 0.04321228388333965\n",
            "Loss: 1.5121658838659569\n",
            "training error 0.025524186951625193, test error 0.04283500123144964\n",
            "Loss: 0.6258720872418744\n",
            "training error 0.025549096836566525, test error 0.042842780896129064\n",
            "Loss: 0.6441476917803834\n",
            "training error 0.025489204993064812, test error 0.042994103598531015\n",
            "Loss: 0.9996275203794092\n",
            "training error 0.025453234083528883, test error 0.04302375442641434\n",
            "Loss: 1.0692817827371082\n",
            "training error 0.025447778444219964, test error 0.04299869643437668\n",
            "Loss: 1.010416783809731\n",
            "training error 0.025520365848453092, test error 0.042733268829135654\n",
            "Loss: 0.3868873456035482\n",
            "training error 0.025447844790352232, test error 0.04283500911629906\n",
            "Loss: 0.6258906099412043\n",
            "training error 0.025408752789538937, test error 0.042910659374286446\n",
            "Loss: 0.8036044646105189\n",
            "training error 0.02538950006108164, test error 0.04309075114636784\n",
            "Loss: 1.2266671726867795\n",
            "training error 0.025398187278338657, test error 0.042798999672551574\n",
            "Loss: 0.5412990008295759\n",
            "training error 0.025415085101597956, test error 0.04294619752384193\n",
            "Loss: 0.8870889326525466\n",
            "training error 0.02536194586552185, test error 0.042795341317194036\n",
            "Loss: 0.5327049728693911\n",
            "training error 0.02537282830424253, test error 0.043159871733637424\n",
            "Loss: 1.389041846977146\n",
            "training error 0.025315324398195206, test error 0.04312905098176037\n",
            "Loss: 1.3166392568790153\n",
            "training error 0.025298624185205832, test error 0.04311899728029409\n",
            "Loss: 1.2930215972860593\n",
            "training error 0.02529380318738708, test error 0.04319422927434427\n",
            "Loss: 1.469752886944642\n",
            "training error 0.025259529442384753, test error 0.0431518974119982\n",
            "Loss: 1.370308963909972\n",
            "training error 0.025286189939450907, test error 0.04310085770682881\n",
            "Loss: 1.2504090060228457\n",
            "training error 0.025246742602079163, test error 0.043320344598725226\n",
            "Loss: 1.766016786432556\n",
            "training error 0.02524441395144888, test error 0.04300172335695721\n",
            "Loss: 1.0175274810351187\n",
            "training error 0.02523424306836381, test error 0.04314218905000826\n",
            "Loss: 1.3475025587811773\n",
            "training error 0.025200519664553393, test error 0.043151651074660125\n",
            "Loss: 1.3697302803857836\n",
            "training error 0.025191515162528863, test error 0.04305735016014983\n",
            "Loss: 1.1482032233431028\n",
            "training error 0.025210899991229958, test error 0.04320036766606641\n",
            "Loss: 1.4841728940065302\n",
            "training error 0.025163379968796813, test error 0.04312620937115281\n",
            "Loss: 1.309963885400145\n",
            "training error 0.025162194974276596, test error 0.0430957089963229\n",
            "Loss: 1.2383139092588147\n",
            "training error 0.02513064164986486, test error 0.043227878773424204\n",
            "Loss: 1.5488006304458457\n",
            "training error 0.02510058723190898, test error 0.04321721647167431\n",
            "Loss: 1.5237532770853823\n",
            "training error 0.025117912119956655, test error 0.043262922106805234\n",
            "Loss: 1.6311227007373086\n",
            "training error 0.025169089837656037, test error 0.04301533336735885\n",
            "Loss: 1.0494994461666574\n",
            "training error 0.025083109120905576, test error 0.04330436707449721\n",
            "Loss: 1.7284831745960183\n",
            "training error 0.02507495305720802, test error 0.04341540904640285\n",
            "Loss: 1.9893374979311984\n",
            "training error 0.02503154512625591, test error 0.043282553732385855\n",
            "Loss: 1.6772403472354647\n",
            "training error 0.025023294922423146, test error 0.04343870450764116\n",
            "Loss: 2.0440620464403425\n",
            "training error 0.02512726209104985, test error 0.04332244810754346\n",
            "Loss: 1.7709582455943007\n",
            "training error 0.02504826561649529, test error 0.04359679996262043\n",
            "Loss: 2.415452091332848\n",
            "training error 0.02498047948553788, test error 0.043449856298965424\n",
            "Loss: 2.0702592845652124\n",
            "training error 0.025060428186925523, test error 0.043605279489480155\n",
            "Loss: 2.435371777585149\n",
            "training error 0.024963161936559213, test error 0.04360620725143896\n",
            "Loss: 2.437551230217583\n",
            "training error 0.02495041893595746, test error 0.043488619494528036\n",
            "Loss: 2.161319871614098\n",
            "training error 0.024962315881836516, test error 0.04358997156134781\n",
            "Loss: 2.3994111478691904\n",
            "training error 0.024931098408496468, test error 0.04356881852038735\n",
            "Loss: 2.3497194674034327\n",
            "training error 0.024982759169305924, test error 0.04315409060928493\n",
            "Loss: 1.375461114798071\n",
            "training error 0.024908924048142937, test error 0.0431604760558783\n",
            "Loss: 1.3904614909776347\n",
            "training error 0.024887090964994068, test error 0.04312799823768819\n",
            "Loss: 1.3141662024305\n",
            "training error 0.024878803492296965, test error 0.043113477478568724\n",
            "Loss: 1.2800547513330818\n",
            "training error 0.02490627059971704, test error 0.043319771757512754\n",
            "Loss: 1.7646710961112033\n",
            "training error 0.02484885104787631, test error 0.04319766872693686\n",
            "Loss: 1.4778326793266183\n",
            "training error 0.02486639978892381, test error 0.043086708445584807\n",
            "Loss: 1.2171702594105938\n",
            "training error 0.024859791971037687, test error 0.04315574624930735\n",
            "Loss: 1.3793504626754771\n",
            "training error 0.024849340083004847, test error 0.04341188145313764\n",
            "Loss: 1.9810506498285863\n",
            "training error 0.024785147265787403, test error 0.04327439277474367\n",
            "Loss: 1.6580690280758148\n",
            "training error 0.02479303974138993, test error 0.043223046156424055\n",
            "Loss: 1.5374480849544625\n",
            "training error 0.024816129385621822, test error 0.043424327506314746\n",
            "Loss: 2.0102883040596042\n",
            "training error 0.02478985069997735, test error 0.04351087382778373\n",
            "Loss: 2.213598653619364\n",
            "training error 0.02476105070681348, test error 0.04316709587169808\n",
            "Loss: 1.4060124357837989\n",
            "training error 0.024775055328084047, test error 0.04305679958104512\n",
            "Loss: 1.146909830067222\n",
            "training error 0.024748079139567586, test error 0.043090940708343094\n",
            "Loss: 1.2271124823290291\n",
            "training error 0.02477255291513014, test error 0.04313595600366752\n",
            "Loss: 1.3328601937580675\n",
            "training error 0.02469952997321816, test error 0.0432176643234224\n",
            "Loss: 1.5248053483220225\n",
            "training error 0.02469013764418886, test error 0.04327374656403082\n",
            "Loss: 1.6565509817430657\n",
            "training error 0.024715305221452828, test error 0.043374325119640525\n",
            "Loss: 1.8928251636163518\n",
            "training error 0.02467762585012352, test error 0.04324280215448514\n",
            "Loss: 1.5838579011491216\n",
            "training error 0.024723446619657516, test error 0.0434718319347551\n",
            "Loss: 2.1218833642298707\n",
            "training error 0.024651083039301247, test error 0.04327988412914453\n",
            "Loss: 1.670969046977211\n",
            "training error 0.024655371086312508, test error 0.0433028046973119\n",
            "Loss: 1.7248129151754332\n",
            "training error 0.024623220820007195, test error 0.04333241924485653\n",
            "Loss: 1.7943819495509716\n",
            "training error 0.024640601802039343, test error 0.043193704547097374\n",
            "Loss: 1.4685202235780181\n",
            "training error 0.024606889422334344, test error 0.04344133801742053\n",
            "Loss: 2.0502485577180707\n",
            "training error 0.024584942682710914, test error 0.04358611618624681\n",
            "Loss: 2.390354290845531\n",
            "training error 0.02458431368377129, test error 0.04356533434139659\n",
            "Loss: 2.3415346059729725\n",
            "training error 0.024571415616319123, test error 0.04361016219654502\n",
            "Loss: 2.4468419921858997\n",
            "training error 0.024588959776252705, test error 0.04359177461063134\n",
            "Loss: 2.4036467823119523\n",
            "training error 0.02452943130516903, test error 0.04349206363105201\n",
            "Loss: 2.169410667256666\n",
            "training error 0.024547204747698524, test error 0.04344959141484574\n",
            "Loss: 2.0696370318564528\n",
            "training error 0.02457559393295782, test error 0.0436517909990316\n",
            "Loss: 2.5446343216765976\n",
            "training error 0.02459111133791105, test error 0.04348563147132622\n",
            "Loss: 2.154300554890498\n",
            "training error 0.024544660922133924, test error 0.04355364500318108\n",
            "Loss: 2.314074589205317\n",
            "training error 0.024488351786149738, test error 0.0436516297297039\n",
            "Loss: 2.5442554757264313\n",
            "training error 0.02448136224324349, test error 0.043569810909017764\n",
            "Loss: 2.352050737825162\n",
            "training error 0.024469914088518705, test error 0.043622417628018655\n",
            "Loss: 2.4756318473134087\n",
            "training error 0.024491225467649702, test error 0.04374105462269996\n",
            "Loss: 2.754328023534147\n",
            "training error 0.02447821676994253, test error 0.04359112514719026\n",
            "Loss: 2.4021210948304894\n",
            "training error 0.02443169475182747, test error 0.04371246951621595\n",
            "Loss: 2.6871773013217526\n",
            "training error 0.02443223928952482, test error 0.04402394547237298\n",
            "Loss: 3.4188812541979807\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9606481481481481 0.042568576393864216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test error, mean test error and std test error')\n",
        "print(error_vals)\n",
        "print(np.mean(error_vals), np.std(error_vals)) \n",
        "\n",
        "print('Test accuracy, mean test accuracy and std test accuracy')\n",
        "print(acc_vals)\n",
        "print(np.mean(acc_vals), np.std(acc_vals)) \n",
        "\n",
        "print('Train error, mean train error and std train error')\n",
        "\n",
        "print(error_trains)\n",
        "print(np.mean(error_trains), np.std(error_trains)) \n",
        "\n",
        "print('Train accuracy, mean train accuracy and std train accuracy')\n",
        "\n",
        "print(acc_trains)\n",
        "print(np.mean(acc_trains), np.std(acc_trains)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwlfu8FBnRab",
        "outputId": "bc5a5a88-c54a-4392-f86c-9d807c693322"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test error, mean test error and std test error\n",
            "[0.042117109128568114, 0.04238018294264801, 0.042593143497150814, 0.042064713083538784, 0.04256066140775207, 0.04226421291424575, 0.04244510415612425, 0.0427091128981698, 0.042581879869812836, 0.042568576393864216]\n",
            "0.04242846962918746 0.00020624504037845479\n",
            "Test accuracy, mean test accuracy and std test accuracy\n",
            "[0.9606481481481481, 0.9606481481481481, 0.9606481481481481, 0.9606481481481481, 0.9606481481481481, 0.9606481481481481, 0.9560185185185185, 0.9583333333333333, 0.9583333333333333, 0.9606481481481481]\n",
            "0.9597222222222224 0.0015354744399793685\n",
            "Train error, mean train error and std train error\n",
            "[0.02449409035076355, 0.025971690548462083, 0.0261468931507741, 0.02612926212410102, 0.026677835367150734, 0.026766582281840666, 0.025110542247187853, 0.024815375356982687, 0.025536844023000625, 0.026597807841580154]\n",
            "0.025824692329184346 0.000762429753477138\n",
            "Train accuracy, mean train accuracy and std train accuracy\n",
            "[0.9426229508196722, 0.9426229508196722, 0.9426229508196722, 0.9426229508196722, 0.9426229508196722, 0.9426229508196722, 0.9508196721311475, 0.9426229508196722, 0.9426229508196722, 0.9426229508196722]\n",
            "0.9434426229508197 0.0024590163934425924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9gqtP0O7nRmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple training trials with lambda = 0.001"
      ],
      "metadata": {
        "id": "YIys02xppPd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_space_def = search_space_dict(layers_range=[1], units_range=[4], eta_0_range=[0.6],\n",
        "                        alpha_range=[0.4], lamb_range=[0.001], lamb0_range = [0.0], minibatch_size_range = [40], num_targets=1, configurations = 0)    \n",
        "\n",
        "search_space_def[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQydHxqrnRr5",
        "outputId": "04fb58a8-8c64-4f0d-b0e3-960246130a04"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'units': array([4, 1]),\n",
              " 'layers': 2,\n",
              " 'eta_0': 0.6,\n",
              " 'alpha': 0.4,\n",
              " 'lamb': 0.001,\n",
              " 'lamb0': 0.0,\n",
              " 'minibatch_size': 40}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "error_vals = []\n",
        "acc_vals = []\n",
        "\n",
        "error_trains = []\n",
        "acc_trains = []\n",
        "\n",
        "models = []\n",
        "\n",
        "for i in range(10):\n",
        "  model, acc_val, error_val, error_train, acc_train = train_test(hyperparams = search_space_def[0][0],\n",
        "           num_inputs = 17, seed = i, activation_output = sigmoid, activation_hidden = sigmoid,\n",
        "           task = 'binary_classification', thr = 0.5, stop_class = 'GL', stop_param = 3, data_train = X_train, data_val = X_test)\n",
        "  \n",
        "  models += [model]\n",
        "\n",
        "  acc_vals += [acc_val]\n",
        "  error_vals += [error_val]\n",
        "  \n",
        "\n",
        "  error_trains += [error_train]\n",
        "  acc_trains += [acc_train]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MxPESJSTpZFy",
        "outputId": "87bacdeb-e6fe-49bf-98de-7706d83a39a3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error 0.1250084139437088, test error 0.25008057932897676\n",
            "training error 0.12501031596457085, test error 0.25015805835806904\n",
            "training error 0.12501963640770625, test error 0.2502542822938578\n",
            "training error 0.12498285266428451, test error 0.2502821510372059\n",
            "training error 0.1249980716473125, test error 0.2503738836777854\n",
            "training error 0.12500092955827138, test error 0.2503835091545526\n",
            "training error 0.1250539221107434, test error 0.2503451287654612\n",
            "training error 0.12497867301171182, test error 0.25040279041681013\n",
            "training error 0.12518038956183247, test error 0.2502678277925596\n",
            "training error 0.12502954371264552, test error 0.2503519645727861\n",
            "training error 0.12503059343458547, test error 0.2503591032919014\n",
            "training error 0.12499679637332678, test error 0.25045027707648304\n",
            "training error 0.12504466394208533, test error 0.2504126623812252\n",
            "training error 0.12510174485953038, test error 0.2505655682765611\n",
            "training error 0.12501862608714565, test error 0.2504307266619356\n",
            "training error 0.12501685562540849, test error 0.25056526039022514\n",
            "training error 0.1249766678184549, test error 0.2506295482231575\n",
            "training error 0.1250519383563301, test error 0.2506247363846358\n",
            "training error 0.12501964409261485, test error 0.25047301910990116\n",
            "training error 0.12501780341283833, test error 0.25058503094180656\n",
            "training error 0.12505925042820867, test error 0.250570066567375\n",
            "training error 0.1250768121129982, test error 0.2504495067901292\n",
            "training error 0.1250442587795129, test error 0.2504027598982403\n",
            "training error 0.12497777374501291, test error 0.25044485645741965\n",
            "training error 0.1249696955660773, test error 0.2503705375029464\n",
            "training error 0.12501658559184042, test error 0.25034527757913966\n",
            "training error 0.12502062389784918, test error 0.2504193801751733\n",
            "training error 0.1249833451086819, test error 0.25034991081344443\n",
            "training error 0.1250277201293271, test error 0.250321092602952\n",
            "training error 0.12498396544124744, test error 0.2503173468437456\n",
            "training error 0.12498003114738117, test error 0.2504120585284087\n",
            "training error 0.125121817833, test error 0.25034379128605694\n",
            "training error 0.12510099960964974, test error 0.25051215627015794\n",
            "training error 0.125002091795735, test error 0.25038703990111466\n",
            "training error 0.1249683298383432, test error 0.2504315145499087\n",
            "training error 0.12498432772855468, test error 0.25050089350215127\n",
            "training error 0.12498186298350376, test error 0.25056401421260593\n",
            "training error 0.12496334832586654, test error 0.25063544325497233\n",
            "training error 0.12505807547158856, test error 0.2507062058984667\n",
            "training error 0.12506088906272417, test error 0.2505674824344443\n",
            "training error 0.12499831316699896, test error 0.25049834685598144\n",
            "training error 0.12504673772050365, test error 0.2505226324509737\n",
            "training error 0.1250249336207892, test error 0.2504922551214698\n",
            "training error 0.12512689025927773, test error 0.25042430564073714\n",
            "training error 0.12495327086769646, test error 0.25045901753546324\n",
            "training error 0.12497837934537372, test error 0.25048251742542715\n",
            "training error 0.1251059261146893, test error 0.2504853684421873\n",
            "training error 0.1249732494305786, test error 0.250486683187764\n",
            "training error 0.12497472978270494, test error 0.25038286352115696\n",
            "training error 0.12496710030898586, test error 0.25044672452384603\n",
            "Loss: 0.14641088718352346\n",
            "training error 0.12520644810100665, test error 0.25050264383666343\n",
            "Loss: 0.16877140512836508\n",
            "training error 0.12495764061136969, test error 0.2504225904797894\n",
            "Loss: 0.13676038008640923\n",
            "training error 0.12498882965236562, test error 0.25041347129811525\n",
            "Loss: 0.13311388274599434\n",
            "training error 0.12498784485012483, test error 0.25043943079174275\n",
            "Loss: 0.14349433439768688\n",
            "training error 0.12500515732384898, test error 0.2504600923623022\n",
            "Loss: 0.1517562996470101\n",
            "training error 0.12522908425751042, test error 0.25056517607497236\n",
            "Loss: 0.19377624096037493\n",
            "training error 0.12506547008220692, test error 0.2505820208477304\n",
            "Loss: 0.20051197901858053\n",
            "training error 0.12503865993469462, test error 0.2505017536616977\n",
            "Loss: 0.16841544987260537\n",
            "training error 0.12526623468114917, test error 0.25036429922975434\n",
            "Loss: 0.11345139296257134\n",
            "training error 0.12495744360792989, test error 0.25042367441255337\n",
            "Loss: 0.1371938134889117\n",
            "training error 0.12501078281179526, test error 0.25043927653022996\n",
            "Loss: 0.1434326496746241\n",
            "training error 0.12494750968600739, test error 0.2505033831832088\n",
            "Loss: 0.1690670484555623\n",
            "training error 0.12505043138017988, test error 0.2504944912315867\n",
            "Loss: 0.16551141384930634\n",
            "training error 0.12497821323857115, test error 0.25050914630806076\n",
            "Loss: 0.17137155561377515\n",
            "training error 0.124954848310452, test error 0.2504608175109607\n",
            "Loss: 0.15204626564935264\n",
            "training error 0.12498088507728852, test error 0.2505530488283829\n",
            "Loss: 0.18892690534941803\n",
            "training error 0.12518663503668276, test error 0.2505065047804819\n",
            "Loss: 0.17031528503652638\n",
            "training error 0.1249827019198055, test error 0.25067738678798773\n",
            "Loss: 0.2386460638456489\n",
            "training error 0.12501502975449666, test error 0.2506827045930755\n",
            "Loss: 0.24077250049339138\n",
            "training error 0.12496323660647243, test error 0.25071545504177467\n",
            "Loss: 0.2538684589188822\n",
            "training error 0.1251657120087629, test error 0.25076562393942475\n",
            "Loss: 0.27392955194127655\n",
            "training error 0.12495664390544924, test error 0.2506635224285661\n",
            "Loss: 0.23310210699027945\n",
            "training error 0.12495629545954581, test error 0.25067711608879834\n",
            "Loss: 0.23853781905904814\n",
            "training error 0.1250746491811161, test error 0.25069276734816537\n",
            "Loss: 0.24479630558729149\n",
            "training error 0.12499527849064002, test error 0.25083673456767575\n",
            "Loss: 0.302364638121011\n",
            "training error 0.12518132135219132, test error 0.2505863687570905\n",
            "Loss: 0.20225058238063642\n",
            "training error 0.12509259778369144, test error 0.2506307152484962\n",
            "Loss: 0.21998346332834373\n",
            "training error 0.12512154434318373, test error 0.25057364599133125\n",
            "Loss: 0.19716311585549207\n",
            "training error 0.12501790514678604, test error 0.2506418602059243\n",
            "Loss: 0.22444000987746904\n",
            "training error 0.12494238698209896, test error 0.2507645266601066\n",
            "Loss: 0.2734907816372756\n",
            "training error 0.12512382157137433, test error 0.25081859336817314\n",
            "Loss: 0.29511049645543075\n",
            "training error 0.1251619431013309, test error 0.25057988081076354\n",
            "Loss: 0.19965624005131577\n",
            "training error 0.12494578708264964, test error 0.250634927941884\n",
            "Loss: 0.22166799772884715\n",
            "training error 0.1249785032275594, test error 0.2507436353939339\n",
            "Loss: 0.2651369677470594\n",
            "training error 0.12493293498688006, test error 0.2506964236303307\n",
            "Loss: 0.2462583472120805\n",
            "training error 0.12506048285431753, test error 0.25073491549364985\n",
            "Loss: 0.26165013150114547\n",
            "training error 0.12508670630254792, test error 0.25069342308099013\n",
            "Loss: 0.24505851420280056\n",
            "training error 0.12506077701040583, test error 0.2505417715798654\n",
            "Loss: 0.1844174594149317\n",
            "training error 0.12502088392182875, test error 0.25055387065174406\n",
            "Loss: 0.18925552877286922\n",
            "training error 0.12499023008815255, test error 0.25055725014696056\n",
            "Loss: 0.19060689129193342\n",
            "training error 0.12498428756302024, test error 0.25060719175030083\n",
            "Loss: 0.21057709588530038\n",
            "training error 0.1250327347752962, test error 0.2507294111812249\n",
            "Loss: 0.25944911595658926\n",
            "training error 0.12500056867669643, test error 0.25052849412720224\n",
            "Loss: 0.17910818961925923\n",
            "training error 0.12504380898066703, test error 0.25060257502074523\n",
            "Loss: 0.20873099909202164\n",
            "training error 0.1249416792991483, test error 0.25045769394627065\n",
            "Loss: 0.15079724235516068\n",
            "training error 0.12498549687641751, test error 0.25045001176729376\n",
            "Loss: 0.1477253608849871\n",
            "training error 0.12505529416876066, test error 0.2505007577682749\n",
            "Loss: 0.16801722085959536\n",
            "training error 0.12494432561278294, test error 0.25038905348634666\n",
            "Loss: 0.12334990513762634\n",
            "training error 0.12494261503323309, test error 0.2505046308655216\n",
            "Loss: 0.1695659605726485\n",
            "training error 0.1249304265098668, test error 0.25048177314779585\n",
            "Loss: 0.160425819508081\n",
            "training error 0.12503424529757312, test error 0.25051364942336873\n",
            "Loss: 0.17317222135120325\n",
            "training error 0.12492827103555051, test error 0.25049523395103107\n",
            "Loss: 0.1658084059013687\n",
            "training error 0.12500441784027364, test error 0.2504569408644394\n",
            "Loss: 0.15049610668389501\n",
            "training error 0.12493041875900665, test error 0.2504764855254151\n",
            "Loss: 0.15831145205305042\n",
            "training error 0.12497718050291656, test error 0.2504972503181034\n",
            "Loss: 0.16661469285006447\n",
            "training error 0.12495875172016423, test error 0.250571943239297\n",
            "Loss: 0.19648223450163815\n",
            "training error 0.12495808402705966, test error 0.25062099824980477\n",
            "Loss: 0.21609791623087293\n",
            "training error 0.12498510536008511, test error 0.25059248838715453\n",
            "Loss: 0.20469764567538906\n",
            "training error 0.12493372670316523, test error 0.25054443359585765\n",
            "Loss: 0.18548192271687913\n",
            "training error 0.12498170833668304, test error 0.25052213235434745\n",
            "Loss: 0.17656430041688154\n",
            "training error 0.12495201245305172, test error 0.2505367704900502\n",
            "Loss: 0.18241766805622284\n",
            "training error 0.12491660954977843, test error 0.2505676757812314\n",
            "Loss: 0.19477580128837158\n",
            "training error 0.12503428868966127, test error 0.2507193613089901\n",
            "Loss: 0.2554304623443082\n",
            "training error 0.12492021597309987, test error 0.25068768551749065\n",
            "Loss: 0.24276422829110356\n",
            "training error 0.12494071387230063, test error 0.25081741979515887\n",
            "Loss: 0.2946412185061309\n",
            "training error 0.12505912699021468, test error 0.2508077020769413\n",
            "Loss: 0.290755383690966\n",
            "training error 0.12495184723008874, test error 0.25076275196403863\n",
            "Loss: 0.2727811319424678\n",
            "training error 0.12492317574090633, test error 0.2507336462500139\n",
            "Loss: 0.2611425976337012\n",
            "training error 0.12492437027923428, test error 0.25066075726336445\n",
            "Loss: 0.23199639729900579\n",
            "training error 0.12497640311478683, test error 0.2507675565242532\n",
            "Loss: 0.2747023367906998\n",
            "training error 0.1249132638116111, test error 0.2507175373318648\n",
            "Loss: 0.25470110657819234\n",
            "training error 0.12508223955200848, test error 0.2507604732707322\n",
            "Loss: 0.2718699483101661\n",
            "training error 0.12493552571633143, test error 0.25082178787938636\n",
            "Loss: 0.29638788921491077\n",
            "training error 0.12497589932757891, test error 0.2507410863524515\n",
            "Loss: 0.2641176796883027\n",
            "training error 0.12496423651140003, test error 0.2507022083412147\n",
            "Loss: 0.24857148600099244\n",
            "training error 0.12491245768041263, test error 0.25075935902068197\n",
            "Loss: 0.27142439190062095\n",
            "training error 0.12489889111977635, test error 0.25079644247078264\n",
            "Loss: 0.2862529924261725\n",
            "training error 0.12489668728964827, test error 0.25079966517824903\n",
            "Loss: 0.2875416600528258\n",
            "training error 0.12507601063132517, test error 0.25092910642856026\n",
            "Loss: 0.3393014770920244\n",
            "training error 0.12492683259682279, test error 0.2508027859763056\n",
            "Loss: 0.288789577050208\n",
            "training error 0.12490759269399196, test error 0.2508144028389444\n",
            "Loss: 0.29343482486190453\n",
            "training error 0.12501279459356146, test error 0.25086466664561696\n",
            "Loss: 0.31353386926089577\n",
            "training error 0.12498064162567363, test error 0.2508127736675054\n",
            "Loss: 0.2927833662626833\n",
            "training error 0.12489838609434381, test error 0.2506586664118592\n",
            "Loss: 0.23116032617709337\n",
            "training error 0.12490638979808087, test error 0.25072965497189803\n",
            "Loss: 0.2595466008047831\n",
            "training error 0.12494925961464953, test error 0.2506824055262445\n",
            "Loss: 0.24065291230632724\n",
            "training error 0.12502991455881332, test error 0.2508402074395966\n",
            "Loss: 0.3037533392869163\n",
            "training error 0.12486439906466273, test error 0.2507923764790229\n",
            "Loss: 0.2846271197691763\n",
            "training error 0.12489916171149419, test error 0.2507522841381497\n",
            "Loss: 0.26859535073666674\n",
            "training error 0.12488577053452234, test error 0.2508027483466171\n",
            "Loss: 0.28877453002471576\n",
            "training error 0.12498276812944571, test error 0.25072909829721945\n",
            "Loss: 0.2593240026805832\n",
            "training error 0.12487605033351047, test error 0.25069190986144985\n",
            "Loss: 0.24445342141858895\n",
            "training error 0.12499030006784982, test error 0.25083592750068073\n",
            "Loss: 0.30204191534215497\n",
            "training error 0.12490584696929441, test error 0.25073018187723317\n",
            "Loss: 0.259757295028451\n",
            "training error 0.12490354131219472, test error 0.2507220129990387\n",
            "Loss: 0.2564907965996621\n",
            "training error 0.12508869358051736, test error 0.2506907622447252\n",
            "Loss: 0.24399452263974553\n",
            "training error 0.12493981316031473, test error 0.2508046404379923\n",
            "Loss: 0.2895311227118613\n",
            "training error 0.12488861598859038, test error 0.25072959648256177\n",
            "Loss: 0.25952321260869216\n",
            "training error 0.12484909641777507, test error 0.25069956460524356\n",
            "Loss: 0.2475143323514617\n",
            "training error 0.12498291995937437, test error 0.2505995480293654\n",
            "Loss: 0.20752059267503675\n",
            "training error 0.12490536318915271, test error 0.25073036259034953\n",
            "Loss: 0.259829556983715\n",
            "training error 0.12506506037982393, test error 0.2506192038107281\n",
            "Loss: 0.21538037187718295\n",
            "training error 0.12486412574689301, test error 0.25071626744325104\n",
            "Loss: 0.2541933148027642\n",
            "training error 0.12484642540638703, test error 0.25077009101868086\n",
            "Loss: 0.27571580790248795\n",
            "training error 0.1248270090265181, test error 0.25085660762397977\n",
            "Loss: 0.3103112992961288\n",
            "training error 0.12483951983384707, test error 0.2507649492279293\n",
            "Loss: 0.2736597543035346\n",
            "training error 0.12499438021048499, test error 0.25085796598037297\n",
            "Loss: 0.3108544667811186\n",
            "training error 0.12481110741231222, test error 0.25076200553251554\n",
            "Loss: 0.27248265553734274\n",
            "training error 0.12484206988699778, test error 0.2506750304517001\n",
            "Loss: 0.23770383302788023\n",
            "training error 0.12492421091505027, test error 0.2508111740062991\n",
            "Loss: 0.2921437079531186\n",
            "training error 0.12490189438121804, test error 0.25066287065169446\n",
            "Loss: 0.2328414802461376\n",
            "training error 0.12480344649904489, test error 0.2506862007201877\n",
            "Loss: 0.24217050073858104\n",
            "training error 0.12477582014235275, test error 0.2506386015444249\n",
            "Loss: 0.22313696527154203\n",
            "training error 0.12487992626548709, test error 0.25052318011889235\n",
            "Loss: 0.17698327119330948\n",
            "training error 0.12485539193168996, test error 0.2506157043534082\n",
            "Loss: 0.21398103997811724\n",
            "training error 0.1249278816688801, test error 0.2504770182095062\n",
            "Loss: 0.15852445703428142\n",
            "training error 0.1249351190353045, test error 0.2506472604750054\n",
            "Loss: 0.2265994214941447\n",
            "training error 0.12475821908422204, test error 0.2505522279701285\n",
            "Loss: 0.18859866784428636\n",
            "training error 0.12479042616994972, test error 0.2504485999697965\n",
            "Loss: 0.1471608238461597\n",
            "training error 0.12474827359212741, test error 0.2504517472480808\n",
            "Loss: 0.14841932952169845\n",
            "training error 0.12480668669305314, test error 0.25041366769771983\n",
            "Loss: 0.1331924172747856\n",
            "training error 0.1247790905210319, test error 0.2504571297619286\n",
            "Loss: 0.15057164133343015\n",
            "training error 0.12478181401650418, test error 0.250547910162228\n",
            "Loss: 0.1868721011864194\n",
            "training error 0.12479956402194446, test error 0.2505876784952335\n",
            "Loss: 0.20277430883173242\n",
            "training error 0.12470720294491282, test error 0.25042414315075895\n",
            "Loss: 0.13738124835764864\n",
            "training error 0.12479781848139763, test error 0.25041812848560874\n",
            "Loss: 0.1349761574999997\n",
            "training error 0.12474790865694341, test error 0.25036362730324246\n",
            "Loss: 0.11318270895932514\n",
            "training error 0.1246795590681791, test error 0.25032035141078507\n",
            "Loss: 0.09587792960639163\n",
            "training error 0.12468879735660289, test error 0.2503891582677545\n",
            "Loss: 0.1233918041959603\n",
            "training error 0.1246739133907738, test error 0.2502951757534985\n",
            "Loss: 0.08581091146604436\n",
            "training error 0.12465164962005833, test error 0.2502312334784598\n",
            "Loss: 0.06024224267526801\n",
            "training error 0.12463973950990759, test error 0.2501916436729943\n",
            "Loss: 0.04441142303635015\n",
            "training error 0.12463146681800637, test error 0.2501381194582991\n",
            "Loss: 0.02300863564725919\n",
            "training error 0.12461867767161891, test error 0.2501754001866582\n",
            "Loss: 0.037916122049885814\n",
            "training error 0.12483004832669535, test error 0.2503546528223601\n",
            "Loss: 0.1095940732857903\n",
            "training error 0.12464914323627584, test error 0.2501215367321474\n",
            "Loss: 0.01637768245759119\n",
            "training error 0.1248261302833114, test error 0.2501667460064783\n",
            "Loss: 0.034455565375268726\n",
            "training error 0.12465873396479928, test error 0.2500485042849678\n",
            "Loss: 0.0\n",
            "training error 0.12456766459084881, test error 0.25011000131416894\n",
            "Loss: 0.02459404001515786\n",
            "training error 0.12456520318724663, test error 0.2500311099931636\n",
            "Loss: 0.0\n",
            "training error 0.12454772534358749, test error 0.25004565692402375\n",
            "Loss: 0.005818048346295512\n",
            "training error 0.12452168791615494, test error 0.2500161551497532\n",
            "Loss: 0.0\n",
            "training error 0.12500850499459723, test error 0.2501307259907255\n",
            "Loss: 0.045825375125718715\n",
            "training error 0.12457063010090101, test error 0.25007567852608154\n",
            "Loss: 0.023807812056242383\n",
            "training error 0.12443194419200819, test error 0.2499495575031687\n",
            "Loss: 0.0\n",
            "training error 0.12442096591489549, test error 0.24992856127862687\n",
            "Loss: 0.0\n",
            "training error 0.12439523478194382, test error 0.24983215851164783\n",
            "Loss: 0.0\n",
            "training error 0.124379009417857, test error 0.24977430325031338\n",
            "Loss: 0.0\n",
            "training error 0.12439938313474296, test error 0.24976216657548936\n",
            "Loss: 0.0\n",
            "training error 0.12444308205666109, test error 0.24976852796461757\n",
            "Loss: 0.0025469786779197534\n",
            "training error 0.12432836137694554, test error 0.24968873472198905\n",
            "Loss: 0.0\n",
            "training error 0.12439971147490754, test error 0.24973387507064876\n",
            "Loss: 0.018078648486063997\n",
            "training error 0.12437116532235161, test error 0.24954364101521417\n",
            "Loss: 0.0\n",
            "training error 0.12424958621043022, test error 0.24956800579379881\n",
            "Loss: 0.00976373450571888\n",
            "training error 0.12425628709058968, test error 0.24939421180651938\n",
            "Loss: 0.0\n",
            "training error 0.12419231468159775, test error 0.249459453317627\n",
            "Loss: 0.02615999410533476\n",
            "training error 0.1241187117894229, test error 0.2493404704531864\n",
            "Loss: 0.0\n",
            "training error 0.1241043547077403, test error 0.24933582985496663\n",
            "Loss: 0.0\n",
            "training error 0.12405937692777332, test error 0.24928507000107344\n",
            "Loss: 0.0\n",
            "training error 0.12405027042788914, test error 0.2491356464740977\n",
            "Loss: 0.0\n",
            "training error 0.12396693870124752, test error 0.24905622306385783\n",
            "Loss: 0.0\n",
            "training error 0.12397801957881664, test error 0.24886126469969588\n",
            "Loss: 0.0\n",
            "training error 0.12396915398365198, test error 0.24895451709806174\n",
            "Loss: 0.03747164046536966\n",
            "training error 0.12389215016361727, test error 0.248917779383874\n",
            "Loss: 0.022709313257851527\n",
            "training error 0.12380448894315048, test error 0.24874116393435766\n",
            "Loss: 0.0\n",
            "training error 0.12376290272213825, test error 0.24863184791060283\n",
            "Loss: 0.0\n",
            "training error 0.12386776110090943, test error 0.24862942425517168\n",
            "Loss: 0.0\n",
            "training error 0.12370586720625128, test error 0.24842470373458161\n",
            "Loss: 0.0\n",
            "training error 0.12367130247373848, test error 0.2483104075521281\n",
            "Loss: 0.0\n",
            "training error 0.12353386739117611, test error 0.24809225205347732\n",
            "Loss: 0.0\n",
            "training error 0.12358215116091611, test error 0.24811377012540517\n",
            "Loss: 0.008673415533833051\n",
            "training error 0.12349185811703739, test error 0.2478712749278342\n",
            "Loss: 0.0\n",
            "training error 0.12336895572883837, test error 0.24786692860100012\n",
            "Loss: 0.0\n",
            "training error 0.12326695884709168, test error 0.24771871867320744\n",
            "Loss: 0.0\n",
            "training error 0.12317549083875945, test error 0.24753204062237866\n",
            "Loss: 0.0\n",
            "training error 0.12311369594772077, test error 0.24741757594577266\n",
            "Loss: 0.0\n",
            "training error 0.12300045871557141, test error 0.24726487090473942\n",
            "Loss: 0.0\n",
            "training error 0.12292212015482822, test error 0.24710920339022446\n",
            "Loss: 0.0\n",
            "training error 0.12289417815734982, test error 0.2468632960046027\n",
            "Loss: 0.0\n",
            "training error 0.12276220885699136, test error 0.24667479945938078\n",
            "Loss: 0.0\n",
            "training error 0.12272310541729464, test error 0.2463686752615917\n",
            "Loss: 0.0\n",
            "training error 0.12255985811703386, test error 0.24613487066135978\n",
            "Loss: 0.0\n",
            "training error 0.12256142012690978, test error 0.24603615155977793\n",
            "Loss: 0.0\n",
            "training error 0.12243915781840467, test error 0.2458153811067409\n",
            "Loss: 0.0\n",
            "training error 0.1222408994524627, test error 0.24547217577288377\n",
            "Loss: 0.0\n",
            "training error 0.1222761238098974, test error 0.24537543023420474\n",
            "Loss: 0.0\n",
            "training error 0.12203695603546491, test error 0.2449300470819812\n",
            "Loss: 0.0\n",
            "training error 0.1219055996340537, test error 0.2448348113159387\n",
            "Loss: 0.0\n",
            "training error 0.12171952325654153, test error 0.2444741649537191\n",
            "Loss: 0.0\n",
            "training error 0.12157454023942627, test error 0.2441891247709549\n",
            "Loss: 0.0\n",
            "training error 0.12142413177384721, test error 0.2439417755487345\n",
            "Loss: 0.0\n",
            "training error 0.1217206969511415, test error 0.24354076749384937\n",
            "Loss: 0.0\n",
            "training error 0.1210852211012363, test error 0.24339073180370027\n",
            "Loss: 0.0\n",
            "training error 0.12102477913180505, test error 0.24307748106638902\n",
            "Loss: 0.0\n",
            "training error 0.12080507340006735, test error 0.24289174967774635\n",
            "Loss: 0.0\n",
            "training error 0.1206718581067984, test error 0.242441753609252\n",
            "Loss: 0.0\n",
            "training error 0.12038910887599344, test error 0.24197305729352664\n",
            "Loss: 0.0\n",
            "training error 0.12022369995399775, test error 0.2415325595215416\n",
            "Loss: 0.0\n",
            "training error 0.12007669254037447, test error 0.24115748234412085\n",
            "Loss: 0.0\n",
            "training error 0.11979759006651894, test error 0.2407075792098708\n",
            "Loss: 0.0\n",
            "training error 0.11961335193759086, test error 0.24019078600521793\n",
            "Loss: 0.0\n",
            "training error 0.11962404589233934, test error 0.23968807482140564\n",
            "Loss: 0.0\n",
            "training error 0.11920019797380796, test error 0.23937103033095225\n",
            "Loss: 0.0\n",
            "training error 0.11904214353187527, test error 0.23887587672580157\n",
            "Loss: 0.0\n",
            "training error 0.1186035936245157, test error 0.23829840084348142\n",
            "Loss: 0.0\n",
            "training error 0.1185066551532015, test error 0.2377307177139176\n",
            "Loss: 0.0\n",
            "training error 0.11810994612867169, test error 0.23709543213224457\n",
            "Loss: 0.0\n",
            "training error 0.11779643657965795, test error 0.2363603719784741\n",
            "Loss: 0.0\n",
            "training error 0.11759579046512271, test error 0.23581707024831572\n",
            "Loss: 0.0\n",
            "training error 0.11720731022187762, test error 0.23522305912631858\n",
            "Loss: 0.0\n",
            "training error 0.11690065663430732, test error 0.2346602025716241\n",
            "Loss: 0.0\n",
            "training error 0.11655480497120164, test error 0.23391367506864047\n",
            "Loss: 0.0\n",
            "training error 0.11626245909954375, test error 0.2332170374948248\n",
            "Loss: 0.0\n",
            "training error 0.11594662938869289, test error 0.23261490799425194\n",
            "Loss: 0.0\n",
            "training error 0.11555604480818643, test error 0.23180084481570323\n",
            "Loss: 0.0\n",
            "training error 0.11520608217190553, test error 0.23119693700194527\n",
            "Loss: 0.0\n",
            "training error 0.11486571327660548, test error 0.23030601987533614\n",
            "Loss: 0.0\n",
            "training error 0.11435733785157975, test error 0.22935557281439942\n",
            "Loss: 0.0\n",
            "training error 0.1139752965260726, test error 0.22848688361363076\n",
            "Loss: 0.0\n",
            "training error 0.1135042224380068, test error 0.22765141096272812\n",
            "Loss: 0.0\n",
            "training error 0.11306476190508645, test error 0.22671575085792173\n",
            "Loss: 0.0\n",
            "training error 0.11266191645203326, test error 0.2257879713854869\n",
            "Loss: 0.0\n",
            "training error 0.11218695217162362, test error 0.22462054084721567\n",
            "Loss: 0.0\n",
            "training error 0.11168412186200843, test error 0.2237944679113986\n",
            "Loss: 0.0\n",
            "training error 0.11119161743448569, test error 0.22281533506284912\n",
            "Loss: 0.0\n",
            "training error 0.11084981737380116, test error 0.22183420974663678\n",
            "Loss: 0.0\n",
            "training error 0.11048979816538629, test error 0.22061041834810835\n",
            "Loss: 0.0\n",
            "training error 0.10964798963480277, test error 0.21967880163650996\n",
            "Loss: 0.0\n",
            "training error 0.10909163313493259, test error 0.21852551361922626\n",
            "Loss: 0.0\n",
            "training error 0.10855614005721129, test error 0.21737013035495248\n",
            "Loss: 0.0\n",
            "training error 0.1080690951697069, test error 0.2160772787710922\n",
            "Loss: 0.0\n",
            "training error 0.1074172864647197, test error 0.21495126690931537\n",
            "Loss: 0.0\n",
            "training error 0.1068641262026603, test error 0.2137813409115667\n",
            "Loss: 0.0\n",
            "training error 0.10622104288123274, test error 0.21248416259785047\n",
            "Loss: 0.0\n",
            "training error 0.10560999772281872, test error 0.21114289898056215\n",
            "Loss: 0.0\n",
            "training error 0.10496282648856993, test error 0.20981385789169882\n",
            "Loss: 0.0\n",
            "training error 0.10433017891047727, test error 0.20849046048424608\n",
            "Loss: 0.0\n",
            "training error 0.10370713729656277, test error 0.20708475710194166\n",
            "Loss: 0.0\n",
            "training error 0.10298546119780111, test error 0.20560362935957938\n",
            "Loss: 0.0\n",
            "training error 0.10240279879873947, test error 0.20438969013614555\n",
            "Loss: 0.0\n",
            "training error 0.10164275010656944, test error 0.20284182275108076\n",
            "Loss: 0.0\n",
            "training error 0.1009432282385569, test error 0.20134219531864078\n",
            "Loss: 0.0\n",
            "training error 0.10028967871700224, test error 0.19981686864998843\n",
            "Loss: 0.0\n",
            "training error 0.09959227964096729, test error 0.1983671489368856\n",
            "Loss: 0.0\n",
            "training error 0.0988204920952102, test error 0.1969362201275893\n",
            "Loss: 0.0\n",
            "training error 0.09826226484023631, test error 0.1952431878142546\n",
            "Loss: 0.0\n",
            "training error 0.09751858458667026, test error 0.1936906012333676\n",
            "Loss: 0.0\n",
            "training error 0.09667606551491062, test error 0.1922597769038109\n",
            "Loss: 0.0\n",
            "training error 0.09590456708901893, test error 0.1906981906846554\n",
            "Loss: 0.0\n",
            "training error 0.095378041903842, test error 0.18884437763868633\n",
            "Loss: 0.0\n",
            "training error 0.09437901002584644, test error 0.18733092820229746\n",
            "Loss: 0.0\n",
            "training error 0.09354758648031594, test error 0.18563397395785608\n",
            "Loss: 0.0\n",
            "training error 0.09285130551096961, test error 0.18383159581249145\n",
            "Loss: 0.0\n",
            "training error 0.09202114695184863, test error 0.18227199445079556\n",
            "Loss: 0.0\n",
            "training error 0.09126178937655205, test error 0.18055439942987178\n",
            "Loss: 0.0\n",
            "training error 0.09047519187843142, test error 0.17906430774907092\n",
            "Loss: 0.0\n",
            "training error 0.0896390600186373, test error 0.17737352355678984\n",
            "Loss: 0.0\n",
            "training error 0.08892149449629692, test error 0.17565817724248511\n",
            "Loss: 0.0\n",
            "training error 0.08805462052461403, test error 0.17402938357732292\n",
            "Loss: 0.0\n",
            "training error 0.08728396081762722, test error 0.17235818809899828\n",
            "Loss: 0.0\n",
            "training error 0.08652437413644706, test error 0.17070294671460967\n",
            "Loss: 0.0\n",
            "training error 0.08571158686649062, test error 0.16906097125375297\n",
            "Loss: 0.0\n",
            "training error 0.08492196778718956, test error 0.1674504379428043\n",
            "Loss: 0.0\n",
            "training error 0.08419393970971784, test error 0.1657698734162132\n",
            "Loss: 0.0\n",
            "training error 0.08338476430146272, test error 0.1640290063229289\n",
            "Loss: 0.0\n",
            "training error 0.08261683566946434, test error 0.16227997772994418\n",
            "Loss: 0.0\n",
            "training error 0.08184778406549886, test error 0.1606153384383447\n",
            "Loss: 0.0\n",
            "training error 0.08126134098377598, test error 0.15909067407395103\n",
            "Loss: 0.0\n",
            "training error 0.08031439422217498, test error 0.15731981589392957\n",
            "Loss: 0.0\n",
            "training error 0.07956615240522422, test error 0.15577206534729662\n",
            "Loss: 0.0\n",
            "training error 0.07896215338011629, test error 0.15419456365241688\n",
            "Loss: 0.0\n",
            "training error 0.0781423850547722, test error 0.15271446970075175\n",
            "Loss: 0.0\n",
            "training error 0.07736974493544378, test error 0.15098128240172146\n",
            "Loss: 0.0\n",
            "training error 0.07664135915391401, test error 0.14951093491132134\n",
            "Loss: 0.0\n",
            "training error 0.07589512567188539, test error 0.1478862041260963\n",
            "Loss: 0.0\n",
            "training error 0.0752111919826418, test error 0.14623239003981417\n",
            "Loss: 0.0\n",
            "training error 0.07450188890415105, test error 0.14479723958724003\n",
            "Loss: 0.0\n",
            "training error 0.07377956429895371, test error 0.1432145601415584\n",
            "Loss: 0.0\n",
            "training error 0.07326688193697943, test error 0.14176272334527246\n",
            "Loss: 0.0\n",
            "training error 0.07246661434742892, test error 0.14013220459427228\n",
            "Loss: 0.0\n",
            "training error 0.0717806050062809, test error 0.13859299637149736\n",
            "Loss: 0.0\n",
            "training error 0.07128701785300967, test error 0.13730609005998448\n",
            "Loss: 0.0\n",
            "training error 0.07053569842917183, test error 0.135472747792441\n",
            "Loss: 0.0\n",
            "training error 0.06995386464437836, test error 0.13414442909150737\n",
            "Loss: 0.0\n",
            "training error 0.06911494484800194, test error 0.13266029444032476\n",
            "Loss: 0.0\n",
            "training error 0.06845459399866169, test error 0.13123761502705986\n",
            "Loss: 0.0\n",
            "training error 0.06793818056296089, test error 0.1298107178018474\n",
            "Loss: 0.0\n",
            "training error 0.06726321376954103, test error 0.12860120209210119\n",
            "Loss: 0.0\n",
            "training error 0.06669938743507284, test error 0.12711356580412694\n",
            "Loss: 0.0\n",
            "training error 0.06610154032503636, test error 0.12589210162330994\n",
            "Loss: 0.0\n",
            "training error 0.06547256761949939, test error 0.12456589961330085\n",
            "Loss: 0.0\n",
            "training error 0.064893174366708, test error 0.12331851579165526\n",
            "Loss: 0.0\n",
            "training error 0.06436588345368081, test error 0.12200897638121508\n",
            "Loss: 0.0\n",
            "training error 0.06379228609611592, test error 0.12094800709591814\n",
            "Loss: 0.0\n",
            "training error 0.0631972752471644, test error 0.11975148639491884\n",
            "Loss: 0.0\n",
            "training error 0.06267585807856547, test error 0.1184837567977564\n",
            "Loss: 0.0\n",
            "training error 0.06230227352625181, test error 0.1170911887326038\n",
            "Loss: 0.0\n",
            "training error 0.0616572060052924, test error 0.11606449946126865\n",
            "Loss: 0.0\n",
            "training error 0.06116174878922987, test error 0.11494786103058935\n",
            "Loss: 0.0\n",
            "training error 0.060607545402987864, test error 0.11395896920761017\n",
            "Loss: 0.0\n",
            "training error 0.06010661038466944, test error 0.11283118898138464\n",
            "Loss: 0.0\n",
            "training error 0.059701897971228775, test error 0.11173420008571468\n",
            "Loss: 0.0\n",
            "training error 0.05932248591557179, test error 0.11085724023284489\n",
            "Loss: 0.0\n",
            "training error 0.05870081936220751, test error 0.10978186640899258\n",
            "Loss: 0.0\n",
            "training error 0.058224565205767104, test error 0.10868803109863377\n",
            "Loss: 0.0\n",
            "training error 0.05778021169895059, test error 0.10760067218626469\n",
            "Loss: 0.0\n",
            "training error 0.05737332793570438, test error 0.10665889044308555\n",
            "Loss: 0.0\n",
            "training error 0.05689630729448832, test error 0.10578396443341773\n",
            "Loss: 0.0\n",
            "training error 0.0564668684355749, test error 0.1047619515457186\n",
            "Loss: 0.0\n",
            "training error 0.05605275821490873, test error 0.10374152064670149\n",
            "Loss: 0.0\n",
            "training error 0.0556395738555787, test error 0.102766736447578\n",
            "Loss: 0.0\n",
            "training error 0.05527679691958579, test error 0.10176339982437689\n",
            "Loss: 0.0\n",
            "training error 0.05490705333699425, test error 0.10082331460100258\n",
            "Loss: 0.0\n",
            "training error 0.05455236961842648, test error 0.10006945223516679\n",
            "Loss: 0.0\n",
            "training error 0.05424695785233506, test error 0.09907510480361868\n",
            "Loss: 0.0\n",
            "training error 0.0537740256923349, test error 0.09836994542320365\n",
            "Loss: 0.0\n",
            "training error 0.05336205346613281, test error 0.09742159398057418\n",
            "Loss: 0.0\n",
            "training error 0.053096499597757744, test error 0.09665749711523532\n",
            "Loss: 0.0\n",
            "training error 0.05272374154421987, test error 0.09589146560346729\n",
            "Loss: 0.0\n",
            "training error 0.052369724440833404, test error 0.09516281904884052\n",
            "Loss: 0.0\n",
            "training error 0.05205401281660282, test error 0.09433937375148863\n",
            "Loss: 0.0\n",
            "training error 0.05181985957568444, test error 0.09349137798280073\n",
            "Loss: 0.0\n",
            "training error 0.051487583032114724, test error 0.09299055810804634\n",
            "Loss: 0.0\n",
            "training error 0.05106480694665497, test error 0.09216909544075129\n",
            "Loss: 0.0\n",
            "training error 0.0508683740656663, test error 0.09139376062847158\n",
            "Loss: 0.0\n",
            "training error 0.050515185028868405, test error 0.0908338613224308\n",
            "Loss: 0.0\n",
            "training error 0.05032097883848444, test error 0.09004498466471265\n",
            "Loss: 0.0\n",
            "training error 0.049926799218989105, test error 0.08940611179500983\n",
            "Loss: 0.0\n",
            "training error 0.049704988919831036, test error 0.08877768355777514\n",
            "Loss: 0.0\n",
            "training error 0.04941734668880047, test error 0.08826407666287883\n",
            "Loss: 0.0\n",
            "training error 0.04909670587520687, test error 0.08743069007776921\n",
            "Loss: 0.0\n",
            "training error 0.04883980976201688, test error 0.08694024506917558\n",
            "Loss: 0.0\n",
            "training error 0.048637501522926174, test error 0.08639914505687823\n",
            "Loss: 0.0\n",
            "training error 0.0484245289553284, test error 0.08567977244723592\n",
            "Loss: 0.0\n",
            "training error 0.048110571410624, test error 0.08525325336320169\n",
            "Loss: 0.0\n",
            "training error 0.04783416031308585, test error 0.0847036217141714\n",
            "Loss: 0.0\n",
            "training error 0.047592129000630125, test error 0.08420046418339996\n",
            "Loss: 0.0\n",
            "training error 0.047461575056470286, test error 0.08357247592330394\n",
            "Loss: 0.0\n",
            "training error 0.047203496811320515, test error 0.08332137914165787\n",
            "Loss: 0.0\n",
            "training error 0.04693616916056568, test error 0.08275309683808951\n",
            "Loss: 0.0\n",
            "training error 0.04672292724562646, test error 0.08217965849316107\n",
            "Loss: 0.0\n",
            "training error 0.046558063541529325, test error 0.08180635655661993\n",
            "Loss: 0.0\n",
            "training error 0.04632707591980705, test error 0.08121591540568905\n",
            "Loss: 0.0\n",
            "training error 0.04622388514072672, test error 0.08055895677807313\n",
            "Loss: 0.0\n",
            "training error 0.045923355942398325, test error 0.08005867171465998\n",
            "Loss: 0.0\n",
            "training error 0.04576815109126855, test error 0.07973670303349323\n",
            "Loss: 0.0\n",
            "training error 0.04555898709078484, test error 0.07927772885938303\n",
            "Loss: 0.0\n",
            "training error 0.04532033915915184, test error 0.07900390024520537\n",
            "Loss: 0.0\n",
            "training error 0.04513239564727242, test error 0.07860402630496302\n",
            "Loss: 0.0\n",
            "training error 0.04496572763730216, test error 0.07812317547460877\n",
            "Loss: 0.0\n",
            "training error 0.044787478722203865, test error 0.0776255317965079\n",
            "Loss: 0.0\n",
            "training error 0.04464169323635345, test error 0.0771950040176952\n",
            "Loss: 0.0\n",
            "training error 0.044476477007587265, test error 0.07689862960448496\n",
            "Loss: 0.0\n",
            "training error 0.04428606737141783, test error 0.0764710164608406\n",
            "Loss: 0.0\n",
            "training error 0.044092169054609984, test error 0.07609908023311203\n",
            "Loss: 0.0\n",
            "training error 0.04393193405877547, test error 0.07565070818144867\n",
            "Loss: 0.0\n",
            "training error 0.04379446288104951, test error 0.07513290590885688\n",
            "Loss: 0.0\n",
            "training error 0.04365198765125142, test error 0.07478593404590565\n",
            "Loss: 0.0\n",
            "training error 0.043490234152621134, test error 0.07437320067937692\n",
            "Loss: 0.0\n",
            "training error 0.04338312810276831, test error 0.07415959706786905\n",
            "Loss: 0.0\n",
            "training error 0.043211464485115744, test error 0.07380378504228417\n",
            "Loss: 0.0\n",
            "training error 0.043031921443649715, test error 0.07360995122589836\n",
            "Loss: 0.0\n",
            "training error 0.042977548706063316, test error 0.07327337675875574\n",
            "Loss: 0.0\n",
            "training error 0.04276296367222761, test error 0.07289264300973328\n",
            "Loss: 0.0\n",
            "training error 0.042719393955739586, test error 0.07241959053148138\n",
            "Loss: 0.0\n",
            "training error 0.042525955345227476, test error 0.07221234004983929\n",
            "Loss: 0.0\n",
            "training error 0.042369547633283385, test error 0.07188918025869465\n",
            "Loss: 0.0\n",
            "training error 0.04223968595854469, test error 0.07163790836102668\n",
            "Loss: 0.0\n",
            "training error 0.042110114999264404, test error 0.07135775386993036\n",
            "Loss: 0.0\n",
            "training error 0.041997806853163865, test error 0.07108824211958865\n",
            "Loss: 0.0\n",
            "training error 0.04188069872541174, test error 0.0707739150147578\n",
            "Loss: 0.0\n",
            "training error 0.04181858140872904, test error 0.0704620340995515\n",
            "Loss: 0.0\n",
            "training error 0.041702317130491225, test error 0.07040533096655562\n",
            "Loss: 0.0\n",
            "training error 0.04155868160951378, test error 0.07003143242708494\n",
            "Loss: 0.0\n",
            "training error 0.041476291575304546, test error 0.06980822710964109\n",
            "Loss: 0.0\n",
            "training error 0.04143337930710909, test error 0.06948594150995169\n",
            "Loss: 0.0\n",
            "training error 0.04121947996527186, test error 0.06892162300626212\n",
            "Loss: 0.0\n",
            "training error 0.04116368564772706, test error 0.06857630925538394\n",
            "Loss: 0.0\n",
            "training error 0.041074699810308875, test error 0.06859987521246853\n",
            "Loss: 0.034364574793355196\n",
            "training error 0.040934696776011845, test error 0.06844387792501537\n",
            "Loss: 0.0\n",
            "training error 0.040827626366061145, test error 0.06823792115021987\n",
            "Loss: 0.0\n",
            "training error 0.04079065717671312, test error 0.0679198355128213\n",
            "Loss: 0.0\n",
            "training error 0.04066303460159725, test error 0.06789450913839179\n",
            "Loss: 0.0\n",
            "training error 0.0406057049708389, test error 0.06766550003435391\n",
            "Loss: 0.0\n",
            "training error 0.04048527553997014, test error 0.06755625021855324\n",
            "Loss: 0.0\n",
            "training error 0.04038947922202836, test error 0.06730226108626927\n",
            "Loss: 0.0\n",
            "training error 0.040271886701753246, test error 0.06715346755186853\n",
            "Loss: 0.0\n",
            "training error 0.04021319141237345, test error 0.06690046420542979\n",
            "Loss: 0.0\n",
            "training error 0.040139052188150726, test error 0.06676292576948792\n",
            "Loss: 0.0\n",
            "training error 0.0400260783283244, test error 0.06653600085768023\n",
            "Loss: 0.0\n",
            "training error 0.03995829193829985, test error 0.06632438887026806\n",
            "Loss: 0.0\n",
            "training error 0.03987944887235537, test error 0.06618948180406858\n",
            "Loss: 0.0\n",
            "training error 0.0398249728327088, test error 0.06605810753239243\n",
            "Loss: 0.0\n",
            "training error 0.03975496076581311, test error 0.06575109419931546\n",
            "Loss: 0.0\n",
            "training error 0.0396953737865626, test error 0.06561816549598634\n",
            "Loss: 0.0\n",
            "training error 0.03962162831482542, test error 0.06532500903221697\n",
            "Loss: 0.0\n",
            "training error 0.03949732171226101, test error 0.06524764959485084\n",
            "Loss: 0.0\n",
            "training error 0.03942451195825688, test error 0.06499208813842765\n",
            "Loss: 0.0\n",
            "training error 0.03936220230292723, test error 0.06473371732917017\n",
            "Loss: 0.0\n",
            "training error 0.039278557805735304, test error 0.06460961674090514\n",
            "Loss: 0.0\n",
            "training error 0.03923523611874763, test error 0.06450105574133379\n",
            "Loss: 0.0\n",
            "training error 0.03915429082827876, test error 0.06444404235816356\n",
            "Loss: 0.0\n",
            "training error 0.03916171901203958, test error 0.06398105310624895\n",
            "Loss: 0.0\n",
            "training error 0.0390287479663827, test error 0.06397300648689706\n",
            "Loss: 0.0\n",
            "training error 0.03894425055993617, test error 0.0638321665649509\n",
            "Loss: 0.0\n",
            "training error 0.03892151376620346, test error 0.06385453363634186\n",
            "Loss: 0.03504043900530718\n",
            "training error 0.03883650917892742, test error 0.06370332963371267\n",
            "Loss: 0.0\n",
            "training error 0.03877771960842207, test error 0.06361976159530545\n",
            "Loss: 0.0\n",
            "training error 0.038685004362645, test error 0.06349415703211975\n",
            "Loss: 0.0\n",
            "training error 0.038660287620493644, test error 0.0633359360633759\n",
            "Loss: 0.0\n",
            "training error 0.03858472275440686, test error 0.06316031148080421\n",
            "Loss: 0.0\n",
            "training error 0.03854520895050854, test error 0.06313306775706903\n",
            "Loss: 0.0\n",
            "training error 0.03852988369807474, test error 0.06280972744665456\n",
            "Loss: 0.0\n",
            "training error 0.03842992736579877, test error 0.06276824730049177\n",
            "Loss: 0.0\n",
            "training error 0.03841295642024079, test error 0.06265591710585838\n",
            "Loss: 0.0\n",
            "training error 0.038407268874867465, test error 0.0628131672822728\n",
            "Loss: 0.25097418356951895\n",
            "training error 0.038264080123383955, test error 0.06257630060994579\n",
            "Loss: 0.0\n",
            "training error 0.03825017992884577, test error 0.062401389375670545\n",
            "Loss: 0.0\n",
            "training error 0.03816341677719452, test error 0.06232217569624216\n",
            "Loss: 0.0\n",
            "training error 0.03815336558558262, test error 0.06225508005469475\n",
            "Loss: 0.0\n",
            "training error 0.03810308721917583, test error 0.06208050682161625\n",
            "Loss: 0.0\n",
            "training error 0.03803889228023539, test error 0.06202261726811552\n",
            "Loss: 0.0\n",
            "training error 0.03801532357840148, test error 0.06195244553475277\n",
            "Loss: 0.0\n",
            "training error 0.03798884617540524, test error 0.06171909701665486\n",
            "Loss: 0.0\n",
            "training error 0.037906930644815366, test error 0.061777100459181354\n",
            "Loss: 0.09397973290317108\n",
            "training error 0.037863717741209016, test error 0.06161754258625495\n",
            "Loss: 0.0\n",
            "training error 0.03787951234092592, test error 0.06143163569142772\n",
            "Loss: 0.0\n",
            "training error 0.037792796046584344, test error 0.061447838187568625\n",
            "Loss: 0.026374840843068625\n",
            "training error 0.03775521614099971, test error 0.06132868346262836\n",
            "Loss: 0.0\n",
            "training error 0.03773086134448958, test error 0.06131241460613839\n",
            "Loss: 0.0\n",
            "training error 0.037787205824254766, test error 0.061348371660360806\n",
            "Loss: 0.05864563392812272\n",
            "training error 0.0376548575804399, test error 0.06106871188641791\n",
            "Loss: 0.0\n",
            "training error 0.03762629809577345, test error 0.06099795679976678\n",
            "Loss: 0.0\n",
            "training error 0.03755949827060799, test error 0.06088561466802956\n",
            "Loss: 0.0\n",
            "training error 0.03751910262268961, test error 0.060917577464453705\n",
            "Loss: 0.05249646669154284\n",
            "training error 0.03747517658424477, test error 0.06072443769344809\n",
            "Loss: 0.0\n",
            "training error 0.03746902273712704, test error 0.060791460562549596\n",
            "Loss: 0.11037215270703893\n",
            "training error 0.03738904319568616, test error 0.060567766086207196\n",
            "Loss: 0.0\n",
            "training error 0.03738946628836473, test error 0.060620688965969574\n",
            "Loss: 0.0873779622102111\n",
            "training error 0.037333296693672934, test error 0.06047869687629961\n",
            "Loss: 0.0\n",
            "training error 0.03732775341904581, test error 0.06028756215170348\n",
            "Loss: 0.0\n",
            "training error 0.03726502102173969, test error 0.06029868451155006\n",
            "Loss: 0.0184488465773347\n",
            "training error 0.03724264903392243, test error 0.06013238617046939\n",
            "Loss: 0.0\n",
            "training error 0.037183345198691174, test error 0.0601514935319905\n",
            "Loss: 0.03177549194031126\n",
            "training error 0.03719666124164603, test error 0.05992658719286935\n",
            "Loss: 0.0\n",
            "training error 0.037128359339768116, test error 0.06001034870293806\n",
            "Loss: 0.13977353624214928\n",
            "training error 0.03710227439662231, test error 0.05982548984833143\n",
            "Loss: 0.0\n",
            "training error 0.03707208130705777, test error 0.05981758718171605\n",
            "Loss: 0.0\n",
            "training error 0.037121481870140305, test error 0.05999187915331843\n",
            "Loss: 0.2913724538452511\n",
            "training error 0.03698931913160786, test error 0.059807368500922144\n",
            "Loss: 0.0\n",
            "training error 0.03697832982654305, test error 0.05987933230131356\n",
            "Loss: 0.12032597687408053\n",
            "training error 0.03694495146804041, test error 0.05960150517539561\n",
            "Loss: 0.0\n",
            "training error 0.036906114342555585, test error 0.05966957072379443\n",
            "Loss: 0.11420105616213316\n",
            "training error 0.03686950468837848, test error 0.0594453556215739\n",
            "Loss: 0.0\n",
            "training error 0.03684703154063463, test error 0.05922655925413667\n",
            "Loss: 0.0\n",
            "training error 0.03683569072968752, test error 0.05904593232024181\n",
            "Loss: 0.0\n",
            "training error 0.036819104911231894, test error 0.058817093394334725\n",
            "Loss: 0.0\n",
            "training error 0.036775866132956365, test error 0.058929237996773194\n",
            "Loss: 0.1906666854252892\n",
            "training error 0.036737508726193524, test error 0.05899533137762508\n",
            "Loss: 0.30303772764725956\n",
            "training error 0.03672044866429687, test error 0.05888231367179286\n",
            "Loss: 0.11088660403681949\n",
            "training error 0.03671561126334906, test error 0.05893376177077124\n",
            "Loss: 0.19835794274007945\n",
            "training error 0.03664390454903596, test error 0.05884006104172421\n",
            "Loss: 0.03904927303275407\n",
            "training error 0.036716971536175556, test error 0.05877488206947652\n",
            "Loss: 0.0\n",
            "training error 0.03661059578597826, test error 0.05874905286897748\n",
            "Loss: 0.0\n",
            "training error 0.03663987638804916, test error 0.05879530718151809\n",
            "Loss: 0.07873201401862318\n",
            "training error 0.03657799620122083, test error 0.058617765183357796\n",
            "Loss: 0.0\n",
            "training error 0.036642368463771814, test error 0.05852075121573837\n",
            "Loss: 0.0\n",
            "training error 0.03651792203382862, test error 0.05853392111270529\n",
            "Loss: 0.0225046614975577\n",
            "training error 0.0365046749578292, test error 0.05867898692470413\n",
            "Loss: 0.2703924773323818\n",
            "training error 0.036488925058864745, test error 0.05858124120103887\n",
            "Loss: 0.10336501846586188\n",
            "training error 0.036484260229107134, test error 0.05857874663586863\n",
            "Loss: 0.09910231657221935\n",
            "training error 0.03646699900631533, test error 0.058483953997003214\n",
            "Loss: 0.0\n",
            "training error 0.03640878215366447, test error 0.058284106697244285\n",
            "Loss: 0.0\n",
            "training error 0.03645032786650332, test error 0.058447398772210085\n",
            "Loss: 0.2801656990541579\n",
            "training error 0.036384967000677844, test error 0.05830819438004723\n",
            "Loss: 0.041328046645827676\n",
            "training error 0.036344681023892975, test error 0.05828343093925244\n",
            "Loss: 0.0\n",
            "training error 0.036400996582115736, test error 0.05838595447354696\n",
            "Loss: 0.17590511169698342\n",
            "training error 0.036322599539781765, test error 0.05838214113573269\n",
            "Loss: 0.16936236403641836\n",
            "training error 0.03628109962301901, test error 0.058232894653454007\n",
            "Loss: 0.0\n",
            "training error 0.03627931052278112, test error 0.058230040202091805\n",
            "Loss: 0.0\n",
            "training error 0.03624302225316295, test error 0.05807701642938238\n",
            "Loss: 0.0\n",
            "training error 0.03628448479951201, test error 0.05806898131218423\n",
            "Loss: 0.0\n",
            "training error 0.036211132512892215, test error 0.05809977811793908\n",
            "Loss: 0.053034864843382046\n",
            "training error 0.036185778694244435, test error 0.05803020599433871\n",
            "Loss: 0.0\n",
            "training error 0.03615939193349903, test error 0.05799129743266042\n",
            "Loss: 0.0\n",
            "training error 0.03618197844910506, test error 0.05798782871348383\n",
            "Loss: 0.0\n",
            "training error 0.036151963540386446, test error 0.057809234985161474\n",
            "Loss: 0.0\n",
            "training error 0.03610642012855892, test error 0.057774648088406606\n",
            "Loss: 0.0\n",
            "training error 0.036093540325822175, test error 0.05772748687995846\n",
            "Loss: 0.0\n",
            "training error 0.03611030946961963, test error 0.057766508657800905\n",
            "Loss: 0.06759652974948072\n",
            "training error 0.03607793494940637, test error 0.05763761826433184\n",
            "Loss: 0.0\n",
            "training error 0.036039189641979, test error 0.05763864396441592\n",
            "Loss: 0.0017795670865128344\n",
            "training error 0.03602203486844814, test error 0.05772462333830381\n",
            "Loss: 0.15095188974143525\n",
            "training error 0.03602273521773599, test error 0.05772537540659732\n",
            "Loss: 0.1522567116896134\n",
            "training error 0.036011362556923325, test error 0.05766756795313206\n",
            "Loss: 0.05196205135136189\n",
            "training error 0.03600586105204998, test error 0.05772651375112549\n",
            "Loss: 0.1542317144090921\n",
            "training error 0.035989114593575705, test error 0.057848679459562666\n",
            "Loss: 0.36618653162050485\n",
            "training error 0.035969158157283146, test error 0.057842082491790243\n",
            "Loss: 0.35474093762983827\n",
            "training error 0.0359416621078879, test error 0.057581327775959985\n",
            "Loss: 0.0\n",
            "training error 0.035927245449294286, test error 0.05761461652285435\n",
            "Loss: 0.05781170421057524\n",
            "training error 0.03594708225489866, test error 0.05730966131355915\n",
            "Loss: 0.0\n",
            "training error 0.03588001157763012, test error 0.05731390262531848\n",
            "Loss: 0.007400692417491861\n",
            "training error 0.03588875147137374, test error 0.05725940525624752\n",
            "Loss: 0.0\n",
            "training error 0.03602415547581322, test error 0.057418448398653\n",
            "Loss: 0.27775898421180667\n",
            "training error 0.035833373726175004, test error 0.05727197058125222\n",
            "Loss: 0.021944560807884983\n",
            "training error 0.03585057872241876, test error 0.05711286802112996\n",
            "Loss: 0.0\n",
            "training error 0.035855651286405395, test error 0.057284191653099996\n",
            "Loss: 0.2999737850787776\n",
            "training error 0.03583850664461782, test error 0.05714691522195631\n",
            "Loss: 0.05961388738828788\n",
            "training error 0.0358297229368699, test error 0.057332255053931684\n",
            "Loss: 0.38412890195000937\n",
            "training error 0.03577465631751773, test error 0.057215391187287\n",
            "Loss: 0.17950974921991936\n",
            "training error 0.03583365272803359, test error 0.05712597879341591\n",
            "Loss: 0.022955898977250477\n",
            "training error 0.03578408425676662, test error 0.057326492560969854\n",
            "Loss: 0.374039244117208\n",
            "training error 0.035771753172778996, test error 0.057303409978507434\n",
            "Loss: 0.3336235142437305\n",
            "training error 0.03574518013914516, test error 0.057284079835551956\n",
            "Loss: 0.29977800162066526\n",
            "training error 0.035724567057787524, test error 0.05728624974243842\n",
            "Loss: 0.3035773325974711\n",
            "training error 0.0356985536562982, test error 0.05724924201799096\n",
            "Loss: 0.23877980844972946\n",
            "training error 0.03572189645571657, test error 0.05714212515374341\n",
            "Loss: 0.0512268664263571\n",
            "training error 0.03568259091868752, test error 0.05709216400867939\n",
            "Loss: 0.0\n",
            "training error 0.03565346383101727, test error 0.05713415640098934\n",
            "Loss: 0.0735519366608095\n",
            "training error 0.035729357524532504, test error 0.05715357452296241\n",
            "Loss: 0.10756382307330892\n",
            "training error 0.035686705968168635, test error 0.056872372115888505\n",
            "Loss: 0.0\n",
            "training error 0.03563274564838638, test error 0.0569951904000211\n",
            "Loss: 0.21595421390605551\n",
            "training error 0.035629762199032196, test error 0.05700688720590909\n",
            "Loss: 0.2365209767345\n",
            "training error 0.03563484696885352, test error 0.05694320851621636\n",
            "Loss: 0.12455327198857002\n",
            "training error 0.03561199467135317, test error 0.05699711267744492\n",
            "Loss: 0.21933419851423164\n",
            "training error 0.03558827178671752, test error 0.056978219750813346\n",
            "Loss: 0.18611433106598962\n",
            "training error 0.03560178937256917, test error 0.05705948511919714\n",
            "Loss: 0.3290050974616543\n",
            "training error 0.03558110388672273, test error 0.05702686208888222\n",
            "Loss: 0.2716432729039431\n",
            "training error 0.035573577013069006, test error 0.057074348124431594\n",
            "Loss: 0.35513906142603524\n",
            "training error 0.03556743354323401, test error 0.05702652451937729\n",
            "Loss: 0.27104971667908373\n",
            "training error 0.035540234769242826, test error 0.05701718675820882\n",
            "Loss: 0.25463091644080116\n",
            "training error 0.03553476281112055, test error 0.0569875073854403\n",
            "Loss: 0.20244499265333893\n",
            "training error 0.03558518537296531, test error 0.05689196015090607\n",
            "Loss: 0.03444209251137753\n",
            "training error 0.03552383758214866, test error 0.05683491696082828\n",
            "Loss: 0.0\n",
            "training error 0.035514772175030566, test error 0.056876230013878495\n",
            "Loss: 0.07268956349262279\n",
            "training error 0.035513158811099316, test error 0.05672612545492927\n",
            "Loss: 0.0\n",
            "training error 0.03551482080905353, test error 0.05682481022646507\n",
            "Loss: 0.17396705793737421\n",
            "training error 0.03547156251951824, test error 0.05678304886128537\n",
            "Loss: 0.10034777785294935\n",
            "training error 0.035492740646417456, test error 0.05688421798029347\n",
            "Loss: 0.27869438304897187\n",
            "training error 0.03548042658165542, test error 0.05689267379934824\n",
            "Loss: 0.2936007758035597\n",
            "training error 0.035459115917939855, test error 0.05680399803786724\n",
            "Loss: 0.13727816295128203\n",
            "training error 0.03553098375182953, test error 0.05673234710215606\n",
            "Loss: 0.010967869172961997\n",
            "training error 0.03542941888975516, test error 0.05660678677695341\n",
            "Loss: 0.0\n",
            "training error 0.035422660180229895, test error 0.056519134123105316\n",
            "Loss: 0.0\n",
            "training error 0.03547584321995784, test error 0.05627034784722491\n",
            "Loss: 0.0\n",
            "training error 0.03540044909779511, test error 0.0561726227733644\n",
            "Loss: 0.0\n",
            "training error 0.03539668288337277, test error 0.05622817982185308\n",
            "Loss: 0.098904138254019\n",
            "training error 0.03538879471435464, test error 0.056246690981784614\n",
            "Loss: 0.13185819846628366\n",
            "training error 0.035395427447625076, test error 0.05624646464757381\n",
            "Loss: 0.1314552722726514\n",
            "training error 0.03543338425119093, test error 0.05598026149094535\n",
            "Loss: 0.0\n",
            "training error 0.035386585284054146, test error 0.056130770915678994\n",
            "Loss: 0.2688615964360652\n",
            "training error 0.03535120049687438, test error 0.05621284804146445\n",
            "Loss: 0.4154795714141457\n",
            "training error 0.035412992839646024, test error 0.05610972174201304\n",
            "Loss: 0.23126053294451054\n",
            "training error 0.035389941772598, test error 0.0561607438087145\n",
            "Loss: 0.32240349180636674\n",
            "training error 0.035356118820724716, test error 0.056023586780948084\n",
            "Loss: 0.07739386856873143\n",
            "training error 0.03534950150722683, test error 0.05609167913380579\n",
            "Loss: 0.1990302293933821\n",
            "training error 0.03532580307244871, test error 0.056339670319684856\n",
            "Loss: 0.642027777590215\n",
            "training error 0.03533951735308653, test error 0.0564501062777275\n",
            "Loss: 0.8393043802736644\n",
            "training error 0.03535770753205811, test error 0.056443685672962096\n",
            "Loss: 0.8278349719600842\n",
            "training error 0.03529627065767875, test error 0.05642012518769957\n",
            "Loss: 0.7857478422557351\n",
            "training error 0.035337610739493305, test error 0.05652194345016408\n",
            "Loss: 0.9676302768009348\n",
            "training error 0.035288723449413355, test error 0.05631295121505807\n",
            "Loss: 0.5942982673750707\n",
            "training error 0.03530303312424946, test error 0.05641015383399783\n",
            "Loss: 0.7679355751526895\n",
            "training error 0.035360916070347005, test error 0.056305064420596004\n",
            "Loss: 0.5802097400048556\n",
            "training error 0.03526921690697751, test error 0.05636812287871567\n",
            "Loss: 0.6928538335482015\n",
            "training error 0.03529419324697551, test error 0.056384302203988\n",
            "Loss: 0.7217556729491204\n",
            "training error 0.03538148955287778, test error 0.05652626477940996\n",
            "Loss: 0.9753496570445996\n",
            "training error 0.035270647765930085, test error 0.056128937739176384\n",
            "Loss: 0.2655869127283106\n",
            "training error 0.03527471707656904, test error 0.056228074280515794\n",
            "Loss: 0.4426788710347962\n",
            "training error 0.03523678995221708, test error 0.05625740148787532\n",
            "Loss: 0.49506734972075694\n",
            "training error 0.0352438258831054, test error 0.05622256878549807\n",
            "Loss: 0.4328441634591451\n",
            "training error 0.03523070629591204, test error 0.05616187428311229\n",
            "Loss: 0.3244229078785521\n",
            "training error 0.03528367289980577, test error 0.05604597780290569\n",
            "Loss: 0.11739193460353903\n",
            "training error 0.03522751760384921, test error 0.05629689351851167\n",
            "Loss: 0.5656136987097327\n",
            "training error 0.03523712228064447, test error 0.05636831455084959\n",
            "Loss: 0.6931962259000946\n",
            "training error 0.03527044892715457, test error 0.05611651330450224\n",
            "Loss: 0.24339259933419122\n",
            "training error 0.03521282996242163, test error 0.056127852979040514\n",
            "Loss: 0.2636491580501721\n",
            "training error 0.035184635711728855, test error 0.056294314994331074\n",
            "Loss: 0.5610075677058513\n",
            "training error 0.03525054106025409, test error 0.05612717159990549\n",
            "Loss: 0.2624319805721198\n",
            "training error 0.03522262495488293, test error 0.05589883096367588\n",
            "Loss: 0.0\n",
            "training error 0.03518325790444079, test error 0.05596802160787645\n",
            "Loss: 0.12377833848713493\n",
            "training error 0.035166663921307434, test error 0.05596043596895288\n",
            "Loss: 0.11020803872809903\n",
            "training error 0.03514563361942196, test error 0.0560155808162193\n",
            "Loss: 0.20885920247470935\n",
            "training error 0.03518322149056326, test error 0.05599575548948693\n",
            "Loss: 0.17339275999177506\n",
            "training error 0.03515445720880894, test error 0.05598608614011389\n",
            "Loss: 0.15609481438836337\n",
            "training error 0.035142737568168984, test error 0.05604490883032471\n",
            "Loss: 0.26132544121317824\n",
            "training error 0.03521377503970163, test error 0.05607340484642245\n",
            "Loss: 0.31230328029581145\n",
            "training error 0.035161916905906226, test error 0.056061788733733846\n",
            "Loss: 0.29152267989980274\n",
            "training error 0.035125696217413724, test error 0.05602819099752989\n",
            "Loss: 0.2314181381325664\n",
            "training error 0.03515476101232979, test error 0.055996816574695574\n",
            "Loss: 0.17529098432016443\n",
            "training error 0.035128959809664836, test error 0.05594162341708528\n",
            "Loss: 0.07655339596852606\n",
            "training error 0.035167790099106415, test error 0.0561283452012054\n",
            "Loss: 0.4105886179957219\n",
            "training error 0.03513594615653486, test error 0.05594570927579059\n",
            "Loss: 0.08386277728271718\n",
            "training error 0.03512473650583311, test error 0.05605739275075002\n",
            "Loss: 0.2836585029428962\n",
            "training error 0.035140024451945875, test error 0.05594161095949421\n",
            "Loss: 0.07653111000860413\n",
            "training error 0.035110415918616245, test error 0.05601394241393819\n",
            "Loss: 0.2059281889761122\n",
            "training error 0.03509177981459057, test error 0.05592852829042613\n",
            "Loss: 0.0531269191828887\n",
            "training error 0.0351568447982694, test error 0.05594072488103189\n",
            "Loss: 0.07494596333013082\n",
            "training error 0.03509184553838436, test error 0.05576683881537086\n",
            "Loss: 0.0\n",
            "training error 0.03510776946473933, test error 0.055713336075305146\n",
            "Loss: 0.0\n",
            "training error 0.03515164120084593, test error 0.055746108018403115\n",
            "Loss: 0.05882243894652639\n",
            "training error 0.03508144325605385, test error 0.0558103551912108\n",
            "Loss: 0.17413984288163764\n",
            "training error 0.03509432020986446, test error 0.055860498151419\n",
            "Loss: 0.2641415619321963\n",
            "training error 0.03505869395792812, test error 0.05578221363678415\n",
            "Loss: 0.12362849962153089\n",
            "training error 0.03504270349268616, test error 0.055851849142551396\n",
            "Loss: 0.24861743525648183\n",
            "training error 0.03504789428592279, test error 0.055878088195238235\n",
            "Loss: 0.29571397359942075\n",
            "training error 0.03505110606719471, test error 0.055953640836618884\n",
            "Loss: 0.4313235900807122\n",
            "training error 0.03510375850595763, test error 0.05597802424609849\n",
            "Loss: 0.4750894299985431\n",
            "training error 0.03505482184130361, test error 0.05598831136825482\n",
            "Loss: 0.49355381012905486\n",
            "training error 0.0350525100298879, test error 0.05598090945551779\n",
            "Loss: 0.48026809927694813\n",
            "training error 0.035068344375476686, test error 0.05599989054409457\n",
            "Loss: 0.5143373005021656\n",
            "training error 0.0350633339495283, test error 0.05606071925404337\n",
            "Loss: 0.6235188972864236\n",
            "training error 0.03504350326115273, test error 0.05617521844395009\n",
            "Loss: 0.829033766745968\n",
            "training error 0.03512272012160083, test error 0.056016567685795944\n",
            "Loss: 0.5442711419774593\n",
            "training error 0.03505790327083716, test error 0.05624489281584336\n",
            "Loss: 0.9540924632833514\n",
            "training error 0.0350982092211271, test error 0.05619709940684862\n",
            "Loss: 0.8683079593180265\n",
            "training error 0.0350397602764013, test error 0.0559981547517437\n",
            "Loss: 0.5112217226654225\n",
            "training error 0.03503080109250717, test error 0.056223873938920046\n",
            "Loss: 0.9163656308874168\n",
            "training error 0.035016052704828556, test error 0.05610793530611981\n",
            "Loss: 0.7082671019400077\n",
            "training error 0.03504753444733726, test error 0.056229467154591414\n",
            "Loss: 0.9264049070560665\n",
            "training error 0.03503180451468173, test error 0.05616211324275878\n",
            "Loss: 0.805511209824239\n",
            "training error 0.035047192402712875, test error 0.05598771376253497\n",
            "Loss: 0.4924811661950246\n",
            "training error 0.03499003675251076, test error 0.055979933522115714\n",
            "Loss: 0.47851639408240665\n",
            "training error 0.03498074444056328, test error 0.05611101191602088\n",
            "Loss: 0.7137893164003151\n",
            "training error 0.03498400755668091, test error 0.05612130972771663\n",
            "Loss: 0.7322728832106717\n",
            "training error 0.03501415454617842, test error 0.05614373528792878\n",
            "Loss: 0.7725245748017695\n",
            "training error 0.035027440356526086, test error 0.05622655742854244\n",
            "Loss: 0.9211822328205166\n",
            "training error 0.035020346816312616, test error 0.05593442132520093\n",
            "Loss: 0.3968264431283508\n",
            "training error 0.03496858356823311, test error 0.055883160509972475\n",
            "Loss: 0.30481828343178474\n",
            "training error 0.03496553091229803, test error 0.05591696085779747\n",
            "Loss: 0.36548660847932624\n",
            "training error 0.03501226958093494, test error 0.05592894579237988\n",
            "Loss: 0.38699839618885346\n",
            "training error 0.034964364037018754, test error 0.05586953004548287\n",
            "Loss: 0.28035293016128726\n",
            "training error 0.034949486053934484, test error 0.05576229791988622\n",
            "Loss: 0.08788173179019587\n",
            "training error 0.03495217024518365, test error 0.05594380526951342\n",
            "Loss: 0.41366970718959006\n",
            "training error 0.03496628493330244, test error 0.056049686550351176\n",
            "Loss: 0.6037162710762933\n",
            "training error 0.03494607937202603, test error 0.055937978516011115\n",
            "Loss: 0.4032112534103627\n",
            "training error 0.03497582388345302, test error 0.05581784213599382\n",
            "Loss: 0.18757817795620468\n",
            "training error 0.0349874984009482, test error 0.056009179991791766\n",
            "Loss: 0.5310109523628981\n",
            "training error 0.034971064272108235, test error 0.055729002871380585\n",
            "Loss: 0.028120369697948178\n",
            "training error 0.03494324975573455, test error 0.055912212390440745\n",
            "Loss: 0.3569635730784171\n",
            "training error 0.03497097827538406, test error 0.05601133763036845\n",
            "Loss: 0.5348837029979903\n",
            "training error 0.03494312549795219, test error 0.055971238729823306\n",
            "Loss: 0.4629100906281547\n",
            "training error 0.03494976177147776, test error 0.055997647786158386\n",
            "Loss: 0.5103117689253978\n",
            "training error 0.034962254003222004, test error 0.05564122568730552\n",
            "Loss: 0.0\n",
            "training error 0.034914560774362675, test error 0.05577006682179112\n",
            "Loss: 0.23155696678873738\n",
            "training error 0.034898894172136846, test error 0.05571842856888932\n",
            "Loss: 0.1387512238095967\n",
            "training error 0.03492893796013691, test error 0.0557109399802824\n",
            "Loss: 0.12529251848019118\n",
            "training error 0.03500481233507015, test error 0.05571182821133647\n",
            "Loss: 0.12688887269975968\n",
            "training error 0.034899373844320965, test error 0.055570688467211526\n",
            "Loss: 0.0\n",
            "training error 0.03495487019296586, test error 0.055639586464788074\n",
            "Loss: 0.12398262371213775\n",
            "training error 0.0350781867239124, test error 0.055677754842158964\n",
            "Loss: 0.19266699387863184\n",
            "training error 0.03488931504133517, test error 0.05553165086431725\n",
            "Loss: 0.0\n",
            "training error 0.03491464117159021, test error 0.05547877475203018\n",
            "Loss: 0.0\n",
            "training error 0.03490921176689539, test error 0.055640815009567454\n",
            "Loss: 0.29207612868440425\n",
            "training error 0.03492376018640584, test error 0.0557401149019119\n",
            "Loss: 0.4710633049302304\n",
            "training error 0.03488112930174702, test error 0.055761175311163556\n",
            "Loss: 0.5090245060306309\n",
            "training error 0.03494746266201838, test error 0.05569129419540598\n",
            "Loss: 0.38306441395954405\n",
            "training error 0.03486509268316226, test error 0.05585087426961796\n",
            "Loss: 0.6707060839950651\n",
            "training error 0.03489675991712647, test error 0.055773825742740736\n",
            "Loss: 0.5318267968773993\n",
            "training error 0.034894568055934126, test error 0.055753436801479435\n",
            "Loss: 0.49507591088102654\n",
            "training error 0.03486027192242044, test error 0.05584572450776446\n",
            "Loss: 0.66142368387625\n",
            "training error 0.034883585902266395, test error 0.055884406847700574\n",
            "Loss: 0.7311482589213991\n",
            "training error 0.03490646132162309, test error 0.05574804613393601\n",
            "Loss: 0.4853592803903295\n",
            "training error 0.034930371353426755, test error 0.05588704862620506\n",
            "Loss: 0.7359100412720254\n",
            "training error 0.03485527255473982, test error 0.05576089345653669\n",
            "Loss: 0.5085164655626917\n",
            "training error 0.034857897934935225, test error 0.05586099757768189\n",
            "Loss: 0.688953257097169\n",
            "training error 0.03489602471971315, test error 0.056001094162382865\n",
            "Loss: 0.9414761098947455\n",
            "training error 0.03488133655513816, test error 0.05587771939718645\n",
            "Loss: 0.719094188614311\n",
            "training error 0.034876321289242576, test error 0.055829224787789046\n",
            "Loss: 0.6316830847927779\n",
            "training error 0.03491808568064459, test error 0.05600066620101817\n",
            "Loss: 0.9407047133262347\n",
            "training error 0.03487735833978013, test error 0.05590999099643296\n",
            "Loss: 0.7772634603596673\n",
            "training error 0.03488973993968907, test error 0.05583476529025662\n",
            "Loss: 0.6416697914068736\n",
            "training error 0.03485534164207227, test error 0.055711150870491694\n",
            "Loss: 0.41885589488979136\n",
            "training error 0.03488301446138514, test error 0.05571542258187247\n",
            "Loss: 0.42655561681024246\n",
            "training error 0.03484140358497984, test error 0.05564421727950544\n",
            "Loss: 0.2982086901066605\n",
            "training error 0.03488311669577701, test error 0.0556861759347363\n",
            "Loss: 0.3738387944454802\n",
            "training error 0.03483129008266605, test error 0.05559298003713715\n",
            "Loss: 0.205854014652318\n",
            "training error 0.03484340463726198, test error 0.0556562368241061\n",
            "Loss: 0.31987381276732396\n",
            "training error 0.03486194187454872, test error 0.05582611967194021\n",
            "Loss: 0.6260861409837171\n",
            "training error 0.03483189938146359, test error 0.05575909068253951\n",
            "Loss: 0.5052669814036737\n",
            "training error 0.03485026040173639, test error 0.05566709360476178\n",
            "Loss: 0.3394430637181811\n",
            "training error 0.034851138312544695, test error 0.05584949441139122\n",
            "Loss: 0.668218901765627\n",
            "training error 0.03485789707283613, test error 0.05565128895897554\n",
            "Loss: 0.3109553297029999\n",
            "training error 0.034884736394511194, test error 0.055426505252435594\n",
            "Loss: 0.0\n",
            "training error 0.03482489704912912, test error 0.05559196316758366\n",
            "Loss: 0.2985176756039376\n",
            "training error 0.03481898045943032, test error 0.05563458719576288\n",
            "Loss: 0.37541956213835714\n",
            "training error 0.03481107873141269, test error 0.05562183209308066\n",
            "Loss: 0.3524069211209735\n",
            "training error 0.03482736259314246, test error 0.055673891231803\n",
            "Loss: 0.4463315488514086\n",
            "training error 0.03481042267413814, test error 0.05578244352090037\n",
            "Loss: 0.642180608074927\n",
            "training error 0.03479714706199341, test error 0.055782384621553216\n",
            "Loss: 0.6420743424049524\n",
            "training error 0.03483867583653652, test error 0.05555560969530697\n",
            "Loss: 0.2329290693746211\n",
            "training error 0.034973207261393406, test error 0.05584688370429548\n",
            "Loss: 0.7584430047417046\n",
            "training error 0.034848003857078896, test error 0.05550865109106877\n",
            "Loss: 0.14820677987732456\n",
            "training error 0.0347983738577165, test error 0.055638241888955345\n",
            "Loss: 0.3820133265761916\n",
            "training error 0.03484151240020804, test error 0.055696996703579464\n",
            "Loss: 0.4880182322734239\n",
            "training error 0.03481234866975308, test error 0.05529780266992489\n",
            "Loss: 0.0\n",
            "training error 0.034793162483121455, test error 0.05533346877046438\n",
            "Loss: 0.06449822383065573\n",
            "training error 0.03477198058717791, test error 0.055386778435155534\n",
            "Loss: 0.160902894752879\n",
            "training error 0.03479699668645803, test error 0.05553056983784141\n",
            "Loss: 0.4209338466953483\n",
            "training error 0.03481765679219575, test error 0.055348751794114276\n",
            "Loss: 0.09213589280119283\n",
            "training error 0.034802804300467526, test error 0.05562533389609805\n",
            "Loss: 0.5923042333674688\n",
            "training error 0.03477258158648884, test error 0.055484240379928225\n",
            "Loss: 0.3371521127452093\n",
            "training error 0.0348112253040702, test error 0.05570743220802473\n",
            "Loss: 0.7407700095154457\n",
            "training error 0.034782436104220105, test error 0.055686989823647934\n",
            "Loss: 0.7038022035814162\n",
            "training error 0.03498005360317761, test error 0.055451788987169015\n",
            "Loss: 0.27846733470273666\n",
            "training error 0.03485064834928099, test error 0.05579317271345806\n",
            "Loss: 0.8958222924155823\n",
            "training error 0.034808679646915766, test error 0.055673008594184135\n",
            "Loss: 0.6785186863551562\n",
            "training error 0.034788966084312714, test error 0.05573743607814057\n",
            "Loss: 0.7950287117914456\n",
            "training error 0.034788626029417596, test error 0.055638736553019305\n",
            "Loss: 0.616541465724163\n",
            "training error 0.0348106088431595, test error 0.05577409349701311\n",
            "Loss: 0.8613196259012579\n",
            "training error 0.03476865044319259, test error 0.05560401774491371\n",
            "Loss: 0.5537563161716053\n",
            "training error 0.03481685329061216, test error 0.05536280359050108\n",
            "Loss: 0.1175470225538211\n",
            "training error 0.034789741307500274, test error 0.05555190126811105\n",
            "Loss: 0.4595093944381068\n",
            "training error 0.03478334343539662, test error 0.055336278897168446\n",
            "Loss: 0.06958002919794648\n",
            "training error 0.034883958541030026, test error 0.05516716281247157\n",
            "Loss: 0.0\n",
            "training error 0.034822441321766776, test error 0.05551713888710579\n",
            "Loss: 0.6343920129151748\n",
            "training error 0.03486728013090059, test error 0.05545897761558038\n",
            "Loss: 0.5289646743313003\n",
            "training error 0.034791940368322956, test error 0.05557096906087457\n",
            "Loss: 0.7319684896170076\n",
            "training error 0.03481602759046158, test error 0.05555705637909887\n",
            "Loss: 0.7067493536919045\n",
            "training error 0.03481516255444767, test error 0.05551915079539125\n",
            "Loss: 0.6380389437756318\n",
            "training error 0.03476220316927395, test error 0.05569920586801796\n",
            "Loss: 0.9644198258934367\n",
            "training error 0.034765454412866444, test error 0.05567941804017605\n",
            "Loss: 0.9285509741470355\n",
            "training error 0.03482193706827738, test error 0.05549784794982604\n",
            "Loss: 0.599423861035886\n",
            "training error 0.034760612891791685, test error 0.05567090471310567\n",
            "Loss: 0.9131191001184202\n",
            "training error 0.034794902697196164, test error 0.055912752861467775\n",
            "Loss: 1.3515105925071325\n",
            "training error 0.03476114982272918, test error 0.055866138897001226\n",
            "Loss: 1.2670147401012244\n",
            "training error 0.03484292913082025, test error 0.05594322845657419\n",
            "Loss: 1.4067528662669915\n",
            "training error 0.03474118966768647, test error 0.055705330504606895\n",
            "Loss: 0.9755217863291366\n",
            "training error 0.034827230075619914, test error 0.055695181686887495\n",
            "Loss: 0.9571253033454186\n",
            "training error 0.03477964031628594, test error 0.05561858978378331\n",
            "Loss: 0.8182892653846752\n",
            "training error 0.03473566274682589, test error 0.05571378609752759\n",
            "Loss: 0.9908490072511933\n",
            "training error 0.034754536382501484, test error 0.05572388814190213\n",
            "Loss: 1.0091607054780516\n",
            "training error 0.034759404034317355, test error 0.055816796856011694\n",
            "Loss: 1.1775737783514595\n",
            "training error 0.034741272835239714, test error 0.05573870245433831\n",
            "Loss: 1.0360142025239938\n",
            "training error 0.03472903172048717, test error 0.05580368574435646\n",
            "Loss: 1.1538076265560582\n",
            "training error 0.03474712671253696, test error 0.05571040340350098\n",
            "Loss: 0.9847172907478274\n",
            "training error 0.03471676813427646, test error 0.05559660793205097\n",
            "Loss: 0.7784433668253099\n",
            "training error 0.03475264676501012, test error 0.055641579167706484\n",
            "Loss: 0.8599614898587182\n",
            "training error 0.03474669103559309, test error 0.0555817533564994\n",
            "Loss: 0.751516885936554\n",
            "training error 0.03475710782286684, test error 0.05561591026391546\n",
            "Loss: 0.8134321733552108\n",
            "training error 0.03476673761026951, test error 0.05553057236646892\n",
            "Loss: 0.6587425117957846\n",
            "training error 0.03472862866960711, test error 0.05577810577351415\n",
            "Loss: 1.1074395163647388\n",
            "training error 0.034711606346861254, test error 0.055710853423477175\n",
            "Loss: 0.9855330295918341\n",
            "training error 0.03473290116746731, test error 0.055813201639620384\n",
            "Loss: 1.171056828397865\n",
            "training error 0.03472171078148752, test error 0.05569100115284596\n",
            "Loss: 0.9495473641721652\n",
            "training error 0.03476202917586727, test error 0.05572221235919928\n",
            "Loss: 1.0061230602242066\n",
            "training error 0.03476471383650456, test error 0.055643051274765924\n",
            "Loss: 0.8626299378708957\n",
            "training error 0.0347763770095536, test error 0.05559850503772674\n",
            "Loss: 0.7818821981500523\n",
            "training error 0.03470830028361911, test error 0.05565180007736001\n",
            "Loss: 0.8784886519102919\n",
            "training error 0.034699633731560424, test error 0.055660517385360965\n",
            "Loss: 0.8942902765662453\n",
            "training error 0.034712463992153636, test error 0.055490227399117076\n",
            "Loss: 0.5856102981835365\n",
            "training error 0.0347321189730595, test error 0.05550414853774549\n",
            "Loss: 0.6108447636131498\n",
            "training error 0.03472229434023384, test error 0.05534467730160243\n",
            "Loss: 0.3217756362317914\n",
            "training error 0.034728449845466164, test error 0.05527092790155161\n",
            "Loss: 0.1880921254420187\n",
            "training error 0.03473438598085083, test error 0.055425168225634605\n",
            "Loss: 0.4676793222810227\n",
            "training error 0.03472464602296462, test error 0.055567212949743504\n",
            "Loss: 0.7251598901901524\n",
            "training error 0.034780671449138725, test error 0.055674002298137605\n",
            "Loss: 0.918734007382116\n",
            "training error 0.03468217938572475, test error 0.05560752140257875\n",
            "Loss: 0.7982259149416082\n",
            "training error 0.03477328939213417, test error 0.055735210988136163\n",
            "Loss: 1.0296853176871679\n",
            "training error 0.03471514170859193, test error 0.05556091466115396\n",
            "Loss: 0.7137431555450213\n",
            "training error 0.034719925430513894, test error 0.055859553833574556\n",
            "Loss: 1.255078176589608\n",
            "training error 0.03475256030956936, test error 0.055613627076950764\n",
            "Loss: 0.8092935030877824\n",
            "training error 0.03470587544652871, test error 0.0555680145728901\n",
            "Loss: 0.7266129704388558\n",
            "training error 0.03470909189196929, test error 0.05560453867547155\n",
            "Loss: 0.7928192074817098\n",
            "training error 0.034725113083690896, test error 0.055555905039842333\n",
            "Loss: 0.704662352661134\n",
            "training error 0.03468498454837968, test error 0.055505965343169164\n",
            "Loss: 0.6141380368776206\n",
            "training error 0.034709150077350176, test error 0.05548739097977944\n",
            "Loss: 0.5804687987968737\n",
            "training error 0.03470814562841587, test error 0.05548468540267285\n",
            "Loss: 0.5755644735268239\n",
            "training error 0.034703546287857955, test error 0.05555166440242879\n",
            "Loss: 0.6969754657571325\n",
            "training error 0.03468488352167675, test error 0.05543568719172665\n",
            "Loss: 0.4867467630479183\n",
            "training error 0.03467677509652547, test error 0.05549665434156499\n",
            "Loss: 0.5972602401422389\n",
            "training error 0.034683067037619975, test error 0.05543984346724828\n",
            "Loss: 0.49428072946877677\n",
            "training error 0.034696713114573625, test error 0.055701847448592044\n",
            "Loss: 0.9692081464077074\n",
            "training error 0.03470680043619693, test error 0.05561032026169187\n",
            "Loss: 0.8032993299414537\n",
            "training error 0.03468845351741865, test error 0.05554434991369881\n",
            "Loss: 0.6837166930433014\n",
            "training error 0.034693347734723504, test error 0.05569637628555428\n",
            "Loss: 0.9592907195203448\n",
            "training error 0.03470339570383173, test error 0.05566338460907563\n",
            "Loss: 0.8994876142006003\n",
            "training error 0.03469691088521331, test error 0.05577989095674995\n",
            "Loss: 1.1106754689582443\n",
            "training error 0.03469228419988701, test error 0.055805578421302994\n",
            "Loss: 1.1572384300450267\n",
            "training error 0.03468732725603036, test error 0.05579868173552991\n",
            "Loss: 1.1447369972696464\n",
            "training error 0.03469429461000278, test error 0.05554251002546631\n",
            "Loss: 0.6803815782056022\n",
            "training error 0.034701114240696905, test error 0.055611745312375234\n",
            "Loss: 0.8058824801538744\n",
            "training error 0.034687683576733436, test error 0.05550403625663751\n",
            "Loss: 0.610641234734266\n",
            "training error 0.034701575800968214, test error 0.05564612409035317\n",
            "Loss: 0.8681999462428758\n",
            "training error 0.03466813289843697, test error 0.05556390849506827\n",
            "Loss: 0.7191699960089615\n",
            "training error 0.034682506118072116, test error 0.05549552742843312\n",
            "Loss: 0.5952175156763939\n",
            "training error 0.034700869120749406, test error 0.05559253856422673\n",
            "Loss: 0.7710669355992295\n",
            "training error 0.034682017247273926, test error 0.055584258820400204\n",
            "Loss: 0.7560584714977336\n",
            "training error 0.03467130685651073, test error 0.05565493025673859\n",
            "Loss: 0.8841626420504456\n",
            "training error 0.034738080430337444, test error 0.055847235457674574\n",
            "Loss: 1.2327489951128312\n",
            "training error 0.03470176500685031, test error 0.05566228160785651\n",
            "Loss: 0.8974882342019042\n",
            "training error 0.034740835495512275, test error 0.055731312210440156\n",
            "Loss: 1.0226181104986187\n",
            "training error 0.03471565155857635, test error 0.055694927588284\n",
            "Loss: 0.956664705789656\n",
            "training error 0.03467064120358311, test error 0.055634029102025286\n",
            "Loss: 0.8462756932792193\n",
            "training error 0.034679133592354514, test error 0.055607688658178465\n",
            "Loss: 0.7985290945709211\n",
            "training error 0.03469686107412995, test error 0.05552969817832535\n",
            "Loss: 0.6571578949712276\n",
            "training error 0.03471009971327207, test error 0.05527315509698612\n",
            "Loss: 0.19212930140135587\n",
            "training error 0.034769607202691774, test error 0.05537949998468031\n",
            "Loss: 0.3848977568966738\n",
            "training error 0.0347064156252996, test error 0.05540771653718588\n",
            "Loss: 0.43604512621397973\n",
            "training error 0.03467671693483549, test error 0.05512103854473272\n",
            "Loss: 0.0\n",
            "training error 0.034715965024940384, test error 0.05494715967134744\n",
            "Loss: 0.0\n",
            "training error 0.03470889896504715, test error 0.05491995404983076\n",
            "Loss: 0.0\n",
            "training error 0.03467884940630826, test error 0.05482564027696229\n",
            "Loss: 0.0\n",
            "training error 0.03470883256288074, test error 0.05496272192485234\n",
            "Loss: 0.250032005458678\n",
            "training error 0.034653840317033516, test error 0.05512148616399681\n",
            "Loss: 0.5396122791088365\n",
            "training error 0.03469421571823813, test error 0.055250893097508524\n",
            "Loss: 0.7756458810111333\n",
            "training error 0.03469371344785417, test error 0.05512203378892157\n",
            "Loss: 0.5406111273156045\n",
            "training error 0.034668879534438174, test error 0.05529057736117084\n",
            "Loss: 0.848028553537783\n",
            "training error 0.03466402307050155, test error 0.05531086578666154\n",
            "Loss: 0.8850339134172325\n",
            "training error 0.03468163619654504, test error 0.05515069572897484\n",
            "Loss: 0.5928894772053273\n",
            "training error 0.03466117758982311, test error 0.05524151212763403\n",
            "Loss: 0.7585353286726493\n",
            "training error 0.034666129357007525, test error 0.05528821532535467\n",
            "Loss: 0.8437202849900016\n",
            "training error 0.034714705402086814, test error 0.05547890009315231\n",
            "Loss: 1.1915224571750649\n",
            "training error 0.03478974815585225, test error 0.055334669854025305\n",
            "Loss: 0.9284516778856666\n",
            "training error 0.03476279910580565, test error 0.05539289780473589\n",
            "Loss: 1.0346573699969497\n",
            "training error 0.03470296623163878, test error 0.05540017122718133\n",
            "Loss: 1.0479238314713424\n",
            "training error 0.03469442224546657, test error 0.05537848691319675\n",
            "Loss: 1.0083724210819156\n",
            "training error 0.03471392611332353, test error 0.055259078766519316\n",
            "Loss: 0.790576247477337\n",
            "training error 0.03468277205849085, test error 0.05555587319761088\n",
            "Loss: 1.3319186368999647\n",
            "training error 0.03469221239067096, test error 0.05543264787071197\n",
            "Loss: 1.1071600635820467\n",
            "training error 0.03467408952195809, test error 0.055632154039905575\n",
            "Loss: 1.4710521552854239\n",
            "training error 0.034684974924139765, test error 0.0556191843522401\n",
            "Loss: 1.4473959105065992\n",
            "training error 0.034731119820509034, test error 0.055446594217361665\n",
            "Loss: 1.1325976992927256\n",
            "training error 0.03469856075521917, test error 0.05547289756321094\n",
            "Loss: 1.1805740580117519\n",
            "training error 0.03466469351149228, test error 0.055569832977045895\n",
            "Loss: 1.3573807735289156\n",
            "training error 0.03466414390853389, test error 0.05541691078468152\n",
            "Loss: 1.0784561835161632\n",
            "training error 0.034677164889385305, test error 0.05546039592694696\n",
            "Loss: 1.1577715221894103\n",
            "training error 0.03469099377639326, test error 0.05562636934279505\n",
            "Loss: 1.4605010753868441\n",
            "training error 0.034693677044674444, test error 0.05570729615344765\n",
            "Loss: 1.6081086732986583\n",
            "training error 0.03471377326823099, test error 0.05551892645671777\n",
            "Loss: 1.2645291076460063\n",
            "training error 0.03471061029459945, test error 0.05553433573045016\n",
            "Loss: 1.2926350698464484\n",
            "training error 0.03467344384572603, test error 0.05561058263510126\n",
            "Loss: 1.4317066871881279\n",
            "training error 0.034663471299640924, test error 0.05567564091250046\n",
            "Loss: 1.550370650017463\n",
            "training error 0.03477152458479269, test error 0.05569667529886968\n",
            "Loss: 1.5887366157644411\n",
            "training error 0.034666511066247116, test error 0.0556332924423334\n",
            "Loss: 1.4731285604529187\n",
            "training error 0.03468828946867705, test error 0.05577373810766469\n",
            "Loss: 1.729296412979231\n",
            "training error 0.03466197427015208, test error 0.05561796834054226\n",
            "Loss: 1.445177948816223\n",
            "training error 0.0346711368861975, test error 0.0556530160960296\n",
            "Loss: 1.5091037968506305\n",
            "training error 0.03468581751762333, test error 0.05541924338976877\n",
            "Loss: 1.082710771470774\n",
            "training error 0.03474710436825228, test error 0.05543133137964501\n",
            "Loss: 1.104758831128927\n",
            "training error 0.03468331479758996, test error 0.05547640285376045\n",
            "Loss: 1.186967582158105\n",
            "training error 0.034691758431482284, test error 0.05542494102471917\n",
            "Loss: 1.093103053114186\n",
            "training error 0.0347636930199851, test error 0.05562760164664841\n",
            "Loss: 1.4627487533841022\n",
            "training error 0.03467228474580859, test error 0.055399955586759454\n",
            "Loss: 1.0475305110818622\n",
            "training error 0.0347114162023666, test error 0.055223659388931735\n",
            "Loss: 0.7259725740707657\n",
            "training error 0.0346857046367837, test error 0.0553271250764323\n",
            "Loss: 0.9146902743619068\n",
            "training error 0.03469094612726233, test error 0.05534880286324399\n",
            "Loss: 0.9542297794222776\n",
            "training error 0.034668459396928746, test error 0.05544769685721322\n",
            "Loss: 1.1346088748047434\n",
            "training error 0.03468284105694211, test error 0.055412560472713326\n",
            "Loss: 1.0705213706326155\n",
            "training error 0.034665733307788715, test error 0.055536784441840945\n",
            "Loss: 1.2971014315312557\n",
            "training error 0.034666206781509695, test error 0.055579815251202834\n",
            "Loss: 1.3755880832958534\n",
            "training error 0.03465529386934499, test error 0.05557162515670142\n",
            "Loss: 1.3606496448935967\n",
            "training error 0.03464873192012249, test error 0.05562659410933545\n",
            "Loss: 1.4609110414889548\n",
            "training error 0.03466034194057425, test error 0.05553634717098026\n",
            "Loss: 1.296303865176407\n",
            "training error 0.03468210142871624, test error 0.05561630869876552\n",
            "Loss: 1.4421508217852486\n",
            "training error 0.03470578868701681, test error 0.055616646319836004\n",
            "Loss: 1.4427666304995146\n",
            "training error 0.03467697986253879, test error 0.05553053068154435\n",
            "Loss: 1.2856947972174515\n",
            "training error 0.03476989313870107, test error 0.055489444535615565\n",
            "Loss: 1.2107551417547269\n",
            "training error 0.03468120924406059, test error 0.05575430606569843\n",
            "Loss: 1.693853065910056\n",
            "training error 0.03470767618728197, test error 0.05562371417114041\n",
            "Loss: 1.4556581375912758\n",
            "training error 0.034669841006690634, test error 0.055841289319282734\n",
            "Loss: 1.8525073983444607\n",
            "training error 0.03473709838557512, test error 0.05561284948888558\n",
            "Loss: 1.4358413471261766\n",
            "training error 0.034661149282505896, test error 0.05557460526886168\n",
            "Loss: 1.3660852625082986\n",
            "training error 0.03471413587524838, test error 0.05552818464063449\n",
            "Loss: 1.281415702804689\n",
            "training error 0.034682517952047125, test error 0.05547372878662236\n",
            "Loss: 1.1820901796789407\n",
            "training error 0.03470295147698219, test error 0.055514195176689055\n",
            "Loss: 1.2558994226941955\n",
            "training error 0.03467263889353955, test error 0.055663793866335416\n",
            "Loss: 1.5287620630402676\n",
            "training error 0.03476284742608873, test error 0.05541689350225537\n",
            "Loss: 1.0784246609911952\n",
            "training error 0.03466983004948178, test error 0.05562795355293681\n",
            "Loss: 1.4633906178231237\n",
            "training error 0.03472831593118417, test error 0.05553710170327259\n",
            "Loss: 1.2976801049950693\n",
            "training error 0.034660290242351594, test error 0.05564828311551893\n",
            "Loss: 1.5004710102807683\n",
            "training error 0.03468739973714649, test error 0.05579525104363319\n",
            "Loss: 1.7685352360186313\n",
            "training error 0.034678606577685074, test error 0.05571070557954081\n",
            "Loss: 1.6143273441175499\n",
            "training error 0.034683447732909246, test error 0.05591053495794173\n",
            "Loss: 1.97880895781406\n",
            "training error 0.03476031088973913, test error 0.05566272535551319\n",
            "Loss: 1.5268131376527494\n",
            "training error 0.03466756746836832, test error 0.055933090095688726\n",
            "Loss: 2.0199487194895394\n",
            "training error 0.034708029432179974, test error 0.055954601521168594\n",
            "Loss: 2.0591847874518843\n",
            "training error 0.03465381720641694, test error 0.05581369810848962\n",
            "Loss: 1.8021820201934213\n",
            "training error 0.03466291594850886, test error 0.05576296010804845\n",
            "Loss: 1.7096377285356068\n",
            "training error 0.034713004387148326, test error 0.05580510300897208\n",
            "Loss: 1.786504867178662\n",
            "training error 0.03467797755952293, test error 0.055776655906988916\n",
            "Loss: 1.7346183742176002\n",
            "training error 0.03470214680235308, test error 0.05579299410786246\n",
            "Loss: 1.7644186661813777\n",
            "training error 0.034653331480626964, test error 0.055608738189641654\n",
            "Loss: 1.4283424848727577\n",
            "training error 0.03466106320552317, test error 0.055682396208800326\n",
            "Loss: 1.562692067999527\n",
            "training error 0.03468584913556701, test error 0.05574192186912373\n",
            "Loss: 1.671264735865674\n",
            "training error 0.034679953098419626, test error 0.05581610993311303\n",
            "Loss: 1.8065811017385025\n",
            "training error 0.034674122845510885, test error 0.0557335485327357\n",
            "Loss: 1.6559920708393738\n",
            "training error 0.03472027439893694, test error 0.055589269236233835\n",
            "Loss: 1.3928318126590566\n",
            "training error 0.034667801095560634, test error 0.05574333442986119\n",
            "Loss: 1.6738411959495325\n",
            "training error 0.03465105917297899, test error 0.05563800417576689\n",
            "Loss: 1.48172259311663\n",
            "training error 0.03471092911387324, test error 0.05566306716667655\n",
            "Loss: 1.5274365889460517\n",
            "training error 0.03467743749484051, test error 0.05548620196724126\n",
            "Loss: 1.2048408134260002\n",
            "training error 0.034692624124263924, test error 0.055417097571299485\n",
            "Loss: 1.078796875602972\n",
            "training error 0.03464193026165547, test error 0.05555285460634641\n",
            "Loss: 1.3264128347803217\n",
            "training error 0.03465504320772695, test error 0.055603243070021006\n",
            "Loss: 1.4183195839218898\n",
            "training error 0.034642676549898624, test error 0.05560879492974307\n",
            "Loss: 1.4284459767811564\n",
            "training error 0.03465158926448261, test error 0.05557254519900041\n",
            "Loss: 1.3623277690237412\n",
            "training error 0.03469072926765222, test error 0.0555314551270133\n",
            "Loss: 1.2873809525715707\n",
            "training error 0.034662851019313344, test error 0.05558694857200242\n",
            "Loss: 1.3885990043969176\n",
            "training error 0.03469174123010555, test error 0.055582486102016194\n",
            "Loss: 1.3804596193141672\n",
            "training error 0.03466200076797174, test error 0.05563513185061812\n",
            "Loss: 1.4764835751420957\n",
            "training error 0.03466485569165955, test error 0.05569989898648127\n",
            "Loss: 1.5946165062596496\n",
            "training error 0.03467575796582271, test error 0.05578470431166781\n",
            "Loss: 1.7492983754692437\n",
            "training error 0.03464445148732965, test error 0.05565937658862682\n",
            "Loss: 1.5207051070498245\n",
            "training error 0.03467580422142717, test error 0.055671810952982397\n",
            "Loss: 1.5433849413258427\n",
            "training error 0.03469391473274203, test error 0.05568729426301578\n",
            "Loss: 1.571625943081889\n",
            "training error 0.03472414205850431, test error 0.05559561841125792\n",
            "Loss: 1.4044124800110636\n",
            "training error 0.034659191166608934, test error 0.05553213839919127\n",
            "Loss: 1.2886272164993828\n",
            "training error 0.03468424895193293, test error 0.05573260315120944\n",
            "Loss: 1.6542677288681862\n",
            "training error 0.03467645403985399, test error 0.055707187313207174\n",
            "Loss: 1.607910152606662\n",
            "training error 0.03464702657430636, test error 0.05564286507802128\n",
            "Loss: 1.4905887043555222\n",
            "training error 0.03466063841265622, test error 0.055585812670890314\n",
            "Loss: 1.386527161539508\n",
            "training error 0.03470500216671577, test error 0.055715517780323916\n",
            "Loss: 1.6231046256208526\n",
            "training error 0.03462120369188822, test error 0.055577005748305756\n",
            "Loss: 1.370463650853515\n",
            "training error 0.0346207133220005, test error 0.05554958349494281\n",
            "Loss: 1.320446444990675\n",
            "training error 0.03463814123446332, test error 0.05558940647506634\n",
            "Loss: 1.393082131363621\n",
            "training error 0.034623756577278016, test error 0.055598556487746364\n",
            "Loss: 1.4097714260691507\n",
            "training error 0.034779908659135474, test error 0.05547737099863324\n",
            "Loss: 1.188733443656309\n",
            "training error 0.03475165036499962, test error 0.055818096617239216\n",
            "Loss: 1.8102047422763112\n",
            "training error 0.03468292586461119, test error 0.055567466826756\n",
            "Loss: 1.3530650003287459\n",
            "training error 0.034628508894659585, test error 0.055650831352949945\n",
            "Loss: 1.5051189038906676\n",
            "training error 0.03467826854013151, test error 0.05573427354548591\n",
            "Loss: 1.657314468072757\n",
            "training error 0.034626548377712105, test error 0.05564378613604908\n",
            "Loss: 1.4922686811385644\n",
            "training error 0.03463920635611678, test error 0.05560959913867978\n",
            "Loss: 1.429912825016122\n",
            "training error 0.03462422647668444, test error 0.055715875121899405\n",
            "Loss: 1.6237564038284358\n",
            "training error 0.034636284304546745, test error 0.055758174602775545\n",
            "Loss: 1.700909138684703\n",
            "training error 0.03467900610308789, test error 0.055752120016750055\n",
            "Loss: 1.6898657910924042\n",
            "training error 0.03465227639061112, test error 0.05564799084848939\n",
            "Loss: 1.4999379257092826\n",
            "training error 0.034659652655244, test error 0.055640190442801114\n",
            "Loss: 1.4857102657150456\n",
            "training error 0.03467087525361628, test error 0.05571252047163038\n",
            "Loss: 1.6176376421467076\n",
            "training error 0.03465014651681591, test error 0.05563104054880502\n",
            "Loss: 1.4690211874847314\n",
            "training error 0.034633055636040286, test error 0.055436522531127616\n",
            "Loss: 1.1142273051063967\n",
            "training error 0.034625237659174454, test error 0.0554176939814172\n",
            "Loss: 1.0798847062506445\n",
            "training error 0.03472925394732995, test error 0.05536708912188495\n",
            "Loss: 0.9875832588318589\n",
            "training error 0.03461873664402098, test error 0.05550308521243916\n",
            "Loss: 1.2356352466740494\n",
            "training error 0.0346230503185592, test error 0.05560014832408151\n",
            "Loss: 1.4126748784084375\n",
            "training error 0.03472689028217685, test error 0.05548999755962833\n",
            "Loss: 1.2117638377042494\n",
            "training error 0.03464344005173655, test error 0.055726935765007715\n",
            "Loss: 1.643930619856615\n",
            "training error 0.03461926294711349, test error 0.05566398911236543\n",
            "Loss: 1.529118184791023\n",
            "training error 0.03464392375904888, test error 0.05552085321724703\n",
            "Loss: 1.2680434496938675\n",
            "training error 0.0346492129754672, test error 0.05568962650766557\n",
            "Loss: 1.5758798736114743\n",
            "training error 0.03464178506147645, test error 0.055437778104136824\n",
            "Loss: 1.1165174252087295\n",
            "training error 0.0346615615589241, test error 0.055583646200054904\n",
            "Loss: 1.382575596497193\n",
            "training error 0.03464107971324606, test error 0.05557703400346829\n",
            "Loss: 1.3705151872558163\n",
            "training error 0.0346320581364134, test error 0.05563822447721841\n",
            "Loss: 1.4821244150568846\n",
            "training error 0.0346267817170435, test error 0.0555171866242984\n",
            "Loss: 1.2613557157611588\n",
            "training error 0.03467744114137854, test error 0.05556010502407545\n",
            "Loss: 1.339637336477728\n",
            "training error 0.034639751690096504, test error 0.05559523575071499\n",
            "Loss: 1.4037145209156554\n",
            "training error 0.034644584393576844, test error 0.055477993405304106\n",
            "Loss: 1.1898686910838174\n",
            "training error 0.034693681196267936, test error 0.05553734540577159\n",
            "Loss: 1.2981246096059884\n",
            "training error 0.03475771288730631, test error 0.05569979682810079\n",
            "Loss: 1.5944301730404398\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.05569979682810079\n",
            "training error 0.1250622256261707, test error 0.250127582552388\n",
            "training error 0.12508988504872862, test error 0.2501475871106597\n",
            "training error 0.12503327268991984, test error 0.2501722508165145\n",
            "training error 0.12513265644180147, test error 0.2502716559426073\n",
            "training error 0.12519803066317423, test error 0.2505311020585626\n",
            "training error 0.12496761231401383, test error 0.2504993421762813\n",
            "training error 0.1249676688083925, test error 0.25049953308808315\n",
            "training error 0.1250554378323319, test error 0.2503851601620691\n",
            "training error 0.12503242349759291, test error 0.2503908357620449\n",
            "training error 0.12500660491303064, test error 0.25056127573523795\n",
            "training error 0.12499633882933944, test error 0.2505443260296248\n",
            "training error 0.12497482407864356, test error 0.2505662436413207\n",
            "training error 0.12497231426007455, test error 0.2506329210936583\n",
            "training error 0.12503761175780856, test error 0.25057296003824053\n",
            "training error 0.12503189326508773, test error 0.25056647138677324\n",
            "training error 0.12498787763151736, test error 0.25055903423438775\n",
            "training error 0.12504663617886777, test error 0.25041933163589675\n",
            "training error 0.12502038084372938, test error 0.25048144814945283\n",
            "training error 0.12509651421840778, test error 0.2503469530384841\n",
            "training error 0.1249720806537572, test error 0.2504426577673503\n",
            "training error 0.12501233825350638, test error 0.2504637939669675\n",
            "training error 0.12497975653317542, test error 0.25055492937383905\n",
            "training error 0.12500948499863418, test error 0.2504764312652064\n",
            "training error 0.12499009465913077, test error 0.25055831292879477\n",
            "training error 0.12504901476470223, test error 0.2505853967051249\n",
            "training error 0.1251609506546138, test error 0.25049895684124496\n",
            "training error 0.12512863860700232, test error 0.25043801909041635\n",
            "training error 0.12495435844674344, test error 0.2505562854572265\n",
            "training error 0.12498782586158032, test error 0.25047292794069786\n",
            "training error 0.12503380377706683, test error 0.2505671941881514\n",
            "training error 0.12495999428055937, test error 0.2505298852822249\n",
            "training error 0.12498498768858755, test error 0.25054940245792556\n",
            "training error 0.12516403301830842, test error 0.25070860387318283\n",
            "training error 0.12498518619582173, test error 0.25068783311828585\n",
            "training error 0.12506888532481703, test error 0.25071373699878274\n",
            "training error 0.1250548751059408, test error 0.25058576060780346\n",
            "training error 0.12511626961575428, test error 0.25078128850237313\n",
            "training error 0.12500653369035009, test error 0.2506086638868105\n",
            "training error 0.12500413859173487, test error 0.25058826592484523\n",
            "training error 0.12527754844647473, test error 0.2507306078412689\n",
            "training error 0.12495356923818011, test error 0.2505469173925114\n",
            "training error 0.1249809621111563, test error 0.2506082011161083\n",
            "training error 0.12507536722789286, test error 0.250585467271574\n",
            "training error 0.12501697822889968, test error 0.25046100849928626\n",
            "training error 0.1250042219675973, test error 0.250459887670986\n",
            "training error 0.12499662836733738, test error 0.25061018233967314\n",
            "training error 0.12502240582612348, test error 0.25054935542879564\n",
            "training error 0.12509317465230968, test error 0.2505537052380764\n",
            "training error 0.1250194662194648, test error 0.2505945917295559\n",
            "training error 0.12507354562545087, test error 0.25052573794975586\n",
            "Loss: 0.1591809241128006\n",
            "training error 0.12495680960016885, test error 0.2506135627527483\n",
            "Loss: 0.19429292659418618\n",
            "training error 0.12512705615408692, test error 0.2505474354460144\n",
            "Loss: 0.16785549572024827\n",
            "training error 0.1249819378099397, test error 0.25053920707894844\n",
            "Loss: 0.1645658277108497\n",
            "training error 0.12494823777096177, test error 0.25051336743481617\n",
            "Loss: 0.15423524206787942\n",
            "training error 0.12495678379039331, test error 0.2504765084258652\n",
            "Loss: 0.13949915875597352\n",
            "training error 0.12499683078066413, test error 0.25052396894649026\n",
            "Loss: 0.1584736837326739\n",
            "training error 0.12524630083264116, test error 0.25055197660883644\n",
            "Loss: 0.16967103432488084\n",
            "training error 0.12496032908516591, test error 0.25050390412036794\n",
            "Loss: 0.1504518470693439\n",
            "training error 0.12495930469028461, test error 0.25046580475532226\n",
            "Loss: 0.13521987438687155\n",
            "training error 0.12499248019754135, test error 0.25056362821680395\n",
            "Loss: 0.17432930025804705\n",
            "training error 0.12502010133215571, test error 0.2504637205081261\n",
            "Loss: 0.13438660075311137\n",
            "training error 0.1249748211480798, test error 0.25059809466084615\n",
            "Loss: 0.18810884575659603\n",
            "training error 0.12500492670119306, test error 0.25047255872659746\n",
            "Loss: 0.13792008489796892\n",
            "training error 0.12503934412184226, test error 0.2504835009605526\n",
            "Loss: 0.1422947459583268\n",
            "training error 0.12502862463429662, test error 0.25054751041812384\n",
            "Loss: 0.1678854692676257\n",
            "training error 0.12499406641719742, test error 0.2504259861436971\n",
            "Loss: 0.11930055384701266\n",
            "training error 0.12502498859636132, test error 0.25046460320888175\n",
            "Loss: 0.1347395009597463\n",
            "training error 0.12516750663103984, test error 0.2505410012796755\n",
            "Loss: 0.16528314193453575\n",
            "training error 0.1249605659350285, test error 0.2505574219318616\n",
            "Loss: 0.1718480525367827\n",
            "training error 0.12518329948364354, test error 0.2504803420734896\n",
            "Loss: 0.1410318356344309\n",
            "training error 0.12494480561858665, test error 0.250709160748164\n",
            "Loss: 0.23251262009627016\n",
            "training error 0.12494577920723604, test error 0.2507241411648013\n",
            "Loss: 0.2385017303273118\n",
            "training error 0.12500254525363913, test error 0.2506339706274908\n",
            "Loss: 0.20245191271408558\n",
            "training error 0.1250010215420178, test error 0.2507412303474499\n",
            "Loss: 0.24533391671563987\n",
            "training error 0.12501084815141528, test error 0.2507513742332221\n",
            "Loss: 0.249389401388167\n",
            "training error 0.12498672192692079, test error 0.2507659131689\n",
            "Loss: 0.2552020093099294\n",
            "training error 0.12495025276472713, test error 0.2506927523743105\n",
            "Loss: 0.22595261832194513\n",
            "training error 0.12495719816416152, test error 0.25070549997830366\n",
            "Loss: 0.23104905905155793\n",
            "training error 0.12497610487571502, test error 0.2506690850872813\n",
            "Loss: 0.2164905322986188\n",
            "training error 0.12493265802317209, test error 0.2506423949034937\n",
            "Loss: 0.20581990432739783\n",
            "training error 0.12494674887429764, test error 0.25068068379500474\n",
            "Loss: 0.22112764892729952\n",
            "training error 0.12493430796200723, test error 0.25072703891147285\n",
            "Loss: 0.2396602377745749\n",
            "training error 0.1249404191376242, test error 0.25073515539831653\n",
            "Loss: 0.24290517652179044\n",
            "training error 0.12493719431015152, test error 0.25061962344186917\n",
            "Loss: 0.1967159656924844\n",
            "training error 0.12509954657335953, test error 0.2507393480180179\n",
            "Loss: 0.24458136899068172\n",
            "training error 0.12493636670956572, test error 0.2506370718708841\n",
            "Loss: 0.20369177733103871\n",
            "training error 0.12494006948359863, test error 0.2505692520711403\n",
            "Loss: 0.1765776945690467\n",
            "training error 0.12494128609646415, test error 0.250519569217407\n",
            "Loss: 0.15671468976712344\n",
            "training error 0.12495219001733114, test error 0.250532316205457\n",
            "Loss: 0.16181088424513934\n",
            "training error 0.12495041339437604, test error 0.25057479598117566\n",
            "Loss: 0.17879412747052736\n",
            "training error 0.12496720118067584, test error 0.250626419345372\n",
            "Loss: 0.19943294053927918\n",
            "training error 0.12492498968310763, test error 0.250713184371919\n",
            "Loss: 0.23412124866650785\n",
            "training error 0.12498548903712318, test error 0.2507507222966833\n",
            "Loss: 0.24912875978595483\n",
            "training error 0.12500520360539333, test error 0.250588174859361\n",
            "Loss: 0.1841429490794244\n",
            "training error 0.12491218830669641, test error 0.25064056594691936\n",
            "Loss: 0.20508869485593717\n",
            "training error 0.12492393085150554, test error 0.25064300858231164\n",
            "Loss: 0.20606525064692516\n",
            "training error 0.12501174167520823, test error 0.2507218088351615\n",
            "Loss: 0.23756927433185027\n",
            "training error 0.12504745410505924, test error 0.25057760842661764\n",
            "Loss: 0.17991853182981465\n",
            "training error 0.12495145837009292, test error 0.2507547981804894\n",
            "Loss: 0.25075828171410297\n",
            "training error 0.12508410858953695, test error 0.2506796655578726\n",
            "Loss: 0.22072056182327326\n",
            "training error 0.12497747452524144, test error 0.2507004457198272\n",
            "Loss: 0.2290283868710219\n",
            "training error 0.12491775088877968, test error 0.2506624734885758\n",
            "Loss: 0.21384724176742864\n",
            "training error 0.12499093343250149, test error 0.2505811604732597\n",
            "Loss: 0.1813386257697891\n",
            "training error 0.12497205474942885, test error 0.2506066646099559\n",
            "Loss: 0.19153507689124716\n",
            "training error 0.12491871391795684, test error 0.25059331987476013\n",
            "Loss: 0.18619990551205223\n",
            "training error 0.12496684066690833, test error 0.25063777124076103\n",
            "Loss: 0.20397138259080627\n",
            "training error 0.12492459224678325, test error 0.25068243037984517\n",
            "Loss: 0.22182592651132094\n",
            "training error 0.12494057501871304, test error 0.25071492352663416\n",
            "Loss: 0.23481655571637727\n",
            "training error 0.1249383876181335, test error 0.2508167992178168\n",
            "Loss: 0.27554604669977056\n",
            "training error 0.12490140951027899, test error 0.25081331072428054\n",
            "Loss: 0.27415136103550175\n",
            "training error 0.12489599078739179, test error 0.2508932786828543\n",
            "Loss: 0.306122228765382\n",
            "training error 0.12491144053479991, test error 0.25085404749099754\n",
            "Loss: 0.2904377562827909\n",
            "training error 0.12520091985642556, test error 0.2507785326662545\n",
            "Loss: 0.26024723352138235\n",
            "training error 0.12491003793032462, test error 0.2506939323707224\n",
            "Loss: 0.22642437613444866\n",
            "training error 0.12488475731140428, test error 0.25062736117750806\n",
            "Loss: 0.19980948123359266\n",
            "training error 0.12495151011966318, test error 0.25059296750952986\n",
            "Loss: 0.18605903131230228\n",
            "training error 0.12488863762372243, test error 0.2506630563721129\n",
            "Loss: 0.21408027625771098\n",
            "training error 0.1248876888358957, test error 0.2506630971430662\n",
            "Loss: 0.21409657632063261\n",
            "training error 0.12494998435214011, test error 0.25057668731632005\n",
            "Loss: 0.17955027564302561\n",
            "training error 0.12487884390822841, test error 0.25066432631285585\n",
            "Loss: 0.21458799345148716\n",
            "training error 0.12499586088103105, test error 0.2507126844502766\n",
            "Loss: 0.23392138200755141\n",
            "training error 0.12488213872355065, test error 0.2506859003796307\n",
            "Loss: 0.22321321844853692\n",
            "training error 0.12496779563201231, test error 0.25076452410183586\n",
            "Loss: 0.25464666589278817\n",
            "training error 0.12487565932583058, test error 0.25069654409411535\n",
            "Loss: 0.2274685326270287\n",
            "training error 0.12496138073375637, test error 0.25077948580282683\n",
            "Loss: 0.2606282936838067\n",
            "training error 0.12492212257365591, test error 0.2508633867545415\n",
            "Loss: 0.2941715562294833\n",
            "training error 0.12495579915471426, test error 0.25070905085637213\n",
            "Loss: 0.23246868580051316\n",
            "training error 0.12494615266403102, test error 0.25070657504746346\n",
            "Loss: 0.23147886737129308\n",
            "training error 0.12485226569830729, test error 0.25069750166301324\n",
            "Loss: 0.22785136481535684\n",
            "training error 0.12493948168119301, test error 0.2506471991530548\n",
            "Loss: 0.2077406239505697\n",
            "training error 0.12485342604841616, test error 0.2507505704132496\n",
            "Loss: 0.24906803740092354\n",
            "training error 0.1249009849880522, test error 0.25059912953097874\n",
            "Loss: 0.18852258266719435\n",
            "training error 0.12485898720381984, test error 0.2507276751563601\n",
            "Loss: 0.23991460591772373\n",
            "training error 0.12488885518348293, test error 0.2507724502638895\n",
            "Loss: 0.2578155135555482\n",
            "training error 0.12483935885613787, test error 0.2506231269180557\n",
            "Loss: 0.19811664136000218\n",
            "training error 0.12484854498848581, test error 0.25063803616645675\n",
            "Loss: 0.20407729881684045\n",
            "training error 0.12496466400503413, test error 0.2507014085314564\n",
            "Loss: 0.2294133150821942\n",
            "training error 0.12485822357806241, test error 0.250631507155472\n",
            "Loss: 0.20146702652374948\n",
            "training error 0.12492730833019856, test error 0.2506665415969496\n",
            "Loss: 0.21547365510909966\n",
            "training error 0.12488518063195059, test error 0.25065073490296047\n",
            "Loss: 0.20915420252098915\n",
            "training error 0.12490632848599503, test error 0.25061098366522694\n",
            "Loss: 0.19326181779160123\n",
            "training error 0.12483272082547313, test error 0.2505565404706157\n",
            "Loss: 0.17149564788117022\n",
            "training error 0.1248365554418741, test error 0.25049440290377334\n",
            "Loss: 0.14665329894536505\n",
            "training error 0.12494798894824967, test error 0.25041098136727596\n",
            "Loss: 0.11330170467249712\n",
            "training error 0.1247969058951539, test error 0.2504056317156926\n",
            "Loss: 0.11116293551765022\n",
            "training error 0.12479347828434462, test error 0.2504357700760112\n",
            "Loss: 0.12321213057686631\n",
            "training error 0.12477233421935285, test error 0.2504759356293976\n",
            "Loss: 0.13927015703540935\n",
            "training error 0.12484096732811226, test error 0.25054258863524015\n",
            "Loss: 0.16591776029548644\n",
            "training error 0.12477516444638298, test error 0.2504710332145868\n",
            "Loss: 0.13731019134080036\n",
            "training error 0.1249278019629573, test error 0.25058958725566016\n",
            "Loss: 0.1847076194307462\n",
            "training error 0.12505052183518023, test error 0.25054849497061665\n",
            "Loss: 0.16827908938852332\n",
            "training error 0.12478607481777458, test error 0.25040950188905003\n",
            "Loss: 0.11271021523706626\n",
            "training error 0.12473665809905916, test error 0.2504088798819594\n",
            "Loss: 0.11246153930766223\n",
            "training error 0.12490559254628102, test error 0.250547887868863\n",
            "Loss: 0.16803637255278225\n",
            "training error 0.12482416758900838, test error 0.25039158566901093\n",
            "Loss: 0.10554738263128804\n",
            "training error 0.12472134961081721, test error 0.2504103821842993\n",
            "Loss: 0.1130621537319243\n",
            "training error 0.12487537769518131, test error 0.250336962731033\n",
            "Loss: 0.08370935204684571\n",
            "training error 0.12472513781855796, test error 0.25040347328956475\n",
            "Loss: 0.11030000544580698\n",
            "training error 0.12475707467700345, test error 0.25045334610409165\n",
            "Loss: 0.13023895580785982\n",
            "training error 0.1246566324183557, test error 0.2504369796653071\n",
            "Loss: 0.1236957195051902\n",
            "training error 0.12465958416649191, test error 0.2504033175769483\n",
            "Loss: 0.11023775216896237\n",
            "training error 0.12467974241837841, test error 0.2503942278105604\n",
            "Loss: 0.10660370018031884\n",
            "training error 0.12478250904445887, test error 0.2505079780470181\n",
            "Loss: 0.15208058653444212\n",
            "training error 0.12461100919073786, test error 0.25040034764221814\n",
            "Loss: 0.10905038422661661\n",
            "training error 0.12463766517740929, test error 0.25035794851882176\n",
            "Loss: 0.09209938547483087\n",
            "training error 0.12457996967626422, test error 0.25037110413291414\n",
            "Loss: 0.0973589469986269\n",
            "training error 0.12457275490299066, test error 0.2503057262246218\n",
            "Loss: 0.07122112260311564\n",
            "training error 0.12457050590020109, test error 0.250170688257439\n",
            "Loss: 0.01723348725124474\n",
            "training error 0.12465662723089133, test error 0.25015741784605205\n",
            "Loss: 0.011928030231467268\n",
            "training error 0.12466431508297557, test error 0.2503020864340408\n",
            "Loss: 0.06976594898975286\n",
            "training error 0.12466898390693718, test error 0.2501611635261005\n",
            "Loss: 0.013425538027367345\n",
            "training error 0.12457623685519445, test error 0.25021148585750597\n",
            "Loss: 0.0335442034268274\n",
            "training error 0.1245541693618605, test error 0.2501218645837121\n",
            "Loss: 0.0\n",
            "training error 0.12449398447180211, test error 0.25003699193017476\n",
            "Loss: 0.0\n",
            "training error 0.12447695594723739, test error 0.25005428237200433\n",
            "Loss: 0.006915153512321304\n",
            "training error 0.12442592832622977, test error 0.25012911076047273\n",
            "Loss: 0.03684208068048811\n",
            "training error 0.12439170321284344, test error 0.250088352649597\n",
            "Loss: 0.02054124832719939\n",
            "training error 0.12446928147673206, test error 0.2501268160606308\n",
            "Loss: 0.03592433654022731\n",
            "training error 0.12434604480120001, test error 0.2500448440057176\n",
            "Loss: 0.003140365544407331\n",
            "training error 0.12431936664892786, test error 0.24996534451789235\n",
            "Loss: 0.0\n",
            "training error 0.12440001058706782, test error 0.2498648712211449\n",
            "Loss: 0.0\n",
            "training error 0.12434231460663277, test error 0.2497384738564816\n",
            "Loss: 0.0\n",
            "training error 0.12423265295981652, test error 0.2497337725368434\n",
            "Loss: 0.0\n",
            "training error 0.12431122696151395, test error 0.24974932694664745\n",
            "Loss: 0.006228396602536712\n",
            "training error 0.1241909166798862, test error 0.2496435934692281\n",
            "Loss: 0.0\n",
            "training error 0.12418902410102219, test error 0.24955451108873217\n",
            "Loss: 0.0\n",
            "training error 0.12411993275426182, test error 0.2496316440557267\n",
            "Loss: 0.030908263953244308\n",
            "training error 0.12407724582432597, test error 0.24946626853056347\n",
            "Loss: 0.0\n",
            "training error 0.12416176725890288, test error 0.24940874076886027\n",
            "Loss: 0.0\n",
            "training error 0.12402526503386588, test error 0.24924593375390994\n",
            "Loss: 0.0\n",
            "training error 0.12401023486456159, test error 0.24920774552519498\n",
            "Loss: 0.0\n",
            "training error 0.12392519804615273, test error 0.24901709745708764\n",
            "Loss: 0.0\n",
            "training error 0.12396267238354128, test error 0.24901129965859528\n",
            "Loss: 0.0\n",
            "training error 0.12381016685687485, test error 0.24888950863043388\n",
            "Loss: 0.0\n",
            "training error 0.12375154583503943, test error 0.24885500769197988\n",
            "Loss: 0.0\n",
            "training error 0.12384137020416518, test error 0.24872936214044022\n",
            "Loss: 0.0\n",
            "training error 0.12400358692469887, test error 0.24869794491308708\n",
            "Loss: 0.0\n",
            "training error 0.12357319157275617, test error 0.24841784314236448\n",
            "Loss: 0.0\n",
            "training error 0.12353090647184146, test error 0.2483129638317243\n",
            "Loss: 0.0\n",
            "training error 0.12352464937585234, test error 0.24816168188727528\n",
            "Loss: 0.0\n",
            "training error 0.1234793779132582, test error 0.2480877836633081\n",
            "Loss: 0.0\n",
            "training error 0.12341146113574013, test error 0.24790384220602035\n",
            "Loss: 0.0\n",
            "training error 0.12328561302959706, test error 0.24788781932295142\n",
            "Loss: 0.0\n",
            "training error 0.12326963253334945, test error 0.2476058087294486\n",
            "Loss: 0.0\n",
            "training error 0.12311860518170711, test error 0.24749181460917732\n",
            "Loss: 0.0\n",
            "training error 0.12306552025518634, test error 0.2473340121895727\n",
            "Loss: 0.0\n",
            "training error 0.12299227292737532, test error 0.24725089917172383\n",
            "Loss: 0.0\n",
            "training error 0.1228546743931433, test error 0.2470128074073928\n",
            "Loss: 0.0\n",
            "training error 0.12288156364159167, test error 0.2468189996735804\n",
            "Loss: 0.0\n",
            "training error 0.12268730240642725, test error 0.2466626046851236\n",
            "Loss: 0.0\n",
            "training error 0.12259670464060657, test error 0.24652024465515665\n",
            "Loss: 0.0\n",
            "training error 0.12251684517092924, test error 0.24637283036379626\n",
            "Loss: 0.0\n",
            "training error 0.12238917137961848, test error 0.2461274067829965\n",
            "Loss: 0.0\n",
            "training error 0.12255062933862801, test error 0.24598962355437054\n",
            "Loss: 0.0\n",
            "training error 0.1221869522541179, test error 0.24564335970449325\n",
            "Loss: 0.0\n",
            "training error 0.12207648361470329, test error 0.24531819360682255\n",
            "Loss: 0.0\n",
            "training error 0.12190842232967739, test error 0.2448521357643313\n",
            "Loss: 0.0\n",
            "training error 0.12178751578949797, test error 0.2444773782956755\n",
            "Loss: 0.0\n",
            "training error 0.1216326968731745, test error 0.2442966109394284\n",
            "Loss: 0.0\n",
            "training error 0.12154476707354797, test error 0.24388273136374383\n",
            "Loss: 0.0\n",
            "training error 0.12130088229423058, test error 0.243649547408274\n",
            "Loss: 0.0\n",
            "training error 0.12114884194332044, test error 0.2432547424952286\n",
            "Loss: 0.0\n",
            "training error 0.1209595343912359, test error 0.24298287645275138\n",
            "Loss: 0.0\n",
            "training error 0.12083035569781288, test error 0.24247942101916725\n",
            "Loss: 0.0\n",
            "training error 0.12060962125268175, test error 0.24219083991881962\n",
            "Loss: 0.0\n",
            "training error 0.12043483804538951, test error 0.2418640962179594\n",
            "Loss: 0.0\n",
            "training error 0.12028739704487404, test error 0.2414623654845205\n",
            "Loss: 0.0\n",
            "training error 0.12002674702385151, test error 0.24098155229333826\n",
            "Loss: 0.0\n",
            "training error 0.11976176714229494, test error 0.24046524915159861\n",
            "Loss: 0.0\n",
            "training error 0.11955413324937342, test error 0.240084603567164\n",
            "Loss: 0.0\n",
            "training error 0.11935634714647515, test error 0.23963096518203061\n",
            "Loss: 0.0\n",
            "training error 0.1192143924718971, test error 0.23924550475776693\n",
            "Loss: 0.0\n",
            "training error 0.1188706495412948, test error 0.2386086021737832\n",
            "Loss: 0.0\n",
            "training error 0.11865695563190154, test error 0.23809783447070273\n",
            "Loss: 0.0\n",
            "training error 0.11832925322775459, test error 0.23766827664372817\n",
            "Loss: 0.0\n",
            "training error 0.11812803207168102, test error 0.23701025336455794\n",
            "Loss: 0.0\n",
            "training error 0.117704779281775, test error 0.23632979755462286\n",
            "Loss: 0.0\n",
            "training error 0.11743174320470663, test error 0.23566310083752703\n",
            "Loss: 0.0\n",
            "training error 0.11707511283468264, test error 0.2350232530294432\n",
            "Loss: 0.0\n",
            "training error 0.11675332669219249, test error 0.234179542413584\n",
            "Loss: 0.0\n",
            "training error 0.11643013996905517, test error 0.23359246877683976\n",
            "Loss: 0.0\n",
            "training error 0.11625546036926801, test error 0.23282396365137273\n",
            "Loss: 0.0\n",
            "training error 0.11566514527222545, test error 0.23204526535323822\n",
            "Loss: 0.0\n",
            "training error 0.1152671400059556, test error 0.2312576861246052\n",
            "Loss: 0.0\n",
            "training error 0.1148905735174697, test error 0.23038899605997734\n",
            "Loss: 0.0\n",
            "training error 0.11455412811531822, test error 0.22960038598570778\n",
            "Loss: 0.0\n",
            "training error 0.11405780283676571, test error 0.2288449882972226\n",
            "Loss: 0.0\n",
            "training error 0.11366403816313804, test error 0.227885395878202\n",
            "Loss: 0.0\n",
            "training error 0.11314956753691553, test error 0.22701629413973134\n",
            "Loss: 0.0\n",
            "training error 0.11280593081285385, test error 0.22605637179641308\n",
            "Loss: 0.0\n",
            "training error 0.11223215883964492, test error 0.22498510437058927\n",
            "Loss: 0.0\n",
            "training error 0.11173772594136297, test error 0.22386026720601032\n",
            "Loss: 0.0\n",
            "training error 0.11129698583726125, test error 0.2227530692995659\n",
            "Loss: 0.0\n",
            "training error 0.11069363042122782, test error 0.22164049165354385\n",
            "Loss: 0.0\n",
            "training error 0.1101463925861684, test error 0.220578640629752\n",
            "Loss: 0.0\n",
            "training error 0.10972924861862007, test error 0.21960705356403726\n",
            "Loss: 0.0\n",
            "training error 0.10918973799617901, test error 0.21840596418008576\n",
            "Loss: 0.0\n",
            "training error 0.10859244237549646, test error 0.21699103013710852\n",
            "Loss: 0.0\n",
            "training error 0.10788369602459649, test error 0.21574012889242292\n",
            "Loss: 0.0\n",
            "training error 0.1073490396800353, test error 0.21443209771601215\n",
            "Loss: 0.0\n",
            "training error 0.10660057616044731, test error 0.21322416659976526\n",
            "Loss: 0.0\n",
            "training error 0.10602858290238044, test error 0.21179990782508765\n",
            "Loss: 0.0\n",
            "training error 0.10533083692927253, test error 0.21058546449598878\n",
            "Loss: 0.0\n",
            "training error 0.10462648281675628, test error 0.20925855801341198\n",
            "Loss: 0.0\n",
            "training error 0.10403008300354095, test error 0.20768537872081427\n",
            "Loss: 0.0\n",
            "training error 0.10346813665054919, test error 0.20637148782047793\n",
            "Loss: 0.0\n",
            "training error 0.10258241275751902, test error 0.20489642539787073\n",
            "Loss: 0.0\n",
            "training error 0.10193009724073335, test error 0.20333199452504971\n",
            "Loss: 0.0\n",
            "training error 0.10110678080460854, test error 0.20181669920341472\n",
            "Loss: 0.0\n",
            "training error 0.10057995609341079, test error 0.20053918710891366\n",
            "Loss: 0.0\n",
            "training error 0.09961392224883772, test error 0.1987608405150141\n",
            "Loss: 0.0\n",
            "training error 0.09882319147739256, test error 0.1971074138045974\n",
            "Loss: 0.0\n",
            "training error 0.09810682504432737, test error 0.195493194047618\n",
            "Loss: 0.0\n",
            "training error 0.09732051114456217, test error 0.19385759046079098\n",
            "Loss: 0.0\n",
            "training error 0.09648362105876193, test error 0.19201490923260453\n",
            "Loss: 0.0\n",
            "training error 0.09569856439220344, test error 0.19038011540426697\n",
            "Loss: 0.0\n",
            "training error 0.09486659790395507, test error 0.1886094222147912\n",
            "Loss: 0.0\n",
            "training error 0.09423047925502828, test error 0.18670584064265466\n",
            "Loss: 0.0\n",
            "training error 0.09324522476085083, test error 0.18506292129359872\n",
            "Loss: 0.0\n",
            "training error 0.0924529041494636, test error 0.18337991112800522\n",
            "Loss: 0.0\n",
            "training error 0.09169868769741246, test error 0.1817901249354466\n",
            "Loss: 0.0\n",
            "training error 0.09082533240979349, test error 0.1800302673654567\n",
            "Loss: 0.0\n",
            "training error 0.0900320065624083, test error 0.1782295901910351\n",
            "Loss: 0.0\n",
            "training error 0.0891433139908858, test error 0.17634214310484084\n",
            "Loss: 0.0\n",
            "training error 0.0883901166384571, test error 0.1746415537910237\n",
            "Loss: 0.0\n",
            "training error 0.08750574701997464, test error 0.1727658078762204\n",
            "Loss: 0.0\n",
            "training error 0.08673564177353475, test error 0.17095342770483993\n",
            "Loss: 0.0\n",
            "training error 0.08585166413867133, test error 0.169410219352573\n",
            "Loss: 0.0\n",
            "training error 0.08504462059292914, test error 0.16759482334123296\n",
            "Loss: 0.0\n",
            "training error 0.08420105128494965, test error 0.16569487896161153\n",
            "Loss: 0.0\n",
            "training error 0.08339716834503011, test error 0.16399396219174395\n",
            "Loss: 0.0\n",
            "training error 0.08254420186117081, test error 0.1623959221648104\n",
            "Loss: 0.0\n",
            "training error 0.08194490595334641, test error 0.16080589285827937\n",
            "Loss: 0.0\n",
            "training error 0.08092815740188966, test error 0.15903825068239463\n",
            "Loss: 0.0\n",
            "training error 0.08015136016274399, test error 0.15717245159552984\n",
            "Loss: 0.0\n",
            "training error 0.07935805242354962, test error 0.15564182607160704\n",
            "Loss: 0.0\n",
            "training error 0.07856675231039094, test error 0.15388342655588666\n",
            "Loss: 0.0\n",
            "training error 0.07787242460376881, test error 0.1519991542674523\n",
            "Loss: 0.0\n",
            "training error 0.07699063331718178, test error 0.15051908265723962\n",
            "Loss: 0.0\n",
            "training error 0.07627165881241556, test error 0.14875464849298742\n",
            "Loss: 0.0\n",
            "training error 0.07550373745642076, test error 0.1470119862088099\n",
            "Loss: 0.0\n",
            "training error 0.07478271012130747, test error 0.14543196015843843\n",
            "Loss: 0.0\n",
            "training error 0.07410808518471014, test error 0.14400701046387102\n",
            "Loss: 0.0\n",
            "training error 0.07350616332951183, test error 0.14252101094942243\n",
            "Loss: 0.0\n",
            "training error 0.0725834887995455, test error 0.14087302815357602\n",
            "Loss: 0.0\n",
            "training error 0.07184507658637519, test error 0.1392537248395207\n",
            "Loss: 0.0\n",
            "training error 0.07120278594424871, test error 0.13758689494484086\n",
            "Loss: 0.0\n",
            "training error 0.07047718921022979, test error 0.13627761014886006\n",
            "Loss: 0.0\n",
            "training error 0.0698535949665121, test error 0.13466735196432572\n",
            "Loss: 0.0\n",
            "training error 0.06918322220000266, test error 0.13336229108073505\n",
            "Loss: 0.0\n",
            "training error 0.06856547706033715, test error 0.13203993514348064\n",
            "Loss: 0.0\n",
            "training error 0.06779154642149905, test error 0.1302502260695711\n",
            "Loss: 0.0\n",
            "training error 0.06723282614163485, test error 0.1288137834405179\n",
            "Loss: 0.0\n",
            "training error 0.06655522852750467, test error 0.1275457855566926\n",
            "Loss: 0.0\n",
            "training error 0.0659007619735725, test error 0.1262519320682825\n",
            "Loss: 0.0\n",
            "training error 0.06527085273079226, test error 0.12488663632003119\n",
            "Loss: 0.0\n",
            "training error 0.06466681649211106, test error 0.12349833415415759\n",
            "Loss: 0.0\n",
            "training error 0.0640807499330346, test error 0.12232429072512009\n",
            "Loss: 0.0\n",
            "training error 0.06355909432814748, test error 0.12075952556972941\n",
            "Loss: 0.0\n",
            "training error 0.06291718844569119, test error 0.11948526146197959\n",
            "Loss: 0.0\n",
            "training error 0.062415123120548976, test error 0.11839133392351199\n",
            "Loss: 0.0\n",
            "training error 0.06183154902048445, test error 0.11699725744249087\n",
            "Loss: 0.0\n",
            "training error 0.061277292700048655, test error 0.11578291740404946\n",
            "Loss: 0.0\n",
            "training error 0.060770993828910644, test error 0.1145733445946356\n",
            "Loss: 0.0\n",
            "training error 0.060281825323636615, test error 0.11341573970972305\n",
            "Loss: 0.0\n",
            "training error 0.05976142043322132, test error 0.11235419343253984\n",
            "Loss: 0.0\n",
            "training error 0.059262170065517136, test error 0.11125080111924358\n",
            "Loss: 0.0\n",
            "training error 0.058727753442462435, test error 0.11009683355592291\n",
            "Loss: 0.0\n",
            "training error 0.05825088859337333, test error 0.10906703347467307\n",
            "Loss: 0.0\n",
            "training error 0.05793383766219307, test error 0.10794153133592384\n",
            "Loss: 0.0\n",
            "training error 0.057417721078398036, test error 0.10724546458279954\n",
            "Loss: 0.0\n",
            "training error 0.056875867760375676, test error 0.10604585328045885\n",
            "Loss: 0.0\n",
            "training error 0.05644450981578467, test error 0.10502095956354014\n",
            "Loss: 0.0\n",
            "training error 0.0560014022826379, test error 0.10412950448707775\n",
            "Loss: 0.0\n",
            "training error 0.0556681395982395, test error 0.10307815696126733\n",
            "Loss: 0.0\n",
            "training error 0.055214585443757964, test error 0.10229732007935169\n",
            "Loss: 0.0\n",
            "training error 0.05474738775245058, test error 0.10133715427520801\n",
            "Loss: 0.0\n",
            "training error 0.05435824313335491, test error 0.10046555960898587\n",
            "Loss: 0.0\n",
            "training error 0.05399456842856505, test error 0.09941682750789099\n",
            "Loss: 0.0\n",
            "training error 0.053609069847217326, test error 0.09852193864813882\n",
            "Loss: 0.0\n",
            "training error 0.0532369206951499, test error 0.0977681530478014\n",
            "Loss: 0.0\n",
            "training error 0.052873384475897615, test error 0.09693381203931714\n",
            "Loss: 0.0\n",
            "training error 0.05250498719288677, test error 0.09611696340528667\n",
            "Loss: 0.0\n",
            "training error 0.05217374525143989, test error 0.0952127658980558\n",
            "Loss: 0.0\n",
            "training error 0.05182503683372152, test error 0.09418091320727805\n",
            "Loss: 0.0\n",
            "training error 0.05144183720505067, test error 0.09335286803743675\n",
            "Loss: 0.0\n",
            "training error 0.05110988728922074, test error 0.09257897297185787\n",
            "Loss: 0.0\n",
            "training error 0.05095025958649247, test error 0.09197029298683183\n",
            "Loss: 0.0\n",
            "training error 0.05049908794326677, test error 0.09121790212829188\n",
            "Loss: 0.0\n",
            "training error 0.05016751023413653, test error 0.090552108888317\n",
            "Loss: 0.0\n",
            "training error 0.049905656982938994, test error 0.08971429334500934\n",
            "Loss: 0.0\n",
            "training error 0.04956670804801617, test error 0.08912412818535491\n",
            "Loss: 0.0\n",
            "training error 0.04924358583730152, test error 0.08846330337667854\n",
            "Loss: 0.0\n",
            "training error 0.04895605049764726, test error 0.08773574280480433\n",
            "Loss: 0.0\n",
            "training error 0.04868998967745537, test error 0.08722015798440706\n",
            "Loss: 0.0\n",
            "training error 0.04841705163739745, test error 0.08652721285168441\n",
            "Loss: 0.0\n",
            "training error 0.048148904747587304, test error 0.08593101746632882\n",
            "Loss: 0.0\n",
            "training error 0.04799072060061654, test error 0.08530466931250275\n",
            "Loss: 0.0\n",
            "training error 0.047737916475667046, test error 0.08502742143683788\n",
            "Loss: 0.0\n",
            "training error 0.04749634117536191, test error 0.08400994289431273\n",
            "Loss: 0.0\n",
            "training error 0.04722291996895636, test error 0.08348919172479723\n",
            "Loss: 0.0\n",
            "training error 0.046967674457252674, test error 0.08280921262181781\n",
            "Loss: 0.0\n",
            "training error 0.04671413513520958, test error 0.08241198410255744\n",
            "Loss: 0.0\n",
            "training error 0.04643581107397209, test error 0.08183083762653615\n",
            "Loss: 0.0\n",
            "training error 0.046264947401944805, test error 0.08133237095065576\n",
            "Loss: 0.0\n",
            "training error 0.046022802665114104, test error 0.0807477073381006\n",
            "Loss: 0.0\n",
            "training error 0.04577973921821075, test error 0.08025170761964745\n",
            "Loss: 0.0\n",
            "training error 0.045607183097110676, test error 0.0797474375026466\n",
            "Loss: 0.0\n",
            "training error 0.04546012320187301, test error 0.07930409002464414\n",
            "Loss: 0.0\n",
            "training error 0.04518283893726237, test error 0.07866969095159816\n",
            "Loss: 0.0\n",
            "training error 0.04498363522459551, test error 0.07819301069649676\n",
            "Loss: 0.0\n",
            "training error 0.04483887900524985, test error 0.07789566521849585\n",
            "Loss: 0.0\n",
            "training error 0.04463930977702168, test error 0.07739575298745836\n",
            "Loss: 0.0\n",
            "training error 0.044463275672826395, test error 0.07702536059509123\n",
            "Loss: 0.0\n",
            "training error 0.04423028335588118, test error 0.07666965719198786\n",
            "Loss: 0.0\n",
            "training error 0.04408403834754155, test error 0.07634937092377886\n",
            "Loss: 0.0\n",
            "training error 0.043900622879213556, test error 0.0756992455487744\n",
            "Loss: 0.0\n",
            "training error 0.043713493846522816, test error 0.07518010996496169\n",
            "Loss: 0.0\n",
            "training error 0.043551881847106115, test error 0.07470487283562913\n",
            "Loss: 0.0\n",
            "training error 0.04343806101107594, test error 0.07433196288068583\n",
            "Loss: 0.0\n",
            "training error 0.043234136981196374, test error 0.07412907139058532\n",
            "Loss: 0.0\n",
            "training error 0.043075637989538146, test error 0.07375833377837238\n",
            "Loss: 0.0\n",
            "training error 0.042924171690038926, test error 0.07340335913172848\n",
            "Loss: 0.0\n",
            "training error 0.04278954919971089, test error 0.07298835286890129\n",
            "Loss: 0.0\n",
            "training error 0.042613382446568336, test error 0.07275197052210634\n",
            "Loss: 0.0\n",
            "training error 0.04247836314039025, test error 0.07240820635190628\n",
            "Loss: 0.0\n",
            "training error 0.04233000769052637, test error 0.07188821187767633\n",
            "Loss: 0.0\n",
            "training error 0.04222189643241095, test error 0.07167091186144797\n",
            "Loss: 0.0\n",
            "training error 0.04209726552929652, test error 0.07126121019586167\n",
            "Loss: 0.0\n",
            "training error 0.04199697143127609, test error 0.07101553493311717\n",
            "Loss: 0.0\n",
            "training error 0.04187029639884214, test error 0.07064931906997485\n",
            "Loss: 0.0\n",
            "training error 0.041732066753453936, test error 0.07044993866123214\n",
            "Loss: 0.0\n",
            "training error 0.04165927750954562, test error 0.07003579166955305\n",
            "Loss: 0.0\n",
            "training error 0.04145768143178978, test error 0.06993720956329674\n",
            "Loss: 0.0\n",
            "training error 0.04136345293142934, test error 0.0697666449604415\n",
            "Loss: 0.0\n",
            "training error 0.04122051703034402, test error 0.06941709762965464\n",
            "Loss: 0.0\n",
            "training error 0.04116282934803507, test error 0.06919407956707214\n",
            "Loss: 0.0\n",
            "training error 0.0410492692469146, test error 0.06880012523696144\n",
            "Loss: 0.0\n",
            "training error 0.040968664046717757, test error 0.06880571085279849\n",
            "Loss: 0.008118612891783172\n",
            "training error 0.04080560125266938, test error 0.06845864148799718\n",
            "Loss: 0.0\n",
            "training error 0.040703113069574366, test error 0.06811244327643105\n",
            "Loss: 0.0\n",
            "training error 0.040580633709407614, test error 0.06804047611125505\n",
            "Loss: 0.0\n",
            "training error 0.04048857262224756, test error 0.06774752479748787\n",
            "Loss: 0.0\n",
            "training error 0.04039734871504221, test error 0.06747010237578419\n",
            "Loss: 0.0\n",
            "training error 0.04027520486629166, test error 0.06712137532049972\n",
            "Loss: 0.0\n",
            "training error 0.04026738102069057, test error 0.06690223234878112\n",
            "Loss: 0.0\n",
            "training error 0.040143084765059255, test error 0.06678814375413097\n",
            "Loss: 0.0\n",
            "training error 0.03999893028904943, test error 0.06655529118866133\n",
            "Loss: 0.0\n",
            "training error 0.039943078015155384, test error 0.06635016473378338\n",
            "Loss: 0.0\n",
            "training error 0.039841945438528904, test error 0.06604506699652132\n",
            "Loss: 0.0\n",
            "training error 0.039780526261003174, test error 0.0660227242529524\n",
            "Loss: 0.0\n",
            "training error 0.03969891377533293, test error 0.06574064055719696\n",
            "Loss: 0.0\n",
            "training error 0.0396022500242581, test error 0.06531104328771231\n",
            "Loss: 0.0\n",
            "training error 0.03949299930917991, test error 0.06525203438593451\n",
            "Loss: 0.0\n",
            "training error 0.03944434143439013, test error 0.06494899595327609\n",
            "Loss: 0.0\n",
            "training error 0.03933234844065738, test error 0.06484957717265193\n",
            "Loss: 0.0\n",
            "training error 0.03926910804909896, test error 0.06468223222412652\n",
            "Loss: 0.0\n",
            "training error 0.039213187234372414, test error 0.0643419092622379\n",
            "Loss: 0.0\n",
            "training error 0.03914725665606726, test error 0.06427477783555868\n",
            "Loss: 0.0\n",
            "training error 0.039055091936553016, test error 0.06427785404490682\n",
            "Loss: 0.004786028753001048\n",
            "training error 0.038974694403287616, test error 0.06402338706175156\n",
            "Loss: 0.0\n",
            "training error 0.03888603001063305, test error 0.06385485497906151\n",
            "Loss: 0.0\n",
            "training error 0.03884966346201403, test error 0.06353197568700156\n",
            "Loss: 0.0\n",
            "training error 0.038783643278046544, test error 0.06350011980562907\n",
            "Loss: 0.0\n",
            "training error 0.038696798815684934, test error 0.06335891555902434\n",
            "Loss: 0.0\n",
            "training error 0.03869809365402043, test error 0.06304127978238248\n",
            "Loss: 0.0\n",
            "training error 0.038608879978453824, test error 0.06309606067246071\n",
            "Loss: 0.08689685594474028\n",
            "training error 0.038527013750409796, test error 0.06295918447772875\n",
            "Loss: 0.0\n",
            "training error 0.03843752087832089, test error 0.0627159348743991\n",
            "Loss: 0.0\n",
            "training error 0.038391925299636365, test error 0.06262521321685305\n",
            "Loss: 0.0\n",
            "training error 0.03835186084924092, test error 0.06258876164594412\n",
            "Loss: 0.0\n",
            "training error 0.0382858944688628, test error 0.062315983542691585\n",
            "Loss: 0.0\n",
            "training error 0.03823445679856515, test error 0.06223827973137575\n",
            "Loss: 0.0\n",
            "training error 0.03814121488420999, test error 0.06214893990612251\n",
            "Loss: 0.0\n",
            "training error 0.03811786853422195, test error 0.06199237501846791\n",
            "Loss: 0.0\n",
            "training error 0.038098532447643566, test error 0.061775477476164305\n",
            "Loss: 0.0\n",
            "training error 0.03803573056849039, test error 0.061699114300371005\n",
            "Loss: 0.0\n",
            "training error 0.03798041531980928, test error 0.06167784980144716\n",
            "Loss: 0.0\n",
            "training error 0.03789685218564711, test error 0.06156807855164322\n",
            "Loss: 0.0\n",
            "training error 0.0379073413157591, test error 0.061320223406454065\n",
            "Loss: 0.0\n",
            "training error 0.03783890357629781, test error 0.06115231380097861\n",
            "Loss: 0.0\n",
            "training error 0.03771772496838401, test error 0.060975178870708475\n",
            "Loss: 0.0\n",
            "training error 0.0376912397130442, test error 0.06095379591390586\n",
            "Loss: 0.0\n",
            "training error 0.03762495714015125, test error 0.06084634708672072\n",
            "Loss: 0.0\n",
            "training error 0.03759821599398273, test error 0.06083103183609295\n",
            "Loss: 0.0\n",
            "training error 0.0375690578427929, test error 0.06068868095992276\n",
            "Loss: 0.0\n",
            "training error 0.03751432140256194, test error 0.06038701530141759\n",
            "Loss: 0.0\n",
            "training error 0.03744684074052884, test error 0.060109011495061475\n",
            "Loss: 0.0\n",
            "training error 0.037433008350546665, test error 0.05995110082283787\n",
            "Loss: 0.0\n",
            "training error 0.03733358577107892, test error 0.05996058898123722\n",
            "Loss: 0.015826495709214683\n",
            "training error 0.03735231541219481, test error 0.0597259556982876\n",
            "Loss: 0.0\n",
            "training error 0.03732149385002995, test error 0.059544025009095926\n",
            "Loss: 0.0\n",
            "training error 0.037259485384936286, test error 0.059506948280189376\n",
            "Loss: 0.0\n",
            "training error 0.037179222142247476, test error 0.05962435006603125\n",
            "Loss: 0.1972908865853551\n",
            "training error 0.037153354248323715, test error 0.05947868098336835\n",
            "Loss: 0.0\n",
            "training error 0.03709335763008234, test error 0.05946475455388694\n",
            "Loss: 0.0\n",
            "training error 0.03707605780962341, test error 0.05932089910388074\n",
            "Loss: 0.0\n",
            "training error 0.03711473839043502, test error 0.05935055797590334\n",
            "Loss: 0.04999734068538153\n",
            "training error 0.03700950722744935, test error 0.059307372759474786\n",
            "Loss: 0.0\n",
            "training error 0.03699911551383193, test error 0.05913834064091542\n",
            "Loss: 0.0\n",
            "training error 0.036940079708233785, test error 0.058929832041662925\n",
            "Loss: 0.0\n",
            "training error 0.0368934673473353, test error 0.05880599945922072\n",
            "Loss: 0.0\n",
            "training error 0.03687123588351401, test error 0.05867697214647011\n",
            "Loss: 0.0\n",
            "training error 0.03680123831003606, test error 0.058828434897077356\n",
            "Loss: 0.2581297995901455\n",
            "training error 0.03682648742298357, test error 0.05882862828922915\n",
            "Loss: 0.25845938740751695\n",
            "training error 0.03675226778691792, test error 0.058774879263509355\n",
            "Loss: 0.16685782080720912\n",
            "training error 0.03670433515834496, test error 0.058663309706013696\n",
            "Loss: 0.0\n",
            "training error 0.036703267291982124, test error 0.05871236607990492\n",
            "Loss: 0.0836236041523497\n",
            "training error 0.03664657509721865, test error 0.0585768945175459\n",
            "Loss: 0.0\n",
            "training error 0.036638051613364, test error 0.05843881945309029\n",
            "Loss: 0.0\n",
            "training error 0.036640005489877794, test error 0.05854552289251799\n",
            "Loss: 0.18258999826878952\n",
            "training error 0.036572317656622125, test error 0.05830313731921755\n",
            "Loss: 0.0\n",
            "training error 0.03655186054667304, test error 0.058261200549398894\n",
            "Loss: 0.0\n",
            "training error 0.036516842341264226, test error 0.0582545776389023\n",
            "Loss: 0.0\n",
            "training error 0.03654916730823108, test error 0.05816267123739078\n",
            "Loss: 0.0\n",
            "training error 0.036490324651406954, test error 0.058197917263145334\n",
            "Loss: 0.06059904919204495\n",
            "training error 0.03646095072000913, test error 0.05833628664536704\n",
            "Loss: 0.2984997151655122\n",
            "training error 0.03644615701719973, test error 0.05838913083094823\n",
            "Loss: 0.38935555871078\n",
            "training error 0.036387557089290645, test error 0.05819560536817241\n",
            "Loss: 0.05662417162239919\n",
            "training error 0.03637010162347882, test error 0.05820155412318421\n",
            "Loss: 0.06685196014248884\n",
            "training error 0.03633184611154689, test error 0.058118322328784944\n",
            "Loss: 0.0\n",
            "training error 0.0363681231039977, test error 0.05805216839098032\n",
            "Loss: 0.0\n",
            "training error 0.03629917368275535, test error 0.05795349070811005\n",
            "Loss: 0.0\n",
            "training error 0.03627942710083465, test error 0.05758721355438368\n",
            "Loss: 0.0\n",
            "training error 0.036249807104455724, test error 0.05764689670868339\n",
            "Loss: 0.10363959395838229\n",
            "training error 0.03625467420387881, test error 0.05741181887400578\n",
            "Loss: 0.0\n",
            "training error 0.036236816194082345, test error 0.05754187011088097\n",
            "Loss: 0.22652345706133392\n",
            "training error 0.03617402025387673, test error 0.057455470686458274\n",
            "Loss: 0.07603279831334131\n",
            "training error 0.036214545072499736, test error 0.057528700735940276\n",
            "Loss: 0.2035850182538157\n",
            "training error 0.03618641830142175, test error 0.05751847457585542\n",
            "Loss: 0.18577307589522363\n",
            "training error 0.03609984560440199, test error 0.057315069538458725\n",
            "Loss: 0.0\n",
            "training error 0.03608758359593041, test error 0.05730893413093805\n",
            "Loss: 0.0\n",
            "training error 0.0361232882292865, test error 0.057214015319121875\n",
            "Loss: 0.0\n",
            "training error 0.03609663084620802, test error 0.057004453576859626\n",
            "Loss: 0.0\n",
            "training error 0.036032179187065236, test error 0.05696051576678091\n",
            "Loss: 0.0\n",
            "training error 0.03599644000218633, test error 0.057025573667604\n",
            "Loss: 0.11421578605337324\n",
            "training error 0.03600242949120739, test error 0.05689475099069366\n",
            "Loss: 0.0\n",
            "training error 0.03597303720025165, test error 0.05694897391342027\n",
            "Loss: 0.09530391078691469\n",
            "training error 0.03597261298741208, test error 0.056849917106622155\n",
            "Loss: 0.0\n",
            "training error 0.03601973995884932, test error 0.05700090077083255\n",
            "Loss: 0.265582910045814\n",
            "training error 0.03595799076143153, test error 0.056759007261724316\n",
            "Loss: 0.0\n",
            "training error 0.03590076240904161, test error 0.056672064778745465\n",
            "Loss: 0.0\n",
            "training error 0.03588857011543698, test error 0.05669748951777649\n",
            "Loss: 0.04486291284830912\n",
            "training error 0.035861037411908964, test error 0.0567503682666548\n",
            "Loss: 0.13816946358853333\n",
            "training error 0.035840080592245575, test error 0.056797386037993836\n",
            "Loss: 0.2211340979680898\n",
            "training error 0.035818894136201845, test error 0.056746956666138\n",
            "Loss: 0.13214956554861157\n",
            "training error 0.03580793972186975, test error 0.05675134939674673\n",
            "Loss: 0.13990070471370242\n",
            "training error 0.03583968825185536, test error 0.05667879958565688\n",
            "Loss: 0.011883821310743059\n",
            "training error 0.035871160228450404, test error 0.056613099381710365\n",
            "Loss: 0.0\n",
            "training error 0.03579081590527591, test error 0.05650329742310831\n",
            "Loss: 0.0\n",
            "training error 0.03574152313021757, test error 0.05632807375037896\n",
            "Loss: 0.0\n",
            "training error 0.035752592432761875, test error 0.05639184892030089\n",
            "Loss: 0.11322093172323466\n",
            "training error 0.035703862650539396, test error 0.05635441466194818\n",
            "Loss: 0.0467633807006429\n",
            "training error 0.035686765421174736, test error 0.05628552763375204\n",
            "Loss: 0.0\n",
            "training error 0.035668905131518776, test error 0.05624298188748262\n",
            "Loss: 0.0\n",
            "training error 0.035737392661458464, test error 0.05614349932379358\n",
            "Loss: 0.0\n",
            "training error 0.03575295220919232, test error 0.05634222397316596\n",
            "Loss: 0.35395843110221925\n",
            "training error 0.03563178610903528, test error 0.05622250766027972\n",
            "Loss: 0.14072570722831745\n",
            "training error 0.03566864758685945, test error 0.056077693707404536\n",
            "Loss: 0.0\n",
            "training error 0.03562645840463302, test error 0.05619526790258323\n",
            "Loss: 0.20966303605878256\n",
            "training error 0.03561900680233555, test error 0.05611285239132555\n",
            "Loss: 0.06269637996252353\n",
            "training error 0.035623047385752206, test error 0.05613025251115117\n",
            "Loss: 0.09372497382089673\n",
            "training error 0.03565740171744612, test error 0.05606213840308636\n",
            "Loss: 0.0\n",
            "training error 0.035555191426175664, test error 0.05618520705375052\n",
            "Loss: 0.21952186300724374\n",
            "training error 0.03556303295884025, test error 0.05613467210746577\n",
            "Loss: 0.1293809091938103\n",
            "training error 0.03555026381542608, test error 0.05608742367454315\n",
            "Loss: 0.04510222438356415\n",
            "training error 0.035518922053853465, test error 0.056106424312501714\n",
            "Loss: 0.07899432785980487\n",
            "training error 0.03550564806182753, test error 0.05609181799905881\n",
            "Loss: 0.05294053494544482\n",
            "training error 0.03550532203854283, test error 0.0559751735973441\n",
            "Loss: 0.0\n",
            "training error 0.035553386219931045, test error 0.05593748886008089\n",
            "Loss: 0.0\n",
            "training error 0.035467980641311285, test error 0.05608380652837728\n",
            "Loss: 0.2615735373148187\n",
            "training error 0.035464803932815245, test error 0.05601950919810304\n",
            "Loss: 0.14662856644729327\n",
            "training error 0.03550202468222122, test error 0.05597221749361675\n",
            "Loss: 0.06208472036119961\n",
            "training error 0.03544794062769439, test error 0.05604804175314031\n",
            "Loss: 0.1976364962251953\n",
            "training error 0.0354332077086819, test error 0.05595196830674846\n",
            "Loss: 0.02588504947689252\n",
            "training error 0.03541565287177872, test error 0.0559295741218074\n",
            "Loss: 0.0\n",
            "training error 0.035419616357537785, test error 0.05592624924672613\n",
            "Loss: 0.0\n",
            "training error 0.03539798584927009, test error 0.05586015835800462\n",
            "Loss: 0.0\n",
            "training error 0.03544398558692073, test error 0.05580011259039623\n",
            "Loss: 0.0\n",
            "training error 0.03540674600713064, test error 0.05605379972527617\n",
            "Loss: 0.4546355250967693\n",
            "training error 0.035372467378948384, test error 0.0559989592672304\n",
            "Loss: 0.3563553326385138\n",
            "training error 0.03534279281129442, test error 0.05582838396738787\n",
            "Loss: 0.05066544793408401\n",
            "training error 0.03542032670988268, test error 0.05566924613117793\n",
            "Loss: 0.0\n",
            "training error 0.03535742493162953, test error 0.0557370130116636\n",
            "Loss: 0.12173127030674635\n",
            "training error 0.03539083788445069, test error 0.055770595098290646\n",
            "Loss: 0.18205557674322304\n",
            "training error 0.03530721443983481, test error 0.05571660975391358\n",
            "Loss: 0.08508040979042786\n",
            "training error 0.03530156933163886, test error 0.05569101537358343\n",
            "Loss: 0.03910461146574562\n",
            "training error 0.03530730270309308, test error 0.055629695252457007\n",
            "Loss: 0.0\n",
            "training error 0.03537165736859155, test error 0.05553148203195151\n",
            "Loss: 0.0\n",
            "training error 0.03526860378436524, test error 0.05572668750173368\n",
            "Loss: 0.351522168397822\n",
            "training error 0.03523822338825585, test error 0.055806436138906575\n",
            "Loss: 0.49513194478918443\n",
            "training error 0.035234720999225824, test error 0.055722363684755825\n",
            "Loss: 0.3437359238755544\n",
            "training error 0.03524853032868844, test error 0.05579825272744236\n",
            "Loss: 0.48039541847155753\n",
            "training error 0.03527366241664138, test error 0.055585831906621065\n",
            "Loss: 0.0978721847154862\n",
            "training error 0.035335958211242344, test error 0.055512375608448544\n",
            "Loss: 0.0\n",
            "training error 0.03521806294146659, test error 0.0558070946122409\n",
            "Loss: 0.5309068483595913\n",
            "training error 0.03521583832030077, test error 0.05566835350918175\n",
            "Loss: 0.2809786088662225\n",
            "training error 0.03519261442844067, test error 0.0556805128204827\n",
            "Loss: 0.3028823936847669\n",
            "training error 0.03524664142392975, test error 0.055626109884660914\n",
            "Loss: 0.20488093864796664\n",
            "training error 0.03517831420800881, test error 0.05572275101123885\n",
            "Loss: 0.37897027551869744\n",
            "training error 0.03524827233021348, test error 0.05567063000927478\n",
            "Loss: 0.2850794964035952\n",
            "training error 0.03514671904679009, test error 0.05575134985929428\n",
            "Loss: 0.43048824379507966\n",
            "training error 0.03522230403995332, test error 0.05585919428932442\n",
            "Loss: 0.6247592128323376\n",
            "training error 0.03516484482282354, test error 0.05586519098351595\n",
            "Loss: 0.6355616584596602\n",
            "training error 0.035145691537905345, test error 0.05580788937836712\n",
            "Loss: 0.5323385401535541\n",
            "training error 0.035187643473362076, test error 0.055623671066300444\n",
            "Loss: 0.20048765096436494\n",
            "training error 0.03515089903888892, test error 0.05578677551254541\n",
            "Loss: 0.4943040197600723\n",
            "training error 0.03510777546562082, test error 0.055726397041474196\n",
            "Loss: 0.3855382348167513\n",
            "training error 0.035152130231090845, test error 0.055434566760160967\n",
            "Loss: 0.0\n",
            "training error 0.035145983894949474, test error 0.055403509769926515\n",
            "Loss: 0.0\n",
            "training error 0.03509735210621722, test error 0.05561990484440551\n",
            "Loss: 0.3905800830626305\n",
            "training error 0.03511783211044151, test error 0.05564373121942207\n",
            "Loss: 0.4335852556870856\n",
            "training error 0.03513530795789583, test error 0.05556198682769431\n",
            "Loss: 0.2860415493998403\n",
            "training error 0.035139019222503294, test error 0.05563689763626575\n",
            "Loss: 0.4212510494523203\n",
            "training error 0.03507461763907657, test error 0.05545451176267666\n",
            "Loss: 0.0920555267381884\n",
            "training error 0.03509175270406854, test error 0.05551438028775237\n",
            "Loss: 0.20011461058382363\n",
            "training error 0.035094302474607715, test error 0.055251180058817184\n",
            "Loss: 0.0\n",
            "training error 0.03507953895082051, test error 0.05537125851931818\n",
            "Loss: 0.21733193820143715\n",
            "training error 0.03507409456087567, test error 0.05554674180928061\n",
            "Loss: 0.5349419689295143\n",
            "training error 0.03503022537232153, test error 0.05535632411237497\n",
            "Loss: 0.19030191472082514\n",
            "training error 0.03509772670872049, test error 0.05540011541669741\n",
            "Loss: 0.26956050119053554\n",
            "training error 0.03504011267022644, test error 0.05544144159114016\n",
            "Loss: 0.3443574094172064\n",
            "training error 0.035060967761877435, test error 0.05524740209836615\n",
            "Loss: 0.0\n",
            "training error 0.03500858545749542, test error 0.05544378401224168\n",
            "Loss: 0.3554590920417988\n",
            "training error 0.034999784921929816, test error 0.05533126581817878\n",
            "Loss: 0.15179667573022027\n",
            "training error 0.035027032512153465, test error 0.05542702936250652\n",
            "Loss: 0.3251325081685241\n",
            "training error 0.03502156610138937, test error 0.05547448041933671\n",
            "Loss: 0.4110208124650905\n",
            "training error 0.035006323836086276, test error 0.05549364610796645\n",
            "Loss: 0.44571147284331314\n",
            "training error 0.03501536159527711, test error 0.055393778491172135\n",
            "Loss: 0.26494710564917145\n",
            "training error 0.03498348980611006, test error 0.05515864976829069\n",
            "Loss: 0.0\n",
            "training error 0.034997837163876694, test error 0.055287379002852126\n",
            "Loss: 0.23337995962953162\n",
            "training error 0.03495525950690604, test error 0.055126409576724836\n",
            "Loss: 0.0\n",
            "training error 0.034952232579752635, test error 0.055230637171046176\n",
            "Loss: 0.18907016640776764\n",
            "training error 0.034936731623284555, test error 0.05520736909831585\n",
            "Loss: 0.14686158995778076\n",
            "training error 0.034963329724648856, test error 0.05493802453194628\n",
            "Loss: 0.0\n",
            "training error 0.03492512408015126, test error 0.05499098959347023\n",
            "Loss: 0.09640874781209341\n",
            "training error 0.034943041666405814, test error 0.05496691387478182\n",
            "Loss: 0.05258533243899244\n",
            "training error 0.034924593062234506, test error 0.054963048628490926\n",
            "Loss: 0.04554968395358294\n",
            "training error 0.034935166553391996, test error 0.05508069390918552\n",
            "Loss: 0.2596914950159457\n",
            "training error 0.034988894754355605, test error 0.05524464925543701\n",
            "Loss: 0.5581284112471607\n",
            "training error 0.03494221017060231, test error 0.055226636544294806\n",
            "Loss: 0.5253410817141813\n",
            "training error 0.03492972339857082, test error 0.05523807469378152\n",
            "Loss: 0.5461611777845432\n",
            "training error 0.03490479073397209, test error 0.05516660314626908\n",
            "Loss: 0.41606631521649007\n",
            "training error 0.034932925466588306, test error 0.055081620485423866\n",
            "Loss: 0.26137807957415937\n",
            "training error 0.034916768611746675, test error 0.055031382273154815\n",
            "Loss: 0.1699328325033722\n",
            "training error 0.03489112772237233, test error 0.05522839692579659\n",
            "Loss: 0.5285453860494282\n",
            "training error 0.03495314735345403, test error 0.05516120620215136\n",
            "Loss: 0.4062426199458624\n",
            "training error 0.03488648731292388, test error 0.055241626922879566\n",
            "Loss: 0.552627062075639\n",
            "training error 0.035021718879111874, test error 0.0553313667129838\n",
            "Loss: 0.7159743809295405\n",
            "training error 0.03487747388762307, test error 0.055099970821446256\n",
            "Loss: 0.29477996502369486\n",
            "training error 0.034938849513291, test error 0.05526517559892317\n",
            "Loss: 0.5954911370842142\n",
            "training error 0.034945323924356855, test error 0.055020788234431295\n",
            "Loss: 0.1506492146198024\n",
            "training error 0.03486235677062853, test error 0.055078438472726705\n",
            "Loss: 0.25558607535800526\n",
            "training error 0.03490156824733317, test error 0.054904139480422914\n",
            "Loss: 0.0\n",
            "training error 0.03488132993194527, test error 0.05504867826090462\n",
            "Loss: 0.2632566175329032\n",
            "training error 0.034914508564731626, test error 0.05506936299962514\n",
            "Loss: 0.30093089658775884\n",
            "training error 0.03496749612813754, test error 0.05494993659236463\n",
            "Loss: 0.08341285807429433\n",
            "training error 0.03487316619654733, test error 0.05495540746012487\n",
            "Loss: 0.09337725750211412\n",
            "training error 0.03485052025565828, test error 0.05498599060835359\n",
            "Loss: 0.14908006701364052\n",
            "training error 0.03485973314750769, test error 0.05496271883775124\n",
            "Loss: 0.10669388115847056\n",
            "training error 0.034874912519116646, test error 0.0549713428118994\n",
            "Loss: 0.12240121075106636\n",
            "training error 0.034894297861419815, test error 0.05509484242390107\n",
            "Loss: 0.3473380063558906\n",
            "training error 0.03484732269884035, test error 0.05500804553554914\n",
            "Loss: 0.18924994747122614\n",
            "training error 0.034836148358410916, test error 0.055014075179447154\n",
            "Loss: 0.20023207733441684\n",
            "training error 0.034899621989528895, test error 0.05509565972607787\n",
            "Loss: 0.3488266048195632\n",
            "training error 0.034904797812749896, test error 0.05493622869495895\n",
            "Loss: 0.05844589285928237\n",
            "training error 0.03484001372116801, test error 0.05488390241431623\n",
            "Loss: 0.0\n",
            "training error 0.03484798861141783, test error 0.05470132483469728\n",
            "Loss: 0.0\n",
            "training error 0.03482654551170881, test error 0.054832185170145924\n",
            "Loss: 0.23922699467351372\n",
            "training error 0.034824793444101956, test error 0.05485979104203861\n",
            "Loss: 0.28969354548578075\n",
            "training error 0.034815585366152806, test error 0.05479940663272374\n",
            "Loss: 0.1793042459627081\n",
            "training error 0.034802879638538925, test error 0.05496632686807251\n",
            "Loss: 0.48445267857049057\n",
            "training error 0.03481549331257831, test error 0.054920402445268905\n",
            "Loss: 0.4004978146940008\n",
            "training error 0.03480396780446913, test error 0.05499173466454141\n",
            "Loss: 0.5309009072846482\n",
            "training error 0.034816048294560774, test error 0.054940368826769496\n",
            "Loss: 0.4369985421643685\n",
            "training error 0.034783751466625504, test error 0.05496387807996374\n",
            "Loss: 0.47997602628431224\n",
            "training error 0.03484840569414812, test error 0.055092130186866574\n",
            "Loss: 0.7144348941278356\n",
            "training error 0.0348413848691117, test error 0.05500649171999374\n",
            "Loss: 0.5578784174216089\n",
            "training error 0.03479224669726102, test error 0.055016864805021576\n",
            "Loss: 0.5768415505069235\n",
            "training error 0.03481322565029931, test error 0.05498795339533814\n",
            "Loss: 0.5239883339334561\n",
            "training error 0.03481580853336709, test error 0.054784628792721474\n",
            "Loss: 0.1522887393969663\n",
            "training error 0.03482885963648677, test error 0.054791381548793296\n",
            "Loss: 0.16463351549191874\n",
            "training error 0.03483296644489669, test error 0.05499426852149198\n",
            "Loss: 0.5355330747106146\n",
            "training error 0.034787890460463086, test error 0.05491560070335956\n",
            "Loss: 0.39171970571061365\n",
            "training error 0.03477706568027426, test error 0.05486204830061632\n",
            "Loss: 0.29382006085727497\n",
            "training error 0.034780719581234354, test error 0.054928585575339424\n",
            "Loss: 0.4154574707813774\n",
            "training error 0.03477582913280702, test error 0.05492975913878709\n",
            "Loss: 0.4176028730201953\n",
            "training error 0.03477368391944351, test error 0.054684674089315204\n",
            "Loss: 0.0\n",
            "training error 0.034767947240360114, test error 0.054692131733275404\n",
            "Loss: 0.013637539373489993\n",
            "training error 0.03475199577227288, test error 0.05483818189276823\n",
            "Loss: 0.2807144890400348\n",
            "training error 0.03477782901135944, test error 0.05493601954508758\n",
            "Loss: 0.45962686979146294\n",
            "training error 0.03477567557660677, test error 0.054968589145235024\n",
            "Loss: 0.5191857876964079\n",
            "training error 0.03473875115066933, test error 0.05477524059193093\n",
            "Loss: 0.1656158770696914\n",
            "training error 0.03473015195311126, test error 0.054939476609180034\n",
            "Loss: 0.4659486850898453\n",
            "training error 0.03475178861114297, test error 0.05491330279281927\n",
            "Loss: 0.41808551904443103\n",
            "training error 0.03479499034630974, test error 0.05489409439751239\n",
            "Loss: 0.3829597811173535\n",
            "training error 0.03476809113198894, test error 0.05486930578810292\n",
            "Loss: 0.3376296958196523\n",
            "training error 0.03472839769966503, test error 0.054789702340593445\n",
            "Loss: 0.1920615840312001\n",
            "training error 0.034821797950770214, test error 0.05492928442547988\n",
            "Loss: 0.44731058607967444\n",
            "training error 0.03472478318718448, test error 0.054898074177606546\n",
            "Loss: 0.39023746935531456\n",
            "training error 0.034741455318792636, test error 0.05482116926043961\n",
            "Loss: 0.24960406804559465\n",
            "training error 0.03473506722202588, test error 0.05471803277019886\n",
            "Loss: 0.061001882957500975\n",
            "training error 0.03485535314438212, test error 0.05476099181722204\n",
            "Loss: 0.13955962831961877\n",
            "training error 0.03476461270764257, test error 0.054805843390701735\n",
            "Loss: 0.22157817232051347\n",
            "training error 0.034716183925107835, test error 0.05480778015839621\n",
            "Loss: 0.22511987340354533\n",
            "training error 0.03473874649793062, test error 0.05484421361574209\n",
            "Loss: 0.2917444952973858\n",
            "training error 0.034721888248909726, test error 0.0549074364990357\n",
            "Loss: 0.4073580275099742\n",
            "training error 0.03472946240727414, test error 0.054553904175369886\n",
            "Loss: 0.0\n",
            "training error 0.03472302488814193, test error 0.05442555726849991\n",
            "Loss: 0.0\n",
            "training error 0.0347272053114701, test error 0.054603346933187004\n",
            "Loss: 0.3266657680875973\n",
            "training error 0.034678509363415606, test error 0.054576045005916896\n",
            "Loss: 0.276501968872056\n",
            "training error 0.03472274575811859, test error 0.05454983366531035\n",
            "Loss: 0.22834198315571097\n",
            "training error 0.034694492582250894, test error 0.05457135391524663\n",
            "Loss: 0.26788268979489605\n",
            "training error 0.03472080757581588, test error 0.05445959588593081\n",
            "Loss: 0.06254160570735579\n",
            "training error 0.034702309836110765, test error 0.054453283591886704\n",
            "Loss: 0.05094357279615824\n",
            "training error 0.03469117123178288, test error 0.05458082496742315\n",
            "Loss: 0.28528453674301346\n",
            "training error 0.03470331974159977, test error 0.05464969678006302\n",
            "Loss: 0.4118276831918344\n",
            "training error 0.03469025046814058, test error 0.05470961765421882\n",
            "Loss: 0.5219246250755649\n",
            "training error 0.034711586469365015, test error 0.054514357291920904\n",
            "Loss: 0.16315868477545514\n",
            "training error 0.0346900210376289, test error 0.054573943924987676\n",
            "Loss: 0.27264150141030896\n",
            "training error 0.03470107744516396, test error 0.05463084058875696\n",
            "Loss: 0.3771818435304608\n",
            "training error 0.03466912727987788, test error 0.054602931397106653\n",
            "Loss: 0.3259022736904704\n",
            "training error 0.03467912478090511, test error 0.05456571321736167\n",
            "Loss: 0.25751862892340327\n",
            "training error 0.03467803720852911, test error 0.05436783980765686\n",
            "Loss: 0.0\n",
            "training error 0.03467856903355106, test error 0.05432335576953196\n",
            "Loss: 0.0\n",
            "training error 0.034728879858363725, test error 0.0543709675912457\n",
            "Loss: 0.08764521454773799\n",
            "training error 0.034701509025756805, test error 0.054473911568935836\n",
            "Loss: 0.2771474576103383\n",
            "training error 0.034683340469674466, test error 0.05436948075161298\n",
            "Loss: 0.08490819727098309\n",
            "training error 0.03467108957492441, test error 0.05425728941282298\n",
            "Loss: 0.0\n",
            "training error 0.0347453150886477, test error 0.05441983199067455\n",
            "Loss: 0.29957740169224145\n",
            "training error 0.03464406670815879, test error 0.05449099287048625\n",
            "Loss: 0.4307319075324889\n",
            "training error 0.03472416065035687, test error 0.05424005535630343\n",
            "Loss: 0.0\n",
            "training error 0.03464050453788304, test error 0.05431395664247528\n",
            "Loss: 0.13624854489249216\n",
            "training error 0.034679013190210245, test error 0.05421983670144678\n",
            "Loss: 0.0\n",
            "training error 0.034652072416559355, test error 0.05426337089303972\n",
            "Loss: 0.08029200056918206\n",
            "training error 0.03465832757330001, test error 0.05431423890636772\n",
            "Loss: 0.17411008712686638\n",
            "training error 0.03466252155519162, test error 0.05441632013316867\n",
            "Loss: 0.362382927864191\n",
            "training error 0.034741073988786356, test error 0.054354048350711004\n",
            "Loss: 0.24753237454999688\n",
            "training error 0.03465534313708408, test error 0.05436807414412635\n",
            "Loss: 0.27340075459063584\n",
            "training error 0.0346544350004437, test error 0.054530857549282545\n",
            "Loss: 0.5736292596164594\n",
            "training error 0.03466253847686955, test error 0.05454916600655907\n",
            "Loss: 0.6073963426442841\n",
            "training error 0.034633497429617306, test error 0.054534154046769215\n",
            "Loss: 0.5797091331225745\n",
            "training error 0.03466569586827529, test error 0.054630810204405154\n",
            "Loss: 0.7579762831476877\n",
            "training error 0.0346692483616984, test error 0.05444724247002892\n",
            "Loss: 0.41941433692305186\n",
            "training error 0.03463400525640158, test error 0.0545016104626699\n",
            "Loss: 0.5196875873578533\n",
            "training error 0.03464774957153348, test error 0.05454648933413095\n",
            "Loss: 0.6024596394172743\n",
            "training error 0.03465752705580823, test error 0.05460391924936871\n",
            "Loss: 0.7083801266994039\n",
            "training error 0.03466669686422401, test error 0.054501607510781744\n",
            "Loss: 0.5196821430623055\n",
            "training error 0.03465657875585427, test error 0.05460877574868511\n",
            "Loss: 0.7173371793426142\n",
            "training error 0.0346604086079456, test error 0.054533429854606554\n",
            "Loss: 0.5783734740599122\n",
            "training error 0.0346777411306391, test error 0.054684728577199\n",
            "Loss: 0.8574202801680642\n",
            "training error 0.034628639346067816, test error 0.05452910788646953\n",
            "Loss: 0.5704022804895237\n",
            "training error 0.034646997781788515, test error 0.054527552634857396\n",
            "Loss: 0.5675338623851056\n",
            "training error 0.03465257504217477, test error 0.05458109320772839\n",
            "Loss: 0.6662810665970964\n",
            "training error 0.03463811169246237, test error 0.054463060194354006\n",
            "Loss: 0.4485876529774391\n",
            "training error 0.03466577438523345, test error 0.054586396637019204\n",
            "Loss: 0.6760624116056047\n",
            "training error 0.03465964632778488, test error 0.05454071452485102\n",
            "Loss: 0.5918089078192912\n",
            "training error 0.03463549922260144, test error 0.05447764689745738\n",
            "Loss: 0.4754905431201939\n",
            "training error 0.03467011552004953, test error 0.05439650809945342\n",
            "Loss: 0.32584273349891557\n",
            "training error 0.03470349708288006, test error 0.05459013392556222\n",
            "Loss: 0.6829552552037876\n",
            "training error 0.03464381535617265, test error 0.05445465167088597\n",
            "Loss: 0.43307944790051955\n",
            "training error 0.03464461432314528, test error 0.05433422694667844\n",
            "Loss: 0.21097489810146808\n",
            "training error 0.03463343169911065, test error 0.05444227107910119\n",
            "Loss: 0.41024538469049965\n",
            "training error 0.034651199598759744, test error 0.05458522098743924\n",
            "Loss: 0.6738941100180584\n",
            "training error 0.03462194622876098, test error 0.05444722009341209\n",
            "Loss: 0.4193730667566564\n",
            "training error 0.03463605390862645, test error 0.05455043490772391\n",
            "Loss: 0.6097366321804243\n",
            "training error 0.034627487332467066, test error 0.054570343190488986\n",
            "Loss: 0.6464543428491121\n",
            "training error 0.034618382601001196, test error 0.05462920425288494\n",
            "Loss: 0.7550143570005163\n",
            "training error 0.034671796382649965, test error 0.05470330484970801\n",
            "Loss: 0.8916813064623774\n",
            "training error 0.03461327910663557, test error 0.05456261968039822\n",
            "Loss: 0.6322095376991133\n",
            "training error 0.034620736921294075, test error 0.054566360807167605\n",
            "Loss: 0.6391094603049208\n",
            "training error 0.034646571150905095, test error 0.05462370243679338\n",
            "Loss: 0.7448671185979805\n",
            "training error 0.03464062574509773, test error 0.05443798209259539\n",
            "Loss: 0.4023350205752063\n",
            "training error 0.03461863744575094, test error 0.05450310164380081\n",
            "Loss: 0.5224378374907079\n",
            "training error 0.03463486141075081, test error 0.054460873026651084\n",
            "Loss: 0.4445537645779485\n",
            "training error 0.034641991269796654, test error 0.05457617793758531\n",
            "Loss: 0.6572156203654256\n",
            "training error 0.0346395295439576, test error 0.05440452550047948\n",
            "Loss: 0.34062957446674424\n",
            "training error 0.03464868091673287, test error 0.054423504858576485\n",
            "Loss: 0.3756340290199889\n",
            "training error 0.0346056351290975, test error 0.05441594543020697\n",
            "Loss: 0.36169184691579037\n",
            "training error 0.034631642796197896, test error 0.05428635873856208\n",
            "Loss: 0.12268948259950285\n",
            "training error 0.03462019064399206, test error 0.05437769121078426\n",
            "Loss: 0.2911379283686788\n",
            "training error 0.03460331232828636, test error 0.054530818322150926\n",
            "Loss: 0.5735569113136885\n",
            "training error 0.034663239710984346, test error 0.05460158220054673\n",
            "Loss: 0.7040698060415851\n",
            "training error 0.03463678851515666, test error 0.05441516758767446\n",
            "Loss: 0.3602572381455893\n",
            "training error 0.03459127794730945, test error 0.05443412648094288\n",
            "Loss: 0.3952239485265352\n",
            "training error 0.034608274567973836, test error 0.05442354081334275\n",
            "Loss: 0.3757003419571969\n",
            "training error 0.034674771907761824, test error 0.054545411977687745\n",
            "Loss: 0.600472624131454\n",
            "training error 0.034629201187244815, test error 0.05436789098661736\n",
            "Loss: 0.2730629492409209\n",
            "training error 0.034637286228794466, test error 0.05452933150238314\n",
            "Loss: 0.5708147050323076\n",
            "training error 0.03461918814205464, test error 0.05456000185232147\n",
            "Loss: 0.6273813636653802\n",
            "training error 0.034649157821789005, test error 0.05439541956609055\n",
            "Loss: 0.323835104134651\n",
            "training error 0.034605935022537636, test error 0.054598532802770106\n",
            "Loss: 0.6984456692640917\n",
            "training error 0.03466109065279408, test error 0.054513867648600875\n",
            "Loss: 0.5422940477912697\n",
            "training error 0.03460479168475572, test error 0.054553366915734085\n",
            "Loss: 0.6151442619125369\n",
            "training error 0.034640907848379074, test error 0.05440928710536154\n",
            "Loss: 0.34941160918269265\n",
            "training error 0.03460460748852945, test error 0.054610584272978026\n",
            "Loss: 0.7206727192537121\n",
            "training error 0.034628148268379115, test error 0.0546201506701191\n",
            "Loss: 0.7383164410409204\n",
            "training error 0.034621732492676784, test error 0.054684896291809025\n",
            "Loss: 0.8577296035084281\n",
            "training error 0.034593069421882165, test error 0.05461206198136906\n",
            "Loss: 0.7233981210271967\n",
            "training error 0.03465217147984543, test error 0.054495425369410815\n",
            "Loss: 0.5082801511954393\n",
            "training error 0.03464315039568674, test error 0.0547058109180829\n",
            "Loss: 0.8963033572234158\n",
            "training error 0.03459202185919874, test error 0.05460550818228489\n",
            "Loss: 0.7113106646959411\n",
            "training error 0.03460777454688291, test error 0.054477600521751574\n",
            "Loss: 0.4754050103915386\n",
            "training error 0.03461227469752092, test error 0.05447270076962586\n",
            "Loss: 0.4663681847133505\n",
            "training error 0.034604634449184685, test error 0.054473000440782175\n",
            "Loss: 0.46692088124389297\n",
            "training error 0.034595987575359366, test error 0.05420391894105844\n",
            "Loss: 0.0\n",
            "training error 0.034611668979134744, test error 0.054332388536902615\n",
            "Loss: 0.23701163744980214\n",
            "training error 0.03464416213721648, test error 0.05448717283459886\n",
            "Loss: 0.5225708750845603\n",
            "training error 0.034597540943103675, test error 0.0542956905890541\n",
            "Loss: 0.16930814189919374\n",
            "training error 0.03458898928425921, test error 0.054318867913682845\n",
            "Loss: 0.21206764173158366\n",
            "training error 0.034609575227231903, test error 0.05436603208966776\n",
            "Loss: 0.29908012515773574\n",
            "training error 0.034637890092275414, test error 0.054499773809615876\n",
            "Loss: 0.5458182255772881\n",
            "training error 0.03459947555563935, test error 0.054269448585870086\n",
            "Loss: 0.12089466240052804\n",
            "training error 0.03458406593392927, test error 0.05432495562484396\n",
            "Loss: 0.22329876907449808\n",
            "training error 0.03460299190731128, test error 0.05441000659524708\n",
            "Loss: 0.3802080333208613\n",
            "training error 0.03461185684831786, test error 0.05425175228710168\n",
            "Loss: 0.08824702526630812\n",
            "training error 0.034601981393203175, test error 0.054215315427367246\n",
            "Loss: 0.021025207275493507\n",
            "training error 0.034628860805775326, test error 0.054398441681337124\n",
            "Loss: 0.35887209648108875\n",
            "training error 0.034633993218239435, test error 0.05438299294018293\n",
            "Loss: 0.33037094480052875\n",
            "training error 0.03459295538866295, test error 0.05436348970207252\n",
            "Loss: 0.2943897122781758\n",
            "training error 0.03457055249588943, test error 0.05441555926470496\n",
            "Loss: 0.39045207022145867\n",
            "training error 0.03457489127470039, test error 0.054424691032719186\n",
            "Loss: 0.4072991325605235\n",
            "training error 0.03460809881745821, test error 0.05455405674044147\n",
            "Loss: 0.6459639934222494\n",
            "training error 0.03460236313989087, test error 0.054535286047416066\n",
            "Loss: 0.611334222379667\n",
            "training error 0.034599873091774494, test error 0.054393046836056315\n",
            "Loss: 0.34891922704616274\n",
            "training error 0.034551768338441494, test error 0.05457468925060864\n",
            "Loss: 0.6840286030856513\n",
            "training error 0.03457248654876872, test error 0.054581746456948105\n",
            "Loss: 0.6970483375943859\n",
            "training error 0.034552254474314904, test error 0.05464290305907599\n",
            "Loss: 0.8098752389009034\n",
            "training error 0.03458038848648603, test error 0.05467123872652121\n",
            "Loss: 0.8621512883061566\n",
            "training error 0.03456052747957607, test error 0.05467655890674173\n",
            "Loss: 0.8719664092871948\n",
            "training error 0.034619439802241206, test error 0.054810843974486684\n",
            "Loss: 1.1197069239370139\n",
            "training error 0.03461710284377067, test error 0.054498616206481554\n",
            "Loss: 0.543682580854643\n",
            "training error 0.03456345619709962, test error 0.054437734602216604\n",
            "Loss: 0.43136301899575624\n",
            "training error 0.03457041009949876, test error 0.05450183932001893\n",
            "Loss: 0.5496288548517025\n",
            "training error 0.034547639919504707, test error 0.054486174161490875\n",
            "Loss: 0.5207284379923793\n",
            "training error 0.03461773933703052, test error 0.054270816790868086\n",
            "Loss: 0.12341884335409503\n",
            "training error 0.034567283874447574, test error 0.0543977275958895\n",
            "Loss: 0.35755469091045544\n",
            "training error 0.03457295566423936, test error 0.05460575581507644\n",
            "Loss: 0.7413428435957936\n",
            "training error 0.03461146703614997, test error 0.054563088882019974\n",
            "Loss: 0.6626272564389524\n",
            "training error 0.03456335927880748, test error 0.05465442185540968\n",
            "Loss: 0.8311260941134435\n",
            "training error 0.034569993841685714, test error 0.054680853697369775\n",
            "Loss: 0.8798898043330672\n",
            "training error 0.03455721934583245, test error 0.054735729817178336\n",
            "Loss: 0.9811299376677685\n",
            "training error 0.03462721242878267, test error 0.05467854895833762\n",
            "Loss: 0.8756378257359865\n",
            "training error 0.034568923310338874, test error 0.05475017389726221\n",
            "Loss: 1.0077776051539145\n",
            "training error 0.03456584344058177, test error 0.054806539121263306\n",
            "Loss: 1.111764964559403\n",
            "training error 0.034571277434565825, test error 0.0547342307141485\n",
            "Loss: 0.9783642648914626\n",
            "training error 0.03455757009885769, test error 0.05462547597472081\n",
            "Loss: 0.7777242714143373\n",
            "training error 0.034592463200195676, test error 0.05445006556096078\n",
            "Loss: 0.45411222050197875\n",
            "training error 0.034546979127822425, test error 0.05448451216370863\n",
            "Loss: 0.5176622431217526\n",
            "training error 0.034534686209252416, test error 0.05450617568042347\n",
            "Loss: 0.5576289413570024\n",
            "training error 0.03461516127427797, test error 0.05427774530898713\n",
            "Loss: 0.13620116288817208\n",
            "training error 0.03453379658139196, test error 0.054445669598724974\n",
            "Loss: 0.4460021754689292\n",
            "training error 0.03453625884489778, test error 0.05449333340099537\n",
            "Loss: 0.5339364119624435\n",
            "training error 0.03453101329665653, test error 0.05446377226946197\n",
            "Loss: 0.4793995221749281\n",
            "training error 0.03453616562996169, test error 0.05447671719093982\n",
            "Loss: 0.5032814143531184\n",
            "training error 0.03452915960495164, test error 0.05452970586323791\n",
            "Loss: 0.6010394239828409\n",
            "training error 0.03456616951539645, test error 0.05444627573595993\n",
            "Loss: 0.447120428995218\n",
            "training error 0.03454453738243871, test error 0.054603351012881086\n",
            "Loss: 0.7369062599643206\n",
            "training error 0.03457249738102038, test error 0.054576781043631034\n",
            "Loss: 0.6878877207717071\n",
            "training error 0.03454851858905364, test error 0.05433894781051175\n",
            "Loss: 0.24911274330576827\n",
            "training error 0.034567584162170734, test error 0.05430853947003744\n",
            "Loss: 0.19301285040433935\n",
            "training error 0.034585648327488866, test error 0.05462621128217569\n",
            "Loss: 0.7790808291489926\n",
            "training error 0.03460611835598248, test error 0.05430119631171313\n",
            "Loss: 0.17946556735217722\n",
            "training error 0.03453874063364198, test error 0.05415247833912792\n",
            "Loss: 0.0\n",
            "training error 0.0345181350812134, test error 0.05419572246606983\n",
            "Loss: 0.07985622868651454\n",
            "training error 0.034531647183947974, test error 0.05421433502782396\n",
            "Loss: 0.11422688414861604\n",
            "training error 0.03468583952501479, test error 0.0545029077784656\n",
            "Loss: 0.6471161617814358\n",
            "training error 0.03452310075899901, test error 0.05424790384304444\n",
            "Loss: 0.17621631888926625\n",
            "training error 0.034574387775325816, test error 0.05420958578900417\n",
            "Loss: 0.10545676140365501\n",
            "training error 0.03455291039782117, test error 0.05430540431942941\n",
            "Loss: 0.28239885780259044\n",
            "training error 0.03462698522215968, test error 0.05431683661723067\n",
            "Loss: 0.30351016822067756\n",
            "training error 0.034541180954240885, test error 0.05451920445105712\n",
            "Loss: 0.6772102093510757\n",
            "training error 0.03456053762736127, test error 0.05469095936132972\n",
            "Loss: 0.9943792762901582\n",
            "training error 0.03455611512153613, test error 0.05454302952940045\n",
            "Loss: 0.7212064936838569\n",
            "training error 0.034531283429780006, test error 0.054593387476120185\n",
            "Loss: 0.8141993691056726\n",
            "training error 0.034534455483699204, test error 0.05452703823137371\n",
            "Loss: 0.6916763622527577\n",
            "training error 0.03456210617209405, test error 0.054608408226869694\n",
            "Loss: 0.8419372514892665\n",
            "training error 0.034548158379131104, test error 0.054524665484492356\n",
            "Loss: 0.6872947587617917\n",
            "training error 0.034539868129879386, test error 0.05453750559960632\n",
            "Loss: 0.711005797494968\n",
            "training error 0.03453812715584492, test error 0.05466970241386673\n",
            "Loss: 0.9551253988778097\n",
            "training error 0.03455966307439896, test error 0.054413590579483674\n",
            "Loss: 0.4821796681595103\n",
            "training error 0.03454292328696049, test error 0.054356854271727456\n",
            "Loss: 0.3774082717316052\n",
            "training error 0.03458007366805411, test error 0.054504924208737454\n",
            "Loss: 0.6508397776411323\n",
            "training error 0.034553231182748896, test error 0.05460082517797922\n",
            "Loss: 0.8279341086543601\n",
            "training error 0.03456257682505985, test error 0.0544387598170449\n",
            "Loss: 0.5286581273790558\n",
            "training error 0.03456495599720452, test error 0.0545472034337527\n",
            "Loss: 0.728914182196494\n",
            "training error 0.03453171419917225, test error 0.054576830163467954\n",
            "Loss: 0.7836240138125339\n",
            "training error 0.03453152392157963, test error 0.05472625833544368\n",
            "Loss: 1.0595636874133119\n",
            "training error 0.03460383774415497, test error 0.054806705744097226\n",
            "Loss: 1.208120893142195\n",
            "training error 0.03452596244828304, test error 0.05456648205190439\n",
            "Loss: 0.7645148024136406\n",
            "training error 0.034534905327201396, test error 0.054650125726868316\n",
            "Loss: 0.9189743535354067\n",
            "training error 0.03452491518439393, test error 0.054595552955631656\n",
            "Loss: 0.8181982248882447\n",
            "training error 0.03462809152405652, test error 0.05448404470289322\n",
            "Loss: 0.6122828980954065\n",
            "training error 0.034524988864512815, test error 0.05459407567801969\n",
            "Loss: 0.8154702285762205\n",
            "training error 0.03453271926728044, test error 0.05456131453398942\n",
            "Loss: 0.7549722697845596\n",
            "training error 0.03456710935824818, test error 0.054507808985509505\n",
            "Loss: 0.6561669147556737\n",
            "training error 0.034533471344792996, test error 0.054495132301938774\n",
            "Loss: 0.6327576748473174\n",
            "training error 0.03465429214889385, test error 0.05471997692916249\n",
            "Loss: 1.047964206699148\n",
            "training error 0.03462351656109546, test error 0.05456627846193823\n",
            "Loss: 0.7641388455370324\n",
            "training error 0.034523552820437134, test error 0.05448738279466472\n",
            "Loss: 0.6184471437105321\n",
            "training error 0.03453534158863475, test error 0.05447776587667547\n",
            "Loss: 0.6006881818232834\n",
            "training error 0.0345404929898566, test error 0.054494277893547174\n",
            "Loss: 0.631179892227185\n",
            "training error 0.03450116821218147, test error 0.05449875391732649\n",
            "Loss: 0.6394454858188237\n",
            "training error 0.034537957633199404, test error 0.054537021541732116\n",
            "Loss: 0.710111918047418\n",
            "training error 0.03452585098135716, test error 0.05452406721238745\n",
            "Loss: 0.68618996702694\n",
            "training error 0.03453099528503257, test error 0.054810504295098386\n",
            "Loss: 1.2151354400616876\n",
            "training error 0.034531039768824184, test error 0.05479905768545656\n",
            "Loss: 1.1939977008613711\n",
            "training error 0.034515060053805946, test error 0.054873026284925866\n",
            "Loss: 1.3305908942625733\n",
            "training error 0.03460166819714828, test error 0.05462994105951151\n",
            "Loss: 0.8817005888327012\n",
            "training error 0.0345201644793883, test error 0.054721212066953503\n",
            "Loss: 1.0502450585251344\n",
            "training error 0.034529856615418526, test error 0.05487263914424772\n",
            "Loss: 1.329875985748652\n",
            "training error 0.03453645346928902, test error 0.054680982136735515\n",
            "Loss: 0.9759549586961791\n",
            "training error 0.03464201577755537, test error 0.05464163955031645\n",
            "Loss: 0.9033034612472957\n",
            "training error 0.034515276134099, test error 0.054664046241179716\n",
            "Loss: 0.944680497997008\n",
            "training error 0.03450267446897704, test error 0.0547498025867519\n",
            "Loss: 1.1030413859976163\n",
            "training error 0.03455604437246133, test error 0.05470979345583014\n",
            "Loss: 1.0291590224404068\n",
            "training error 0.03453929925872312, test error 0.05462414505565528\n",
            "Loss: 0.8709974704639745\n",
            "training error 0.03456426161118277, test error 0.054463310139663906\n",
            "Loss: 0.5739936750251973\n",
            "training error 0.03451907906658055, test error 0.054422241091077406\n",
            "Loss: 0.49815402770692074\n",
            "training error 0.03453980655625571, test error 0.05417197790435132\n",
            "Loss: 0.03600862937664928\n",
            "training error 0.034496631748002146, test error 0.05434519149884232\n",
            "Loss: 0.35587135736898645\n",
            "training error 0.034499008802526605, test error 0.05418775092042995\n",
            "Loss: 0.06513567316557278\n",
            "training error 0.0344824126744689, test error 0.054083096777715785\n",
            "Loss: 0.0\n",
            "training error 0.03452824646780507, test error 0.0542030596827042\n",
            "Loss: 0.2218121966674147\n",
            "training error 0.0345002573470405, test error 0.05408963080832855\n",
            "Loss: 0.0120814653783885\n",
            "training error 0.03447105755955155, test error 0.05415948868142763\n",
            "Loss: 0.14124913006705597\n",
            "training error 0.03449547639094433, test error 0.054287108981662384\n",
            "Loss: 0.37721990067451916\n",
            "training error 0.03446250280319778, test error 0.05424048421857295\n",
            "Loss: 0.29101040849053295\n",
            "training error 0.03454624142636451, test error 0.054151522676395406\n",
            "Loss: 0.12651993461256783\n",
            "training error 0.034485119338237405, test error 0.054345404028711346\n",
            "Loss: 0.48500782429981015\n",
            "training error 0.03459784086051443, test error 0.05433406638524108\n",
            "Loss: 0.464044447300771\n",
            "training error 0.034503144040759645, test error 0.054280851848284926\n",
            "Loss: 0.36565042009693016\n",
            "training error 0.03448578722254033, test error 0.054382109160334924\n",
            "Loss: 0.5528758529639966\n",
            "training error 0.03448526444729282, test error 0.05447870197534263\n",
            "Loss: 0.731476600263492\n",
            "training error 0.03448559065397561, test error 0.05450946786931088\n",
            "Loss: 0.788362939621412\n",
            "training error 0.034475150711842636, test error 0.05448606952463525\n",
            "Loss: 0.745099247137615\n",
            "training error 0.03451396257672855, test error 0.0542910861967213\n",
            "Loss: 0.384573797355503\n",
            "training error 0.03449346452890342, test error 0.054310782640871545\n",
            "Loss: 0.42099265153316967\n",
            "training error 0.034495277849452013, test error 0.05438250861201268\n",
            "Loss: 0.5536144417311872\n",
            "training error 0.03447809615569254, test error 0.054375467519069284\n",
            "Loss: 0.5405954147839509\n",
            "training error 0.034470065085788607, test error 0.0543839362137237\n",
            "Loss: 0.5562540866407506\n",
            "training error 0.03452398501804866, test error 0.05433730423770115\n",
            "Loss: 0.470031257696224\n",
            "training error 0.03452102884430902, test error 0.05429638713438584\n",
            "Loss: 0.3943752658001287\n",
            "training error 0.03460257742548599, test error 0.05421453346843163\n",
            "Loss: 0.24302730159120323\n",
            "training error 0.03449172874099158, test error 0.0541516200403363\n",
            "Loss: 0.12669996117669058\n",
            "training error 0.03449290952289073, test error 0.054275489052585085\n",
            "Loss: 0.3557345757400743\n",
            "training error 0.03448742866597987, test error 0.05440300815924145\n",
            "Loss: 0.591518238758626\n",
            "training error 0.034501895352338934, test error 0.054333420941198564\n",
            "Loss: 0.46285101704073295\n",
            "training error 0.03453068189475209, test error 0.054213800371725324\n",
            "Loss: 0.2416718009820018\n",
            "training error 0.034522055186118526, test error 0.05419245880231136\n",
            "Loss: 0.2022110994218007\n",
            "training error 0.03452255163206934, test error 0.054218568948435755\n",
            "Loss: 0.25048893053734034\n",
            "training error 0.03453635247002759, test error 0.05442142438949044\n",
            "Loss: 0.6255699690518757\n",
            "training error 0.03454281991879505, test error 0.05419334714819178\n",
            "Loss: 0.2038536567703142\n",
            "training error 0.03449823583034261, test error 0.054368621896960366\n",
            "Loss: 0.5279378146893166\n",
            "training error 0.034591356220318815, test error 0.05442426565850783\n",
            "Loss: 0.6308234940655577\n",
            "training error 0.0345008062970689, test error 0.05432202390670868\n",
            "Loss: 0.4417778256576188\n",
            "training error 0.03453521988991326, test error 0.05457429721110203\n",
            "Loss: 0.9082328170021414\n",
            "training error 0.034510474027564214, test error 0.05442861252859344\n",
            "Loss: 0.6388608853108879\n",
            "training error 0.03450992107869659, test error 0.05450218648157794\n",
            "Loss: 0.7748996060352065\n",
            "training error 0.034605296895997645, test error 0.05450335983752398\n",
            "Loss: 0.777069148860865\n",
            "training error 0.03461819303943445, test error 0.054095345380028886\n",
            "Loss: 0.022647745863091373\n",
            "training error 0.03451685671355214, test error 0.054109909665727006\n",
            "Loss: 0.04957720546481248\n",
            "training error 0.03449343518553457, test error 0.0540773922931561\n",
            "Loss: 0.0\n",
            "training error 0.034500347067207206, test error 0.054148015706326345\n",
            "Loss: 0.1305969281717445\n",
            "training error 0.034531377805021905, test error 0.05414213729646866\n",
            "Loss: 0.11972656329575226\n",
            "training error 0.03454921868685947, test error 0.05394581844665801\n",
            "Loss: 0.0\n",
            "training error 0.03448744619604636, test error 0.05403201054735596\n",
            "Loss: 0.15977531378670307\n",
            "training error 0.034491837915717354, test error 0.05413967789419894\n",
            "Loss: 0.35935954467467557\n",
            "training error 0.034491401739667, test error 0.0542244145871205\n",
            "Loss: 0.516436951898247\n",
            "training error 0.034477322192439694, test error 0.054147159969733606\n",
            "Loss: 0.37322915635191745\n",
            "training error 0.03447502753510384, test error 0.05429376753390624\n",
            "Loss: 0.6449973274430532\n",
            "training error 0.034481931205771324, test error 0.05426176172433191\n",
            "Loss: 0.5856677806942656\n",
            "training error 0.0344726475398836, test error 0.05430745489792444\n",
            "Loss: 0.6703697555798804\n",
            "training error 0.034499242588223025, test error 0.054068220021628824\n",
            "Loss: 0.22689724337363604\n",
            "training error 0.03446138022014317, test error 0.054151334690586646\n",
            "Loss: 0.38096788564223605\n",
            "training error 0.034486343345117, test error 0.05410423307297275\n",
            "Loss: 0.29365506146019626\n",
            "training error 0.03451695628829154, test error 0.05406948755629156\n",
            "Loss: 0.2292468873298148\n",
            "training error 0.0344952557978024, test error 0.05409194918621743\n",
            "Loss: 0.2708842756810048\n",
            "training error 0.034516254717365115, test error 0.054272243387757366\n",
            "Loss: 0.6050977638278443\n",
            "training error 0.034525131750237206, test error 0.054054670170336626\n",
            "Loss: 0.20177972419910262\n",
            "training error 0.03448542550019766, test error 0.05402767431398628\n",
            "Loss: 0.15173718683907378\n",
            "training error 0.03451482183212856, test error 0.05402506363150864\n",
            "Loss: 0.14689773393463668\n",
            "training error 0.03450825743627914, test error 0.05406996005360374\n",
            "Loss: 0.23012276117098107\n",
            "training error 0.034541381562882424, test error 0.05413326854077083\n",
            "Loss: 0.34747845062017557\n",
            "training error 0.03453855183601143, test error 0.053994429588378225\n",
            "Loss: 0.09011104682428162\n",
            "training error 0.03454316256624697, test error 0.05397146383976805\n",
            "Loss: 0.04753916772139988\n",
            "training error 0.03450948420424785, test error 0.053940737091634806\n",
            "Loss: 0.0\n",
            "training error 0.03450523899755918, test error 0.05394156244654201\n",
            "Loss: 0.001530114254477688\n",
            "training error 0.03448143649568153, test error 0.05403941398316732\n",
            "Loss: 0.18293574921841582\n",
            "training error 0.03447584882344196, test error 0.054115273392583235\n",
            "Loss: 0.32357047819335794\n",
            "training error 0.03449072515569599, test error 0.05413715629572337\n",
            "Loss: 0.36413889516355713\n",
            "training error 0.03451251320666568, test error 0.05421296298472877\n",
            "Loss: 0.5046758864853951\n",
            "training error 0.03448603808397519, test error 0.054336535252102997\n",
            "Loss: 0.7337648349072579\n",
            "training error 0.034506896792125444, test error 0.0544265484727142\n",
            "Loss: 0.9006391222539101\n",
            "training error 0.03451113315696382, test error 0.054249722283768856\n",
            "Loss: 0.572823451798854\n",
            "training error 0.034490174511721344, test error 0.05422134790661304\n",
            "Loss: 0.5202205793026771\n",
            "training error 0.034544012758646804, test error 0.05436016017867323\n",
            "Loss: 0.777562765458506\n",
            "training error 0.03455115957110197, test error 0.05415144261970367\n",
            "Loss: 0.3906241171879321\n",
            "training error 0.03460910954174991, test error 0.05413585460623043\n",
            "Loss: 0.3617257106890426\n",
            "training error 0.034580154825973146, test error 0.05432359985204213\n",
            "Loss: 0.7097840723921056\n",
            "training error 0.03454579140647153, test error 0.05450574806484509\n",
            "Loss: 1.047466170605782\n",
            "training error 0.03450127139309206, test error 0.05449562283392907\n",
            "Loss: 1.0286951425072743\n",
            "training error 0.034522264974206386, test error 0.05459797150595977\n",
            "Loss: 1.2184379557299119\n",
            "training error 0.0345372895809087, test error 0.0544645529345836\n",
            "Loss: 0.9710950780278171\n",
            "training error 0.03454748520260227, test error 0.054675473980680234\n",
            "Loss: 1.3621187411607893\n",
            "training error 0.03453163224474876, test error 0.054457219653761185\n",
            "Loss: 0.9575000082942431\n",
            "training error 0.034508416041116007, test error 0.05446122809552575\n",
            "Loss: 0.9649312040484803\n",
            "training error 0.03451752043132257, test error 0.05430560686227201\n",
            "Loss: 0.6764271129950705\n",
            "training error 0.034495520736721116, test error 0.054389257893480265\n",
            "Loss: 0.8315066238036461\n",
            "training error 0.034515675524802315, test error 0.054387394422762386\n",
            "Loss: 0.8280519607449799\n",
            "training error 0.03449853983807295, test error 0.05435456473712279\n",
            "Loss: 0.7671894523520573\n",
            "training error 0.03453368995559755, test error 0.05438394703258228\n",
            "Loss: 0.8216608909042966\n",
            "training error 0.0345160482132761, test error 0.05436586572925488\n",
            "Loss: 0.7881402082026812\n",
            "training error 0.03453037263532741, test error 0.054411865573402544\n",
            "Loss: 0.8734186946080857\n",
            "training error 0.03454949808860426, test error 0.05442546930105191\n",
            "Loss: 0.8986384605639275\n",
            "training error 0.03451386960068621, test error 0.05435567783232413\n",
            "Loss: 0.769253004430448\n",
            "training error 0.03452688952141332, test error 0.05421659325145127\n",
            "Loss: 0.5114059886646372\n",
            "training error 0.03451544938229719, test error 0.05425883800337015\n",
            "Loss: 0.5897229605797749\n",
            "training error 0.034543652071345333, test error 0.05437910362112673\n",
            "Loss: 0.8126817561785016\n",
            "training error 0.034531119034753824, test error 0.05425814584845124\n",
            "Loss: 0.5884397839748035\n",
            "training error 0.034513758717156735, test error 0.05430926352045882\n",
            "Loss: 0.6832061419515867\n",
            "training error 0.03460135916376547, test error 0.05440786446554103\n",
            "Loss: 0.8660010950771069\n",
            "training error 0.03450066336238622, test error 0.054366797516298235\n",
            "Loss: 0.7898676355490464\n",
            "training error 0.03453220014696673, test error 0.05430338798056831\n",
            "Loss: 0.6723135583361239\n",
            "training error 0.03450809487903411, test error 0.05430921708321976\n",
            "Loss: 0.683120052584707\n",
            "training error 0.034518527429036355, test error 0.05420790354574306\n",
            "Loss: 0.49529626125499604\n",
            "training error 0.0345236345936581, test error 0.05436481587963247\n",
            "Loss: 0.7861939062442369\n",
            "training error 0.03452744964810396, test error 0.054267876954746365\n",
            "Loss: 0.6064801497907002\n",
            "training error 0.034499744472851875, test error 0.054320205919473445\n",
            "Loss: 0.7034921068912903\n",
            "training error 0.03454411149263622, test error 0.054279732745161374\n",
            "Loss: 0.6284594386440912\n",
            "training error 0.03452906492763009, test error 0.05441432679449267\n",
            "Loss: 0.8779815189646545\n",
            "training error 0.03450362213104928, test error 0.05449519597647942\n",
            "Loss: 1.0279037972779292\n",
            "training error 0.03456570547700017, test error 0.05433742161898307\n",
            "Loss: 0.7354080584297096\n",
            "training error 0.03449636857368606, test error 0.05445740921622042\n",
            "Loss: 0.9578514355632395\n",
            "training error 0.03458269790565085, test error 0.0543968625251804\n",
            "Loss: 0.8456047472445993\n",
            "training error 0.03453666346280992, test error 0.05437547091561307\n",
            "Loss: 0.8059471327574341\n",
            "training error 0.03450357688236133, test error 0.05446305692011729\n",
            "Loss: 0.9683216371240144\n",
            "training error 0.034514654110978815, test error 0.05447733950916472\n",
            "Loss: 0.9947999350070624\n",
            "training error 0.03450061774792513, test error 0.05453756571692454\n",
            "Loss: 1.1064524837245626\n",
            "training error 0.0345181291562579, test error 0.05444739642229459\n",
            "Loss: 0.9392888528739762\n",
            "training error 0.03452435798531528, test error 0.05438955805294634\n",
            "Loss: 0.8320630853617672\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.05438955805294634\n",
            "training error 0.1250424112972171, test error 0.2501096557458432\n",
            "training error 0.12500661527196813, test error 0.2501622128534978\n",
            "training error 0.12502733080070522, test error 0.25029546109879997\n",
            "training error 0.12508151482919322, test error 0.250304197070603\n",
            "training error 0.12504023694584612, test error 0.25043194506224764\n",
            "training error 0.12497597891288122, test error 0.2505510571454266\n",
            "training error 0.12497291974963964, test error 0.2505516298252313\n",
            "training error 0.12502985024586832, test error 0.2506836672465807\n",
            "training error 0.1250352080852144, test error 0.250646641003996\n",
            "training error 0.12510559223237291, test error 0.25066792114783903\n",
            "training error 0.1250593203606304, test error 0.2505910370298258\n",
            "training error 0.12497326258952168, test error 0.25068664724099954\n",
            "training error 0.124976195436617, test error 0.25066255841585355\n",
            "training error 0.12498623761626948, test error 0.25059099485952047\n",
            "training error 0.12516593188018377, test error 0.25076879837926863\n",
            "training error 0.1251302962184713, test error 0.2506690417416908\n",
            "training error 0.12500774269464654, test error 0.25055794024708916\n",
            "training error 0.12505266731928724, test error 0.25067899199712873\n",
            "training error 0.12510204570073677, test error 0.25062593986601156\n",
            "training error 0.1249951970449593, test error 0.25048826792114015\n",
            "training error 0.1251194378267671, test error 0.25058048922976756\n",
            "training error 0.12503788301552363, test error 0.25044545895196135\n",
            "training error 0.1250200729806181, test error 0.25044482096141224\n",
            "training error 0.12510065202519027, test error 0.25054351919285484\n",
            "training error 0.12495784174091788, test error 0.25048355350543183\n",
            "training error 0.12497133643809662, test error 0.25039099903086726\n",
            "training error 0.12496962920139656, test error 0.25043512132982215\n",
            "training error 0.1249754341880002, test error 0.250466701329676\n",
            "training error 0.1250211749325605, test error 0.25045945364017086\n",
            "training error 0.12502037155150694, test error 0.25052843255134466\n",
            "training error 0.12506781032013464, test error 0.2506264545063014\n",
            "training error 0.12496094647520517, test error 0.25049387459465705\n",
            "training error 0.1249557918235617, test error 0.25050352503417217\n",
            "training error 0.12504502708726617, test error 0.2504392190088705\n",
            "training error 0.12527913631833115, test error 0.25040106056258465\n",
            "training error 0.1250438957244207, test error 0.2505534236582614\n",
            "training error 0.12498187640199682, test error 0.2507527683434396\n",
            "training error 0.12496472232055218, test error 0.25072825010585076\n",
            "training error 0.1250184867856027, test error 0.250565408996986\n",
            "training error 0.12497352871409727, test error 0.2505244762085938\n",
            "training error 0.12502022293802312, test error 0.2506260588030533\n",
            "training error 0.1249538201492536, test error 0.25054719507369366\n",
            "training error 0.12502159529546347, test error 0.2506162779663535\n",
            "training error 0.12504516421262427, test error 0.2506852532213998\n",
            "training error 0.12503032222502283, test error 0.2505159446129327\n",
            "training error 0.12503454755316168, test error 0.25056553261730524\n",
            "training error 0.12505376646110405, test error 0.25066018472859775\n",
            "training error 0.12511183237821924, test error 0.25069544453735054\n",
            "training error 0.12500212810757597, test error 0.2506882433719812\n",
            "training error 0.12507134175252227, test error 0.2507909809513236\n",
            "Loss: 0.2724105966435575\n",
            "training error 0.12505113603445192, test error 0.25065954949988595\n",
            "Loss: 0.21986106550062612\n",
            "training error 0.12504249641951495, test error 0.2505784839207943\n",
            "Loss: 0.18744905051866212\n",
            "training error 0.12498985379044812, test error 0.2506622655958183\n",
            "Loss: 0.2209470275456482\n",
            "training error 0.12494282680347644, test error 0.2507440345165587\n",
            "Loss: 0.25364025584047045\n",
            "training error 0.12498291208954439, test error 0.2506877477133866\n",
            "Loss: 0.2311354057161319\n",
            "training error 0.1250329596464466, test error 0.25072284318376764\n",
            "Loss: 0.24516743909621663\n",
            "training error 0.12495612342331638, test error 0.2506671520625218\n",
            "Loss: 0.22290075727628977\n",
            "training error 0.1251208750591046, test error 0.25078828353659394\n",
            "Loss: 0.2713321038034344\n",
            "training error 0.12506539905878886, test error 0.2505246089701561\n",
            "Loss: 0.1659085184358311\n",
            "training error 0.12502238306073404, test error 0.25059520801547797\n",
            "Loss: 0.19413575544966477\n",
            "training error 0.12508545975149746, test error 0.2507502435534806\n",
            "Loss: 0.25612278171631697\n",
            "training error 0.12495572736045933, test error 0.2507620186925908\n",
            "Loss: 0.26083077232752494\n",
            "training error 0.12496008316994935, test error 0.25069406630823443\n",
            "Loss: 0.2336617355489512\n",
            "training error 0.12497885502638904, test error 0.2507509613600692\n",
            "Loss: 0.25640977846839164\n",
            "training error 0.12500084157039199, test error 0.25078422523742605\n",
            "Loss: 0.26970949584943504\n",
            "training error 0.12493797771940013, test error 0.250723533542148\n",
            "Loss: 0.24544346137862494\n",
            "training error 0.12511413882727251, test error 0.2505879901709337\n",
            "Loss: 0.19124988344174643\n",
            "training error 0.1250078011622896, test error 0.25070893602444677\n",
            "Loss: 0.23960701429797115\n",
            "training error 0.12499677653085654, test error 0.25079738070884544\n",
            "Loss: 0.2749693773122841\n",
            "training error 0.12494038756328964, test error 0.25075715076345034\n",
            "Loss: 0.2588844543711222\n",
            "training error 0.1250808266411275, test error 0.2506129932654032\n",
            "Loss: 0.2012467363800896\n",
            "training error 0.1250193745336541, test error 0.2506384971127379\n",
            "Loss: 0.21144380264634943\n",
            "training error 0.12503755322983368, test error 0.2508359785824247\n",
            "Loss: 0.2904017577472384\n",
            "training error 0.12493335509666872, test error 0.2507386025999528\n",
            "Loss: 0.25146844180565786\n",
            "training error 0.12504409120934157, test error 0.250614973883756\n",
            "Loss: 0.20203863637566766\n",
            "training error 0.1250289302738547, test error 0.2506570623224375\n",
            "Loss: 0.21886663070320722\n",
            "training error 0.12497592228606287, test error 0.25067145155232196\n",
            "Loss: 0.22461979918506625\n",
            "training error 0.12493183047832142, test error 0.25066516568928776\n",
            "Loss: 0.22210655633745358\n",
            "training error 0.1250163374311788, test error 0.25069063151914484\n",
            "Loss: 0.23228842227986934\n",
            "training error 0.12495424473746847, test error 0.2507098868724273\n",
            "Loss: 0.23998718673781116\n",
            "training error 0.12494824983014792, test error 0.25075164293671504\n",
            "Loss: 0.2566822895970988\n",
            "training error 0.12495250126361873, test error 0.2507201957219851\n",
            "Loss: 0.2441089186745904\n",
            "training error 0.12494364964820501, test error 0.2507554794515704\n",
            "Loss: 0.2582162227209084\n",
            "training error 0.1249343094459108, test error 0.25075245109274924\n",
            "Loss: 0.25700541028261803\n",
            "training error 0.1249415569759876, test error 0.2507837294438903\n",
            "Loss: 0.26951126538357073\n",
            "training error 0.12495336041157865, test error 0.2508693951530079\n",
            "Loss: 0.30376252564063844\n",
            "training error 0.12498588733198276, test error 0.2507832742115369\n",
            "Loss: 0.2693292522773394\n",
            "training error 0.12495909628033623, test error 0.2508511789694791\n",
            "Loss: 0.29647924684259586\n",
            "training error 0.12493661570255515, test error 0.25088632318243775\n",
            "Loss: 0.31053076870561913\n",
            "training error 0.1249539811227724, test error 0.2508018888580171\n",
            "Loss: 0.2767718463765778\n",
            "training error 0.12494818833610172, test error 0.25086332133536854\n",
            "Loss: 0.30133406376409155\n",
            "training error 0.12526963545942693, test error 0.2507455473249957\n",
            "Loss: 0.25424511391063387\n",
            "training error 0.12491185492690521, test error 0.250876357334154\n",
            "Loss: 0.3065461771255551\n",
            "training error 0.12495617306600017, test error 0.2508479633093627\n",
            "Loss: 0.29519354673366305\n",
            "training error 0.12491752268935685, test error 0.25084317917857396\n",
            "Loss: 0.2932807334220522\n",
            "training error 0.12499923433157682, test error 0.25086994493472475\n",
            "Loss: 0.30398234191091333\n",
            "training error 0.1249219815493459, test error 0.25084922557002937\n",
            "Loss: 0.29569822963479986\n",
            "training error 0.12503124568882323, test error 0.2507361961857055\n",
            "Loss: 0.25050629812506386\n",
            "training error 0.12498637395032262, test error 0.2506407930636796\n",
            "Loss: 0.21236178037689069\n",
            "training error 0.12499130085150816, test error 0.2507724531441122\n",
            "Loss: 0.2650027230226115\n",
            "training error 0.1249293624297551, test error 0.25068256699257496\n",
            "Loss: 0.2290640259462684\n",
            "training error 0.1249238055766198, test error 0.25061671416972287\n",
            "Loss: 0.20273444556451192\n",
            "training error 0.12490813541458011, test error 0.25060669445552153\n",
            "Loss: 0.19872831706402305\n",
            "training error 0.12494090348514791, test error 0.2505966403690502\n",
            "Loss: 0.194708445683478\n",
            "training error 0.12503872380778652, test error 0.2507828178963774\n",
            "Loss: 0.2691468062385738\n",
            "training error 0.12512864620190597, test error 0.25074986372622615\n",
            "Loss: 0.25597091742572964\n",
            "training error 0.12491822748128023, test error 0.2508189133815626\n",
            "Loss: 0.2835786701654186\n",
            "training error 0.12500716641568438, test error 0.25067420078006136\n",
            "Loss: 0.2257190081424998\n",
            "training error 0.12498601399036624, test error 0.25062570785147426\n",
            "Loss: 0.20633034102268688\n",
            "training error 0.12492573947544632, test error 0.2506851756709644\n",
            "Loss: 0.2301070398121663\n",
            "training error 0.12505657455487174, test error 0.25059794583209016\n",
            "Loss: 0.1952304019574358\n",
            "training error 0.12509206033595344, test error 0.2507910371480088\n",
            "Loss: 0.2724330654622964\n",
            "training error 0.12488636911213373, test error 0.2506892182644387\n",
            "Loss: 0.23172336824308637\n",
            "training error 0.1249459278045215, test error 0.2506519630907479\n",
            "Loss: 0.21682783229119984\n",
            "training error 0.1249043465355012, test error 0.25064462978017676\n",
            "Loss: 0.2138957941220765\n",
            "training error 0.1249371172027895, test error 0.25063338964464027\n",
            "Loss: 0.20940171111556438\n",
            "training error 0.1250411737486443, test error 0.2505820507622042\n",
            "Loss: 0.18887516155754458\n",
            "training error 0.12490777330418634, test error 0.2507464813341945\n",
            "Loss: 0.25461855379085296\n",
            "training error 0.12490431087743137, test error 0.25076541730811525\n",
            "Loss: 0.26218962251438604\n",
            "training error 0.12493755546923338, test error 0.25067416589945685\n",
            "Loss: 0.22570506201780027\n",
            "training error 0.12492474178872474, test error 0.2506922006242892\n",
            "Loss: 0.23291578916009303\n",
            "training error 0.1251024249629854, test error 0.25069484248079005\n",
            "Loss: 0.23397206845203744\n",
            "training error 0.12490213898442784, test error 0.2506615869759238\n",
            "Loss: 0.22067569859895997\n",
            "training error 0.12492843548295778, test error 0.2506879452926175\n",
            "Loss: 0.23121440275859495\n",
            "training error 0.12489084622519811, test error 0.25075706984836615\n",
            "Loss: 0.25885210252771795\n",
            "training error 0.12488318093555076, test error 0.2506758452009297\n",
            "Loss: 0.22637648810404087\n",
            "training error 0.12488802872744066, test error 0.250620988013936\n",
            "Loss: 0.20444323373600692\n",
            "training error 0.12504641094767477, test error 0.250636658278407\n",
            "Loss: 0.21070859139453724\n",
            "training error 0.12499387610805612, test error 0.2505917436391053\n",
            "Loss: 0.19275061245616065\n",
            "training error 0.12487769955337785, test error 0.250657888411157\n",
            "Loss: 0.21919692131793234\n",
            "training error 0.12494138745008382, test error 0.250598639938401\n",
            "Loss: 0.19550792275475626\n",
            "training error 0.12488570789851297, test error 0.2506409999269354\n",
            "Loss: 0.21244448940114236\n",
            "training error 0.12488309620938277, test error 0.250762257048069\n",
            "Loss: 0.2609260727178597\n",
            "training error 0.12492697465582359, test error 0.2508174798601207\n",
            "Loss: 0.2830055129885878\n",
            "training error 0.1251319418361671, test error 0.25061313813546554\n",
            "Loss: 0.20130465899883543\n",
            "training error 0.12499670844444571, test error 0.2507250694945583\n",
            "Loss: 0.24605757297930797\n",
            "training error 0.12490167352283098, test error 0.25064924401156263\n",
            "Loss: 0.21574067746819026\n",
            "training error 0.12498187302455477, test error 0.25075733888191143\n",
            "Loss: 0.25895966876479104\n",
            "training error 0.12489462439280129, test error 0.25066575846205447\n",
            "Loss: 0.2223435614882252\n",
            "training error 0.12497429184617487, test error 0.2506371673923177\n",
            "Loss: 0.21091214767434696\n",
            "training error 0.1248684211708297, test error 0.25066038767990706\n",
            "Loss: 0.2201961905155203\n",
            "training error 0.12499601713580287, test error 0.25067736491309545\n",
            "Loss: 0.2269841064549416\n",
            "training error 0.12510035157448157, test error 0.25080723960691503\n",
            "Loss: 0.278911207562782\n",
            "training error 0.12491650557134452, test error 0.2507762530444159\n",
            "Loss: 0.26652201674697285\n",
            "training error 0.12489035555406723, test error 0.2507104614370195\n",
            "Loss: 0.24021691181201899\n",
            "training error 0.12485368344226748, test error 0.2507035360047876\n",
            "Loss: 0.23744795344802228\n",
            "training error 0.12487289829550523, test error 0.25068776314693647\n",
            "Loss: 0.2311415764294722\n",
            "training error 0.12495556306103059, test error 0.2506048987085996\n",
            "Loss: 0.19801033321946715\n",
            "training error 0.12489743847008833, test error 0.2508015157196856\n",
            "Loss: 0.27662265648211726\n",
            "training error 0.12491351403722185, test error 0.25073259819964056\n",
            "Loss: 0.24906773468609256\n",
            "training error 0.1249256509888316, test error 0.2509005870110511\n",
            "Loss: 0.3162337986709396\n",
            "training error 0.12483066831655376, test error 0.25083821302464326\n",
            "Loss: 0.29129514277546065\n",
            "training error 0.1248287701022404, test error 0.25074131124483684\n",
            "Loss: 0.2525514247380789\n",
            "training error 0.12484088996761077, test error 0.2507094168848609\n",
            "Loss: 0.2397992741340449\n",
            "training error 0.12481891868347847, test error 0.2507353728254314\n",
            "Loss: 0.25017709840999824\n",
            "training error 0.12486096124574636, test error 0.2507701335669964\n",
            "Loss: 0.2640752989658157\n",
            "training error 0.12497289459784044, test error 0.2506575869958462\n",
            "Loss: 0.21907640805352013\n",
            "training error 0.12485913901892855, test error 0.2507965236507966\n",
            "Loss: 0.2746267043969519\n",
            "training error 0.1248292040846231, test error 0.25069914682600697\n",
            "Loss: 0.2356930516760114\n",
            "training error 0.12483809147239687, test error 0.2507088851793734\n",
            "Loss: 0.23958668518544446\n",
            "training error 0.1248725505144007, test error 0.2506519996759419\n",
            "Loss: 0.21684245995277074\n",
            "training error 0.12481998352911515, test error 0.25071378155660085\n",
            "Loss: 0.2415443773876369\n",
            "training error 0.1247940456332218, test error 0.2506456760269298\n",
            "Loss: 0.21431410934060757\n",
            "training error 0.12491771008218498, test error 0.2507180447599814\n",
            "Loss: 0.2432489110921754\n",
            "training error 0.12480192039363242, test error 0.2506175971288715\n",
            "Loss: 0.20308747437742802\n",
            "training error 0.12479892773157877, test error 0.2506593498676699\n",
            "Loss: 0.2197812476241623\n",
            "training error 0.12495821625121285, test error 0.25077578268925343\n",
            "Loss: 0.26633395716921804\n",
            "training error 0.12485435856911957, test error 0.2506987679620864\n",
            "Loss: 0.23554157255001318\n",
            "training error 0.12486233442982686, test error 0.25058314881789073\n",
            "Loss: 0.18931419126364535\n",
            "training error 0.12478661612274557, test error 0.25066372676288\n",
            "Loss: 0.22153123812214126\n",
            "training error 0.12488801567951677, test error 0.2508036586121322\n",
            "Loss: 0.2774794376568357\n",
            "training error 0.1248535595377219, test error 0.25072226807685855\n",
            "Loss: 0.24493749719036728\n",
            "training error 0.12488351174137907, test error 0.2505538873737121\n",
            "Loss: 0.17761474523809717\n",
            "training error 0.12478118354152999, test error 0.25056426426963724\n",
            "Loss: 0.18176368378834962\n",
            "training error 0.12475744877959576, test error 0.25058255313622685\n",
            "Loss: 0.18907602306412752\n",
            "training error 0.12473379693927159, test error 0.25062856405437195\n",
            "Loss: 0.20747232128297544\n",
            "training error 0.12474537688372442, test error 0.2505405284120402\n",
            "Loss: 0.17227350336079805\n",
            "training error 0.12484447169234632, test error 0.25060607220418424\n",
            "Loss: 0.1984795256547267\n",
            "training error 0.12470502280405331, test error 0.25053426181977473\n",
            "Loss: 0.1697679654411255\n",
            "training error 0.12469307080444209, test error 0.25050587174692135\n",
            "Loss: 0.15841691513132083\n",
            "training error 0.12475740961930128, test error 0.25055568075258816\n",
            "Loss: 0.1783317822796171\n",
            "training error 0.12468043803136215, test error 0.25042900996528517\n",
            "Loss: 0.12768568190204554\n",
            "training error 0.12469973298049629, test error 0.2503400623009212\n",
            "Loss: 0.09212221511036844\n",
            "training error 0.1247365643772115, test error 0.2503251578030631\n",
            "Loss: 0.08616302980275492\n",
            "training error 0.12486690581617081, test error 0.25047106343612263\n",
            "Loss: 0.1444996952243649\n",
            "training error 0.12465581264106412, test error 0.2504206055628869\n",
            "Loss: 0.12432539484188432\n",
            "training error 0.12473539821127091, test error 0.2504139497011377\n",
            "Loss: 0.12166421739578936\n",
            "training error 0.1246301835664266, test error 0.2503486662596591\n",
            "Loss: 0.09556228970974967\n",
            "training error 0.1246535910040271, test error 0.2503260318117333\n",
            "Loss: 0.08651247999396094\n",
            "training error 0.12492635860166472, test error 0.2504345757771619\n",
            "Loss: 0.1299110305636786\n",
            "training error 0.12476425489184384, test error 0.2504145979138425\n",
            "Loss: 0.1219233887991944\n",
            "training error 0.12462854705356659, test error 0.25022602444065484\n",
            "Loss: 0.04652707008236945\n",
            "training error 0.12469355631473292, test error 0.25012405852889485\n",
            "Loss: 0.005758587371884616\n",
            "training error 0.12455348048121245, test error 0.25020001204394265\n",
            "Loss: 0.0361266732505694\n",
            "training error 0.12458074741178563, test error 0.25017907215529095\n",
            "Loss: 0.02775439006572622\n",
            "training error 0.12453932957684634, test error 0.2501214088860451\n",
            "Loss: 0.004699194905866122\n",
            "training error 0.12460763707034297, test error 0.25012392120558036\n",
            "Loss: 0.005703682128799237\n",
            "training error 0.12449483194714012, test error 0.2500533939737815\n",
            "Loss: 0.0\n",
            "training error 0.1245016882752106, test error 0.250103246590923\n",
            "Loss: 0.01993678883907002\n",
            "training error 0.12444748963182455, test error 0.25005043288112755\n",
            "Loss: 0.0\n",
            "training error 0.12443576212836105, test error 0.2499723810437259\n",
            "Loss: 0.0\n",
            "training error 0.1244251583674327, test error 0.24997039027349122\n",
            "Loss: 0.0\n",
            "training error 0.1245919976416271, test error 0.2498670871587854\n",
            "Loss: 0.0\n",
            "training error 0.12442128348363458, test error 0.24996641958288\n",
            "Loss: 0.039754104961997605\n",
            "training error 0.12450005982266978, test error 0.2498277277459357\n",
            "Loss: 0.0\n",
            "training error 0.12432726810109923, test error 0.2498875834635531\n",
            "Loss: 0.02395879679066848\n",
            "training error 0.12440094316674781, test error 0.24973077184868353\n",
            "Loss: 0.0\n",
            "training error 0.12430377928631062, test error 0.2497540302617373\n",
            "Loss: 0.009313394933907482\n",
            "training error 0.12440393199079379, test error 0.24965614283868331\n",
            "Loss: 0.0\n",
            "training error 0.12422505130149976, test error 0.2497628278217844\n",
            "Loss: 0.04273276911517243\n",
            "training error 0.12429581029070352, test error 0.24963439106537821\n",
            "Loss: 0.0\n",
            "training error 0.1241964853699855, test error 0.24970683588364703\n",
            "Loss: 0.029020367730447916\n",
            "training error 0.1241283471641043, test error 0.2497485491891592\n",
            "Loss: 0.04573012688426914\n",
            "training error 0.12414018068189375, test error 0.24976885039924773\n",
            "Loss: 0.05386250399861403\n",
            "training error 0.12406556834481264, test error 0.249586748431374\n",
            "Loss: 0.0\n",
            "training error 0.12416683521590226, test error 0.24963602602108953\n",
            "Loss: 0.019743672300420734\n",
            "training error 0.12416594519135145, test error 0.24958430442857998\n",
            "Loss: 0.0\n",
            "training error 0.12392668825463055, test error 0.24936272006587115\n",
            "Loss: 0.0\n",
            "training error 0.12404802478906024, test error 0.24923900588905465\n",
            "Loss: 0.0\n",
            "training error 0.12384981720970845, test error 0.249112898324771\n",
            "Loss: 0.0\n",
            "training error 0.123875658091224, test error 0.2489204393469747\n",
            "Loss: 0.0\n",
            "training error 0.12378943107576126, test error 0.24888139345797158\n",
            "Loss: 0.0\n",
            "training error 0.1237749362953829, test error 0.2486837239876494\n",
            "Loss: 0.0\n",
            "training error 0.12374722774718977, test error 0.2486355428781994\n",
            "Loss: 0.0\n",
            "training error 0.12361666555401221, test error 0.2484605129004567\n",
            "Loss: 0.0\n",
            "training error 0.12367055875415602, test error 0.2483991442964601\n",
            "Loss: 0.0\n",
            "training error 0.12350005905249802, test error 0.2482176072601635\n",
            "Loss: 0.0\n",
            "training error 0.12342339464400201, test error 0.248063626168008\n",
            "Loss: 0.0\n",
            "training error 0.12355178492481114, test error 0.24810323266659412\n",
            "Loss: 0.015966266073719204\n",
            "training error 0.12344838305838234, test error 0.24782644474951837\n",
            "Loss: 0.0\n",
            "training error 0.12326636577897818, test error 0.24773021134804454\n",
            "Loss: 0.0\n",
            "training error 0.1231771594999123, test error 0.24754595817603284\n",
            "Loss: 0.0\n",
            "training error 0.12310523168942461, test error 0.24733351499163675\n",
            "Loss: 0.0\n",
            "training error 0.12313434827538255, test error 0.24713605189697901\n",
            "Loss: 0.0\n",
            "training error 0.12287632569163348, test error 0.2470804349456375\n",
            "Loss: 0.0\n",
            "training error 0.12288184433740514, test error 0.24690458793554548\n",
            "Loss: 0.0\n",
            "training error 0.12272343865310092, test error 0.2466866801012561\n",
            "Loss: 0.0\n",
            "training error 0.12280156262575559, test error 0.246376251939041\n",
            "Loss: 0.0\n",
            "training error 0.12250635771696583, test error 0.2462500023629757\n",
            "Loss: 0.0\n",
            "training error 0.1224449207680038, test error 0.24602443882808966\n",
            "Loss: 0.0\n",
            "training error 0.12230193206005473, test error 0.24592192991489778\n",
            "Loss: 0.0\n",
            "training error 0.12227093559905355, test error 0.24559551609468067\n",
            "Loss: 0.0\n",
            "training error 0.12217899572077288, test error 0.24533020797866445\n",
            "Loss: 0.0\n",
            "training error 0.12202606323248835, test error 0.24513181309691004\n",
            "Loss: 0.0\n",
            "training error 0.12179652631667703, test error 0.2449281611611726\n",
            "Loss: 0.0\n",
            "training error 0.12172897386236779, test error 0.24460741010109757\n",
            "Loss: 0.0\n",
            "training error 0.12153491090337308, test error 0.24447360201404977\n",
            "Loss: 0.0\n",
            "training error 0.12139710974803267, test error 0.2441486585415271\n",
            "Loss: 0.0\n",
            "training error 0.12122677728439069, test error 0.24378391167056307\n",
            "Loss: 0.0\n",
            "training error 0.12118283644450728, test error 0.24340482838937277\n",
            "Loss: 0.0\n",
            "training error 0.1209707997787435, test error 0.24311177371000173\n",
            "Loss: 0.0\n",
            "training error 0.12078644298413181, test error 0.24254955749738852\n",
            "Loss: 0.0\n",
            "training error 0.12062474626422108, test error 0.2422224948206975\n",
            "Loss: 0.0\n",
            "training error 0.12038037005685895, test error 0.2418444229936613\n",
            "Loss: 0.0\n",
            "training error 0.1201824427406809, test error 0.2415106352935591\n",
            "Loss: 0.0\n",
            "training error 0.11993033576095201, test error 0.24097521556696352\n",
            "Loss: 0.0\n",
            "training error 0.11970094138973325, test error 0.24048817211570114\n",
            "Loss: 0.0\n",
            "training error 0.1196317259785461, test error 0.24001569461566485\n",
            "Loss: 0.0\n",
            "training error 0.11927529652006506, test error 0.239548077992414\n",
            "Loss: 0.0\n",
            "training error 0.11902422647467102, test error 0.2390229553943901\n",
            "Loss: 0.0\n",
            "training error 0.11879119904025763, test error 0.2386227313043275\n",
            "Loss: 0.0\n",
            "training error 0.11853757512110027, test error 0.23806487761815592\n",
            "Loss: 0.0\n",
            "training error 0.11830422613613961, test error 0.23755847185373105\n",
            "Loss: 0.0\n",
            "training error 0.11803203433705753, test error 0.2369505552834673\n",
            "Loss: 0.0\n",
            "training error 0.11767633952238447, test error 0.23628444518235242\n",
            "Loss: 0.0\n",
            "training error 0.11743457431596192, test error 0.23588918360294872\n",
            "Loss: 0.0\n",
            "training error 0.11712331711170922, test error 0.2350582189697564\n",
            "Loss: 0.0\n",
            "training error 0.11680010062034375, test error 0.23447525183443557\n",
            "Loss: 0.0\n",
            "training error 0.11644413101675524, test error 0.2337906117595578\n",
            "Loss: 0.0\n",
            "training error 0.11602706905988959, test error 0.2330986255863825\n",
            "Loss: 0.0\n",
            "training error 0.11573803177933999, test error 0.23232935727629078\n",
            "Loss: 0.0\n",
            "training error 0.1154214840603077, test error 0.23153993436319242\n",
            "Loss: 0.0\n",
            "training error 0.11491926207428806, test error 0.23071489017745242\n",
            "Loss: 0.0\n",
            "training error 0.11451675799620889, test error 0.22971864349714805\n",
            "Loss: 0.0\n",
            "training error 0.11417524933508658, test error 0.22884957592358665\n",
            "Loss: 0.0\n",
            "training error 0.11381746248491953, test error 0.22787599101106232\n",
            "Loss: 0.0\n",
            "training error 0.1131838230068719, test error 0.22701534653379796\n",
            "Loss: 0.0\n",
            "training error 0.11277768414930021, test error 0.22609518318761246\n",
            "Loss: 0.0\n",
            "training error 0.11228404942734682, test error 0.22517182574725442\n",
            "Loss: 0.0\n",
            "training error 0.11187099308050194, test error 0.22415536845776968\n",
            "Loss: 0.0\n",
            "training error 0.11141791302105251, test error 0.2230750478297681\n",
            "Loss: 0.0\n",
            "training error 0.11078818295248642, test error 0.22202507268912408\n",
            "Loss: 0.0\n",
            "training error 0.11034979185100115, test error 0.22081146561466117\n",
            "Loss: 0.0\n",
            "training error 0.10972063583200685, test error 0.21977291203102423\n",
            "Loss: 0.0\n",
            "training error 0.10918251892541724, test error 0.21866081915562505\n",
            "Loss: 0.0\n",
            "training error 0.10869073619211768, test error 0.21736942821914293\n",
            "Loss: 0.0\n",
            "training error 0.1081863785706145, test error 0.21625627207553857\n",
            "Loss: 0.0\n",
            "training error 0.107425858850851, test error 0.21506265154072254\n",
            "Loss: 0.0\n",
            "training error 0.10684483035210991, test error 0.21375753189699326\n",
            "Loss: 0.0\n",
            "training error 0.10623871297240683, test error 0.2123940500683834\n",
            "Loss: 0.0\n",
            "training error 0.10575827785737785, test error 0.21089128423984643\n",
            "Loss: 0.0\n",
            "training error 0.10495918178962349, test error 0.20964999196946799\n",
            "Loss: 0.0\n",
            "training error 0.1043113854659091, test error 0.20831819812256305\n",
            "Loss: 0.0\n",
            "training error 0.10367179361816485, test error 0.2070069222744646\n",
            "Loss: 0.0\n",
            "training error 0.1029380982974394, test error 0.20542931823957258\n",
            "Loss: 0.0\n",
            "training error 0.10225339975544559, test error 0.2039633314311588\n",
            "Loss: 0.0\n",
            "training error 0.10157096708585794, test error 0.20247714116973628\n",
            "Loss: 0.0\n",
            "training error 0.10084108436395492, test error 0.20105247935232473\n",
            "Loss: 0.0\n",
            "training error 0.10009889865797768, test error 0.19957880951995458\n",
            "Loss: 0.0\n",
            "training error 0.09946321023817065, test error 0.1980788978462576\n",
            "Loss: 0.0\n",
            "training error 0.09864946857241098, test error 0.19655154359811336\n",
            "Loss: 0.0\n",
            "training error 0.0980579727125078, test error 0.1950077292414514\n",
            "Loss: 0.0\n",
            "training error 0.0971612908859819, test error 0.1934637983344739\n",
            "Loss: 0.0\n",
            "training error 0.09638004968668956, test error 0.19178602664393923\n",
            "Loss: 0.0\n",
            "training error 0.0956781168341438, test error 0.19005804457655684\n",
            "Loss: 0.0\n",
            "training error 0.09490505864338945, test error 0.1884963734265557\n",
            "Loss: 0.0\n",
            "training error 0.09412650252608598, test error 0.18690299408038677\n",
            "Loss: 0.0\n",
            "training error 0.0933994647068461, test error 0.185289368550863\n",
            "Loss: 0.0\n",
            "training error 0.09251407162084664, test error 0.18338338896872644\n",
            "Loss: 0.0\n",
            "training error 0.0918222085192957, test error 0.1817298795567285\n",
            "Loss: 0.0\n",
            "training error 0.09091681700411078, test error 0.1801822394678633\n",
            "Loss: 0.0\n",
            "training error 0.09017606491307671, test error 0.17867896244662945\n",
            "Loss: 0.0\n",
            "training error 0.08948605545884515, test error 0.17670078241080764\n",
            "Loss: 0.0\n",
            "training error 0.08851024536720764, test error 0.17507325489344072\n",
            "Loss: 0.0\n",
            "training error 0.08781264571416347, test error 0.1733859079110531\n",
            "Loss: 0.0\n",
            "training error 0.08695073630664361, test error 0.17177607134023362\n",
            "Loss: 0.0\n",
            "training error 0.08611563049553202, test error 0.1699647491946979\n",
            "Loss: 0.0\n",
            "training error 0.08537679080198865, test error 0.1681388015502795\n",
            "Loss: 0.0\n",
            "training error 0.08469325634441184, test error 0.16645551242262224\n",
            "Loss: 0.0\n",
            "training error 0.08373751929188218, test error 0.16473360808018706\n",
            "Loss: 0.0\n",
            "training error 0.08309992375766478, test error 0.16323511067875715\n",
            "Loss: 0.0\n",
            "training error 0.08220290752793465, test error 0.16143204846570622\n",
            "Loss: 0.0\n",
            "training error 0.08144228458134553, test error 0.15970144280848164\n",
            "Loss: 0.0\n",
            "training error 0.08063698479895755, test error 0.15810936631850897\n",
            "Loss: 0.0\n",
            "training error 0.0798916848002, test error 0.15655071556355682\n",
            "Loss: 0.0\n",
            "training error 0.07913059548076067, test error 0.1549137947379991\n",
            "Loss: 0.0\n",
            "training error 0.07836860357789775, test error 0.15328060707988667\n",
            "Loss: 0.0\n",
            "training error 0.07765660011620791, test error 0.15160539677654336\n",
            "Loss: 0.0\n",
            "training error 0.07689491040938219, test error 0.15004217683419038\n",
            "Loss: 0.0\n",
            "training error 0.07621196309405395, test error 0.14838122498445455\n",
            "Loss: 0.0\n",
            "training error 0.07547118285775949, test error 0.1467084447526351\n",
            "Loss: 0.0\n",
            "training error 0.07469520512196756, test error 0.1449779413213264\n",
            "Loss: 0.0\n",
            "training error 0.07400898863544289, test error 0.14354335067196058\n",
            "Loss: 0.0\n",
            "training error 0.07324983677183841, test error 0.14212511620137908\n",
            "Loss: 0.0\n",
            "training error 0.07261668925963026, test error 0.1406162775628166\n",
            "Loss: 0.0\n",
            "training error 0.0718914057419711, test error 0.1392667093815678\n",
            "Loss: 0.0\n",
            "training error 0.0713087972952058, test error 0.13783429102799097\n",
            "Loss: 0.0\n",
            "training error 0.07055678422209023, test error 0.13642245751668178\n",
            "Loss: 0.0\n",
            "training error 0.06988754316425926, test error 0.134817526902518\n",
            "Loss: 0.0\n",
            "training error 0.0692841115268224, test error 0.13327821841129872\n",
            "Loss: 0.0\n",
            "training error 0.06859303104107768, test error 0.13176288395480282\n",
            "Loss: 0.0\n",
            "training error 0.06796264039360092, test error 0.1303999226313278\n",
            "Loss: 0.0\n",
            "training error 0.06743373911380238, test error 0.12891333042580416\n",
            "Loss: 0.0\n",
            "training error 0.0667788606071181, test error 0.12758730907062452\n",
            "Loss: 0.0\n",
            "training error 0.06613508441162495, test error 0.12631861073668668\n",
            "Loss: 0.0\n",
            "training error 0.06554402202547825, test error 0.12509349827195887\n",
            "Loss: 0.0\n",
            "training error 0.065017708437658, test error 0.12381967068123877\n",
            "Loss: 0.0\n",
            "training error 0.0644700220527663, test error 0.12245429493914868\n",
            "Loss: 0.0\n",
            "training error 0.06388818186725584, test error 0.12129901844790442\n",
            "Loss: 0.0\n",
            "training error 0.06332403399325107, test error 0.12012466387756798\n",
            "Loss: 0.0\n",
            "training error 0.06275667219786822, test error 0.11892366676539884\n",
            "Loss: 0.0\n",
            "training error 0.062222629431362135, test error 0.11774027276054198\n",
            "Loss: 0.0\n",
            "training error 0.06178652780822016, test error 0.11661683658835874\n",
            "Loss: 0.0\n",
            "training error 0.061272458095591, test error 0.11556091898210659\n",
            "Loss: 0.0\n",
            "training error 0.060738197572864715, test error 0.11432468756091002\n",
            "Loss: 0.0\n",
            "training error 0.06026086010530846, test error 0.11311762515578098\n",
            "Loss: 0.0\n",
            "training error 0.05970780552301845, test error 0.11201779275242912\n",
            "Loss: 0.0\n",
            "training error 0.05926656211571554, test error 0.11113159565921488\n",
            "Loss: 0.0\n",
            "training error 0.058820846959760024, test error 0.10981406051728158\n",
            "Loss: 0.0\n",
            "training error 0.05844279325980827, test error 0.10894875808809233\n",
            "Loss: 0.0\n",
            "training error 0.057899034652087164, test error 0.10794220025251787\n",
            "Loss: 0.0\n",
            "training error 0.057377055628525145, test error 0.10675168028711242\n",
            "Loss: 0.0\n",
            "training error 0.056958547246246875, test error 0.1058041399594995\n",
            "Loss: 0.0\n",
            "training error 0.056527398100151525, test error 0.10481634881832438\n",
            "Loss: 0.0\n",
            "training error 0.056131989710981385, test error 0.1039880965111487\n",
            "Loss: 0.0\n",
            "training error 0.055850706707145134, test error 0.102825073578513\n",
            "Loss: 0.0\n",
            "training error 0.055317809856565404, test error 0.10213375518676189\n",
            "Loss: 0.0\n",
            "training error 0.054912282614851725, test error 0.10134446907541345\n",
            "Loss: 0.0\n",
            "training error 0.05463767763550144, test error 0.10045205778440826\n",
            "Loss: 0.0\n",
            "training error 0.054162598881007655, test error 0.09952812484701584\n",
            "Loss: 0.0\n",
            "training error 0.053768831469074685, test error 0.09860683938414118\n",
            "Loss: 0.0\n",
            "training error 0.053497889411814416, test error 0.09771013994055064\n",
            "Loss: 0.0\n",
            "training error 0.05318741623634834, test error 0.09698588111976851\n",
            "Loss: 0.0\n",
            "training error 0.05271284652319606, test error 0.0960443719612492\n",
            "Loss: 0.0\n",
            "training error 0.052509502583812595, test error 0.09547065353699549\n",
            "Loss: 0.0\n",
            "training error 0.05207590328627316, test error 0.094563281829172\n",
            "Loss: 0.0\n",
            "training error 0.05176781952601381, test error 0.09381539246029803\n",
            "Loss: 0.0\n",
            "training error 0.05148189794132128, test error 0.09309135220121621\n",
            "Loss: 0.0\n",
            "training error 0.051103441297838406, test error 0.09231862241301292\n",
            "Loss: 0.0\n",
            "training error 0.05076446955547644, test error 0.09156459832691588\n",
            "Loss: 0.0\n",
            "training error 0.05046019546470367, test error 0.09087317991239321\n",
            "Loss: 0.0\n",
            "training error 0.05019834875472595, test error 0.09012264640605389\n",
            "Loss: 0.0\n",
            "training error 0.049921087070105, test error 0.08944880246510299\n",
            "Loss: 0.0\n",
            "training error 0.04965843080990761, test error 0.08882660315473415\n",
            "Loss: 0.0\n",
            "training error 0.049326064262217015, test error 0.08816984537322291\n",
            "Loss: 0.0\n",
            "training error 0.04907202987343067, test error 0.08758359324253966\n",
            "Loss: 0.0\n",
            "training error 0.04892659858614347, test error 0.08691852990973285\n",
            "Loss: 0.0\n",
            "training error 0.04858825269879144, test error 0.0863603669173927\n",
            "Loss: 0.0\n",
            "training error 0.048371036369216185, test error 0.08578379387870497\n",
            "Loss: 0.0\n",
            "training error 0.04809569342154445, test error 0.08517176907582025\n",
            "Loss: 0.0\n",
            "training error 0.04790944034534262, test error 0.08445902318687583\n",
            "Loss: 0.0\n",
            "training error 0.04757319061996021, test error 0.08393061604667423\n",
            "Loss: 0.0\n",
            "training error 0.04739674625500927, test error 0.08338801235549402\n",
            "Loss: 0.0\n",
            "training error 0.04714130014572221, test error 0.08296519167649177\n",
            "Loss: 0.0\n",
            "training error 0.046868178391311695, test error 0.08244433435496056\n",
            "Loss: 0.0\n",
            "training error 0.04672393827545864, test error 0.08204298409184452\n",
            "Loss: 0.0\n",
            "training error 0.04655557691687344, test error 0.08141252530719786\n",
            "Loss: 0.0\n",
            "training error 0.046272212790455125, test error 0.080946830332432\n",
            "Loss: 0.0\n",
            "training error 0.046039131136825805, test error 0.08044493276935795\n",
            "Loss: 0.0\n",
            "training error 0.04595083401844656, test error 0.07987276381000218\n",
            "Loss: 0.0\n",
            "training error 0.045655970509553716, test error 0.07957872723645473\n",
            "Loss: 0.0\n",
            "training error 0.04543352838659867, test error 0.07921185156941626\n",
            "Loss: 0.0\n",
            "training error 0.0452528501815204, test error 0.07870118595592693\n",
            "Loss: 0.0\n",
            "training error 0.04508803151240314, test error 0.07815793875177339\n",
            "Loss: 0.0\n",
            "training error 0.04487899090463784, test error 0.07776600831011726\n",
            "Loss: 0.0\n",
            "training error 0.04478281779223952, test error 0.07742486993756093\n",
            "Loss: 0.0\n",
            "training error 0.044606460705694746, test error 0.07703700518956402\n",
            "Loss: 0.0\n",
            "training error 0.04435425308891544, test error 0.07654582292227252\n",
            "Loss: 0.0\n",
            "training error 0.04424640840985321, test error 0.07610141896154723\n",
            "Loss: 0.0\n",
            "training error 0.04408534591383374, test error 0.0757512615552562\n",
            "Loss: 0.0\n",
            "training error 0.04397347382325487, test error 0.07554669330805078\n",
            "Loss: 0.0\n",
            "training error 0.043725555116526385, test error 0.07490090912095294\n",
            "Loss: 0.0\n",
            "training error 0.04356557688961654, test error 0.07454584289222367\n",
            "Loss: 0.0\n",
            "training error 0.04346208116748655, test error 0.0742005435793009\n",
            "Loss: 0.0\n",
            "training error 0.04326863753382016, test error 0.0740765397789413\n",
            "Loss: 0.0\n",
            "training error 0.04312471910285291, test error 0.07369690760263663\n",
            "Loss: 0.0\n",
            "training error 0.042992265137269864, test error 0.07352673983289536\n",
            "Loss: 0.0\n",
            "training error 0.04286098524005922, test error 0.07308404166961464\n",
            "Loss: 0.0\n",
            "training error 0.04271616784445828, test error 0.07291007505602233\n",
            "Loss: 0.0\n",
            "training error 0.042560224555428655, test error 0.07243341864833876\n",
            "Loss: 0.0\n",
            "training error 0.04246026122960837, test error 0.07217174446384023\n",
            "Loss: 0.0\n",
            "training error 0.04233578385337541, test error 0.0717534473651159\n",
            "Loss: 0.0\n",
            "training error 0.04224384992092024, test error 0.07160726477930623\n",
            "Loss: 0.0\n",
            "training error 0.04216802582359811, test error 0.07115495155776377\n",
            "Loss: 0.0\n",
            "training error 0.041963814428305685, test error 0.07098956961055293\n",
            "Loss: 0.0\n",
            "training error 0.041841500594453136, test error 0.07054834710776763\n",
            "Loss: 0.0\n",
            "training error 0.041753708963967, test error 0.07034798131497687\n",
            "Loss: 0.0\n",
            "training error 0.04162326520236325, test error 0.07021104756951857\n",
            "Loss: 0.0\n",
            "training error 0.04151903412083945, test error 0.06982381629275089\n",
            "Loss: 0.0\n",
            "training error 0.04143270605901072, test error 0.06959122552322532\n",
            "Loss: 0.0\n",
            "training error 0.04130761114066557, test error 0.06924830047898776\n",
            "Loss: 0.0\n",
            "training error 0.04119936317431179, test error 0.06905297802149381\n",
            "Loss: 0.0\n",
            "training error 0.04107722280255203, test error 0.06877527016014362\n",
            "Loss: 0.0\n",
            "training error 0.04100910002256835, test error 0.06860798173800385\n",
            "Loss: 0.0\n",
            "training error 0.040893348789892085, test error 0.06840867836785533\n",
            "Loss: 0.0\n",
            "training error 0.040794459872611155, test error 0.06817569833270648\n",
            "Loss: 0.0\n",
            "training error 0.04071288763611315, test error 0.0678786078033661\n",
            "Loss: 0.0\n",
            "training error 0.040601017643581046, test error 0.0677307438768645\n",
            "Loss: 0.0\n",
            "training error 0.04055995325325342, test error 0.06735813372139005\n",
            "Loss: 0.0\n",
            "training error 0.04041751187017867, test error 0.06727412373009968\n",
            "Loss: 0.0\n",
            "training error 0.0403330433412038, test error 0.06710225182065807\n",
            "Loss: 0.0\n",
            "training error 0.0402923144073926, test error 0.0669632397862027\n",
            "Loss: 0.0\n",
            "training error 0.040188070059546575, test error 0.06691058437372421\n",
            "Loss: 0.0\n",
            "training error 0.04010087493366693, test error 0.06654684113608776\n",
            "Loss: 0.0\n",
            "training error 0.04003079713878591, test error 0.06630120047881885\n",
            "Loss: 0.0\n",
            "training error 0.03992742561733184, test error 0.06619200691321951\n",
            "Loss: 0.0\n",
            "training error 0.03986408169322449, test error 0.06611879753979015\n",
            "Loss: 0.0\n",
            "training error 0.03977090661208035, test error 0.06592688845722414\n",
            "Loss: 0.0\n",
            "training error 0.03972229603702633, test error 0.0658276136660793\n",
            "Loss: 0.0\n",
            "training error 0.039629060782002914, test error 0.06563989493598975\n",
            "Loss: 0.0\n",
            "training error 0.039546626213538605, test error 0.0654669659478759\n",
            "Loss: 0.0\n",
            "training error 0.03953228217489606, test error 0.06512640397100067\n",
            "Loss: 0.0\n",
            "training error 0.03943389819797643, test error 0.06504155158156981\n",
            "Loss: 0.0\n",
            "training error 0.0393485148287985, test error 0.06500967202803785\n",
            "Loss: 0.0\n",
            "training error 0.03926220888879191, test error 0.06484904503975943\n",
            "Loss: 0.0\n",
            "training error 0.03920578010124557, test error 0.06464060920568841\n",
            "Loss: 0.0\n",
            "training error 0.03914032046445376, test error 0.06452525097685889\n",
            "Loss: 0.0\n",
            "training error 0.039097953697914654, test error 0.0643654153396575\n",
            "Loss: 0.0\n",
            "training error 0.03909142056103413, test error 0.06424266655443343\n",
            "Loss: 0.0\n",
            "training error 0.03896995474909473, test error 0.0640195267467057\n",
            "Loss: 0.0\n",
            "training error 0.03895020484818877, test error 0.06398048439540255\n",
            "Loss: 0.0\n",
            "training error 0.03885615979983748, test error 0.0636915486602443\n",
            "Loss: 0.0\n",
            "training error 0.03877955797315714, test error 0.0636074330267793\n",
            "Loss: 0.0\n",
            "training error 0.03875536735899166, test error 0.06364038435752688\n",
            "Loss: 0.05180421403532254\n",
            "training error 0.03863866557353725, test error 0.06341442954692837\n",
            "Loss: 0.0\n",
            "training error 0.038616929010786026, test error 0.06316574136228999\n",
            "Loss: 0.0\n",
            "training error 0.03855351868573249, test error 0.06302642534095464\n",
            "Loss: 0.0\n",
            "training error 0.038608518158424296, test error 0.06284056994997615\n",
            "Loss: 0.0\n",
            "training error 0.03849070058041603, test error 0.06301490676580102\n",
            "Loss: 0.27742717159255914\n",
            "training error 0.038421736252017086, test error 0.06266291005759299\n",
            "Loss: 0.0\n",
            "training error 0.038356637378120666, test error 0.06264005011383057\n",
            "Loss: 0.0\n",
            "training error 0.0382735519277585, test error 0.06262752078012528\n",
            "Loss: 0.0\n",
            "training error 0.03824408078149188, test error 0.0624006042786159\n",
            "Loss: 0.0\n",
            "training error 0.03820264751917037, test error 0.062376816246690066\n",
            "Loss: 0.0\n",
            "training error 0.038147881327604716, test error 0.06210153688245399\n",
            "Loss: 0.0\n",
            "training error 0.03810377154221881, test error 0.06212536625432951\n",
            "Loss: 0.038371629869038415\n",
            "training error 0.038105038324049906, test error 0.06184443067170685\n",
            "Loss: 0.0\n",
            "training error 0.0380440609673395, test error 0.06178989936592229\n",
            "Loss: 0.0\n",
            "training error 0.03798102878787617, test error 0.06167503174302718\n",
            "Loss: 0.0\n",
            "training error 0.03796685960594754, test error 0.06164485839970643\n",
            "Loss: 0.0\n",
            "training error 0.03787126134186147, test error 0.06155964936871411\n",
            "Loss: 0.0\n",
            "training error 0.03783532978771954, test error 0.06143635912944313\n",
            "Loss: 0.0\n",
            "training error 0.03780342880556893, test error 0.061110965274742794\n",
            "Loss: 0.0\n",
            "training error 0.0377498820524903, test error 0.06106757414244234\n",
            "Loss: 0.0\n",
            "training error 0.03770810642761103, test error 0.06089828953796977\n",
            "Loss: 0.0\n",
            "training error 0.03768659769015665, test error 0.060781865932103205\n",
            "Loss: 0.0\n",
            "training error 0.037660027344594324, test error 0.060741162565456565\n",
            "Loss: 0.0\n",
            "training error 0.03758818064796873, test error 0.06068327858798546\n",
            "Loss: 0.0\n",
            "training error 0.03765901289688356, test error 0.06077619920728946\n",
            "Loss: 0.1531239271610474\n",
            "training error 0.03752707814795242, test error 0.06058806210265076\n",
            "Loss: 0.0\n",
            "training error 0.03750594419522654, test error 0.060584791655139476\n",
            "Loss: 0.0\n",
            "training error 0.03743779817023591, test error 0.06044100700943116\n",
            "Loss: 0.0\n",
            "training error 0.037426615342495397, test error 0.0602555679189509\n",
            "Loss: 0.0\n",
            "training error 0.03739284920977157, test error 0.060069282497315635\n",
            "Loss: 0.0\n",
            "training error 0.037324436051897114, test error 0.06019368609976676\n",
            "Loss: 0.20710019710437066\n",
            "training error 0.03731463861538789, test error 0.06014146158732725\n",
            "Loss: 0.1201597339119953\n",
            "training error 0.037274626643156984, test error 0.06003987293419777\n",
            "Loss: 0.0\n",
            "training error 0.03726116265605707, test error 0.059965515724076664\n",
            "Loss: 0.0\n",
            "training error 0.03719158254541423, test error 0.05990559607616593\n",
            "Loss: 0.0\n",
            "training error 0.037189519651452854, test error 0.059870865759015984\n",
            "Loss: 0.0\n",
            "training error 0.03714511036003269, test error 0.059818759333326074\n",
            "Loss: 0.0\n",
            "training error 0.037141365656677056, test error 0.05971403225932212\n",
            "Loss: 0.0\n",
            "training error 0.03707606551491117, test error 0.0596552802226958\n",
            "Loss: 0.0\n",
            "training error 0.037080696512048886, test error 0.05962696261840833\n",
            "Loss: 0.0\n",
            "training error 0.03704665767756142, test error 0.059379015632036965\n",
            "Loss: 0.0\n",
            "training error 0.037012622234798925, test error 0.05924223893960124\n",
            "Loss: 0.0\n",
            "training error 0.036929190950819604, test error 0.059180894379607\n",
            "Loss: 0.0\n",
            "training error 0.03692787784609837, test error 0.059143719933723284\n",
            "Loss: 0.0\n",
            "training error 0.03692783906878493, test error 0.05900687996991465\n",
            "Loss: 0.0\n",
            "training error 0.03687610939146783, test error 0.05912527144530308\n",
            "Loss: 0.20064012103129247\n",
            "training error 0.036876325231730206, test error 0.05900180986456327\n",
            "Loss: 0.0\n",
            "training error 0.036823820269922244, test error 0.05911875505054114\n",
            "Loss: 0.19820609951850532\n",
            "training error 0.036777506510441524, test error 0.05899946444654963\n",
            "Loss: 0.0\n",
            "training error 0.036752099778974076, test error 0.05898517833780311\n",
            "Loss: 0.0\n",
            "training error 0.03671342579609403, test error 0.05897127438589099\n",
            "Loss: 0.0\n",
            "training error 0.03670505445088891, test error 0.058887081675878056\n",
            "Loss: 0.0\n",
            "training error 0.03666872009782454, test error 0.05886041001925105\n",
            "Loss: 0.0\n",
            "training error 0.036640783217573594, test error 0.058810415815756606\n",
            "Loss: 0.0\n",
            "training error 0.036637806367644514, test error 0.058778576099873706\n",
            "Loss: 0.0\n",
            "training error 0.03663882102043468, test error 0.05862523307098124\n",
            "Loss: 0.0\n",
            "training error 0.03658607343528108, test error 0.05875710655724271\n",
            "Loss: 0.2249432187362066\n",
            "training error 0.03660386473761922, test error 0.05883536823468339\n",
            "Loss: 0.3584380866302528\n",
            "training error 0.03654668653093025, test error 0.058628360468025234\n",
            "Loss: 0.005334557971314879\n",
            "training error 0.036549525413616966, test error 0.05872201688232201\n",
            "Loss: 0.1650890005393757\n",
            "training error 0.03647371865805111, test error 0.05854104639100876\n",
            "Loss: 0.0\n",
            "training error 0.036490959245747655, test error 0.05854376010334427\n",
            "Loss: 0.00463557196670461\n",
            "training error 0.036501391143275864, test error 0.058318674690055694\n",
            "Loss: 0.0\n",
            "training error 0.03642718241623681, test error 0.05835307670273927\n",
            "Loss: 0.05898970246909219\n",
            "training error 0.03642468875238587, test error 0.058290959257173425\n",
            "Loss: 0.0\n",
            "training error 0.03642620698885547, test error 0.058309454804731195\n",
            "Loss: 0.03172970181561929\n",
            "training error 0.036360578163616195, test error 0.058334488767175376\n",
            "Loss: 0.07467626293453655\n",
            "training error 0.03635541163225437, test error 0.0582497547808751\n",
            "Loss: 0.0\n",
            "training error 0.03633264153188969, test error 0.05816167009622421\n",
            "Loss: 0.0\n",
            "training error 0.036363356088854014, test error 0.058012546398367094\n",
            "Loss: 0.0\n",
            "training error 0.036330103093498496, test error 0.0580525275430392\n",
            "Loss: 0.06891809988402997\n",
            "training error 0.036384481685755074, test error 0.057998200569146965\n",
            "Loss: 0.0\n",
            "training error 0.03625959757308762, test error 0.0579114801063462\n",
            "Loss: 0.0\n",
            "training error 0.03626081811440089, test error 0.05789122678693089\n",
            "Loss: 0.0\n",
            "training error 0.03636099032873751, test error 0.05776607828380219\n",
            "Loss: 0.0\n",
            "training error 0.03621166437239243, test error 0.057825321905974995\n",
            "Loss: 0.10255780543340087\n",
            "training error 0.03621229913520433, test error 0.05790502318203388\n",
            "Loss: 0.24053025990280474\n",
            "training error 0.036254020073088866, test error 0.057798683014213544\n",
            "Loss: 0.05644269332456542\n",
            "training error 0.036234089763527086, test error 0.05781485493110473\n",
            "Loss: 0.08443821833101772\n",
            "training error 0.0361648955762657, test error 0.057738985065882584\n",
            "Loss: 0.0\n",
            "training error 0.03614289691166351, test error 0.057882708258110524\n",
            "Loss: 0.24891880600939054\n",
            "training error 0.0362075413199883, test error 0.057534017546109234\n",
            "Loss: 0.0\n",
            "training error 0.036154063639111084, test error 0.05775626627424736\n",
            "Loss: 0.3862909937760062\n",
            "training error 0.036115842876667276, test error 0.05760338564313269\n",
            "Loss: 0.12056883906614857\n",
            "training error 0.03606742839597698, test error 0.057576243481599496\n",
            "Loss: 0.07339298955859608\n",
            "training error 0.03610321717877243, test error 0.057689940921159026\n",
            "Loss: 0.27101075450681744\n",
            "training error 0.03602266586422039, test error 0.05761184053600829\n",
            "Loss: 0.1352643066107495\n",
            "training error 0.03606136546994296, test error 0.05736448376859184\n",
            "Loss: 0.0\n",
            "training error 0.03600405994726716, test error 0.05756322955668718\n",
            "Loss: 0.34646139046083935\n",
            "training error 0.03597314328527083, test error 0.05755645760739596\n",
            "Loss: 0.33465626497841683\n",
            "training error 0.03596946142496936, test error 0.057467966919040055\n",
            "Loss: 0.18039585410665193\n",
            "training error 0.03595543446713499, test error 0.05754376995382239\n",
            "Loss: 0.31253865362721456\n",
            "training error 0.035944480108431896, test error 0.057456612444787426\n",
            "Loss: 0.16060229281804617\n",
            "training error 0.035908452812589815, test error 0.05738057196345884\n",
            "Loss: 0.028045567239654012\n",
            "training error 0.035923697494649055, test error 0.057526586289905514\n",
            "Loss: 0.28258342211811005\n",
            "training error 0.035910350144421344, test error 0.05728428077466024\n",
            "Loss: 0.0\n",
            "training error 0.035861195355883814, test error 0.057279907869505195\n",
            "Loss: 0.0\n",
            "training error 0.035912274369074695, test error 0.05716587304649852\n",
            "Loss: 0.0\n",
            "training error 0.035882181750855176, test error 0.05724190697346943\n",
            "Loss: 0.13300580034711906\n",
            "training error 0.03586851690230919, test error 0.057290299863028055\n",
            "Loss: 0.21765926049677198\n",
            "training error 0.03584749160838778, test error 0.05712721706217772\n",
            "Loss: 0.0\n",
            "training error 0.03578832333137307, test error 0.05723604578859324\n",
            "Loss: 0.19050241200628815\n",
            "training error 0.03577394542352823, test error 0.05722212205039\n",
            "Loss: 0.16612919916785263\n",
            "training error 0.0357909155308104, test error 0.05727170940824294\n",
            "Loss: 0.25293083314028575\n",
            "training error 0.03576489396828706, test error 0.05722303579747069\n",
            "Loss: 0.16772869434316107\n",
            "training error 0.03575527335122074, test error 0.05717014136178906\n",
            "Loss: 0.07513808972108293\n",
            "training error 0.03576956547225701, test error 0.0571778518704553\n",
            "Loss: 0.08863517405803822\n",
            "training error 0.03572126806114066, test error 0.05720042757983198\n",
            "Loss: 0.12815348168382368\n",
            "training error 0.035740789758370535, test error 0.05723728050841784\n",
            "Loss: 0.19266376326423362\n",
            "training error 0.035722130081878696, test error 0.05717319580517758\n",
            "Loss: 0.08048482906111154\n",
            "training error 0.03572180302074568, test error 0.057043534488810735\n",
            "Loss: 0.0\n",
            "training error 0.03573416876187385, test error 0.05720806613761167\n",
            "Loss: 0.2884317219740362\n",
            "training error 0.03569099969136833, test error 0.056975091495935014\n",
            "Loss: 0.0\n",
            "training error 0.03574667795636072, test error 0.056991115666357124\n",
            "Loss: 0.028124870011403402\n",
            "training error 0.035681448253781736, test error 0.05723999647446994\n",
            "Loss: 0.464948754937633\n",
            "training error 0.03565833247513121, test error 0.057079954762287666\n",
            "Loss: 0.18405107144081612\n",
            "training error 0.03569322188010741, test error 0.057054158347430654\n",
            "Loss: 0.13877441776690702\n",
            "training error 0.03562810373382101, test error 0.05719379057814578\n",
            "Loss: 0.3838503396284443\n",
            "training error 0.035623694497382404, test error 0.05709579980698762\n",
            "Loss: 0.21186154841228433\n",
            "training error 0.03565389357895466, test error 0.05712807263537948\n",
            "Loss: 0.26850529841691007\n",
            "training error 0.03562759465900259, test error 0.057017432557436847\n",
            "Loss: 0.07431503906378811\n",
            "training error 0.035604966020863764, test error 0.05702307454584158\n",
            "Loss: 0.08421759166457043\n",
            "training error 0.035591979183917224, test error 0.056947086962991465\n",
            "Loss: 0.0\n",
            "training error 0.035623265408411396, test error 0.05698924307225338\n",
            "Loss: 0.07402680542607509\n",
            "training error 0.03561409546630789, test error 0.0567933922693536\n",
            "Loss: 0.0\n",
            "training error 0.035601066799277443, test error 0.05679547002945407\n",
            "Loss: 0.003658453945876161\n",
            "training error 0.03555559596228438, test error 0.05674338253926498\n",
            "Loss: 0.0\n",
            "training error 0.03555881408595728, test error 0.05688086684307723\n",
            "Loss: 0.24229134334230462\n",
            "training error 0.035533330017400146, test error 0.05685683684429535\n",
            "Loss: 0.19994279500672452\n",
            "training error 0.03558943964475304, test error 0.05669621766223027\n",
            "Loss: 0.0\n",
            "training error 0.03555767773689467, test error 0.05668678536984563\n",
            "Loss: 0.0\n",
            "training error 0.03551801911750803, test error 0.05663354215774225\n",
            "Loss: 0.0\n",
            "training error 0.03554041059985488, test error 0.056718032850616835\n",
            "Loss: 0.14918843084059663\n",
            "training error 0.03547047592463064, test error 0.05661353451411368\n",
            "Loss: 0.0\n",
            "training error 0.03546594185871863, test error 0.056644832371396094\n",
            "Loss: 0.05528334796798262\n",
            "training error 0.03547884408893122, test error 0.05667418872660768\n",
            "Loss: 0.10713730031972624\n",
            "training error 0.035467499501173, test error 0.056708670253651096\n",
            "Loss: 0.16804416179614368\n",
            "training error 0.03542526384605975, test error 0.056813191048361446\n",
            "Loss: 0.3526657290722435\n",
            "training error 0.03545409767158864, test error 0.05687173511154618\n",
            "Loss: 0.45607574169057585\n",
            "training error 0.03544685813804376, test error 0.05665058904008639\n",
            "Loss: 0.0654517091906115\n",
            "training error 0.03540040272195544, test error 0.056729816608597904\n",
            "Loss: 0.20539628108757757\n",
            "training error 0.0354292567102574, test error 0.05682678745367462\n",
            "Loss: 0.376681903702325\n",
            "training error 0.03540848658184844, test error 0.0567919611904324\n",
            "Loss: 0.3151661132802852\n",
            "training error 0.03539045343249015, test error 0.05664937167850077\n",
            "Loss: 0.06330140786061644\n",
            "training error 0.03538560518536119, test error 0.056678934694003785\n",
            "Loss: 0.11552039711246742\n",
            "training error 0.0354125103238246, test error 0.05643289561404708\n",
            "Loss: 0.0\n",
            "training error 0.03537220635779012, test error 0.05646639618634567\n",
            "Loss: 0.05936355371112256\n",
            "training error 0.035390354634502054, test error 0.05644979053886677\n",
            "Loss: 0.02993807890920852\n",
            "training error 0.035352349077720814, test error 0.056629143115120004\n",
            "Loss: 0.3477537328849589\n",
            "training error 0.03535146335093592, test error 0.05657604238886701\n",
            "Loss: 0.2536583906644241\n",
            "training error 0.03536841864845108, test error 0.05666372380358743\n",
            "Loss: 0.4090312698448395\n",
            "training error 0.03535173788290316, test error 0.05667291923331997\n",
            "Loss: 0.42532571944287056\n",
            "training error 0.035344256987478485, test error 0.0564203236763169\n",
            "Loss: 0.0\n",
            "training error 0.03533837616364774, test error 0.05620940809491664\n",
            "Loss: 0.0\n",
            "training error 0.03533838230895972, test error 0.05633810857441335\n",
            "Loss: 0.22896608211810943\n",
            "training error 0.03536523778295949, test error 0.056005426905450864\n",
            "Loss: 0.0\n",
            "training error 0.035334834597738805, test error 0.055888291295316764\n",
            "Loss: 0.0\n",
            "training error 0.03535324082794355, test error 0.05609311963681879\n",
            "Loss: 0.3664959810986135\n",
            "training error 0.0353524479918828, test error 0.0561466900945488\n",
            "Loss: 0.46234871963903856\n",
            "training error 0.035310730633439244, test error 0.05605922861519376\n",
            "Loss: 0.3058553337652592\n",
            "training error 0.03527311780142527, test error 0.05608794434808696\n",
            "Loss: 0.3572359221276189\n",
            "training error 0.03529455096779739, test error 0.05612862757682139\n",
            "Loss: 0.4300297538793574\n",
            "training error 0.035312900448422446, test error 0.0561444603640155\n",
            "Loss: 0.4583590994849507\n",
            "training error 0.03530763225130407, test error 0.056288596492558185\n",
            "Loss: 0.7162595025963281\n",
            "training error 0.03527590931413744, test error 0.05619677959531956\n",
            "Loss: 0.5519730391697397\n",
            "training error 0.03528994230232005, test error 0.05618735361685293\n",
            "Loss: 0.5351072909993038\n",
            "training error 0.03522983855888114, test error 0.056221971982318354\n",
            "Loss: 0.5970493626981765\n",
            "training error 0.03531479172628782, test error 0.05625478996677837\n",
            "Loss: 0.6557700422884727\n",
            "training error 0.03523701607299488, test error 0.05615120688460047\n",
            "Loss: 0.47043053775690513\n",
            "training error 0.03526493667325352, test error 0.056306176692054094\n",
            "Loss: 0.747715464280696\n",
            "training error 0.035227749602488506, test error 0.05622899800147836\n",
            "Loss: 0.6096209031714483\n",
            "training error 0.03523927307035705, test error 0.05619262606334528\n",
            "Loss: 0.5445411927525123\n",
            "training error 0.03523560968314878, test error 0.05632003388239841\n",
            "Loss: 0.772509906950436\n",
            "training error 0.03526315817535985, test error 0.056070253275198906\n",
            "Loss: 0.3255815765070391\n",
            "training error 0.03524888367236919, test error 0.055992005130108276\n",
            "Loss: 0.18557345803162928\n",
            "training error 0.03519813678865597, test error 0.05608727842407554\n",
            "Loss: 0.3560443952514536\n",
            "training error 0.03520474608333901, test error 0.056049335786308156\n",
            "Loss: 0.2881542578219598\n",
            "training error 0.03520391739963661, test error 0.05597861481668056\n",
            "Loss: 0.16161439054653837\n",
            "training error 0.03521011752591701, test error 0.05598152325637393\n",
            "Loss: 0.1668184138329032\n",
            "training error 0.035223265741654665, test error 0.05596907482196685\n",
            "Loss: 0.14454463498128067\n",
            "training error 0.03521659643054033, test error 0.05615713767910613\n",
            "Loss: 0.4810424107775324\n",
            "training error 0.035188785289684076, test error 0.05621463252196708\n",
            "Loss: 0.5839169870589078\n",
            "training error 0.03518045255987673, test error 0.05614281377082184\n",
            "Loss: 0.45541287737740355\n",
            "training error 0.03517180921240692, test error 0.05610161329869484\n",
            "Loss: 0.3816935505343455\n",
            "training error 0.03520517575588816, test error 0.05626385394550077\n",
            "Loss: 0.6719880702730885\n",
            "training error 0.035164141187085815, test error 0.056219154313202166\n",
            "Loss: 0.5920077537119539\n",
            "training error 0.0352493360665324, test error 0.056371643662858115\n",
            "Loss: 0.8648544379131096\n",
            "training error 0.035172804372031255, test error 0.05629656141667038\n",
            "Loss: 0.7305110102513224\n",
            "training error 0.03519302816139835, test error 0.056158661229060346\n",
            "Loss: 0.4837684736413328\n",
            "training error 0.03516469458185727, test error 0.05621647441408317\n",
            "Loss: 0.5872126543147838\n",
            "training error 0.035186428354624176, test error 0.0562905115512395\n",
            "Loss: 0.7196860855834375\n",
            "training error 0.03521820227160711, test error 0.056065971555812465\n",
            "Loss: 0.31792036646249855\n",
            "training error 0.0351920077734313, test error 0.05601716387168923\n",
            "Loss: 0.23058958036754618\n",
            "training error 0.035218779090766233, test error 0.0563407264095953\n",
            "Loss: 0.8095347053783453\n",
            "training error 0.03518068956465192, test error 0.056264320942131296\n",
            "Loss: 0.6728236596599046\n",
            "training error 0.03515994836100218, test error 0.0558896461564966\n",
            "Loss: 0.002424230815489814\n",
            "training error 0.03513032344476321, test error 0.05593701526681018\n",
            "Loss: 0.08718100046385402\n",
            "training error 0.03515965457215708, test error 0.05587036807294963\n",
            "Loss: 0.0\n",
            "training error 0.035137653147330944, test error 0.056057738974941776\n",
            "Loss: 0.3353672231897642\n",
            "training error 0.03516595146864127, test error 0.05592777235652983\n",
            "Loss: 0.10274549024851254\n",
            "training error 0.03521535097180106, test error 0.056216912606928265\n",
            "Loss: 0.6202653498293742\n",
            "training error 0.035107683457865184, test error 0.056092295488313064\n",
            "Loss: 0.39721845947688905\n",
            "training error 0.035159723820705306, test error 0.055988476860249846\n",
            "Loss: 0.21139790442403772\n",
            "training error 0.03513277322237145, test error 0.05597293951269562\n",
            "Loss: 0.1835882656295862\n",
            "training error 0.03511507069149705, test error 0.0561673525358962\n",
            "Loss: 0.5315598826175716\n",
            "training error 0.035117409584434996, test error 0.05604611265039711\n",
            "Loss: 0.3145577584490189\n",
            "training error 0.03512252102388281, test error 0.05601227726434368\n",
            "Loss: 0.25399723733474655\n",
            "training error 0.0351008710698735, test error 0.05595688355092582\n",
            "Loss: 0.1548503812669022\n",
            "training error 0.035170480836662035, test error 0.05587693262039379\n",
            "Loss: 0.011749604791555512\n",
            "training error 0.035098193016352947, test error 0.05596115656764698\n",
            "Loss: 0.16249847249765192\n",
            "training error 0.035099629689346885, test error 0.05594788478064785\n",
            "Loss: 0.13874386436296682\n",
            "training error 0.035102661002054926, test error 0.0559712903798601\n",
            "Loss: 0.18063655277642532\n",
            "training error 0.03511722362516352, test error 0.05597003223634087\n",
            "Loss: 0.1783846551021595\n",
            "training error 0.0351014060231075, test error 0.055965230750866055\n",
            "Loss: 0.16979068008387\n",
            "training error 0.03508644675002234, test error 0.05598536728567315\n",
            "Loss: 0.20583220889716092\n",
            "training error 0.035090939620091945, test error 0.0561545876544428\n",
            "Loss: 0.5087125631999756\n",
            "training error 0.03517594095118646, test error 0.056309784078693956\n",
            "Loss: 0.7864920545548992\n",
            "training error 0.03508835138060653, test error 0.05606686731724548\n",
            "Loss: 0.35170565556197353\n",
            "training error 0.03510356202530204, test error 0.056080350561857006\n",
            "Loss: 0.37583874270026296\n",
            "training error 0.03507942843684304, test error 0.05603132530447358\n",
            "Loss: 0.2880905157341962\n",
            "training error 0.03513965741032727, test error 0.05599170000929777\n",
            "Loss: 0.2171668820755146\n",
            "training error 0.035064874561198764, test error 0.056095677104460596\n",
            "Loss: 0.40327107066269186\n",
            "training error 0.03507072159001536, test error 0.05619791450153984\n",
            "Loss: 0.5862614475038663\n",
            "training error 0.0350585007009057, test error 0.05606479629288201\n",
            "Loss: 0.3479988169730941\n",
            "training error 0.03519333770082465, test error 0.056090775166826375\n",
            "Loss: 0.39449730058866983\n",
            "training error 0.03506025068271525, test error 0.05606220246213568\n",
            "Loss: 0.34335622943377775\n",
            "training error 0.035089850343037174, test error 0.0560085531270605\n",
            "Loss: 0.24733156210898777\n",
            "training error 0.03505576450234316, test error 0.05612182564883401\n",
            "Loss: 0.45007324017636297\n",
            "training error 0.03508340657971963, test error 0.0561153766201635\n",
            "Loss: 0.4385303975337429\n",
            "training error 0.03511622493211226, test error 0.0559662461678279\n",
            "Loss: 0.17160813179730727\n",
            "training error 0.035070371484703114, test error 0.056005056558098544\n",
            "Loss: 0.24107320176063318\n",
            "training error 0.03507262873651654, test error 0.0558754549539062\n",
            "Loss: 0.00910479227544947\n",
            "training error 0.03506461362644296, test error 0.05606404361505576\n",
            "Loss: 0.34665163088463125\n",
            "training error 0.03501704043042447, test error 0.056081195201174126\n",
            "Loss: 0.37735052675726966\n",
            "training error 0.03504888344925087, test error 0.05601147070758619\n",
            "Loss: 0.25255361563454315\n",
            "training error 0.035044026157668745, test error 0.05602676594228612\n",
            "Loss: 0.2799299069093042\n",
            "training error 0.035013964511773105, test error 0.056026303268910116\n",
            "Loss: 0.27910178747503345\n",
            "training error 0.03503224682454872, test error 0.055935854519719455\n",
            "Loss: 0.11721141103693267\n",
            "training error 0.03502933440333834, test error 0.05609093623765296\n",
            "Loss: 0.3947855944233858\n",
            "training error 0.0350251175701018, test error 0.05603312702759454\n",
            "Loss: 0.2913153434614957\n",
            "training error 0.035043990324973136, test error 0.056105985772297876\n",
            "Loss: 0.4217221176001651\n",
            "training error 0.035017181989531826, test error 0.05603582177934155\n",
            "Loss: 0.2961385652156201\n",
            "training error 0.03499792771107423, test error 0.05608794726262826\n",
            "Loss: 0.3894357549149019\n",
            "training error 0.035030148247884536, test error 0.05597985483873232\n",
            "Loss: 0.19596571413265096\n",
            "training error 0.03500141315006036, test error 0.05605309935386018\n",
            "Loss: 0.3270629623774113\n",
            "training error 0.0350171239844708, test error 0.056028065206116896\n",
            "Loss: 0.28225540408355343\n",
            "training error 0.03503252967802468, test error 0.05606991396619493\n",
            "Loss: 0.3571587231799844\n",
            "training error 0.035062979541140266, test error 0.05611189943499939\n",
            "Loss: 0.4323067314222717\n",
            "training error 0.035053130528633784, test error 0.05615575529994521\n",
            "Loss: 0.5108024823157642\n",
            "training error 0.03502150961622884, test error 0.05616557714120543\n",
            "Loss: 0.5283821790297738\n",
            "training error 0.035068966519030215, test error 0.056179566751682807\n",
            "Loss: 0.5534215889350547\n",
            "training error 0.03506319434528187, test error 0.055994992103664966\n",
            "Loss: 0.22305926202708104\n",
            "training error 0.03501222763826452, test error 0.05604029122711901\n",
            "Loss: 0.30413824005510737\n",
            "training error 0.0350781552116958, test error 0.056062829999566265\n",
            "Loss: 0.3444794320405009\n",
            "training error 0.0350114437216093, test error 0.05619811272216695\n",
            "Loss: 0.5866162341894521\n",
            "training error 0.035026514184579756, test error 0.056122227846011494\n",
            "Loss: 0.4507931158302281\n",
            "training error 0.0349960390663559, test error 0.05617750001353978\n",
            "Loss: 0.5497224220701868\n",
            "training error 0.035013754371726304, test error 0.05614949822058476\n",
            "Loss: 0.4996032015945673\n",
            "training error 0.03498992947023595, test error 0.05618352964494466\n",
            "Loss: 0.5605146033513586\n",
            "training error 0.03503613967307428, test error 0.0562711184653102\n",
            "Loss: 0.7172861145953391\n",
            "training error 0.03509153133679057, test error 0.05608622257428654\n",
            "Loss: 0.3863488084686839\n",
            "training error 0.03499800332971801, test error 0.05608115570128145\n",
            "Loss: 0.3772798275762046\n",
            "training error 0.03499392581872839, test error 0.05610365134386946\n",
            "Loss: 0.4175438232574935\n",
            "training error 0.0349759115642844, test error 0.056063364753191805\n",
            "Loss: 0.34543656485344076\n",
            "training error 0.03500586638140811, test error 0.0559994914650551\n",
            "Loss: 0.2311124779719842\n",
            "training error 0.03498610022744922, test error 0.05597880245690839\n",
            "Loss: 0.19408210058178277\n",
            "training error 0.03497778369510882, test error 0.05596569093850163\n",
            "Loss: 0.17061435039686934\n",
            "training error 0.03496388007090887, test error 0.05599222556496618\n",
            "Loss: 0.21810755185547315\n",
            "training error 0.03500972316704766, test error 0.05610018442326062\n",
            "Loss: 0.41133852923775915\n",
            "training error 0.0349796602899903, test error 0.0560147823581216\n",
            "Loss: 0.2584809983414038\n",
            "training error 0.034969101248551925, test error 0.056002847803778254\n",
            "Loss: 0.23711984616898718\n",
            "training error 0.034973590622860734, test error 0.05610256709041441\n",
            "Loss: 0.41560316402713404\n",
            "training error 0.03495684003877266, test error 0.05611528664942145\n",
            "Loss: 0.4383693627219909\n",
            "training error 0.034953071050501, test error 0.056062245993200255\n",
            "Loss: 0.34343414383826154\n",
            "training error 0.03495585091783272, test error 0.05611360291365296\n",
            "Loss: 0.4353557155480692\n",
            "training error 0.034965984945985525, test error 0.05596074786933597\n",
            "Loss: 0.16176696074086117\n",
            "training error 0.03494339677572366, test error 0.055964099375136196\n",
            "Loss: 0.16776567869425385\n",
            "training error 0.034946662498449264, test error 0.05591381348940328\n",
            "Loss: 0.07776110656174673\n",
            "training error 0.03493640663781238, test error 0.05593036657908942\n",
            "Loss: 0.10738877907774391\n",
            "training error 0.035011867292361123, test error 0.055997164732420675\n",
            "Loss: 0.22694795800430878\n",
            "training error 0.03494518553642593, test error 0.05582478419353351\n",
            "Loss: 0.0\n",
            "training error 0.034941439231431295, test error 0.05577592936955783\n",
            "Loss: 0.0\n",
            "training error 0.03500987438442746, test error 0.0558219323419527\n",
            "Loss: 0.08247818174407229\n",
            "training error 0.03495105184837737, test error 0.05579165874240613\n",
            "Loss: 0.0282010053908488\n",
            "training error 0.03492843983310822, test error 0.05579172367343276\n",
            "Loss: 0.028317419455770043\n",
            "training error 0.0349452269202019, test error 0.05579662657667426\n",
            "Loss: 0.03710777632999207\n",
            "training error 0.03494482488255648, test error 0.05579303573688245\n",
            "Loss: 0.03066980240038486\n",
            "training error 0.0349380909424913, test error 0.05591004782317156\n",
            "Loss: 0.24045937939483064\n",
            "training error 0.03492023294849199, test error 0.05582729891881493\n",
            "Loss: 0.09209985353491046\n",
            "training error 0.03492187168330512, test error 0.05588785612035632\n",
            "Loss: 0.20067213951182605\n",
            "training error 0.0349284946542399, test error 0.05595302982516169\n",
            "Loss: 0.31752129925157746\n",
            "training error 0.03492975249572053, test error 0.05573241151727946\n",
            "Loss: 0.0\n",
            "training error 0.03503537940769033, test error 0.055718470831677996\n",
            "Loss: 0.0\n",
            "training error 0.03495807462040926, test error 0.05562418514096822\n",
            "Loss: 0.0\n",
            "training error 0.034964048302192574, test error 0.055617427230214955\n",
            "Loss: 0.0\n",
            "training error 0.03491279805801512, test error 0.05566340552636045\n",
            "Loss: 0.08266886556109743\n",
            "training error 0.034918184014887076, test error 0.05552735287593838\n",
            "Loss: 0.0\n",
            "training error 0.034946548021561004, test error 0.055712466214642126\n",
            "Loss: 0.33337324600604124\n",
            "training error 0.03490054274631114, test error 0.055630727951899654\n",
            "Loss: 0.1861696454218409\n",
            "training error 0.03490288321111397, test error 0.05565142779177557\n",
            "Loss: 0.22344828163229913\n",
            "training error 0.03491876331882105, test error 0.055761788086522594\n",
            "Loss: 0.422197706971561\n",
            "training error 0.034994365641954614, test error 0.05586684202650936\n",
            "Loss: 0.6113908425086967\n",
            "training error 0.034937083035303, test error 0.05589822575680188\n",
            "Loss: 0.6679102490120936\n",
            "training error 0.03489013755188574, test error 0.055906889875832214\n",
            "Loss: 0.6835135842722551\n",
            "training error 0.03490757489292055, test error 0.055881569750327265\n",
            "Loss: 0.6379142099215418\n",
            "training error 0.03497680445122447, test error 0.05600904963445\n",
            "Loss: 0.8674945473951201\n",
            "training error 0.03489325345674372, test error 0.055890617262096536\n",
            "Loss: 0.6542080026213037\n",
            "training error 0.034903081470541074, test error 0.05583570213218027\n",
            "Loss: 0.5553105636618705\n",
            "training error 0.03498810479551809, test error 0.05600330086365637\n",
            "Loss: 0.8571415042625485\n",
            "training error 0.034929842212062176, test error 0.05571548831953991\n",
            "Loss: 0.33881579772381265\n",
            "training error 0.03490312618389427, test error 0.0558559461714254\n",
            "Loss: 0.5917683420298747\n",
            "training error 0.03489649437138454, test error 0.0558577614813589\n",
            "Loss: 0.5950375595226598\n",
            "training error 0.03495548011858939, test error 0.056002810338356385\n",
            "Loss: 0.8562581102691702\n",
            "training error 0.03497852290676247, test error 0.05560558171944178\n",
            "Loss: 0.14088343753426713\n",
            "training error 0.03491545753875158, test error 0.05576682144179729\n",
            "Loss: 0.4312623481150579\n",
            "training error 0.03489342156188778, test error 0.055937871790280654\n",
            "Loss: 0.7393093549038277\n",
            "training error 0.03490412132974608, test error 0.05584176439593005\n",
            "Loss: 0.5662281807204739\n",
            "training error 0.034936589920796665, test error 0.05558550891766004\n",
            "Loss: 0.1047340431509447\n",
            "training error 0.034890207302207835, test error 0.05573892127409184\n",
            "Loss: 0.38101653904907273\n",
            "training error 0.03492895601518505, test error 0.05570363678006504\n",
            "Loss: 0.3174721916251233\n",
            "training error 0.034900525720396564, test error 0.055797262923649636\n",
            "Loss: 0.4860848459934619\n",
            "training error 0.03490134121366836, test error 0.055879503072304805\n",
            "Loss: 0.6341923000601435\n",
            "training error 0.034882768187206405, test error 0.055878214278207064\n",
            "Loss: 0.6318712924287873\n",
            "training error 0.0349279973821505, test error 0.055848548730578255\n",
            "Loss: 0.5784461855358192\n",
            "training error 0.034890596456405326, test error 0.05585172807735659\n",
            "Loss: 0.5841719163939718\n",
            "training error 0.034918627940703176, test error 0.055980249141615715\n",
            "Loss: 0.8156273299921413\n",
            "training error 0.03489767608065264, test error 0.05583964612378965\n",
            "Loss: 0.5624133542778686\n",
            "training error 0.03489451609745077, test error 0.05583377542801981\n",
            "Loss: 0.5518407347205212\n",
            "training error 0.0349019920265694, test error 0.05567483578348392\n",
            "Loss: 0.2656040670172999\n",
            "training error 0.0349688322927251, test error 0.05560843496217361\n",
            "Loss: 0.14602188297430096\n",
            "training error 0.03489537238041239, test error 0.055560550518095664\n",
            "Loss: 0.05978610619428171\n",
            "training error 0.034913703878001016, test error 0.05547662574305765\n",
            "Loss: 0.0\n",
            "training error 0.034897648792140655, test error 0.055555608045143944\n",
            "Loss: 0.1423704146176874\n",
            "training error 0.034897743242520256, test error 0.05551140578622215\n",
            "Loss: 0.06269314814779658\n",
            "training error 0.034889816305284094, test error 0.05563751049831881\n",
            "Loss: 0.29000457959773573\n",
            "training error 0.034979007265859056, test error 0.055517480629789034\n",
            "Loss: 0.07364342402618185\n",
            "training error 0.034870631434379566, test error 0.055679379354571415\n",
            "Loss: 0.36547574550194195\n",
            "training error 0.0349087580395054, test error 0.055672738983494084\n",
            "Loss: 0.3535060718089511\n",
            "training error 0.03489888177652881, test error 0.055621514825111705\n",
            "Loss: 0.26117140347561296\n",
            "training error 0.0348977970112594, test error 0.05566974253639727\n",
            "Loss: 0.3481047932403891\n",
            "training error 0.0349532784066461, test error 0.0554985885694289\n",
            "Loss: 0.03958933348429117\n",
            "training error 0.03492174025869204, test error 0.05550805848685585\n",
            "Loss: 0.056659436974015875\n",
            "training error 0.034896381273269904, test error 0.05551127466231355\n",
            "Loss: 0.0624567893086736\n",
            "training error 0.03488430030766558, test error 0.055573581246710635\n",
            "Loss: 0.17476820616675326\n",
            "training error 0.034903424430334305, test error 0.05553460401590734\n",
            "Loss: 0.10450937142105765\n",
            "training error 0.034909715276030734, test error 0.05553113132876035\n",
            "Loss: 0.09824964112838952\n",
            "training error 0.0348896763392176, test error 0.0556054655110314\n",
            "Loss: 0.23224153640934997\n",
            "training error 0.03490688945719145, test error 0.05548170495925806\n",
            "Loss: 0.009155596852505887\n",
            "training error 0.03488240313526089, test error 0.05553090714172523\n",
            "Loss: 0.09784553033016508\n",
            "training error 0.0349153617459865, test error 0.055539266820559395\n",
            "Loss: 0.1129143610713923\n",
            "training error 0.034959614690253354, test error 0.0556383816133487\n",
            "Loss: 0.29157481754609194\n",
            "training error 0.034871967520813237, test error 0.05577135461308684\n",
            "Loss: 0.5312667561906848\n",
            "training error 0.03490751280944275, test error 0.05565519308789354\n",
            "Loss: 0.32187852531428973\n",
            "training error 0.03488804150482317, test error 0.055828575604595164\n",
            "Loss: 0.6344110818267579\n",
            "training error 0.03490471904531927, test error 0.055740317521299675\n",
            "Loss: 0.475320506087229\n",
            "training error 0.03494412177180985, test error 0.055971762488788185\n",
            "Loss: 0.8925141698844063\n",
            "training error 0.03487664358684097, test error 0.0557428309359931\n",
            "Loss: 0.4798510893009089\n",
            "training error 0.03488052433980328, test error 0.05577629120079973\n",
            "Loss: 0.540165256499181\n",
            "training error 0.03495644502594515, test error 0.05557878927905438\n",
            "Loss: 0.1841560019708144\n",
            "training error 0.034876086902778415, test error 0.05572482472485701\n",
            "Loss: 0.4473937959905916\n",
            "training error 0.0349047721881621, test error 0.05568627855371416\n",
            "Loss: 0.3779119725621394\n",
            "training error 0.034926804257659594, test error 0.05565813246562115\n",
            "Loss: 0.3271769328656582\n",
            "training error 0.03493348617024744, test error 0.05550155922425526\n",
            "Loss: 0.04494411991293745\n",
            "training error 0.03489449115321396, test error 0.05572785263050539\n",
            "Loss: 0.4528517805161947\n",
            "training error 0.03488645445416902, test error 0.055734002983071414\n",
            "Loss: 0.46393816596888016\n",
            "training error 0.03487635898019507, test error 0.0557086813153735\n",
            "Loss: 0.4182943162235997\n",
            "training error 0.03488772770715159, test error 0.05587398069087411\n",
            "Loss: 0.7162565179375191\n",
            "training error 0.03495341890772151, test error 0.05596398556268315\n",
            "Loss: 0.8784957864645948\n",
            "training error 0.034888415641268544, test error 0.055811159774189156\n",
            "Loss: 0.603017985774601\n",
            "training error 0.034903144572128994, test error 0.05592424415396741\n",
            "Loss: 0.8068594744441215\n",
            "training error 0.03486730562498958, test error 0.05592927329284903\n",
            "Loss: 0.8159248038765554\n",
            "training error 0.034942107250072754, test error 0.055941155693636425\n",
            "Loss: 0.8373435556990438\n",
            "training error 0.03489745406673902, test error 0.05587166258845164\n",
            "Loss: 0.7120779969993496\n",
            "training error 0.0348762858000269, test error 0.055832283192691755\n",
            "Loss: 0.6410942354016624\n",
            "training error 0.034859230303357476, test error 0.05585815474478185\n",
            "Loss: 0.6877292852872197\n",
            "training error 0.034865396640091535, test error 0.05589208634342593\n",
            "Loss: 0.7488930604620636\n",
            "training error 0.03488821264175969, test error 0.0558681826674183\n",
            "Loss: 0.7058052271855075\n",
            "training error 0.0348905927867978, test error 0.05578252325080129\n",
            "Loss: 0.5513989065600633\n",
            "training error 0.03492246513526751, test error 0.055912230679612165\n",
            "Loss: 0.7852044545247461\n",
            "training error 0.03502104971068359, test error 0.05559459924622284\n",
            "Loss: 0.2126544316368273\n",
            "training error 0.03490178639286867, test error 0.055928148090497405\n",
            "Loss: 0.8138965580404944\n",
            "training error 0.034987909364093225, test error 0.05571109134047435\n",
            "Loss: 0.42263853339357116\n",
            "training error 0.03488441179640975, test error 0.05586951703487181\n",
            "Loss: 0.7082105058693555\n",
            "training error 0.03486748053541694, test error 0.0559458732968288\n",
            "Loss: 0.8458473230590569\n",
            "training error 0.03490418987920765, test error 0.056007643319492505\n",
            "Loss: 0.957191554681569\n",
            "training error 0.03488703290328826, test error 0.05576983485356661\n",
            "Loss: 0.5285272970042776\n",
            "training error 0.03489011351857591, test error 0.05583017043779784\n",
            "Loss: 0.6372858659025926\n",
            "training error 0.03489039424400514, test error 0.055726481047533326\n",
            "Loss: 0.4503794185913401\n",
            "training error 0.034908617881840374, test error 0.05574612846870097\n",
            "Loss: 0.4857950930388766\n",
            "training error 0.034863368813546146, test error 0.05574924284476041\n",
            "Loss: 0.49140894575203653\n",
            "training error 0.03487854960451239, test error 0.05574216525794707\n",
            "Loss: 0.47865116404028907\n",
            "training error 0.034892071393870586, test error 0.05573208289801785\n",
            "Loss: 0.4604770956030402\n",
            "training error 0.0348656298335968, test error 0.05581549208019609\n",
            "Loss: 0.6108272314684582\n",
            "training error 0.03489244223135606, test error 0.05578982735693318\n",
            "Loss: 0.5645650031530947\n",
            "training error 0.03486050630400545, test error 0.05577509076523514\n",
            "Loss: 0.5380013982101861\n",
            "training error 0.03484783873781852, test error 0.05578683670097549\n",
            "Loss: 0.559174163465892\n",
            "training error 0.03485446784568422, test error 0.055869736477886206\n",
            "Loss: 0.7086060652810033\n",
            "training error 0.03492025978902934, test error 0.05585700290915283\n",
            "Loss: 0.6856530313449793\n",
            "training error 0.03487829554692512, test error 0.05576262378076587\n",
            "Loss: 0.5155288986623541\n",
            "training error 0.03485208312377899, test error 0.0559051211624918\n",
            "Loss: 0.7723891164879815\n",
            "training error 0.034866481039580996, test error 0.055849771444791446\n",
            "Loss: 0.6726178759718371\n",
            "training error 0.034888753275329504, test error 0.055904038250094414\n",
            "Loss: 0.7704371008726119\n",
            "training error 0.03490324971672726, test error 0.05573095162666287\n",
            "Loss: 0.4584379100184277\n",
            "training error 0.03485957518135346, test error 0.055896567949491896\n",
            "Loss: 0.7569714286143281\n",
            "training error 0.034843580550640206, test error 0.05586466530300018\n",
            "Loss: 0.699464963387908\n",
            "training error 0.03484037445291621, test error 0.05579285092964622\n",
            "Loss: 0.5700151772985995\n",
            "training error 0.03484796304472158, test error 0.05575965266628781\n",
            "Loss: 0.5101732836834794\n",
            "training error 0.03486851234219667, test error 0.055879319636360045\n",
            "Loss: 0.7258802926614294\n",
            "training error 0.034862778236929876, test error 0.0559061456818968\n",
            "Loss: 0.7742358751746892\n",
            "training error 0.03490031115264955, test error 0.05578451743032337\n",
            "Loss: 0.5549935367225478\n",
            "training error 0.03484863964883594, test error 0.055849858150004504\n",
            "Loss: 0.6727741674042997\n",
            "training error 0.03485667497167852, test error 0.05584567623205133\n",
            "Loss: 0.6652360053456707\n",
            "training error 0.034861430896191134, test error 0.055925095225728534\n",
            "Loss: 0.8083935831785904\n",
            "training error 0.034885950689340126, test error 0.05586727131117068\n",
            "Loss: 0.7041624519889211\n",
            "training error 0.03489908660717927, test error 0.05580217276763822\n",
            "Loss: 0.5868183585792552\n",
            "training error 0.03489747418609674, test error 0.05599998657809943\n",
            "Loss: 0.9433898115320583\n",
            "training error 0.03485923892587287, test error 0.056035379973281556\n",
            "Loss: 1.0071885640842604\n",
            "training error 0.03486516132305134, test error 0.05592296851289626\n",
            "Loss: 0.8045600536446873\n",
            "training error 0.03487862819126008, test error 0.05573310207950983\n",
            "Loss: 0.46231423237610425\n",
            "training error 0.03487035665407979, test error 0.055745042993800106\n",
            "Loss: 0.48383845835475725\n",
            "training error 0.03487738101176599, test error 0.05561962595886404\n",
            "Loss: 0.2577666069106366\n",
            "training error 0.03485484093057466, test error 0.055695407471996104\n",
            "Loss: 0.3943674043042078\n",
            "training error 0.03491241788942203, test error 0.05571057070239924\n",
            "Loss: 0.42170005152279977\n",
            "training error 0.03487009200985068, test error 0.055835179603204686\n",
            "Loss: 0.6463151919291032\n",
            "training error 0.03487012169753178, test error 0.05576250451965125\n",
            "Loss: 0.5153139232325632\n",
            "training error 0.03488047049658464, test error 0.05557181342436455\n",
            "Loss: 0.17158159861372368\n",
            "training error 0.03488820392095772, test error 0.05569110617797269\n",
            "Loss: 0.3866140596012757\n",
            "training error 0.034864324813752616, test error 0.05576281772034393\n",
            "Loss: 0.5158784865752164\n",
            "training error 0.03486105817655052, test error 0.055486140594403786\n",
            "Loss: 0.017151099618417653\n",
            "training error 0.03485352741992885, test error 0.05549300686828333\n",
            "Loss: 0.02952797688444697\n",
            "training error 0.034858581448243525, test error 0.055476627780989496\n",
            "Loss: 3.673496395961706e-06\n",
            "training error 0.03484334294366226, test error 0.055502886351301524\n",
            "Loss: 0.047336347321302874\n",
            "training error 0.03487765972373813, test error 0.05558959020445089\n",
            "Loss: 0.20362532846975867\n",
            "training error 0.034896551292951924, test error 0.055464553615618084\n",
            "Loss: 0.0\n",
            "training error 0.034854752723766765, test error 0.05562246734367972\n",
            "Loss: 0.28471107719718614\n",
            "training error 0.03486316388914992, test error 0.05571250982025634\n",
            "Loss: 0.4470534575228946\n",
            "training error 0.03488193374497664, test error 0.05557416596925534\n",
            "Loss: 0.19762595476182643\n",
            "training error 0.03483328829806495, test error 0.0555204136483879\n",
            "Loss: 0.10071303044631996\n",
            "training error 0.03482974981141174, test error 0.055635633228043095\n",
            "Loss: 0.3084485518636493\n",
            "training error 0.03485903880913505, test error 0.055714579393623315\n",
            "Loss: 0.450784801655435\n",
            "training error 0.034853225409925456, test error 0.055745540936966834\n",
            "Loss: 0.5066070184140514\n",
            "training error 0.034866140639380074, test error 0.055677249823281164\n",
            "Loss: 0.3834813296021755\n",
            "training error 0.034880688156785855, test error 0.055538748187799095\n",
            "Loss: 0.13376934879021363\n",
            "training error 0.034877014135420326, test error 0.05556139167759275\n",
            "Loss: 0.17459450344767813\n",
            "training error 0.03483379267439304, test error 0.05554814555530932\n",
            "Loss: 0.1507123635584362\n",
            "training error 0.034844877098154654, test error 0.05561399157933509\n",
            "Loss: 0.26942966989809136\n",
            "training error 0.03488082787449279, test error 0.055721871796173474\n",
            "Loss: 0.463932662901545\n",
            "training error 0.03483261064753124, test error 0.055627180230856235\n",
            "Loss: 0.2932081926867891\n",
            "training error 0.03482068490194091, test error 0.05563164183378704\n",
            "Loss: 0.30125225441623993\n",
            "training error 0.03485375470755905, test error 0.0556187394654511\n",
            "Loss: 0.277989886841179\n",
            "training error 0.03490545857981007, test error 0.05559415529612046\n",
            "Loss: 0.2336657776073503\n",
            "training error 0.03486532705755645, test error 0.055424573905525396\n",
            "Loss: 0.0\n",
            "training error 0.034848133173599055, test error 0.05523833873873191\n",
            "Loss: 0.0\n",
            "training error 0.03488622314925412, test error 0.05543718117403397\n",
            "Loss: 0.3599717874256836\n",
            "training error 0.034851218792942305, test error 0.055302035956454006\n",
            "Loss: 0.1153134203100814\n",
            "training error 0.0348814572851518, test error 0.055117565884439675\n",
            "Loss: 0.0\n",
            "training error 0.03487935455145022, test error 0.05511699052796482\n",
            "Loss: 0.0\n",
            "training error 0.03482616052906971, test error 0.05533354206587896\n",
            "Loss: 0.39289434317766325\n",
            "training error 0.03486493282453054, test error 0.055299979121676404\n",
            "Loss: 0.33200033593767664\n",
            "training error 0.03487235129636806, test error 0.055272362088957284\n",
            "Loss: 0.2818941301115263\n",
            "training error 0.03483750948981997, test error 0.05550118366323888\n",
            "Loss: 0.6970502772264631\n",
            "training error 0.0348617537322764, test error 0.05540932423392044\n",
            "Loss: 0.5303876411889563\n",
            "training error 0.034872304333326956, test error 0.05573262791760519\n",
            "Loss: 1.1169648120174624\n",
            "training error 0.03484832579629533, test error 0.05578267029545653\n",
            "Loss: 1.2077578276955325\n",
            "training error 0.03484114680260933, test error 0.05566928216284357\n",
            "Loss: 1.0020351793310178\n",
            "training error 0.03487715916131935, test error 0.05574182216004148\n",
            "Loss: 1.1336461335994752\n",
            "training error 0.03485927988916511, test error 0.055668089376529374\n",
            "Loss: 0.9998710801979227\n",
            "training error 0.03497535180722047, test error 0.05582313281509607\n",
            "Loss: 1.2811698903861135\n",
            "training error 0.034900383111216814, test error 0.05557696721363334\n",
            "Loss: 0.8345460832719809\n",
            "training error 0.034845904449841994, test error 0.055642575484568364\n",
            "Loss: 0.9535806501207222\n",
            "training error 0.03483246976432334, test error 0.05561218995547529\n",
            "Loss: 0.8984514988335857\n",
            "training error 0.034881880644861515, test error 0.055746728091596705\n",
            "Loss: 1.1425470759554024\n",
            "training error 0.03486284244392326, test error 0.055571697825698686\n",
            "Loss: 0.8249857138030059\n",
            "training error 0.03491248369602304, test error 0.05577418571081501\n",
            "Loss: 1.1923640542687908\n",
            "training error 0.03485516581339194, test error 0.05561541003176097\n",
            "Loss: 0.9042937559213593\n",
            "training error 0.034876471416645266, test error 0.05563597369056059\n",
            "Loss: 0.9416028662385889\n",
            "training error 0.03484277177910267, test error 0.055687136206138625\n",
            "Loss: 1.0344281730776395\n",
            "training error 0.03487946960599197, test error 0.055479759743223334\n",
            "Loss: 0.6581803755675963\n",
            "training error 0.03483118377585011, test error 0.055602122987052215\n",
            "Loss: 0.8801867707948441\n",
            "training error 0.034886473315084626, test error 0.05567784489505973\n",
            "Loss: 1.0175707376663512\n",
            "training error 0.03482961855795788, test error 0.0554695611282489\n",
            "Loss: 0.6396767982192131\n",
            "training error 0.03483184583856176, test error 0.055610731450907086\n",
            "Loss: 0.895805301074537\n",
            "training error 0.034878426965519856, test error 0.05550433783744445\n",
            "Loss: 0.702772966682752\n",
            "training error 0.034842143419531985, test error 0.05572612716449287\n",
            "Loss: 1.1051703489126208\n",
            "training error 0.03485348927552873, test error 0.0555907198880777\n",
            "Loss: 0.8594978709378642\n",
            "training error 0.03483033566756062, test error 0.05571969624580816\n",
            "Loss: 1.0935025879860838\n",
            "training error 0.034830365426895125, test error 0.05581782371243242\n",
            "Loss: 1.2715374655878797\n",
            "training error 0.034875135007832465, test error 0.05587594954996737\n",
            "Loss: 1.3769964846275018\n",
            "training error 0.034828798210193475, test error 0.05578929148677529\n",
            "Loss: 1.2197708045568412\n",
            "training error 0.034845059973773865, test error 0.055641405116330624\n",
            "Loss: 0.9514572246097641\n",
            "training error 0.03483424233617907, test error 0.05575786860087034\n",
            "Loss: 1.1627595533910018\n",
            "training error 0.03486731421707595, test error 0.05559768499567522\n",
            "Loss: 0.8721348228664771\n",
            "training error 0.03483189430402649, test error 0.05546850719349511\n",
            "Loss: 0.6377646206062959\n",
            "training error 0.034868628991449505, test error 0.055523768901279205\n",
            "Loss: 0.7380271843906216\n",
            "training error 0.03487151391287428, test error 0.05551695761791633\n",
            "Loss: 0.7256693192429831\n",
            "training error 0.03484313765382986, test error 0.05556280903680712\n",
            "Loss: 0.8088585834818085\n",
            "training error 0.03487436267495911, test error 0.05568773396456677\n",
            "Loss: 1.0355126996862474\n",
            "training error 0.03487595544449977, test error 0.055653468571522834\n",
            "Loss: 0.9733442236578904\n",
            "training error 0.0348566538558494, test error 0.05568877690399328\n",
            "Loss: 1.0374049282287023\n",
            "training error 0.03486075856993441, test error 0.0556871906000026\n",
            "Loss: 1.0345268610928215\n",
            "training error 0.03488433651250532, test error 0.055386198633634656\n",
            "Loss: 0.4884303426059722\n",
            "training error 0.034826077111677047, test error 0.05547546411021233\n",
            "Loss: 0.6503867116359086\n",
            "training error 0.03481328582243564, test error 0.0555261377355788\n",
            "Loss: 0.7423250139290349\n",
            "training error 0.03482542716845946, test error 0.055553283026190864\n",
            "Loss: 0.7915753274023274\n",
            "training error 0.034982131841261, test error 0.05553459155354812\n",
            "Loss: 0.7576629666879553\n",
            "training error 0.03488836088060294, test error 0.05568140361273186\n",
            "Loss: 1.024027399465277\n",
            "training error 0.03482900872650043, test error 0.05553046142095528\n",
            "Loss: 0.750169573900572\n",
            "training error 0.0348637976245725, test error 0.055453415807775644\n",
            "Loss: 0.6103839788569987\n",
            "training error 0.03483933745181641, test error 0.05537462283174583\n",
            "Loss: 0.467428103953349\n",
            "training error 0.03482063776002016, test error 0.0554414300274583\n",
            "Loss: 0.5886379070875858\n",
            "training error 0.03489485834767125, test error 0.05541778002070149\n",
            "Loss: 0.5457291660074581\n",
            "training error 0.034821208952035256, test error 0.055619290613884396\n",
            "Loss: 0.9113343836592902\n",
            "training error 0.034893174890127436, test error 0.05549912194497323\n",
            "Loss: 0.6933096552405704\n",
            "training error 0.034843242047187695, test error 0.055644031181357\n",
            "Loss: 0.9562217536619233\n",
            "training error 0.034834377709759445, test error 0.05571689689505621\n",
            "Loss: 1.0884236627306754\n",
            "training error 0.03484308184136233, test error 0.05554767427052604\n",
            "Loss: 0.7813992354003707\n",
            "training error 0.03483977767441834, test error 0.05562066830300237\n",
            "Loss: 0.9138339561228337\n",
            "training error 0.03482573132204012, test error 0.055606874776897644\n",
            "Loss: 0.8888080503674622\n",
            "training error 0.03485931391215468, test error 0.055711794592842284\n",
            "Loss: 1.0791664406562207\n",
            "training error 0.03485985213569255, test error 0.055576570740011245\n",
            "Loss: 0.8338267522303155\n",
            "training error 0.03482601873945131, test error 0.05550349829846248\n",
            "Loss: 0.7012497721579303\n",
            "training error 0.03483072123024222, test error 0.05550573818880608\n",
            "Loss: 0.705313655766493\n",
            "training error 0.0348514779213945, test error 0.055567743394323854\n",
            "Loss: 0.8178110997013377\n",
            "training error 0.03490885873563827, test error 0.05539237583720244\n",
            "Loss: 0.49963778246908674\n",
            "training error 0.03481543491329258, test error 0.055702324376073836\n",
            "Loss: 1.0619844126141587\n",
            "training error 0.034866369272496964, test error 0.0557100809211663\n",
            "Loss: 1.0760572874539687\n",
            "training error 0.03481959620493706, test error 0.05571719542234962\n",
            "Loss: 1.088965287537369\n",
            "training error 0.034954790118930795, test error 0.055784639936392134\n",
            "Loss: 1.2113313917031965\n",
            "training error 0.03480551622094642, test error 0.05591811739545233\n",
            "Loss: 1.4535025584915262\n",
            "training error 0.034840442216511724, test error 0.05605958041650231\n",
            "Loss: 1.7101621106458076\n",
            "training error 0.03487964742625668, test error 0.055900834935924855\n",
            "Loss: 1.4221466020760465\n",
            "training error 0.034814701487465946, test error 0.05592095393774413\n",
            "Loss: 1.4586489611971842\n",
            "training error 0.03486460975335461, test error 0.05584188492373054\n",
            "Loss: 1.3151922643489078\n",
            "training error 0.034827163227403896, test error 0.05570295161271981\n",
            "Loss: 1.0631224222187763\n",
            "training error 0.03481279725765712, test error 0.055709096227691844\n",
            "Loss: 1.0742707358570414\n",
            "training error 0.03483724336234224, test error 0.05588205824669774\n",
            "Loss: 1.3880796309891696\n",
            "training error 0.03483671767254945, test error 0.055769960453988604\n",
            "Loss: 1.1846980754373515\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.055769960453988604\n",
            "training error 0.12505934142619402, test error 0.25005176406276125\n",
            "training error 0.12503477567443586, test error 0.250205847459126\n",
            "training error 0.12509678925992007, test error 0.25032102879401047\n",
            "training error 0.12505140927545, test error 0.25022072205622303\n",
            "training error 0.12501170592899985, test error 0.25030452179862384\n",
            "training error 0.12510408730618383, test error 0.25035341730670096\n",
            "training error 0.12499839113521423, test error 0.2504450718728104\n",
            "training error 0.12506184423162323, test error 0.2505142487137012\n",
            "training error 0.1250202127677789, test error 0.25050673512394755\n",
            "training error 0.12510783170158876, test error 0.250443208920162\n",
            "training error 0.12500495038985193, test error 0.25053689948365415\n",
            "training error 0.12499271743969244, test error 0.25057237860716564\n",
            "training error 0.12499332435653455, test error 0.25056420410708047\n",
            "training error 0.12511179999058425, test error 0.2505878576830556\n",
            "training error 0.125132274828606, test error 0.2505142314719471\n",
            "training error 0.12496607540430667, test error 0.2505807329238859\n",
            "training error 0.12504267391182305, test error 0.2506797457483669\n",
            "training error 0.1250009723912273, test error 0.2506685491325915\n",
            "training error 0.124972912650628, test error 0.2506376410215276\n",
            "training error 0.12496875257974813, test error 0.2506310091232822\n",
            "training error 0.12508385502944983, test error 0.25055864916849546\n",
            "training error 0.12497544752509812, test error 0.2506558155311981\n",
            "training error 0.1250838992454462, test error 0.25066928780953285\n",
            "training error 0.12514739484100423, test error 0.250518553075731\n",
            "training error 0.1249691541875609, test error 0.25056185660855074\n",
            "training error 0.12496671011403972, test error 0.2504715233788236\n",
            "training error 0.1250657312384799, test error 0.25034999738226477\n",
            "training error 0.12499663608815234, test error 0.2504848409926852\n",
            "training error 0.12503995282045408, test error 0.2505760562558607\n",
            "training error 0.1250270612332505, test error 0.250395574352488\n",
            "training error 0.12496475319590244, test error 0.2504710280314024\n",
            "training error 0.12497645499098116, test error 0.2504505677676574\n",
            "training error 0.12496943602231102, test error 0.25046824005111223\n",
            "training error 0.1250585180353702, test error 0.2505289238892024\n",
            "training error 0.12508147276753626, test error 0.25044505513063336\n",
            "training error 0.12504505615368397, test error 0.2505818080474\n",
            "training error 0.12515821936693855, test error 0.2504157195817737\n",
            "training error 0.12498951855090333, test error 0.2504709770051999\n",
            "training error 0.12497355642521057, test error 0.25053964598347067\n",
            "training error 0.12500425400911822, test error 0.25066393787734453\n",
            "training error 0.12498523017064038, test error 0.2507512716473036\n",
            "training error 0.1250299290119197, test error 0.2505873178339397\n",
            "training error 0.12508983484283204, test error 0.2507085264235539\n",
            "training error 0.1249871673000038, test error 0.25068720512365333\n",
            "training error 0.12498394357055555, test error 0.25057015789961856\n",
            "training error 0.12497113511151467, test error 0.25066187890348024\n",
            "training error 0.12509168456860426, test error 0.2508228871549023\n",
            "training error 0.12501794564064908, test error 0.25075475830790295\n",
            "training error 0.1251204917307678, test error 0.2508908782287938\n",
            "training error 0.12513563344288942, test error 0.25074757838417916\n",
            "Loss: 0.2782681114152252\n",
            "training error 0.12523650596796135, test error 0.2508972692372775\n",
            "Loss: 0.3381320574503244\n",
            "training error 0.12494150089463231, test error 0.25080594984014554\n",
            "Loss: 0.3016118603326401\n",
            "training error 0.12500748195461817, test error 0.25079006646333163\n",
            "Loss: 0.2952598248357363\n",
            "training error 0.12498928174431276, test error 0.25086182448039807\n",
            "Loss: 0.3239570897142219\n",
            "training error 0.12500079398595257, test error 0.2508513866573389\n",
            "Loss: 0.3197828247982093\n",
            "training error 0.12495102970826621, test error 0.2506895480033615\n",
            "Loss: 0.255060764314452\n",
            "training error 0.12501103575184266, test error 0.250563725514098\n",
            "Loss: 0.2047421873849542\n",
            "training error 0.12501694188169177, test error 0.25062487419054497\n",
            "Loss: 0.22919659452587116\n",
            "training error 0.1250396042873703, test error 0.2505872648002511\n",
            "Loss: 0.21415595266724985\n",
            "training error 0.1251603146780913, test error 0.25054388275964307\n",
            "Loss: 0.19680672868931381\n",
            "training error 0.12492953603113718, test error 0.2506585665150851\n",
            "Loss: 0.2426707344370449\n",
            "training error 0.12495790568416058, test error 0.2507147590085304\n",
            "Loss: 0.2651430787757869\n",
            "training error 0.12494441408564502, test error 0.2506972501137264\n",
            "Loss: 0.25814097068443953\n",
            "training error 0.1249362948574805, test error 0.2506697779685307\n",
            "Loss: 0.24715438744686846\n",
            "training error 0.1249374657833961, test error 0.25067026544793775\n",
            "Loss: 0.24734933884380528\n",
            "training error 0.12497391849272431, test error 0.2506768823567049\n",
            "Loss: 0.2499955544351673\n",
            "training error 0.1249656356720006, test error 0.2505936950414967\n",
            "Loss: 0.21672751670707147\n",
            "training error 0.12497402967091308, test error 0.2505680534868733\n",
            "Loss: 0.20647301811573815\n",
            "training error 0.1250275709387364, test error 0.2505933181459187\n",
            "Loss: 0.2165767896848303\n",
            "training error 0.12515741718746712, test error 0.25065346371573666\n",
            "Loss: 0.24063003723675536\n",
            "training error 0.12494875607523866, test error 0.25053464663411495\n",
            "Loss: 0.19311304327871426\n",
            "training error 0.12496231502480382, test error 0.2505172842579103\n",
            "Loss: 0.1861695304945732\n",
            "training error 0.12507645767308687, test error 0.2505427325069833\n",
            "Loss: 0.1963467228724758\n",
            "training error 0.12495052356190679, test error 0.25064767978104235\n",
            "Loss: 0.23831694229980993\n",
            "training error 0.12497763721386305, test error 0.2506808647435599\n",
            "Loss: 0.2515881794142194\n",
            "training error 0.1249527955912067, test error 0.2506378077874097\n",
            "Loss: 0.2343689623006684\n",
            "training error 0.1250034196835938, test error 0.2505753022463279\n",
            "Loss: 0.2093719216614831\n",
            "training error 0.12506191334086042, test error 0.25074649295726004\n",
            "Loss: 0.2778340305267468\n",
            "training error 0.1249239524721892, test error 0.25065736173382297\n",
            "Loss: 0.24218892169451944\n",
            "training error 0.12504762313653303, test error 0.2505722506176098\n",
            "Loss: 0.2081515228654407\n",
            "training error 0.12493647446644406, test error 0.2506812574082976\n",
            "Loss: 0.25174521279456297\n",
            "training error 0.12496287438096837, test error 0.250652658003712\n",
            "Loss: 0.24030781914416988\n",
            "training error 0.12499393917680536, test error 0.25065886293174344\n",
            "Loss: 0.24278927655547733\n",
            "training error 0.12492837858856558, test error 0.25062606473067367\n",
            "Loss: 0.22967271199425543\n",
            "training error 0.12495416600651875, test error 0.25055625765166684\n",
            "Loss: 0.20175566079148055\n",
            "training error 0.12509312492944308, test error 0.2505150767047809\n",
            "Loss: 0.185286692040032\n",
            "training error 0.1249929505457996, test error 0.2506094434136763\n",
            "Loss: 0.22302556152935527\n",
            "training error 0.12496525850848117, test error 0.2505829675681302\n",
            "Loss: 0.2124374156527109\n",
            "training error 0.12494818930667499, test error 0.25064229903981605\n",
            "Loss: 0.23616509136348274\n",
            "training error 0.12498355658718198, test error 0.25064319768500865\n",
            "Loss: 0.23652447502788831\n",
            "training error 0.12501296632084644, test error 0.25059642035775936\n",
            "Loss: 0.21781741754136608\n",
            "training error 0.1250074391866945, test error 0.25078896350567864\n",
            "Loss: 0.2948187331053509\n",
            "training error 0.12494313310777093, test error 0.2507783525743377\n",
            "Loss: 0.29057523921092177\n",
            "training error 0.12500035549843908, test error 0.25070294504745416\n",
            "Loss: 0.2604184726045178\n",
            "training error 0.12492043891925247, test error 0.25072984271049065\n",
            "Loss: 0.2711753105485881\n",
            "training error 0.1251006962100345, test error 0.25063826052821264\n",
            "Loss: 0.2345500211325069\n",
            "training error 0.1250931790517441, test error 0.25091010300744\n",
            "Loss: 0.3432645028104364\n",
            "training error 0.12491881791543942, test error 0.25078187703957816\n",
            "Loss: 0.29198473346248033\n",
            "training error 0.12494843030132016, test error 0.2508301320552831\n",
            "Loss: 0.3112827439707644\n",
            "training error 0.1249347256879808, test error 0.2508122818149136\n",
            "Loss: 0.30414412591845963\n",
            "training error 0.1251562871957353, test error 0.2507106603826293\n",
            "Loss: 0.26350396780350227\n",
            "training error 0.12494240902026706, test error 0.25095304794913165\n",
            "Loss: 0.3604389234159511\n",
            "training error 0.12495512422859494, test error 0.2510193424439673\n",
            "Loss: 0.386951231811028\n",
            "training error 0.1249826016882081, test error 0.2510048566432412\n",
            "Loss: 0.38115811102243935\n",
            "training error 0.1251341966476717, test error 0.25083710829061884\n",
            "Loss: 0.31407266043541604\n",
            "training error 0.12491137034363575, test error 0.25082276323896713\n",
            "Loss: 0.30833582762181955\n",
            "training error 0.12492906632532265, test error 0.2508305627952762\n",
            "Loss: 0.3114550043004316\n",
            "training error 0.12491987405309557, test error 0.250869272288981\n",
            "Loss: 0.32693559642895664\n",
            "training error 0.1249382809214506, test error 0.2508218071313475\n",
            "Loss: 0.30795346374479227\n",
            "training error 0.12494247610439452, test error 0.2508951518916792\n",
            "Loss: 0.3372852945385585\n",
            "training error 0.12520247987784636, test error 0.2510864036066719\n",
            "Loss: 0.4137701438694741\n",
            "training error 0.1250175400515756, test error 0.25094274784882403\n",
            "Loss: 0.35631973619636437\n",
            "training error 0.12489369743193458, test error 0.25079830296457667\n",
            "Loss: 0.2985537433073393\n",
            "training error 0.12504267664470398, test error 0.2507916181015419\n",
            "Loss: 0.2958803516358799\n",
            "training error 0.12493037468938864, test error 0.2508898495850021\n",
            "Loss: 0.33516481092710215\n",
            "training error 0.1251356127365243, test error 0.25071138572870677\n",
            "Loss: 0.2637940461719612\n",
            "training error 0.124887932935323, test error 0.25083711524438\n",
            "Loss: 0.31407544136408116\n",
            "training error 0.1251153089967303, test error 0.250682387219578\n",
            "Loss: 0.252197043752278\n",
            "training error 0.12495168126250027, test error 0.2506668184806445\n",
            "Loss: 0.24597083735384828\n",
            "training error 0.12500074779956707, test error 0.25079803699045644\n",
            "Loss: 0.2984473756833417\n",
            "training error 0.12514048463800234, test error 0.250574037146333\n",
            "Loss: 0.20886598642058107\n",
            "training error 0.12525701231823355, test error 0.25066225600846403\n",
            "Loss: 0.24414622627879634\n",
            "training error 0.12495685942408409, test error 0.2506359998277214\n",
            "Loss: 0.23364592813410745\n",
            "training error 0.1251432576435255, test error 0.2506174467616363\n",
            "Loss: 0.22622623799328778\n",
            "training error 0.12500082632232104, test error 0.2506168408774178\n",
            "Loss: 0.22598393447634813\n",
            "training error 0.12503144631952104, test error 0.2507764828709972\n",
            "Loss: 0.28982751269615026\n",
            "training error 0.12491971414698692, test error 0.2507433470326914\n",
            "Loss: 0.27657592119869623\n",
            "training error 0.12495112095347645, test error 0.2507211495730177\n",
            "Loss: 0.26769877539774\n",
            "training error 0.12498738466927202, test error 0.2506390796728999\n",
            "Loss: 0.23487761117784167\n",
            "training error 0.12488898244187623, test error 0.2506309537127097\n",
            "Loss: 0.2316278999747734\n",
            "training error 0.12498499295766653, test error 0.2505621586043785\n",
            "Loss: 0.2041155532456651\n",
            "training error 0.12487864745236915, test error 0.2505823416616248\n",
            "Loss: 0.2121871048789581\n",
            "training error 0.12500743101250628, test error 0.2506651171412278\n",
            "Loss: 0.24529044246719423\n",
            "training error 0.1249210579552212, test error 0.25064263233843503\n",
            "Loss: 0.23629838321215235\n",
            "training error 0.12493394821443075, test error 0.2506440218241267\n",
            "Loss: 0.23685406243196816\n",
            "training error 0.1249249689716423, test error 0.2507706316364487\n",
            "Loss: 0.2874875033902935\n",
            "training error 0.12488749634564932, test error 0.2507160108405534\n",
            "Loss: 0.2656437079265883\n",
            "training error 0.12491636958377507, test error 0.2506578142426266\n",
            "Loss: 0.24236988774581913\n",
            "training error 0.1249572422290276, test error 0.2507879558379396\n",
            "Loss: 0.2944157494500166\n",
            "training error 0.12500389934894734, test error 0.25071721408591324\n",
            "Loss: 0.266124906435361\n",
            "training error 0.12486774198097841, test error 0.2506981409124361\n",
            "Loss: 0.2584972164054067\n",
            "training error 0.12486012875739666, test error 0.25068820289978017\n",
            "Loss: 0.25452283426370403\n",
            "training error 0.1248797538633008, test error 0.2506636478380198\n",
            "Loss: 0.24470284285016408\n",
            "training error 0.12497208362521493, test error 0.25076024657543894\n",
            "Loss: 0.28333433892506754\n",
            "training error 0.12493360502576846, test error 0.25071416516511963\n",
            "Loss: 0.26490559058487406\n",
            "training error 0.12487260776784043, test error 0.25066439716033656\n",
            "Loss: 0.2450025097289643\n",
            "training error 0.12496761124442766, test error 0.2507925575322633\n",
            "Loss: 0.2962560461345687\n",
            "training error 0.12486503822046631, test error 0.2506321995866403\n",
            "Loss: 0.23212614638197238\n",
            "training error 0.12489525931459325, test error 0.2506137494678079\n",
            "Loss: 0.22474762661766068\n",
            "training error 0.12493924836176588, test error 0.25072311295785205\n",
            "Loss: 0.26848396675269726\n",
            "training error 0.12488499552127716, test error 0.25065731734408253\n",
            "Loss: 0.24217116947404804\n",
            "training error 0.124988158088529, test error 0.25061627228286026\n",
            "Loss: 0.22575654373599985\n",
            "training error 0.12490979756815766, test error 0.2506853896346658\n",
            "Loss: 0.25339776117137003\n",
            "training error 0.1248474195978892, test error 0.25078872926665907\n",
            "Loss: 0.29472505689376316\n",
            "training error 0.12482945530779004, test error 0.2507664528885849\n",
            "Loss: 0.285816350267476\n",
            "training error 0.12485203758210024, test error 0.25085434458557465\n",
            "Loss: 0.3209657511602071\n",
            "training error 0.1248356187382367, test error 0.2508560612683103\n",
            "Loss: 0.3216522821039458\n",
            "training error 0.12486533314912038, test error 0.2508674803682011\n",
            "Loss: 0.32621897649764975\n",
            "training error 0.12486427307194506, test error 0.250783873592297\n",
            "Loss: 0.29278318922476565\n",
            "training error 0.12491949908350991, test error 0.2507991778089592\n",
            "Loss: 0.29890360861857257\n",
            "training error 0.12487954169685284, test error 0.250757545997094\n",
            "Loss: 0.28225433120943055\n",
            "training error 0.12490589352898034, test error 0.2506518662536072\n",
            "Loss: 0.23999118466340175\n",
            "training error 0.12483641792080855, test error 0.2507352446803098\n",
            "Loss: 0.2733356511642082\n",
            "training error 0.12482873760833936, test error 0.25066393505141193\n",
            "Loss: 0.24481770442421613\n",
            "training error 0.12481210718004844, test error 0.2507154501914578\n",
            "Loss: 0.2654194947130817\n",
            "training error 0.12488494059886335, test error 0.2508596876140909\n",
            "Loss: 0.32310252013534324\n",
            "training error 0.12488053446964387, test error 0.2507907120801748\n",
            "Loss: 0.2955180181124728\n",
            "training error 0.12478830799270024, test error 0.25068022409809615\n",
            "Loss: 0.2513319742776021\n",
            "training error 0.12480859662078375, test error 0.2506023082855775\n",
            "Loss: 0.22017210111666685\n",
            "training error 0.12486795855884765, test error 0.2506408696784092\n",
            "Loss: 0.23559346515951152\n",
            "training error 0.12483419499912303, test error 0.25070363388383277\n",
            "Loss: 0.26069395011663055\n",
            "training error 0.12485325324232362, test error 0.2507465650500991\n",
            "Loss: 0.27786286169271257\n",
            "training error 0.12497739358264749, test error 0.2507785950430017\n",
            "Loss: 0.29067220659879656\n",
            "training error 0.1247725422682919, test error 0.2506923641283665\n",
            "Loss: 0.2561869811262163\n",
            "training error 0.12477660242687759, test error 0.25073372943088473\n",
            "Loss: 0.27272967686495786\n",
            "training error 0.12475496492593674, test error 0.25074496255533063\n",
            "Loss: 0.277221996480459\n",
            "training error 0.12483946021019332, test error 0.25071976916240196\n",
            "Loss: 0.26714672545682383\n",
            "training error 0.12475029457566308, test error 0.250606129555582\n",
            "Loss: 0.2217002926968359\n",
            "training error 0.12473295866334487, test error 0.25060552687682697\n",
            "Loss: 0.2214592710998442\n",
            "training error 0.12472550567763802, test error 0.25060979769848696\n",
            "Loss: 0.22316724611695093\n",
            "training error 0.12474649397093197, test error 0.2505771967626024\n",
            "Loss: 0.21012957129520338\n",
            "training error 0.12470552471469512, test error 0.2505663760259299\n",
            "Loss: 0.2058021726411452\n",
            "training error 0.12476580599768375, test error 0.2505438228307561\n",
            "Loss: 0.19678276209695866\n",
            "training error 0.12487280778712154, test error 0.25064353619335367\n",
            "Loss: 0.23665985033558545\n",
            "training error 0.12477270640858987, test error 0.25062449654419544\n",
            "Loss: 0.22904556725720226\n",
            "training error 0.12488567662819469, test error 0.2506190106147038\n",
            "Loss: 0.2268516497248907\n",
            "training error 0.12491164077446322, test error 0.25063373007762957\n",
            "Loss: 0.23273821604483658\n",
            "training error 0.12465520258971745, test error 0.2505518301271511\n",
            "Loss: 0.19998501760791765\n",
            "training error 0.124679096700165, test error 0.2505066736791067\n",
            "Loss: 0.18192617758587826\n",
            "training error 0.12469006334634114, test error 0.250489545289398\n",
            "Loss: 0.17507624002479627\n",
            "training error 0.1246430424326601, test error 0.25050591018394136\n",
            "Loss: 0.1816208427412258\n",
            "training error 0.12469033616720557, test error 0.25055856564130524\n",
            "Loss: 0.20267866553294045\n",
            "training error 0.12461767038841308, test error 0.2503918267666942\n",
            "Loss: 0.1359969225602331\n",
            "training error 0.12464331569428724, test error 0.25043215080180553\n",
            "Loss: 0.15212319755872894\n",
            "training error 0.12483361014502582, test error 0.250342428708786\n",
            "Loss: 0.11624178982068134\n",
            "training error 0.12457865473109835, test error 0.25037835064295794\n",
            "Loss: 0.13060758896095148\n",
            "training error 0.12458004006796215, test error 0.25034995287375134\n",
            "Loss: 0.11925083276567161\n",
            "training error 0.12458642199589288, test error 0.25034307936367656\n",
            "Loss: 0.11650199789918769\n",
            "training error 0.12453321931100393, test error 0.2503803509085159\n",
            "Loss: 0.13140752955143054\n",
            "training error 0.12457538123689348, test error 0.2503874457127233\n",
            "Loss: 0.13424486374662425\n",
            "training error 0.1245201812512352, test error 0.2502383132324423\n",
            "Loss: 0.0746042206021702\n",
            "training error 0.12448318127208637, test error 0.25019243557436033\n",
            "Loss: 0.056256956285172244\n",
            "training error 0.12455203961407539, test error 0.2501989044544701\n",
            "Loss: 0.058843972671152045\n",
            "training error 0.12445705657247946, test error 0.2501567699528023\n",
            "Loss: 0.04199366096639956\n",
            "training error 0.12444500076243495, test error 0.2501640962739002\n",
            "Loss: 0.04492358274694741\n",
            "training error 0.12448647124939691, test error 0.25010300754817455\n",
            "Loss: 0.0204931509303119\n",
            "training error 0.12442763233386536, test error 0.2500261714041105\n",
            "Loss: 0.0\n",
            "training error 0.12444520308274963, test error 0.25000733538916214\n",
            "Loss: 0.0\n",
            "training error 0.12434706154062257, test error 0.25010204870843367\n",
            "Loss: 0.03788421612673787\n",
            "training error 0.12442738794931396, test error 0.25012752348967326\n",
            "Loss: 0.048073829643446864\n",
            "training error 0.12434997088885995, test error 0.24995946693247587\n",
            "Loss: 0.0\n",
            "training error 0.12433087232021484, test error 0.24988925528319783\n",
            "Loss: 0.0\n",
            "training error 0.1242467502042171, test error 0.24983049340515132\n",
            "Loss: 0.0\n",
            "training error 0.12426771084647559, test error 0.2498580636094309\n",
            "Loss: 0.01103556411541362\n",
            "training error 0.12420586186498739, test error 0.24987150814739525\n",
            "Loss: 0.016417028075688123\n",
            "training error 0.12416587705761718, test error 0.24967814271348038\n",
            "Loss: 0.0\n",
            "training error 0.1241377596620217, test error 0.24969889675772788\n",
            "Loss: 0.008312319221048803\n",
            "training error 0.12411250206005482, test error 0.2496779612617406\n",
            "Loss: 0.0\n",
            "training error 0.12405960070747366, test error 0.24954278420437453\n",
            "Loss: 0.0\n",
            "training error 0.12402879705846684, test error 0.24949800590395094\n",
            "Loss: 0.0\n",
            "training error 0.12422212922071764, test error 0.24949107500781534\n",
            "Loss: 0.0\n",
            "training error 0.12399874045336588, test error 0.24930843012405046\n",
            "Loss: 0.0\n",
            "training error 0.1239233747289823, test error 0.2492401150689722\n",
            "Loss: 0.0\n",
            "training error 0.12399230240204316, test error 0.2491457411112833\n",
            "Loss: 0.0\n",
            "training error 0.12405363757851438, test error 0.24907596305813395\n",
            "Loss: 0.0\n",
            "training error 0.12380134212459383, test error 0.24886910834323903\n",
            "Loss: 0.0\n",
            "training error 0.12380466050446767, test error 0.24888173871188635\n",
            "Loss: 0.0050751050346908855\n",
            "training error 0.1236684739086766, test error 0.2486245675839126\n",
            "Loss: 0.0\n",
            "training error 0.12360263573962012, test error 0.24849189047886835\n",
            "Loss: 0.0\n",
            "training error 0.1235839839825148, test error 0.24846969915047287\n",
            "Loss: 0.0\n",
            "training error 0.1234878547369771, test error 0.24822561431378387\n",
            "Loss: 0.0\n",
            "training error 0.12362561340290058, test error 0.24810777903156214\n",
            "Loss: 0.0\n",
            "training error 0.12357689507245131, test error 0.24779864157112533\n",
            "Loss: 0.0\n",
            "training error 0.12334679353023449, test error 0.2477435249644206\n",
            "Loss: 0.0\n",
            "training error 0.1235234247723905, test error 0.24780422098386268\n",
            "Loss: 0.02449953816181516\n",
            "training error 0.12322194126161919, test error 0.24751714721368823\n",
            "Loss: 0.0\n",
            "training error 0.12316665074863349, test error 0.24725024790773484\n",
            "Loss: 0.0\n",
            "training error 0.12301240796991746, test error 0.24717972395614007\n",
            "Loss: 0.0\n",
            "training error 0.12296896976027506, test error 0.24703609161742018\n",
            "Loss: 0.0\n",
            "training error 0.12286237004273223, test error 0.24692380029607544\n",
            "Loss: 0.0\n",
            "training error 0.12271771446525132, test error 0.2466315391768493\n",
            "Loss: 0.0\n",
            "training error 0.12265574526455215, test error 0.24640159796798888\n",
            "Loss: 0.0\n",
            "training error 0.12256101639827431, test error 0.2461370984216401\n",
            "Loss: 0.0\n",
            "training error 0.12242538903085179, test error 0.24597352357152222\n",
            "Loss: 0.0\n",
            "training error 0.12230664621946888, test error 0.2456993887723586\n",
            "Loss: 0.0\n",
            "training error 0.12226540733335488, test error 0.24539495706252842\n",
            "Loss: 0.0\n",
            "training error 0.1220913224628588, test error 0.245263516846338\n",
            "Loss: 0.0\n",
            "training error 0.12192592104536061, test error 0.24504609560583157\n",
            "Loss: 0.0\n",
            "training error 0.12178753489655024, test error 0.24481750520320653\n",
            "Loss: 0.0\n",
            "training error 0.12167699547045947, test error 0.24456477450358294\n",
            "Loss: 0.0\n",
            "training error 0.12165003375373008, test error 0.2443525724202061\n",
            "Loss: 0.0\n",
            "training error 0.12142341516537396, test error 0.24394836861413452\n",
            "Loss: 0.0\n",
            "training error 0.12122008725863147, test error 0.2435304604232929\n",
            "Loss: 0.0\n",
            "training error 0.12112696745742713, test error 0.24325403127575138\n",
            "Loss: 0.0\n",
            "training error 0.12091621160687678, test error 0.24274727086718806\n",
            "Loss: 0.0\n",
            "training error 0.12069948226935871, test error 0.24244479304562327\n",
            "Loss: 0.0\n",
            "training error 0.12057777210895301, test error 0.24213362846862937\n",
            "Loss: 0.0\n",
            "training error 0.12034631946418763, test error 0.24185274839369572\n",
            "Loss: 0.0\n",
            "training error 0.12014161745478599, test error 0.24150880213875914\n",
            "Loss: 0.0\n",
            "training error 0.11991312576861299, test error 0.24109125084896854\n",
            "Loss: 0.0\n",
            "training error 0.11970089929270623, test error 0.24055648246514377\n",
            "Loss: 0.0\n",
            "training error 0.1195004245065424, test error 0.24001163236899492\n",
            "Loss: 0.0\n",
            "training error 0.11930126885253681, test error 0.23960028286459256\n",
            "Loss: 0.0\n",
            "training error 0.11902237371330439, test error 0.2391812446758786\n",
            "Loss: 0.0\n",
            "training error 0.11883392321197643, test error 0.23868229329294166\n",
            "Loss: 0.0\n",
            "training error 0.1184962954660553, test error 0.23810265443678344\n",
            "Loss: 0.0\n",
            "training error 0.11824358016293132, test error 0.23753696427509213\n",
            "Loss: 0.0\n",
            "training error 0.11795168964563767, test error 0.23693794051347375\n",
            "Loss: 0.0\n",
            "training error 0.11777926665063133, test error 0.23611292087652405\n",
            "Loss: 0.0\n",
            "training error 0.11732569789515855, test error 0.23561182896188057\n",
            "Loss: 0.0\n",
            "training error 0.11706440597108403, test error 0.2351348819467102\n",
            "Loss: 0.0\n",
            "training error 0.11673425539129376, test error 0.23430868225284682\n",
            "Loss: 0.0\n",
            "training error 0.11639466139492126, test error 0.23374054496865898\n",
            "Loss: 0.0\n",
            "training error 0.11595916698440577, test error 0.23288532371338652\n",
            "Loss: 0.0\n",
            "training error 0.11558733619719971, test error 0.23213631530633286\n",
            "Loss: 0.0\n",
            "training error 0.1152565365222701, test error 0.2313131396835784\n",
            "Loss: 0.0\n",
            "training error 0.11499479931417991, test error 0.23050454366362882\n",
            "Loss: 0.0\n",
            "training error 0.1144472160851962, test error 0.22968423767065338\n",
            "Loss: 0.0\n",
            "training error 0.11399623774029878, test error 0.22885880084904442\n",
            "Loss: 0.0\n",
            "training error 0.11363852422394131, test error 0.2278976318359013\n",
            "Loss: 0.0\n",
            "training error 0.11317096019453489, test error 0.22701208395132325\n",
            "Loss: 0.0\n",
            "training error 0.11283401149011907, test error 0.22610038635804655\n",
            "Loss: 0.0\n",
            "training error 0.11220205655149142, test error 0.22518555642295143\n",
            "Loss: 0.0\n",
            "training error 0.11221147065821929, test error 0.22387123600490608\n",
            "Loss: 0.0\n",
            "training error 0.11111361867528032, test error 0.2229403472037934\n",
            "Loss: 0.0\n",
            "training error 0.11078336105138867, test error 0.22190403281280643\n",
            "Loss: 0.0\n",
            "training error 0.11011590523202575, test error 0.22075267789337688\n",
            "Loss: 0.0\n",
            "training error 0.10957397033395058, test error 0.2195439022949213\n",
            "Loss: 0.0\n",
            "training error 0.10896732073058503, test error 0.21835028151561464\n",
            "Loss: 0.0\n",
            "training error 0.10840736311628998, test error 0.21730582327552783\n",
            "Loss: 0.0\n",
            "training error 0.10783265794483714, test error 0.21587894402293642\n",
            "Loss: 0.0\n",
            "training error 0.10714497207091186, test error 0.21467938023418626\n",
            "Loss: 0.0\n",
            "training error 0.10656514733223667, test error 0.21336956407329496\n",
            "Loss: 0.0\n",
            "training error 0.10590635416570182, test error 0.21187353920742186\n",
            "Loss: 0.0\n",
            "training error 0.10521364961945216, test error 0.21043029015437578\n",
            "Loss: 0.0\n",
            "training error 0.10453426434310614, test error 0.20897063782471276\n",
            "Loss: 0.0\n",
            "training error 0.10384889692342235, test error 0.2075251566841587\n",
            "Loss: 0.0\n",
            "training error 0.10324174899256978, test error 0.20605797586339164\n",
            "Loss: 0.0\n",
            "training error 0.10262488675814964, test error 0.20438587747144082\n",
            "Loss: 0.0\n",
            "training error 0.1016886652929437, test error 0.20295607029276072\n",
            "Loss: 0.0\n",
            "training error 0.10095107685160373, test error 0.20144845266677655\n",
            "Loss: 0.0\n",
            "training error 0.10059092279694902, test error 0.19978573596945856\n",
            "Loss: 0.0\n",
            "training error 0.09948998309484519, test error 0.1984338306543302\n",
            "Loss: 0.0\n",
            "training error 0.09873832289682856, test error 0.19683043970206437\n",
            "Loss: 0.0\n",
            "training error 0.09795420446721043, test error 0.19530105035687584\n",
            "Loss: 0.0\n",
            "training error 0.0971670416124479, test error 0.1936652788977754\n",
            "Loss: 0.0\n",
            "training error 0.09636160009989485, test error 0.1919989952004858\n",
            "Loss: 0.0\n",
            "training error 0.09565137469341957, test error 0.19022299571912757\n",
            "Loss: 0.0\n",
            "training error 0.09477806941619943, test error 0.18850574255724367\n",
            "Loss: 0.0\n",
            "training error 0.09394544812611641, test error 0.1868110069817569\n",
            "Loss: 0.0\n",
            "training error 0.09322683294045071, test error 0.18491583358802285\n",
            "Loss: 0.0\n",
            "training error 0.09233785146976102, test error 0.18324904663502292\n",
            "Loss: 0.0\n",
            "training error 0.09162405010968426, test error 0.18147558476585002\n",
            "Loss: 0.0\n",
            "training error 0.09058484677599576, test error 0.17949212751583782\n",
            "Loss: 0.0\n",
            "training error 0.08976905332648065, test error 0.17777929687586722\n",
            "Loss: 0.0\n",
            "training error 0.08891633262873615, test error 0.1759770769818635\n",
            "Loss: 0.0\n",
            "training error 0.08812714541265956, test error 0.1743171050891222\n",
            "Loss: 0.0\n",
            "training error 0.08731453979932427, test error 0.1725277095891299\n",
            "Loss: 0.0\n",
            "training error 0.08641035863418224, test error 0.17064618626656955\n",
            "Loss: 0.0\n",
            "training error 0.08563072565432814, test error 0.16878693498289388\n",
            "Loss: 0.0\n",
            "training error 0.08474679515393994, test error 0.1669768108294396\n",
            "Loss: 0.0\n",
            "training error 0.08385913522155508, test error 0.16514112627875094\n",
            "Loss: 0.0\n",
            "training error 0.08310901659450272, test error 0.1633321970606494\n",
            "Loss: 0.0\n",
            "training error 0.0822310399445306, test error 0.16164442515046842\n",
            "Loss: 0.0\n",
            "training error 0.08142745060960473, test error 0.1599561481228014\n",
            "Loss: 0.0\n",
            "training error 0.08064454574661271, test error 0.15821474617923492\n",
            "Loss: 0.0\n",
            "training error 0.07982342072541636, test error 0.15647104801724374\n",
            "Loss: 0.0\n",
            "training error 0.0790670097979008, test error 0.15472713841170938\n",
            "Loss: 0.0\n",
            "training error 0.07824766248619133, test error 0.15310236402257493\n",
            "Loss: 0.0\n",
            "training error 0.07752648917035353, test error 0.15138793656399033\n",
            "Loss: 0.0\n",
            "training error 0.0767524802103156, test error 0.149885492467034\n",
            "Loss: 0.0\n",
            "training error 0.07597583923164943, test error 0.14815676889883098\n",
            "Loss: 0.0\n",
            "training error 0.0751235052543968, test error 0.14657774234323498\n",
            "Loss: 0.0\n",
            "training error 0.07439138710568631, test error 0.14492394656576496\n",
            "Loss: 0.0\n",
            "training error 0.07379460995456698, test error 0.143082437999512\n",
            "Loss: 0.0\n",
            "training error 0.07288114209110162, test error 0.1416258954643212\n",
            "Loss: 0.0\n",
            "training error 0.07228567656963719, test error 0.13999513166963898\n",
            "Loss: 0.0\n",
            "training error 0.07148743693874235, test error 0.13853220798153257\n",
            "Loss: 0.0\n",
            "training error 0.07079908387434533, test error 0.13708605631166967\n",
            "Loss: 0.0\n",
            "training error 0.07015625119900874, test error 0.1356533208081913\n",
            "Loss: 0.0\n",
            "training error 0.06957899588078253, test error 0.1339537119607493\n",
            "Loss: 0.0\n",
            "training error 0.06875978623829632, test error 0.13252041867008305\n",
            "Loss: 0.0\n",
            "training error 0.06808516538155347, test error 0.13106227827046155\n",
            "Loss: 0.0\n",
            "training error 0.06744447420256214, test error 0.12957829962566877\n",
            "Loss: 0.0\n",
            "training error 0.06681424842060227, test error 0.12830535188050826\n",
            "Loss: 0.0\n",
            "training error 0.06619277118512035, test error 0.12690287580719894\n",
            "Loss: 0.0\n",
            "training error 0.06559849437059753, test error 0.1255138833954209\n",
            "Loss: 0.0\n",
            "training error 0.06495348684173956, test error 0.12433155090874279\n",
            "Loss: 0.0\n",
            "training error 0.0643956673148903, test error 0.1229745915201613\n",
            "Loss: 0.0\n",
            "training error 0.06376559526619466, test error 0.12162635291081862\n",
            "Loss: 0.0\n",
            "training error 0.06325970106487504, test error 0.12023488249682751\n",
            "Loss: 0.0\n",
            "training error 0.06266834056362804, test error 0.11899425379637242\n",
            "Loss: 0.0\n",
            "training error 0.06213564476695742, test error 0.11780613991903376\n",
            "Loss: 0.0\n",
            "training error 0.061975544630477775, test error 0.11642792813594656\n",
            "Loss: 0.0\n",
            "training error 0.061043457131561396, test error 0.11526181427428252\n",
            "Loss: 0.0\n",
            "training error 0.060534538158960795, test error 0.11426743963171129\n",
            "Loss: 0.0\n",
            "training error 0.060208095215899364, test error 0.1131093626130828\n",
            "Loss: 0.0\n",
            "training error 0.05948303261050894, test error 0.11171655714540443\n",
            "Loss: 0.0\n",
            "training error 0.058960539748554114, test error 0.11056049515623853\n",
            "Loss: 0.0\n",
            "training error 0.058528004016193555, test error 0.10953835487922962\n",
            "Loss: 0.0\n",
            "training error 0.05815783743992728, test error 0.10838031946855647\n",
            "Loss: 0.0\n",
            "training error 0.05772606170055115, test error 0.10754237639014454\n",
            "Loss: 0.0\n",
            "training error 0.05711370571098138, test error 0.10650642759131841\n",
            "Loss: 0.0\n",
            "training error 0.056680218849784796, test error 0.10545067954534221\n",
            "Loss: 0.0\n",
            "training error 0.05625893042655834, test error 0.10435594917781939\n",
            "Loss: 0.0\n",
            "training error 0.05578054036012066, test error 0.10336260554319625\n",
            "Loss: 0.0\n",
            "training error 0.05534787472916648, test error 0.1024237195015978\n",
            "Loss: 0.0\n",
            "training error 0.0549982254787062, test error 0.10150444588045028\n",
            "Loss: 0.0\n",
            "training error 0.05453508527041015, test error 0.10056357515854936\n",
            "Loss: 0.0\n",
            "training error 0.054190265346910974, test error 0.09946299147170329\n",
            "Loss: 0.0\n",
            "training error 0.053787798722287276, test error 0.09847143943109113\n",
            "Loss: 0.0\n",
            "training error 0.053382587919236925, test error 0.09773289053702008\n",
            "Loss: 0.0\n",
            "training error 0.05312969252280107, test error 0.09669967319726258\n",
            "Loss: 0.0\n",
            "training error 0.05263858964693973, test error 0.09600429135001566\n",
            "Loss: 0.0\n",
            "training error 0.05225736739030412, test error 0.09515647474393137\n",
            "Loss: 0.0\n",
            "training error 0.05189294456116043, test error 0.09448875155052801\n",
            "Loss: 0.0\n",
            "training error 0.051509082177588275, test error 0.09361809425547954\n",
            "Loss: 0.0\n",
            "training error 0.0512186130862465, test error 0.09271549747624307\n",
            "Loss: 0.0\n",
            "training error 0.05085699847972533, test error 0.09200059455633808\n",
            "Loss: 0.0\n",
            "training error 0.050594107046268356, test error 0.09124195837737667\n",
            "Loss: 0.0\n",
            "training error 0.05025639922659412, test error 0.09070626557908798\n",
            "Loss: 0.0\n",
            "training error 0.04994642617039393, test error 0.08982452617707046\n",
            "Loss: 0.0\n",
            "training error 0.049836844899191886, test error 0.08880113450230147\n",
            "Loss: 0.0\n",
            "training error 0.04932109491727217, test error 0.08827752110114562\n",
            "Loss: 0.0\n",
            "training error 0.04908291746908666, test error 0.08757183203400712\n",
            "Loss: 0.0\n",
            "training error 0.048781101988795254, test error 0.08699465781329939\n",
            "Loss: 0.0\n",
            "training error 0.04847361108888994, test error 0.08615980826130533\n",
            "Loss: 0.0\n",
            "training error 0.04820235695866145, test error 0.08560136070187442\n",
            "Loss: 0.0\n",
            "training error 0.04797347521064618, test error 0.08502456512126365\n",
            "Loss: 0.0\n",
            "training error 0.0477533910370447, test error 0.08448138921937139\n",
            "Loss: 0.0\n",
            "training error 0.04741812048091651, test error 0.0837224959773078\n",
            "Loss: 0.0\n",
            "training error 0.04718463207115224, test error 0.08311195852115001\n",
            "Loss: 0.0\n",
            "training error 0.0469846063783407, test error 0.08264801050254095\n",
            "Loss: 0.0\n",
            "training error 0.046714895842165725, test error 0.0820527040271074\n",
            "Loss: 0.0\n",
            "training error 0.04648488295717343, test error 0.08165947342982054\n",
            "Loss: 0.0\n",
            "training error 0.04622624958480223, test error 0.0811240636484199\n",
            "Loss: 0.0\n",
            "training error 0.04600914935526939, test error 0.08057074914040614\n",
            "Loss: 0.0\n",
            "training error 0.045883900630053986, test error 0.08018645936105435\n",
            "Loss: 0.0\n",
            "training error 0.045566940169508595, test error 0.07951287130701663\n",
            "Loss: 0.0\n",
            "training error 0.04538841963627221, test error 0.07907784529385635\n",
            "Loss: 0.0\n",
            "training error 0.045238319715472514, test error 0.07861581731756404\n",
            "Loss: 0.0\n",
            "training error 0.04514102665816776, test error 0.07817742814529162\n",
            "Loss: 0.0\n",
            "training error 0.04486613667544019, test error 0.07789878088635832\n",
            "Loss: 0.0\n",
            "training error 0.04461827179043037, test error 0.07729290417178854\n",
            "Loss: 0.0\n",
            "training error 0.0443954191966288, test error 0.07682525658765244\n",
            "Loss: 0.0\n",
            "training error 0.04423838684894384, test error 0.07643475621955446\n",
            "Loss: 0.0\n",
            "training error 0.04407917438362395, test error 0.07625623713459469\n",
            "Loss: 0.0\n",
            "training error 0.04390855781588032, test error 0.07576314312900104\n",
            "Loss: 0.0\n",
            "training error 0.043689349107191015, test error 0.07527951966996628\n",
            "Loss: 0.0\n",
            "training error 0.04354398124104239, test error 0.07482000413862663\n",
            "Loss: 0.0\n",
            "training error 0.04337846954862147, test error 0.0744755983984434\n",
            "Loss: 0.0\n",
            "training error 0.04322370502383263, test error 0.07400314498819964\n",
            "Loss: 0.0\n",
            "training error 0.04307545751772556, test error 0.07355374130135416\n",
            "Loss: 0.0\n",
            "training error 0.04290269004590089, test error 0.07320337805511729\n",
            "Loss: 0.0\n",
            "training error 0.042789657184839895, test error 0.07268165063015004\n",
            "Loss: 0.0\n",
            "training error 0.04263254182673064, test error 0.07246388227608166\n",
            "Loss: 0.0\n",
            "training error 0.04249673385772971, test error 0.07219363559694807\n",
            "Loss: 0.0\n",
            "training error 0.04230517966127067, test error 0.07175203000683214\n",
            "Loss: 0.0\n",
            "training error 0.042160190620478696, test error 0.071487485569225\n",
            "Loss: 0.0\n",
            "training error 0.04206060020673486, test error 0.0709326817861239\n",
            "Loss: 0.0\n",
            "training error 0.04198503218033389, test error 0.07057538901095255\n",
            "Loss: 0.0\n",
            "training error 0.04186538572645517, test error 0.07020006508920265\n",
            "Loss: 0.0\n",
            "training error 0.04168631938463225, test error 0.07003042735842725\n",
            "Loss: 0.0\n",
            "training error 0.04150314525359437, test error 0.069928942301537\n",
            "Loss: 0.0\n",
            "training error 0.04142614050181973, test error 0.06956129232914252\n",
            "Loss: 0.0\n",
            "training error 0.04128217366512332, test error 0.0693423050786416\n",
            "Loss: 0.0\n",
            "training error 0.04117735814874325, test error 0.06907088037434349\n",
            "Loss: 0.0\n",
            "training error 0.041072750580405853, test error 0.06879057402613492\n",
            "Loss: 0.0\n",
            "training error 0.04096028160574802, test error 0.06861538565862435\n",
            "Loss: 0.0\n",
            "training error 0.040874571964225455, test error 0.06819419117131453\n",
            "Loss: 0.0\n",
            "training error 0.04073941005733912, test error 0.06781717285249049\n",
            "Loss: 0.0\n",
            "training error 0.040591370095358945, test error 0.06774547534439919\n",
            "Loss: 0.0\n",
            "training error 0.04051745641536038, test error 0.06740206825521682\n",
            "Loss: 0.0\n",
            "training error 0.040423843444001975, test error 0.0671187100141976\n",
            "Loss: 0.0\n",
            "training error 0.04037959427508067, test error 0.06717379296911158\n",
            "Loss: 0.08206795825236313\n",
            "training error 0.0402324083665685, test error 0.06689431498323889\n",
            "Loss: 0.0\n",
            "training error 0.0401433378312597, test error 0.06667877780423434\n",
            "Loss: 0.0\n",
            "training error 0.04002494679298872, test error 0.06638426422521103\n",
            "Loss: 0.0\n",
            "training error 0.039975308045902304, test error 0.06592875448994773\n",
            "Loss: 0.0\n",
            "training error 0.03985975923133622, test error 0.0656924764610388\n",
            "Loss: 0.0\n",
            "training error 0.039793770195346564, test error 0.06558778322722142\n",
            "Loss: 0.0\n",
            "training error 0.03970731016758787, test error 0.06537735197890344\n",
            "Loss: 0.0\n",
            "training error 0.039614102106861726, test error 0.06530278138548404\n",
            "Loss: 0.0\n",
            "training error 0.039520435408681664, test error 0.06497176043843025\n",
            "Loss: 0.0\n",
            "training error 0.03941095033964334, test error 0.06469651540260142\n",
            "Loss: 0.0\n",
            "training error 0.03948505922374795, test error 0.064517810429418\n",
            "Loss: 0.0\n",
            "training error 0.03926882193878055, test error 0.06437765171868962\n",
            "Loss: 0.0\n",
            "training error 0.03921558606939941, test error 0.0642893215841952\n",
            "Loss: 0.0\n",
            "training error 0.03910823608500787, test error 0.06410301700751735\n",
            "Loss: 0.0\n",
            "training error 0.03907095340482757, test error 0.06400740342445388\n",
            "Loss: 0.0\n",
            "training error 0.03897262625707588, test error 0.06378240851195596\n",
            "Loss: 0.0\n",
            "training error 0.03897005088053357, test error 0.0636815263399473\n",
            "Loss: 0.0\n",
            "training error 0.038845481733014456, test error 0.06331120337262255\n",
            "Loss: 0.0\n",
            "training error 0.0387864778492377, test error 0.06315771840228647\n",
            "Loss: 0.0\n",
            "training error 0.0389276428336584, test error 0.06327435967866564\n",
            "Loss: 0.18468253656063194\n",
            "training error 0.038632257225530084, test error 0.06277771638398692\n",
            "Loss: 0.0\n",
            "training error 0.03861720828472212, test error 0.06282833243174195\n",
            "Loss: 0.08062741155703268\n",
            "training error 0.03852278783260583, test error 0.06267078697919248\n",
            "Loss: 0.0\n",
            "training error 0.03844128046788789, test error 0.06245066523073934\n",
            "Loss: 0.0\n",
            "training error 0.03838944699594583, test error 0.06235424418237487\n",
            "Loss: 0.0\n",
            "training error 0.03833885887676669, test error 0.062060265087031816\n",
            "Loss: 0.0\n",
            "training error 0.038273830768853, test error 0.06197934108830986\n",
            "Loss: 0.0\n",
            "training error 0.038245071915792794, test error 0.06188546475000681\n",
            "Loss: 0.0\n",
            "training error 0.038150890580815756, test error 0.06181745400321994\n",
            "Loss: 0.0\n",
            "training error 0.03810266509685875, test error 0.061840697970125556\n",
            "Loss: 0.03760097739451407\n",
            "training error 0.038107233351745734, test error 0.06158757820693827\n",
            "Loss: 0.0\n",
            "training error 0.03801199965455355, test error 0.0616032623337923\n",
            "Loss: 0.025466380251759624\n",
            "training error 0.037951754829021064, test error 0.06141891013957907\n",
            "Loss: 0.0\n",
            "training error 0.03789020026008225, test error 0.06119883151210194\n",
            "Loss: 0.0\n",
            "training error 0.03788944158892873, test error 0.061191215806628334\n",
            "Loss: 0.0\n",
            "training error 0.03779190201450444, test error 0.06101662818255798\n",
            "Loss: 0.0\n",
            "training error 0.03778070392013971, test error 0.06093842595451921\n",
            "Loss: 0.0\n",
            "training error 0.03770128370547571, test error 0.060909790330935784\n",
            "Loss: 0.0\n",
            "training error 0.037657845041821125, test error 0.060741049572859813\n",
            "Loss: 0.0\n",
            "training error 0.03763530894815859, test error 0.06050413023768662\n",
            "Loss: 0.0\n",
            "training error 0.037549686709620105, test error 0.060436350523767675\n",
            "Loss: 0.0\n",
            "training error 0.03752474583817362, test error 0.060480503768102614\n",
            "Loss: 0.07305742976253526\n",
            "training error 0.03749837320476345, test error 0.060033580996713104\n",
            "Loss: 0.0\n",
            "training error 0.037431501579592144, test error 0.060033859413043754\n",
            "Loss: 0.00046376765474587245\n",
            "training error 0.03739814771654965, test error 0.059920313064242975\n",
            "Loss: 0.0\n",
            "training error 0.03736835084668797, test error 0.05975162216843197\n",
            "Loss: 0.0\n",
            "training error 0.03727910592207307, test error 0.059753794768678495\n",
            "Loss: 0.003636052324074157\n",
            "training error 0.037264433475975314, test error 0.059599831009112196\n",
            "Loss: 0.0\n",
            "training error 0.037237946676694396, test error 0.05946689146547017\n",
            "Loss: 0.0\n",
            "training error 0.03718512357602454, test error 0.059395400114432936\n",
            "Loss: 0.0\n",
            "training error 0.03714138497266596, test error 0.059349400103283684\n",
            "Loss: 0.0\n",
            "training error 0.037110554461523844, test error 0.059353202285694455\n",
            "Loss: 0.0064064378142880685\n",
            "training error 0.03707183841225259, test error 0.05932734273920866\n",
            "Loss: 0.0\n",
            "training error 0.03700973785315736, test error 0.059219998080980524\n",
            "Loss: 0.0\n",
            "training error 0.03697414156427781, test error 0.059206756231117846\n",
            "Loss: 0.0\n",
            "training error 0.0369686698570423, test error 0.05907236236265708\n",
            "Loss: 0.0\n",
            "training error 0.03698494988965101, test error 0.05899033336210408\n",
            "Loss: 0.0\n",
            "training error 0.03687277479400111, test error 0.058968085714150784\n",
            "Loss: 0.0\n",
            "training error 0.03686138264231766, test error 0.05899051029575579\n",
            "Loss: 0.03802833572335018\n",
            "training error 0.036824051321928696, test error 0.058764371333765664\n",
            "Loss: 0.0\n",
            "training error 0.036792026761558586, test error 0.05874817054352182\n",
            "Loss: 0.0\n",
            "training error 0.036840456664202506, test error 0.05849605401519286\n",
            "Loss: 0.0\n",
            "training error 0.03673573909636478, test error 0.058553690531799034\n",
            "Loss: 0.09853060616911158\n",
            "training error 0.036774774052192725, test error 0.058558156189143076\n",
            "Loss: 0.10616472340867489\n",
            "training error 0.036708684853962656, test error 0.05840007939681904\n",
            "Loss: 0.0\n",
            "training error 0.03665945227150245, test error 0.058497900018802874\n",
            "Loss: 0.16750083731764676\n",
            "training error 0.03660554374428664, test error 0.05842891371529913\n",
            "Loss: 0.049373765888516274\n",
            "training error 0.036593488570640376, test error 0.058331346052989404\n",
            "Loss: 0.0\n",
            "training error 0.03658053206846811, test error 0.0584444817969431\n",
            "Loss: 0.19395359718070715\n",
            "training error 0.03654387580978797, test error 0.0582524846845907\n",
            "Loss: 0.0\n",
            "training error 0.036541569064798454, test error 0.05823907872491707\n",
            "Loss: 0.0\n",
            "training error 0.03646415526335496, test error 0.05819981169683114\n",
            "Loss: 0.0\n",
            "training error 0.03644641795929129, test error 0.05807457403783956\n",
            "Loss: 0.0\n",
            "training error 0.03644098953984128, test error 0.05803549492026884\n",
            "Loss: 0.0\n",
            "training error 0.03642763444527981, test error 0.05814895250492388\n",
            "Loss: 0.1954968848131733\n",
            "training error 0.0364072733566345, test error 0.05804950635947019\n",
            "Loss: 0.02414287880305732\n",
            "training error 0.03642087677898541, test error 0.058128433761595814\n",
            "Loss: 0.16014137805606676\n",
            "training error 0.03633070730801779, test error 0.057662959374787305\n",
            "Loss: 0.0\n",
            "training error 0.03631997303262411, test error 0.05759886101918516\n",
            "Loss: 0.0\n",
            "training error 0.03632830460716614, test error 0.05749505326597687\n",
            "Loss: 0.0\n",
            "training error 0.03625699571721546, test error 0.05732924350912035\n",
            "Loss: 0.0\n",
            "training error 0.03623151447135101, test error 0.05735813133541778\n",
            "Loss: 0.05038933802228307\n",
            "training error 0.03625844338446777, test error 0.05737598373992385\n",
            "Loss: 0.08152947421338208\n",
            "training error 0.03620953437163828, test error 0.056965116228526104\n",
            "Loss: 0.0\n",
            "training error 0.036164987522264394, test error 0.05705363114778637\n",
            "Loss: 0.15538442668170394\n",
            "training error 0.03618051688192025, test error 0.05713129329327556\n",
            "Loss: 0.2917172398680057\n",
            "training error 0.036148810580315494, test error 0.05707006644503498\n",
            "Loss: 0.1842359385133996\n",
            "training error 0.03607501382533401, test error 0.05696515018123735\n",
            "Loss: 5.9602636648392604e-05\n",
            "training error 0.03605053679421697, test error 0.056921885430255566\n",
            "Loss: 0.0\n",
            "training error 0.036045673435670327, test error 0.05691224211695365\n",
            "Loss: 0.0\n",
            "training error 0.03606552257578378, test error 0.05696330762128799\n",
            "Loss: 0.08972674847249618\n",
            "training error 0.036016320200571064, test error 0.05687941269671843\n",
            "Loss: 0.0\n",
            "training error 0.035999698435119624, test error 0.05682400996056366\n",
            "Loss: 0.0\n",
            "training error 0.036062425588854086, test error 0.05653591095361868\n",
            "Loss: 0.0\n",
            "training error 0.03600500067226284, test error 0.05616319981810953\n",
            "Loss: 0.0\n",
            "training error 0.03592185226231894, test error 0.056289269190579284\n",
            "Loss: 0.22446971126652837\n",
            "training error 0.03593596522184052, test error 0.05643197305582827\n",
            "Loss: 0.47855755831076596\n",
            "training error 0.03589683891824412, test error 0.05630940417093041\n",
            "Loss: 0.2603205538401987\n",
            "training error 0.03589158423966841, test error 0.056238256867132186\n",
            "Loss: 0.13364097712691425\n",
            "training error 0.03591031664084959, test error 0.056221891059488396\n",
            "Loss: 0.10450124203917888\n",
            "training error 0.03587810535554951, test error 0.05647875044521223\n",
            "Loss: 0.5618458850717989\n",
            "training error 0.035814369156109145, test error 0.05635672884404201\n",
            "Loss: 0.3445833331420678\n",
            "training error 0.03580125924125346, test error 0.05633764033953171\n",
            "Loss: 0.3105957673122717\n",
            "training error 0.035788301239661265, test error 0.056359653413571124\n",
            "Loss: 0.34979060327373634\n",
            "training error 0.035776125444156655, test error 0.05623334706408529\n",
            "Loss: 0.12489894842697691\n",
            "training error 0.035777849612589535, test error 0.0562048059239778\n",
            "Loss: 0.0740807254626219\n",
            "training error 0.03575348180777245, test error 0.056192529648884784\n",
            "Loss: 0.0522225066774018\n",
            "training error 0.0357342695074727, test error 0.05611842695935743\n",
            "Loss: 0.0\n",
            "training error 0.03572329424649384, test error 0.05600064504762014\n",
            "Loss: 0.0\n",
            "training error 0.0357040163421294, test error 0.05602945549944767\n",
            "Loss: 0.05144664280747335\n",
            "training error 0.03566755006326856, test error 0.05607952041588072\n",
            "Loss: 0.14084724951561878\n",
            "training error 0.035687516434013855, test error 0.056019994578973394\n",
            "Loss: 0.03455233656113332\n",
            "training error 0.035688645492614385, test error 0.05607722528327455\n",
            "Loss: 0.13674884564149892\n",
            "training error 0.035669543119829665, test error 0.055892936032271405\n",
            "Loss: 0.0\n",
            "training error 0.03567329561963775, test error 0.05569497844815386\n",
            "Loss: 0.0\n",
            "training error 0.0356379595020505, test error 0.055884233258885946\n",
            "Loss: 0.3398058783850022\n",
            "training error 0.035590905641411215, test error 0.055851156826808704\n",
            "Loss: 0.2804173428314183\n",
            "training error 0.035585684815531496, test error 0.05585294596925089\n",
            "Loss: 0.2836297373632668\n",
            "training error 0.03557674248299619, test error 0.05582543760166199\n",
            "Loss: 0.23423862822673502\n",
            "training error 0.0355964109696055, test error 0.05595397807081924\n",
            "Loss: 0.46503227020096016\n",
            "training error 0.03557135453001411, test error 0.05589192725565294\n",
            "Loss: 0.35362040346673673\n",
            "training error 0.035550297820090615, test error 0.05584776276609155\n",
            "Loss: 0.274323327155801\n",
            "training error 0.035532168820674254, test error 0.05576267285150168\n",
            "Loss: 0.12154489548970027\n",
            "training error 0.035512610241079524, test error 0.055689925351484246\n",
            "Loss: 0.0\n",
            "training error 0.03554012013131503, test error 0.055749753467737063\n",
            "Loss: 0.10743077114077071\n",
            "training error 0.03549497856234134, test error 0.05551046864964688\n",
            "Loss: 0.0\n",
            "training error 0.035442511506824294, test error 0.05544181180856555\n",
            "Loss: 0.0\n",
            "training error 0.03548607736770243, test error 0.0555104871763784\n",
            "Loss: 0.12386927045238139\n",
            "training error 0.03541545874521126, test error 0.05550310148786626\n",
            "Loss: 0.11054775683077\n",
            "training error 0.035431787815599323, test error 0.055392074698421834\n",
            "Loss: 0.0\n",
            "training error 0.035403175930683706, test error 0.055412923634505167\n",
            "Loss: 0.03763884309595511\n",
            "training error 0.035450351523170914, test error 0.05542724777385881\n",
            "Loss: 0.06349838966761556\n",
            "training error 0.035384431859610224, test error 0.055318260756982665\n",
            "Loss: 0.0\n",
            "training error 0.035376887453317823, test error 0.05544939888789357\n",
            "Loss: 0.23706119663993164\n",
            "training error 0.03538908919828933, test error 0.05533822284457362\n",
            "Loss: 0.03608589156236519\n",
            "training error 0.03539775029393866, test error 0.055402704670325424\n",
            "Loss: 0.15265106347743806\n",
            "training error 0.03534380114705814, test error 0.0552505898772296\n",
            "Loss: 0.0\n",
            "training error 0.035339061920564, test error 0.05522505467477581\n",
            "Loss: 0.0\n",
            "training error 0.03533162701421156, test error 0.05531979680062652\n",
            "Loss: 0.17155641838411118\n",
            "training error 0.035383047267396245, test error 0.055186055646049455\n",
            "Loss: 0.0\n",
            "training error 0.03534359758143545, test error 0.05528350901910304\n",
            "Loss: 0.17659057512395826\n",
            "training error 0.035358422636990273, test error 0.055296856896226515\n",
            "Loss: 0.2007776219552948\n",
            "training error 0.03530601613608429, test error 0.05521689412846554\n",
            "Loss: 0.055880932338925327\n",
            "training error 0.03529460115204074, test error 0.05531370343168822\n",
            "Loss: 0.2313044194668823\n",
            "training error 0.035263331652236794, test error 0.055333062873416945\n",
            "Loss: 0.2663847336913605\n",
            "training error 0.03544237968420667, test error 0.055211117027402716\n",
            "Loss: 0.04541252506611304\n",
            "training error 0.03526151561892109, test error 0.055431064206600454\n",
            "Loss: 0.4439682410397827\n",
            "training error 0.03525462148323019, test error 0.05554289310076485\n",
            "Loss: 0.6466080072909497\n",
            "training error 0.03523305242660168, test error 0.055454928042016745\n",
            "Loss: 0.48721075065008\n",
            "training error 0.035214451634105366, test error 0.0555638474255755\n",
            "Loss: 0.6845783325213795\n",
            "training error 0.035199501391035566, test error 0.05546556444678329\n",
            "Loss: 0.5064844686972103\n",
            "training error 0.03518099583387852, test error 0.05548905682091607\n",
            "Loss: 0.5490538711626636\n",
            "training error 0.035176532065572697, test error 0.05538519534638001\n",
            "Loss: 0.36085148322213545\n",
            "training error 0.03516735194819295, test error 0.05541629408571599\n",
            "Loss: 0.41720401462141954\n",
            "training error 0.035215774121341595, test error 0.05544531749228855\n",
            "Loss: 0.4697959352303549\n",
            "training error 0.03518923656552927, test error 0.055239661477013174\n",
            "Loss: 0.09713655077567829\n",
            "training error 0.03515599579084096, test error 0.05525218591251648\n",
            "Loss: 0.11983147860967769\n",
            "training error 0.035138413633387114, test error 0.05517596076970902\n",
            "Loss: 0.0\n",
            "training error 0.03515952666863403, test error 0.055278728104550805\n",
            "Loss: 0.18625382033801152\n",
            "training error 0.035163232215070284, test error 0.05504054156294491\n",
            "Loss: 0.0\n",
            "training error 0.03514009182678206, test error 0.054939989929866775\n",
            "Loss: 0.0\n",
            "training error 0.035194658579889515, test error 0.05507482545232256\n",
            "Loss: 0.24542327479111226\n",
            "training error 0.035127236376131035, test error 0.054958131923266505\n",
            "Loss: 0.0330214720149824\n",
            "training error 0.03510446998258354, test error 0.055047889042664465\n",
            "Loss: 0.19639448957931016\n",
            "training error 0.03510835553295436, test error 0.05498212764894532\n",
            "Loss: 0.0766977189699869\n",
            "training error 0.03509751087085845, test error 0.05492130631911041\n",
            "Loss: 0.0\n",
            "training error 0.03507922223880697, test error 0.05500703406179986\n",
            "Loss: 0.15609195854036084\n",
            "training error 0.03508910061336674, test error 0.05507439919080914\n",
            "Loss: 0.27874950899604567\n",
            "training error 0.03511428663220872, test error 0.055115816707273176\n",
            "Loss: 0.3541619841170629\n",
            "training error 0.03508743470254907, test error 0.055071869067813695\n",
            "Loss: 0.27414269396373037\n",
            "training error 0.03504871583208734, test error 0.055141672828119694\n",
            "Loss: 0.40124047255700734\n",
            "training error 0.03505929230718437, test error 0.055001837339000305\n",
            "Loss: 0.1466298332781557\n",
            "training error 0.03507286445170453, test error 0.055002706827301955\n",
            "Loss: 0.1482129862654391\n",
            "training error 0.035036762991374766, test error 0.05516763807862634\n",
            "Loss: 0.4485176628623133\n",
            "training error 0.0350625235535003, test error 0.0552226957783459\n",
            "Loss: 0.5487660061913369\n",
            "training error 0.03508077597832058, test error 0.05506758080528255\n",
            "Loss: 0.2663346813388534\n",
            "training error 0.03504358365644002, test error 0.05521645944531735\n",
            "Loss: 0.5374109721498765\n",
            "training error 0.03503774505615969, test error 0.05521060743557858\n",
            "Loss: 0.5267557089542629\n",
            "training error 0.03505394905782348, test error 0.055187214454761206\n",
            "Loss: 0.4841620738330388\n",
            "training error 0.035014642177406144, test error 0.055184706906714745\n",
            "Loss: 0.479596362974144\n",
            "training error 0.03502851557986859, test error 0.054987911430832204\n",
            "Loss: 0.12127372086672139\n",
            "training error 0.03498235215225067, test error 0.05514582431288742\n",
            "Loss: 0.40879944200979956\n",
            "training error 0.03501567688316667, test error 0.05521322201653858\n",
            "Loss: 0.5315163039496085\n",
            "training error 0.03499988518036107, test error 0.05498585101844738\n",
            "Loss: 0.11752214880313971\n",
            "training error 0.03496237055547339, test error 0.05508546119785229\n",
            "Loss: 0.2988910674995404\n",
            "training error 0.03493813848806117, test error 0.05509223133436927\n",
            "Loss: 0.31121804398774877\n",
            "training error 0.034940222122666294, test error 0.055037038433968746\n",
            "Loss: 0.2107235290178533\n",
            "training error 0.034993535089052584, test error 0.05518843610936413\n",
            "Loss: 0.48638644663987485\n",
            "training error 0.03495527210781615, test error 0.05511222915980102\n",
            "Loss: 0.34762982435503353\n",
            "training error 0.03491746703057385, test error 0.055099378288899804\n",
            "Loss: 0.3242311258125241\n",
            "training error 0.03493136408257217, test error 0.05515018436633145\n",
            "Loss: 0.41673817059482143\n",
            "training error 0.034931836945798954, test error 0.05501863238116124\n",
            "Loss: 0.17721002753527948\n",
            "training error 0.03492750758037142, test error 0.05508964736805387\n",
            "Loss: 0.3065131917389996\n",
            "training error 0.03497251828084827, test error 0.055002111235421945\n",
            "Loss: 0.14712854031919687\n",
            "training error 0.034909276750963364, test error 0.05508298429114716\n",
            "Loss: 0.2943811479962921\n",
            "training error 0.03494182652718321, test error 0.05472201240509671\n",
            "Loss: 0.0\n",
            "training error 0.03490314245586075, test error 0.054734115956568914\n",
            "Loss: 0.02211824993314515\n",
            "training error 0.034916129141993275, test error 0.05490362317551343\n",
            "Loss: 0.3318788224970559\n",
            "training error 0.034897315991960866, test error 0.05487921372965933\n",
            "Loss: 0.28727255752016845\n",
            "training error 0.03489785674854645, test error 0.05471192386605799\n",
            "Loss: 0.0\n",
            "training error 0.03496194459790007, test error 0.05486872846030014\n",
            "Loss: 0.286600402914039\n",
            "training error 0.034918362596544045, test error 0.054771785054539196\n",
            "Loss: 0.10941159486139718\n",
            "training error 0.034868451727478635, test error 0.05485371067054519\n",
            "Loss: 0.2591515605159689\n",
            "training error 0.03485201740389234, test error 0.05485492144788692\n",
            "Loss: 0.261364565024258\n",
            "training error 0.03488233376325299, test error 0.05477857272554414\n",
            "Loss: 0.12181779542119742\n",
            "training error 0.03491429514996385, test error 0.054758151709238374\n",
            "Loss: 0.08449317792873501\n",
            "training error 0.0348349230631156, test error 0.05480406225906698\n",
            "Loss: 0.16840642130324657\n",
            "training error 0.03486949432749364, test error 0.054797825899637394\n",
            "Loss: 0.157007883308391\n",
            "training error 0.03484476612230836, test error 0.054659795909697384\n",
            "Loss: 0.0\n",
            "training error 0.03483027451697502, test error 0.05472489337207714\n",
            "Loss: 0.11909569235732942\n",
            "training error 0.03488241505137961, test error 0.05485194479846602\n",
            "Loss: 0.35153605235938024\n",
            "training error 0.034818119121060045, test error 0.05473302743165194\n",
            "Loss: 0.13397693997163618\n",
            "training error 0.03482695643218241, test error 0.054719895011960944\n",
            "Loss: 0.10995120135985914\n",
            "training error 0.03484994215071748, test error 0.054895275173785237\n",
            "Loss: 0.4308088974150026\n",
            "training error 0.03481813843754994, test error 0.05467964753247845\n",
            "Loss: 0.036318508788180104\n",
            "training error 0.03482861625463208, test error 0.05460409483211374\n",
            "Loss: 0.0\n",
            "training error 0.034800014881890017, test error 0.05454876360870156\n",
            "Loss: 0.0\n",
            "training error 0.034813827121855885, test error 0.05460416492395688\n",
            "Loss: 0.10156291653597993\n",
            "training error 0.03479709110446173, test error 0.05436775058509683\n",
            "Loss: 0.0\n",
            "training error 0.03492659037986579, test error 0.054185842814505655\n",
            "Loss: 0.0\n",
            "training error 0.03478056885590536, test error 0.05441885295527758\n",
            "Loss: 0.4300203312691764\n",
            "training error 0.034838202566275435, test error 0.054422161359078076\n",
            "Loss: 0.43612599213673775\n",
            "training error 0.03478072924711216, test error 0.054484601612568455\n",
            "Loss: 0.5513595111652059\n",
            "training error 0.034765421017508985, test error 0.05458327856249834\n",
            "Loss: 0.7334678715863463\n",
            "training error 0.03483846972174349, test error 0.05440746732524343\n",
            "Loss: 0.40900814534980956\n",
            "training error 0.03475703671231019, test error 0.05458946051503696\n",
            "Loss: 0.744876668086536\n",
            "training error 0.03477930122341997, test error 0.05446510830887651\n",
            "Loss: 0.5153846094576142\n",
            "training error 0.0347682718535133, test error 0.05455314548649917\n",
            "Loss: 0.6778572647673009\n",
            "training error 0.034774448691415404, test error 0.0545774958725565\n",
            "Loss: 0.7227959144081142\n",
            "training error 0.03474378624707674, test error 0.05459331046795757\n",
            "Loss: 0.751981758126008\n",
            "training error 0.03476981398978527, test error 0.05449999652728434\n",
            "Loss: 0.5797708339688068\n",
            "training error 0.03480531399096167, test error 0.054619751459706246\n",
            "Loss: 0.800778621615228\n",
            "training error 0.03475774525296308, test error 0.05443417216094715\n",
            "Loss: 0.4582919329899404\n",
            "training error 0.03473341461622823, test error 0.05454617027834331\n",
            "Loss: 0.6649845146289746\n",
            "training error 0.03475867001476599, test error 0.05466367380742339\n",
            "Loss: 0.8818373362826426\n",
            "training error 0.03477874777394357, test error 0.05461226924471522\n",
            "Loss: 0.7869701901091597\n",
            "training error 0.034760379969600776, test error 0.05468350337043262\n",
            "Loss: 0.9184328047283641\n",
            "training error 0.03472872956509893, test error 0.05453039246099372\n",
            "Loss: 0.6358665448234602\n",
            "training error 0.034797174051979345, test error 0.05467261015533132\n",
            "Loss: 0.898329370813733\n",
            "training error 0.0348119903918665, test error 0.05436519523680617\n",
            "Loss: 0.33099498500834823\n",
            "training error 0.03479799854978956, test error 0.05469871290918736\n",
            "Loss: 0.9465020161030235\n",
            "training error 0.03472410997404198, test error 0.05445052647399901\n",
            "Loss: 0.48847382590218036\n",
            "training error 0.034770146031233094, test error 0.05444068022824972\n",
            "Loss: 0.4703025744500211\n",
            "training error 0.03471710062408139, test error 0.0543901524675059\n",
            "Loss: 0.37705356674002655\n",
            "training error 0.03474477448864862, test error 0.0544669610079328\n",
            "Loss: 0.518803766492093\n",
            "training error 0.034730432477493474, test error 0.05455225631959591\n",
            "Loss: 0.676216306802857\n",
            "training error 0.034751894975109464, test error 0.05431146960357486\n",
            "Loss: 0.2318443020241645\n",
            "training error 0.03471973026075149, test error 0.05444183146862836\n",
            "Loss: 0.4724271891442866\n",
            "training error 0.0347141983217921, test error 0.05453498212146196\n",
            "Loss: 0.6443367655118326\n",
            "training error 0.034697068367662794, test error 0.054511137702792735\n",
            "Loss: 0.6003318789386869\n",
            "training error 0.03473407393680937, test error 0.0543749864058219\n",
            "Loss: 0.3490645923211577\n",
            "training error 0.034718361547721906, test error 0.05458177579529529\n",
            "Loss: 0.730694513961927\n",
            "training error 0.034687478462512036, test error 0.05448040887177909\n",
            "Loss: 0.5436218059426068\n",
            "training error 0.03468057030130176, test error 0.054530302019244734\n",
            "Loss: 0.6356996345304955\n",
            "training error 0.034749188213784345, test error 0.054515824486631156\n",
            "Loss: 0.6089813408552525\n",
            "training error 0.03469344722864444, test error 0.05467689583676069\n",
            "Loss: 0.9062385980339149\n",
            "training error 0.03473668305946939, test error 0.05456064261580715\n",
            "Loss: 0.6916932206527449\n",
            "training error 0.03467506405706597, test error 0.0545918977793417\n",
            "Loss: 0.7493746405792701\n",
            "training error 0.0347387917700269, test error 0.054540378258509055\n",
            "Loss: 0.6542953391295958\n",
            "training error 0.03469670960004337, test error 0.05458445023708128\n",
            "Loss: 0.7356301976148449\n",
            "training error 0.034680090241300865, test error 0.05471372337863044\n",
            "Loss: 0.9742038449634727\n",
            "training error 0.03471047667330557, test error 0.05457036062047861\n",
            "Loss: 0.7096278031316627\n",
            "training error 0.03470751430617197, test error 0.054669418446104984\n",
            "Loss: 0.8924390698410889\n",
            "training error 0.03466927197125409, test error 0.05470521908778992\n",
            "Loss: 0.9585091719662708\n",
            "training error 0.03469282603366919, test error 0.05457037033662412\n",
            "Loss: 0.7096457342830487\n",
            "training error 0.034685057759191, test error 0.05460880889713591\n",
            "Loss: 0.780584116921812\n",
            "training error 0.03465175672013612, test error 0.054713783687485015\n",
            "Loss: 0.9743151449847609\n",
            "training error 0.03465369801272307, test error 0.05466230061597371\n",
            "Loss: 0.879303110775842\n",
            "training error 0.03469899620391978, test error 0.054750346095793254\n",
            "Loss: 1.0417910877940306\n",
            "training error 0.03466945549990008, test error 0.05469270263859491\n",
            "Loss: 0.9354100587202874\n",
            "training error 0.034652459426554295, test error 0.054553624429696045\n",
            "Loss: 0.6787411546765432\n",
            "training error 0.03464841385743703, test error 0.05457644828460987\n",
            "Loss: 0.7208625903289523\n",
            "training error 0.034673799647220685, test error 0.054492818699407614\n",
            "Loss: 0.5665241490343309\n",
            "training error 0.034723271859687446, test error 0.05458987353959818\n",
            "Loss: 0.7456389051207379\n",
            "training error 0.03465707145538367, test error 0.05455576718704788\n",
            "Loss: 0.6826956144404539\n",
            "training error 0.03465722065945996, test error 0.05454664142472412\n",
            "Loss: 0.6658540155102477\n",
            "training error 0.03464890403559335, test error 0.054567485349206815\n",
            "Loss: 0.7043214885623206\n",
            "training error 0.03465606146773538, test error 0.05457544816200256\n",
            "Loss: 0.7190168635572203\n",
            "training error 0.03463233902285877, test error 0.054617360114579884\n",
            "Loss: 0.7963653929891734\n",
            "training error 0.03468545304830308, test error 0.05467764297361662\n",
            "Loss: 0.9076174394750058\n",
            "training error 0.034649528307965714, test error 0.054501616562059985\n",
            "Loss: 0.5827606089570558\n",
            "training error 0.034720868439253184, test error 0.05450111048213762\n",
            "Loss: 0.5818266382073611\n",
            "training error 0.034637372056433656, test error 0.05441443578042021\n",
            "Loss: 0.4218684328618716\n",
            "training error 0.03460794570238726, test error 0.05447635617921599\n",
            "Loss: 0.536142559791597\n",
            "training error 0.03462591029139791, test error 0.05443618390802467\n",
            "Loss: 0.4620046132271183\n",
            "training error 0.03466159328641129, test error 0.054588895434634015\n",
            "Loss: 0.7438338119204468\n",
            "training error 0.034677248038612635, test error 0.054376201684394\n",
            "Loss: 0.35130738953346174\n",
            "training error 0.03464460324347929, test error 0.054390492417526116\n",
            "Loss: 0.3776809446722762\n",
            "training error 0.034652305517995814, test error 0.05455857683302522\n",
            "Loss: 0.6878808174960893\n",
            "training error 0.034676165504669405, test error 0.05457040321239472\n",
            "Loss: 0.7097064065341385\n",
            "training error 0.034701053028032953, test error 0.05447568536771511\n",
            "Loss: 0.5349045768313854\n",
            "training error 0.034632265346447935, test error 0.05455492798074217\n",
            "Loss: 0.6811468587837766\n",
            "training error 0.03463271372342362, test error 0.05453457494196018\n",
            "Loss: 0.6435853155377425\n",
            "training error 0.03466088658239783, test error 0.05440359623489582\n",
            "Loss: 0.40186404617825744\n",
            "training error 0.03464092092800206, test error 0.054438706638272064\n",
            "Loss: 0.4666603131597169\n",
            "training error 0.03462586392225716, test error 0.05446595682516214\n",
            "Loss: 0.5169505466869007\n",
            "training error 0.03461103544711334, test error 0.05434719801304561\n",
            "Loss: 0.29778109956197785\n",
            "training error 0.03462902246859256, test error 0.05441099456979102\n",
            "Loss: 0.4155176769255364\n",
            "training error 0.03460794948887631, test error 0.05440604181460079\n",
            "Loss: 0.4063773647462465\n",
            "training error 0.0346275958825671, test error 0.054353017900499775\n",
            "Loss: 0.3085217047678057\n",
            "training error 0.03463237871733858, test error 0.05445195651193075\n",
            "Loss: 0.4911129616200327\n",
            "training error 0.03461412547030008, test error 0.05463251038308858\n",
            "Loss: 0.8243252210951013\n",
            "training error 0.03460870350680775, test error 0.054586487862639296\n",
            "Loss: 0.7393906366007341\n",
            "training error 0.03462011433163353, test error 0.05466789544683945\n",
            "Loss: 0.8896283739352473\n",
            "training error 0.034591783895130616, test error 0.05457518250664991\n",
            "Loss: 0.718526596471114\n",
            "training error 0.03460773532421384, test error 0.054570132356394165\n",
            "Loss: 0.7092065416497206\n",
            "training error 0.03461388282021933, test error 0.05463933095529448\n",
            "Loss: 0.8369125905105079\n",
            "training error 0.03458076714302008, test error 0.05454532182632615\n",
            "Loss: 0.6634186960071675\n",
            "training error 0.03465013920728797, test error 0.05444529656076499\n",
            "Loss: 0.4788220184145153\n",
            "training error 0.03459197137989492, test error 0.054671258530548744\n",
            "Loss: 0.8958349466018589\n",
            "training error 0.034609359563837, test error 0.05466510584925517\n",
            "Loss: 0.8844801701990201\n",
            "training error 0.034613619186519846, test error 0.054546392463762174\n",
            "Loss: 0.665394557930532\n",
            "training error 0.03459249451118315, test error 0.05458310964752767\n",
            "Loss: 0.7331561389235519\n",
            "training error 0.03459084070734484, test error 0.054563304724252415\n",
            "Loss: 0.6966061431191939\n",
            "training error 0.03463073080008273, test error 0.054571525360617705\n",
            "Loss: 0.711777331640584\n",
            "training error 0.03458144960717379, test error 0.054626033772574994\n",
            "Loss: 0.8123726331548342\n",
            "training error 0.034585188531410994, test error 0.05461781863160836\n",
            "Loss: 0.7972115863944085\n",
            "training error 0.03460948813488644, test error 0.054494630886419135\n",
            "Loss: 0.5698685410699511\n",
            "training error 0.034576292841933004, test error 0.05451991380355139\n",
            "Loss: 0.6165281772756659\n",
            "training error 0.034587204870096755, test error 0.054559112249891815\n",
            "Loss: 0.6888689295910355\n",
            "training error 0.03457066737004444, test error 0.054575895280424655\n",
            "Loss: 0.7198420208287093\n",
            "training error 0.034560571159932794, test error 0.054712466247459283\n",
            "Loss: 0.9718838087587223\n",
            "training error 0.034647342318660866, test error 0.054689026175339266\n",
            "Loss: 0.9286251439442506\n",
            "training error 0.03456840221991924, test error 0.054505580593092115\n",
            "Loss: 0.5900762302083473\n",
            "training error 0.034571755661450695, test error 0.05455171987224207\n",
            "Loss: 0.6752262929431874\n",
            "training error 0.034624265154778774, test error 0.05455738028987781\n",
            "Loss: 0.6856725965194377\n",
            "training error 0.034565028511426565, test error 0.05448428002860108\n",
            "Loss: 0.5507660277926707\n",
            "training error 0.034606930073149675, test error 0.05448331244729981\n",
            "Loss: 0.5489803560174922\n",
            "training error 0.034579435651141584, test error 0.05448041596239038\n",
            "Loss: 0.5436348916692868\n",
            "training error 0.0345629597333055, test error 0.054568762852722746\n",
            "Loss: 0.7066791219395352\n",
            "training error 0.03462226235113399, test error 0.05462252229799057\n",
            "Loss: 0.805892205054004\n",
            "training error 0.034592791784013234, test error 0.054467880199360864\n",
            "Loss: 0.5205001347320604\n",
            "training error 0.034607652347385554, test error 0.05451068138912818\n",
            "Loss: 0.5994897518426523\n",
            "training error 0.03457127120984505, test error 0.054488828246082116\n",
            "Loss: 0.5591597654274194\n",
            "training error 0.03460813323354704, test error 0.054212538968507085\n",
            "Loss: 0.049267765554228404\n",
            "training error 0.03460814896223927, test error 0.054202500726676414\n",
            "Loss: 0.030742185237908615\n",
            "training error 0.03456760416575764, test error 0.054317056114365284\n",
            "Loss: 0.2421542104804164\n",
            "training error 0.03458124986731246, test error 0.05452039096103421\n",
            "Loss: 0.6174087716487309\n",
            "training error 0.034568123095685, test error 0.05458825487400283\n",
            "Loss: 0.7426516569553954\n",
            "training error 0.03454486425563779, test error 0.054494608360163746\n",
            "Loss: 0.56982696885437\n",
            "training error 0.03457000908028501, test error 0.05441399153487901\n",
            "Loss: 0.4210485774935213\n",
            "training error 0.03466170778796959, test error 0.05459833091273371\n",
            "Loss: 0.7612469914699327\n",
            "training error 0.03459043665145748, test error 0.05437753895755319\n",
            "Loss: 0.35377532781721843\n",
            "training error 0.034558859196522004, test error 0.05447212111705554\n",
            "Loss: 0.5283267504574862\n",
            "training error 0.034553704039440025, test error 0.054364593809904715\n",
            "Loss: 0.3298850513610674\n",
            "training error 0.03455363947335316, test error 0.054455043006475225\n",
            "Loss: 0.4968090888447074\n",
            "training error 0.03466447392501054, test error 0.054360241305994345\n",
            "Loss: 0.3218525032187891\n",
            "training error 0.03456148712504098, test error 0.054350014352068675\n",
            "Loss: 0.3029786546368429\n",
            "training error 0.0345806273818405, test error 0.05462913791069739\n",
            "Loss: 0.8181013216113753\n",
            "training error 0.03452762841443115, test error 0.054539869219377764\n",
            "Loss: 0.6533559071583372\n",
            "training error 0.03455887029207096, test error 0.0544191848572232\n",
            "Loss: 0.43063285647571004\n",
            "training error 0.03454903369648072, test error 0.05441370607831176\n",
            "Loss: 0.4205217672559769\n",
            "training error 0.03455424699357211, test error 0.0544915085809472\n",
            "Loss: 0.5641063247607603\n",
            "training error 0.03453351584438779, test error 0.054340735731657035\n",
            "Loss: 0.2858549560290635\n",
            "training error 0.03452953350344136, test error 0.05436541165235848\n",
            "Loss: 0.3313943800183017\n",
            "training error 0.03453734687283271, test error 0.05432053612275223\n",
            "Loss: 0.2485765676980778\n",
            "training error 0.034530702258477826, test error 0.054298131511091724\n",
            "Loss: 0.2072288456792526\n",
            "training error 0.03463592536251035, test error 0.05434275133755069\n",
            "Loss: 0.28957475771334007\n",
            "training error 0.03456457389740443, test error 0.054224015966257075\n",
            "Loss: 0.07044857063882581\n",
            "training error 0.03451821099613374, test error 0.054324842785928376\n",
            "Loss: 0.25652451674242904\n",
            "training error 0.03452240596816003, test error 0.05432834610331124\n",
            "Loss: 0.26298989072370826\n",
            "training error 0.03457122842334462, test error 0.05440326059013209\n",
            "Loss: 0.40124461359902774\n",
            "training error 0.0345595211156567, test error 0.05437618866202587\n",
            "Loss: 0.35128335674656785\n",
            "training error 0.03458597115359899, test error 0.05443843890050599\n",
            "Loss: 0.4661662029785907\n",
            "training error 0.03454462768889226, test error 0.054565026748131844\n",
            "Loss: 0.6997841390494752\n",
            "training error 0.03457854984344216, test error 0.05439031582907313\n",
            "Loss: 0.3773550505940193\n",
            "training error 0.03455041756136751, test error 0.0545088855114323\n",
            "Loss: 0.5961754586571999\n",
            "training error 0.034520232029105515, test error 0.054442187181952505\n",
            "Loss: 0.473083658261797\n",
            "training error 0.03453918455481102, test error 0.054386116794863715\n",
            "Loss: 0.36960573086157744\n",
            "training error 0.03455785970728091, test error 0.05445916491326115\n",
            "Loss: 0.5044160698785438\n",
            "training error 0.034549482820846314, test error 0.0543392988531513\n",
            "Loss: 0.2832031960284809\n",
            "training error 0.0345231389797958, test error 0.054428408796475765\n",
            "Loss: 0.4476556409770849\n",
            "training error 0.034520717459646626, test error 0.05442487676900947\n",
            "Loss: 0.44113728252248574\n",
            "training error 0.03457349819680245, test error 0.05436710092998217\n",
            "Loss: 0.3345119427172527\n",
            "training error 0.034529338590803156, test error 0.054475496742650474\n",
            "Loss: 0.5345564691803251\n",
            "training error 0.03458702894428285, test error 0.054207151701037654\n",
            "Loss: 0.03932556074646154\n",
            "training error 0.03454735397933555, test error 0.05429921415826993\n",
            "Loss: 0.20922687158779762\n",
            "training error 0.03451302904525485, test error 0.05433255323835173\n",
            "Loss: 0.2707541605439401\n",
            "training error 0.03453978612076419, test error 0.05440023201073077\n",
            "Loss: 0.3956553687999964\n",
            "training error 0.03450669305258013, test error 0.054546534128468875\n",
            "Loss: 0.6656560002175738\n",
            "training error 0.034535362499165265, test error 0.05463064901684\n",
            "Loss: 0.8208900687529219\n",
            "training error 0.03452932779928038, test error 0.05453386566235356\n",
            "Loss: 0.6422763396691789\n",
            "training error 0.03455791005291411, test error 0.05424488992025964\n",
            "Loss: 0.10897146318480377\n",
            "training error 0.03451101605780271, test error 0.054390854914221694\n",
            "Loss: 0.378349932505162\n",
            "training error 0.03453638623787623, test error 0.054255168751005604\n",
            "Loss: 0.12794105046454174\n",
            "training error 0.0345163710661269, test error 0.05419793772554377\n",
            "Loss: 0.022321164366712054\n",
            "training error 0.03453713659850401, test error 0.054425444643476976\n",
            "Loss: 0.4421852951361327\n",
            "training error 0.034502006851570774, test error 0.0543005759212286\n",
            "Loss: 0.21174000580872487\n",
            "training error 0.03454274514265422, test error 0.05427827384075637\n",
            "Loss: 0.1705815051491033\n",
            "training error 0.03464896177190721, test error 0.054404534828554084\n",
            "Loss: 0.4035962212437516\n",
            "training error 0.03452650901782461, test error 0.05431467138650998\n",
            "Loss: 0.23775319403140927\n",
            "training error 0.03454800343543223, test error 0.054261555857793006\n",
            "Loss: 0.13972845923342359\n",
            "training error 0.034526150145159536, test error 0.054439391821418055\n",
            "Loss: 0.4679248189981555\n",
            "training error 0.03451873751561981, test error 0.0544290908979662\n",
            "Loss: 0.4489144596186323\n",
            "training error 0.034580343935519356, test error 0.05469000214734746\n",
            "Loss: 0.9304263007732461\n",
            "training error 0.034539965506963505, test error 0.054359260947517814\n",
            "Loss: 0.32004325116030685\n",
            "training error 0.03453664806222031, test error 0.05433940164335594\n",
            "Loss: 0.2833928954025122\n",
            "training error 0.03451017641611258, test error 0.054384162139212364\n",
            "Loss: 0.36599841287994206\n",
            "training error 0.03464044664636747, test error 0.054481627805283886\n",
            "Loss: 0.5458713483350142\n",
            "training error 0.03449781461526331, test error 0.05428560398971768\n",
            "Loss: 0.18410929872132442\n",
            "training error 0.03450722815275093, test error 0.05429277495108577\n",
            "Loss: 0.19734331151066709\n",
            "training error 0.03452389740450197, test error 0.0541386725134862\n",
            "Loss: 0.0\n",
            "training error 0.03451044897300816, test error 0.05416026542951608\n",
            "Loss: 0.039884457869754364\n",
            "training error 0.03453148897729083, test error 0.05427981332232249\n",
            "Loss: 0.2607023820193177\n",
            "training error 0.03451222790516747, test error 0.05419745507190881\n",
            "Loss: 0.10857776094892468\n",
            "training error 0.03449680118479453, test error 0.054216465641583875\n",
            "Loss: 0.14369234502065176\n",
            "training error 0.03452120133082924, test error 0.05423913794892232\n",
            "Loss: 0.18557055570784797\n",
            "training error 0.03454476779724062, test error 0.05417945871086035\n",
            "Loss: 0.07533653021136733\n",
            "training error 0.034540226662132176, test error 0.05418515248050336\n",
            "Loss: 0.0858535402869176\n",
            "training error 0.03457270597626319, test error 0.05423100803677427\n",
            "Loss: 0.17055372620204956\n",
            "training error 0.034531279555769116, test error 0.05417160202925679\n",
            "Loss: 0.060824387155755666\n",
            "training error 0.034519819998698624, test error 0.0542157206002675\n",
            "Loss: 0.14231617290230147\n",
            "training error 0.03451364234647252, test error 0.0542415979896657\n",
            "Loss: 0.1901145177762853\n",
            "training error 0.034536824859414075, test error 0.05416753513463629\n",
            "Loss: 0.05331239169727997\n",
            "training error 0.03450418609278976, test error 0.05438310620728279\n",
            "Loss: 0.4514955436628165\n",
            "training error 0.03451448785572988, test error 0.05429461440560285\n",
            "Loss: 0.28804158816011505\n",
            "training error 0.034506907492909154, test error 0.0542893767620424\n",
            "Loss: 0.27836709243038715\n",
            "training error 0.03451262030902814, test error 0.05432765082123057\n",
            "Loss: 0.349063430946317\n",
            "training error 0.034527083942003475, test error 0.054255203268519195\n",
            "Loss: 0.21524494344402623\n",
            "training error 0.03449812782489168, test error 0.05434769677371208\n",
            "Loss: 0.3860904793589359\n",
            "training error 0.034497469692153324, test error 0.054387072883965104\n",
            "Loss: 0.45882242571986254\n",
            "training error 0.03448485838164061, test error 0.054344554699191645\n",
            "Loss: 0.38028672693104504\n",
            "training error 0.034494023758931076, test error 0.05421479454926569\n",
            "Loss: 0.1406056562626823\n",
            "training error 0.03455683233888756, test error 0.054368960887025454\n",
            "Loss: 0.42536760294942244\n",
            "training error 0.03451526694699365, test error 0.05415325849682749\n",
            "Loss: 0.02694189322367535\n",
            "training error 0.034600802383305235, test error 0.05437210537050117\n",
            "Loss: 0.43117580497900043\n",
            "training error 0.03454938160859859, test error 0.05430254974332233\n",
            "Loss: 0.30269901759285567\n",
            "training error 0.034504939928767395, test error 0.05424376170580218\n",
            "Loss: 0.1941111361565273\n",
            "training error 0.03455915612390618, test error 0.05419225987881357\n",
            "Loss: 0.09898167583259898\n",
            "training error 0.03456737062724301, test error 0.054435602894249695\n",
            "Loss: 0.5484626182689079\n",
            "training error 0.034495886215177886, test error 0.054435732941802546\n",
            "Loss: 0.548702830203962\n",
            "training error 0.03449066101774918, test error 0.05430990280577231\n",
            "Loss: 0.3162809214493745\n",
            "training error 0.03448162153323872, test error 0.054335804144708046\n",
            "Loss: 0.3641235037167512\n",
            "training error 0.03454909201895601, test error 0.054502604878102025\n",
            "Loss: 0.6722225494634637\n",
            "training error 0.034490362093979585, test error 0.05440109595770677\n",
            "Loss: 0.48472456385997287\n",
            "training error 0.03449685238687441, test error 0.05445032950460414\n",
            "Loss: 0.5756642648382382\n",
            "training error 0.03449197988962397, test error 0.054362997918039696\n",
            "Loss: 0.4143533524905374\n",
            "training error 0.03449095323779089, test error 0.05447612589766672\n",
            "Loss: 0.6233130006955134\n",
            "training error 0.034520607764557225, test error 0.05456315577353708\n",
            "Loss: 0.7840666206677671\n",
            "training error 0.03448178519446782, test error 0.05447075449783848\n",
            "Loss: 0.6133914426319942\n",
            "training error 0.03451599666853671, test error 0.05438618347667688\n",
            "Loss: 0.4571795939936063\n",
            "training error 0.03450127185298087, test error 0.05442632267552117\n",
            "Loss: 0.5313210477470021\n",
            "training error 0.03448582832571877, test error 0.05442795847340374\n",
            "Loss: 0.5343425438543603\n",
            "training error 0.03463021152333157, test error 0.05448975550116068\n",
            "Loss: 0.64848835661242\n",
            "training error 0.03455855282269177, test error 0.05448057860826656\n",
            "Loss: 0.6315376401133443\n",
            "training error 0.0345132141816405, test error 0.05434291062398281\n",
            "Loss: 0.37724994170431625\n",
            "training error 0.0345849251097065, test error 0.05425983768336375\n",
            "Loss: 0.22380521030944678\n",
            "training error 0.03457551460028589, test error 0.0543129480543276\n",
            "Loss: 0.32190582581792526\n",
            "training error 0.034555062033808565, test error 0.054176921783546154\n",
            "Loss: 0.070650550307505\n",
            "training error 0.03450179930543335, test error 0.05418783408632217\n",
            "Loss: 0.09080675708057484\n",
            "training error 0.03448546730764389, test error 0.05417068613442337\n",
            "Loss: 0.05913263006069869\n",
            "training error 0.03447525220507152, test error 0.05422898083746909\n",
            "Loss: 0.1668092692158174\n",
            "training error 0.03448187981233257, test error 0.05431889756443122\n",
            "Loss: 0.332895216261786\n",
            "training error 0.03459826913488239, test error 0.05410218665111481\n",
            "Loss: 0.0\n",
            "training error 0.03448836738165756, test error 0.05438543740207728\n",
            "Loss: 0.5235476946413575\n",
            "training error 0.034496485839433766, test error 0.05437246611503605\n",
            "Loss: 0.4995721627005123\n",
            "training error 0.0345002007745638, test error 0.05438436669824787\n",
            "Loss: 0.5215686547989984\n",
            "training error 0.034495389312299694, test error 0.05449823729175321\n",
            "Loss: 0.7320418362983805\n",
            "training error 0.03453682406246043, test error 0.05439415109701667\n",
            "Loss: 0.5396536886477898\n",
            "training error 0.03452370023425172, test error 0.05444637643095191\n",
            "Loss: 0.6361846001838201\n",
            "training error 0.03447841888441704, test error 0.054298379889214494\n",
            "Loss: 0.36263458141694294\n",
            "training error 0.034486730967138016, test error 0.054194741040583264\n",
            "Loss: 0.171073287786494\n",
            "training error 0.03450304286252956, test error 0.05415853672370351\n",
            "Loss: 0.10415488925072935\n",
            "training error 0.03451007723761811, test error 0.05421842090516387\n",
            "Loss: 0.21484206322124333\n",
            "training error 0.03450652078584311, test error 0.05439513621729244\n",
            "Loss: 0.5414745397755327\n",
            "training error 0.03449162374125315, test error 0.054391098145343195\n",
            "Loss: 0.5340107528212679\n",
            "training error 0.03450481216942313, test error 0.05439834169075818\n",
            "Loss: 0.5473993898863494\n",
            "training error 0.03451306642638082, test error 0.054302825746739475\n",
            "Loss: 0.3708521005232335\n",
            "training error 0.03448777988282329, test error 0.054360049618237316\n",
            "Loss: 0.47662207959424663\n",
            "training error 0.03458808506473157, test error 0.05419974852514801\n",
            "Loss: 0.1803288925498192\n",
            "training error 0.03447886764027897, test error 0.05434906297281871\n",
            "Loss: 0.456314868188068\n",
            "training error 0.03450552652516065, test error 0.05453113861719422\n",
            "Loss: 0.7928551369754455\n",
            "training error 0.03450577451011835, test error 0.054567558930679744\n",
            "Loss: 0.8601727737289844\n",
            "training error 0.034468600959114706, test error 0.054444964595771764\n",
            "Loss: 0.6335750288013253\n",
            "training error 0.034495933270552295, test error 0.05442340469713534\n",
            "Loss: 0.5937247011695623\n",
            "training error 0.034559498637276684, test error 0.054502869071454595\n",
            "Loss: 0.7406030054268209\n",
            "training error 0.034492324350226006, test error 0.05444263281573726\n",
            "Loss: 0.629265073550278\n",
            "training error 0.03453540196734113, test error 0.05446220892370856\n",
            "Loss: 0.665448653518208\n",
            "training error 0.03452146980525907, test error 0.054433440165924045\n",
            "Loss: 0.6122738013259355\n",
            "training error 0.03448563987298132, test error 0.05442942852612991\n",
            "Loss: 0.6048588703546631\n",
            "training error 0.03448406801083736, test error 0.05445373659454673\n",
            "Loss: 0.6497887889429066\n",
            "training error 0.034461703307115094, test error 0.05442642518920599\n",
            "Loss: 0.5993076401552333\n",
            "training error 0.03451946279387932, test error 0.054414881429582194\n",
            "Loss: 0.5779706844084354\n",
            "training error 0.03450727872489126, test error 0.05417975350263375\n",
            "Loss: 0.1433710101574004\n",
            "training error 0.034471997391535295, test error 0.054155040845703224\n",
            "Loss: 0.09769326871988948\n",
            "training error 0.03447904022504691, test error 0.054102718920163044\n",
            "Loss: 0.0009838216922020138\n",
            "training error 0.03449240771760945, test error 0.05420459488122669\n",
            "Loss: 0.18928667481088457\n",
            "training error 0.034522659233178404, test error 0.05438217048136272\n",
            "Loss: 0.517509268254579\n",
            "training error 0.03450206700197244, test error 0.05419847561812728\n",
            "Loss: 0.1779761096042165\n",
            "training error 0.03451668610903804, test error 0.054035382478017174\n",
            "Loss: 0.0\n",
            "training error 0.0345225825783684, test error 0.054097104816897126\n",
            "Loss: 0.1142257832727589\n",
            "training error 0.034461490608254125, test error 0.05392484885051093\n",
            "Loss: 0.0\n",
            "training error 0.03448714460474908, test error 0.054076716848430274\n",
            "Loss: 0.28162897283281474\n",
            "training error 0.03450386147834279, test error 0.05412187693703327\n",
            "Loss: 0.36537531531806167\n",
            "training error 0.03448624557982353, test error 0.0542804544593684\n",
            "Loss: 0.6594466492493511\n",
            "training error 0.03449962545032857, test error 0.05431612811471443\n",
            "Loss: 0.7256010402332258\n",
            "training error 0.034571240660650246, test error 0.0542987396377642\n",
            "Loss: 0.6933552809573307\n",
            "training error 0.03446942982090482, test error 0.05426134543261554\n",
            "Loss: 0.6240102462548247\n",
            "training error 0.03448739861738292, test error 0.054342537001129426\n",
            "Loss: 0.7745745412776284\n",
            "training error 0.03451109307299927, test error 0.0543123863271672\n",
            "Loss: 0.7186621472608845\n",
            "training error 0.034519244830109305, test error 0.05425020010911334\n",
            "Loss: 0.603341994530826\n",
            "training error 0.03448618555532858, test error 0.05413670902167793\n",
            "Loss: 0.3928804172531164\n",
            "training error 0.034464331608476506, test error 0.05424214140419659\n",
            "Loss: 0.588397669069507\n",
            "training error 0.034465266434843926, test error 0.05426556308243346\n",
            "Loss: 0.6318315937556829\n",
            "training error 0.03449878973963737, test error 0.054542447845055235\n",
            "Loss: 1.1452957360277294\n",
            "training error 0.034523480705422646, test error 0.0543064402512879\n",
            "Loss: 0.7076355500500453\n",
            "training error 0.034457330687349755, test error 0.054511194588363614\n",
            "Loss: 1.0873386766055315\n",
            "training error 0.03445112964689845, test error 0.05453360093566286\n",
            "Loss: 1.1288897384571284\n",
            "training error 0.034484327277710634, test error 0.05445552032943861\n",
            "Loss: 0.9840945134566681\n",
            "training error 0.03448468476457164, test error 0.054534744944012635\n",
            "Loss: 1.131011224885281\n",
            "training error 0.034485505665271865, test error 0.05435535650385921\n",
            "Loss: 0.7983474456121797\n",
            "training error 0.03452433483233508, test error 0.0545801153773715\n",
            "Loss: 1.2151476375521941\n",
            "training error 0.03448385843158247, test error 0.05453681432028112\n",
            "Loss: 1.1348487437890853\n",
            "training error 0.03452185448722568, test error 0.05435952922731405\n",
            "Loss: 0.8060854801988038\n",
            "training error 0.03444684600839823, test error 0.054459497706932675\n",
            "Loss: 0.9914702921168805\n",
            "training error 0.03450196339696081, test error 0.05467768867787176\n",
            "Loss: 1.396090751126322\n",
            "training error 0.034449012026212124, test error 0.054456638955606665\n",
            "Loss: 0.9861689303384935\n",
            "training error 0.03449775346268819, test error 0.05453867095786813\n",
            "Loss: 1.1382917531374437\n",
            "training error 0.0344913543577447, test error 0.054540444531088444\n",
            "Loss: 1.1415807252127097\n",
            "training error 0.03446135479946582, test error 0.05455192658149755\n",
            "Loss: 1.1628734143047659\n",
            "training error 0.03447548647130226, test error 0.054496919466418985\n",
            "Loss: 1.060866424482576\n",
            "training error 0.03444421235556425, test error 0.05466274483123375\n",
            "Loss: 1.3683783941024874\n",
            "training error 0.03447530124622765, test error 0.054538022257439665\n",
            "Loss: 1.1370887818870967\n",
            "training error 0.03447090384333048, test error 0.054522424381217224\n",
            "Loss: 1.1081635710521454\n",
            "training error 0.03445856609739207, test error 0.054666307505242644\n",
            "Loss: 1.3749851330824514\n",
            "training error 0.03442609468765241, test error 0.05455099512041728\n",
            "Loss: 1.1611460824714204\n",
            "training error 0.034462729563975336, test error 0.0543791234426448\n",
            "Loss: 0.8424216327303835\n",
            "training error 0.034420782502903445, test error 0.05430150815634285\n",
            "Loss: 0.6984893121834812\n",
            "training error 0.03450016994474927, test error 0.05432711363983867\n",
            "Loss: 0.7459729566287487\n",
            "training error 0.03445282876461591, test error 0.05422377989134631\n",
            "Loss: 0.5543474802573334\n",
            "training error 0.03445433859122048, test error 0.054460723676871076\n",
            "Loss: 0.9937437707905117\n",
            "training error 0.03445855152150557, test error 0.05446143675668538\n",
            "Loss: 0.9950661292755125\n",
            "training error 0.03446908411913097, test error 0.05418996728087984\n",
            "Loss: 0.4916442716489833\n",
            "training error 0.03443071200987402, test error 0.05431249595382398\n",
            "Loss: 0.7188654425118068\n",
            "training error 0.03444829924682008, test error 0.05434875169508775\n",
            "Loss: 0.7860992726228311\n",
            "training error 0.03449560469081025, test error 0.05430902666442974\n",
            "Loss: 0.7124318790096451\n",
            "training error 0.03442774871776927, test error 0.0543451417557837\n",
            "Loss: 0.7794048833366096\n",
            "training error 0.034445008785791154, test error 0.05445575660857549\n",
            "Loss: 0.9845326772010665\n",
            "training error 0.03449104324616275, test error 0.05433953459009867\n",
            "Loss: 0.7690067722532223\n",
            "training error 0.03443192090643379, test error 0.05437914742214162\n",
            "Loss: 0.8424661010920653\n",
            "training error 0.03443869874298545, test error 0.05427995603013696\n",
            "Loss: 0.6585223458121403\n",
            "training error 0.0344503855012076, test error 0.054368003108487634\n",
            "Loss: 0.8217997220635764\n",
            "training error 0.03446112287463978, test error 0.054336236202821\n",
            "Loss: 0.7628901352148443\n",
            "training error 0.03447949127726631, test error 0.054266502873407264\n",
            "Loss: 0.6335743728155041\n",
            "training error 0.03444903485683036, test error 0.05431629183095777\n",
            "Loss: 0.7259046409791248\n",
            "training error 0.03447664953393052, test error 0.0543443910187148\n",
            "Loss: 0.778012692009411\n",
            "training error 0.03442958291289703, test error 0.05449995830995258\n",
            "Loss: 1.0665017551295453\n",
            "training error 0.03454713864172146, test error 0.05426629255617445\n",
            "Loss: 0.6331843536734993\n",
            "training error 0.0344070558360962, test error 0.05443236801592993\n",
            "Loss: 0.9411601075154374\n",
            "training error 0.034436301665881956, test error 0.054463627825803954\n",
            "Loss: 0.9991293193729822\n",
            "training error 0.03444564031612082, test error 0.05437077651379969\n",
            "Loss: 0.8269428154076941\n",
            "training error 0.03445934814869317, test error 0.05450598130499737\n",
            "Loss: 1.077670993751778\n",
            "training error 0.03445016909534866, test error 0.05441924905052773\n",
            "Loss: 0.9168318698256517\n",
            "training error 0.034482291858939015, test error 0.0542938307279888\n",
            "Loss: 0.6842520384262141\n",
            "training error 0.034443642663382804, test error 0.054489394691369056\n",
            "Loss: 1.0469122359955785\n",
            "training error 0.03444710137517155, test error 0.05426644603571072\n",
            "Loss: 0.6334689711356534\n",
            "training error 0.03444537054071979, test error 0.05421678990610663\n",
            "Loss: 0.5413850234518236\n",
            "training error 0.034459544134671574, test error 0.054157561980124655\n",
            "Loss: 0.43155082410866186\n",
            "training error 0.03441432150410763, test error 0.054274103500091875\n",
            "Loss: 0.647669223049907\n",
            "training error 0.03453319220460627, test error 0.05425767087205829\n",
            "Loss: 0.6171960212072225\n",
            "training error 0.034442335858701094, test error 0.054187776529510796\n",
            "Loss: 0.48758167079661074\n",
            "training error 0.03443021792210651, test error 0.05431123852022691\n",
            "Loss: 0.7165336166024661\n",
            "training error 0.03446739433858768, test error 0.05419339303337516\n",
            "Loss: 0.4979970989045901\n",
            "training error 0.034440304397988654, test error 0.05422942530724593\n",
            "Loss: 0.5648165237872815\n",
            "training error 0.03448881519938895, test error 0.05438999967977874\n",
            "Loss: 0.8625908819091643\n",
            "training error 0.03447597542152125, test error 0.05404407414707001\n",
            "Loss: 0.22109528186087246\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xU9Z3/8ddnEhJAFAGpNyLgSlWsEkoKDCriHVertdpFVjdW3Q3Wttq1Lcj215ttVeiv1fqra6Fb17XSLa3Wel8sVhQlXqCCCipQGyUu2BQVBIVc5vP745wJM8mE3OZkJpP3k8c8cr7fc5nvmRPyme/3e873a+6OiIhIS7FcF0BERPKTAoSIiGSkACEiIhkpQIiISEYKECIikpEChIiIZKQAIdJFZnaimb2e63KIRMX0HIT0RmZWA/yzuy/NdVlECpVqECJtMLOiXJehuwrhHCR3FCCkoJhZzMyuM7M/m9lWM/uNmQ1NWf9bM9tiZtvM7CkzOyZl3Z1mdruZPWJmO4GTzazGzL5mZi+F+yw2s/7h9tPMrDZl/za3DdfPNrPNZva/ZvbPZuZmdkQb5zHUzP4z3PY9M/t9mP95M3u6xbbNx8lwDl8Lz7coZfvzzeyljnxe0rcpQEih+TLwGeAk4BDgPeC2lPWPAmOAjwF/Aha12P8fgR8A+wLJP8T/AEwHRgPHAZ/fy/tn3NbMpgPXAqcBRwDT2jmPXwIDgWPCst7czvZtncNPgJ3AKS3W/ypcbu/zkj5MAUIKzZXAN9y91t13A98BLjSzYgB3v8PdP0hZN87MBqfsf7+7P+PuCXffFebd6u7/6+7vAg8C5Xt5/7a2/QfgP919rbt/GL53RmZ2MHAWcKW7v+fuDe7+ZCc+g5bn8N/AzPDY+wJ/H+ZBO5+X9G0KEFJoRgL3mdn7ZvY+8CrQBBxoZkVmdlPYnLIdqAn3OSBl/00ZjrklZflDYNBe3r+tbQ9pcexM75NUBrzr7u/tZZu9aXnsXwGfNbNS4LPAn9z9zXBdm59XF99bCogChBSaTcBZ7r5/yqu/u79N0LRyHkEzz2BgVLiPpewf1W19m4ERKemyvWy7CRhqZvtnWLeToOkJADM7KMM2aefg7uuANwlqJanNS8n3auvzkj5OAUJ6s35m1j/lVQz8DPiBmY0EMLPhZnZeuP2+wG5gK8Ef2Rt6sKy/AS4zs6PNbCDwzbY2dPfNBH0l/25mQ8ysn5lNDVevAY4xs/KwA/w7HXz/XwHXAFOB36bk7+3zkj5OAUJ6s0eAj1Je3yHolH0AeMzMPgCeBSaF299F8E36bWBduK5HuPujwK3AE8DGlPfe3cYu/wQ0AK8BfwW+Eh5nPXA9sBTYwJ6O9Pb8N0FH9B/d/W8p+Xv7vKSP04NyIjlgZkcDrwCl7t6Y6/KIZKIahEgPCZ8/KDWzIcA84EEFB8lnChAiPWcWQXPRnwnuFPpCbosjsndqYhIRkYxUgxARkYwK5mnJAw44wEeNGpXrYoiI9CqrVq36m7sPz7SuYALEqFGjWLlyZa6LISLSq5jZm22tUxOTiIhkpAAhIiIZKUCIiEhGBdMHISL5oaGhgdraWnbt2tX+xtJj+vfvz4gRI+jXr1+H91GAEJGsqq2tZd9992XUqFGYWfs7SOTcna1bt1JbW8vo0aM7vJ+amEQkq3bt2sWwYcMUHPKImTFs2LBO1+oirUGE0yz+BCgC/sPdb2qx/lrgn4FGoA64PDmRiZk1AS+Hm77l7udGWdauWLhqIbc8ewvv7XqPoQOGcs2ka6iaUNWpfd/Z+Q67GnZRn6gnZjEO2+8wTjv8NCrHVRIvi7d7nDlL5/C7db/js2M/y2eO/AzLapYxbdS0Du0rEhUFh/zTlWsS2VAb4STp64HTgVrgBWBmOHlJcpuTgefc/UMz+wIwzd1nhOt2uPveZu5KU1FR4V19DmLO0jnc9vxt7GrchZkRI0bCEyQ8QcyCSpbjaesa2xhjLRZWypL7JY9hZrg7CU/g4b/2FFFELBa8n7s3X2B3J2YxmrypzeMUhXPUJ7dNLlvMiFnrY8YsxtEHHM3tZ9+u4CLd8uqrr3L00UfnuhiSQaZrY2ar3L0i0/ZRNjFNBDa6+xvuXg/8mmA2r2bu/kQ4Py8E49CPoId9+ZEvM/+Z+exs2EmTN9GYaKQ+UU+jN5IgCASN3thqXVsS4b/kfsnlhkRDc7ojwQGgiSYaEg00eRMJEjR5U/Nyozfu9Tgtt230RpoIz6GpnsbEnnNK5q15Zw1T7phC8fXFlH6/lPKflVO9qbrTn6lILm3dupXy8nLKy8s56KCDOPTQQ5vT9fX1e9135cqVXH311e2+x5QpU7JS1mXLljF48ODm8pWXl7N06dKsHDsbomxiOpT0uXFr2ftEJFcQzKKV1N/MVhI0P93k7r9vuYOZVQFVAIcddliXCvnIhke6tF8ha/ImmpqamgPGoJJBXPWpq5h32rxcF02kXcOGDWP16tUAfOc732HQoEF87Wtfa17f2NhIcXHmP30VFRVUVGT8Mp1mxYoV2SkscOKJJ/LQQw+1ud7dg5aAWCxjui17O8+OyotOajO7BKgAfpiSPTKs9vwjcIuZ/V3L/dx9obtXuHvF8OEZhxJp14XHXNil/bLNyN822x31O5j/zHzKflymGoVEoroabrwx+BmFz3/+81x55ZVMmjSJ2bNn8/zzzxOPxxk/fjxTpkzh9ddfB4Jv9Oeccw4QBJfLL7+cadOmcfjhh3Prrbc2H2/QoEHN20+bNo0LL7yQo446iosvvphks/0jjzzCUUcdxYQJE7j66qubj9sRNTU1HHnkkVRWVvKJT3yC5cuXp6U3bdrE17/+dT7xiU9w7LHHsnjx4ubynHjiiZx77rmMHTu2259blDWIt0mfmH1EmJfGzE4DvgGc5O7N0y8mJ0139zfMbBkwnmAc/axKfitO64PI0EafXE5dV1JUwoj9RlAcK6Yx0cjmDzazq2lXu/uZGf1i/RjcfzCTR0xm9pTZxMvizR3Xtdtr+ajhIzDaLUtJUQn7l+7P+7vepz6RXn3uyDmYGYlE0CzWntoPaplyxxQuPvZi7v7s3Vm7BlK4vvIVCL/Mt2nbNnjpJUgkIBaD446DwYPb3r68HG65pfNlqa2tZcWKFRQVFbF9+3aWL19OcXExS5cu5d/+7d+49957W+3z2muv8cQTT/DBBx9w5JFH8oUvfKHVcwQvvvgia9eu5ZBDDuH444/nmWeeoaKiglmzZvHUU08xevRoZs6c2Wa5li9fTnl5eXP63nvvpaioiA0bNvBf//VfTJ48mZqamrT0vffey+rVq1mzZg1/+9vf+NSnPsXUqcG05X/605945ZVXOnU7a1uiDBAvAGPMbDRBYLiIoDbQzMzGAwuA6e7+15T8IcCH7r7bzA4AjgfmR1XQeafNy4vmk6oJVR2+CyrbFq5ayA3Lb6BuZx27GnftNWAsenkRj/35Me6/6H51aEu3bdsWBAcIfm7btvcA0VWf+9znKCoqCt9zG5deeikbNmzAzGhoaMi4z9lnn01paSmlpaV87GMf45133mHEiPSu0okTJzbnlZeXU1NTw6BBgzj88MOb/0jPnDmThQsXZnyPTE1MNTU1jBw5ksmTJzfnpaaffvppZs6cSVFREQceeCAnnXQSL7zwAvvttx8TJ07MSnCACAOEuzea2ZeAJQS3ud7h7mvN7Hpgpbs/QNCkNAj4bfgtN3k769HAAjNLEDSD3ZR695NkX8vgtHDVQr79xLfZsnNLxu3rPqxjyh1TWHDOgpwFNcl/HfmmX10Np54K9fVQUgKLFkE8gu8d++yzT/PyN7/5TU4++WTuu+8+ampqmDZtWsZ9SktLm5eLiopobGx9g0pHtulueTOlO7pfd0TaB+Huj7j7x93979z9B2Het8LggLuf5u4Hunt5+Do3zF/h7se6+7jw5y+iLKe0VjWhis1f28yKy1cwZsiYNreb9dAsFq7K/M1IpCPicXj8cfje94KfUQSHlrZt28ahhx4KwJ133pn14x955JG88cYb1NTUADT3EWTLiSeeyOLFi2lqaqKuro6nnnqKiRMnZvU9IE86qSV/xcvirL96PQvOWcDA4oEZt5n10Cx1Xku3xOMwd27PBAeA2bNnM3fuXMaPH5+1b/ypBgwYwL//+78zffp0JkyYwL777svgNtrNkn0Qydc999zT7vHPP/98jjvuOMaNG8cpp5zC/PnzOeigg7J9GoUzJ3V3HpSTjpv080k8/7/Pt8ofM2QM669en4MSSb7Rg3KBHTt2MGjQINydL37xi4wZM4Z//dd/zWmZ8ulBOSlAz/3Lc1x87MWt8je8t4E5S+fkoEQi+ennP/855eXlHHPMMWzbto1Zs2blukidphqEdMmZvzyTx954rFX+istX6M6mPk41iPylGoT0iCX/tIQjhhzRKv+qh6/KQWlEJAoKENJld51/V6u81e+sVlOTSIFQgJAui5fFmX387Fb5P3zmh7qrSaQAKEBIt8w7bR7jDhyXluc4d61pXbsQkd5FAUK67fazb2+V92ztszkoiUj3hvuGYMC7tkZrvfPOOxk+fHjacwvr1hXuIA+ak1q6LV4WZ+zwsayr2/MfZfU7q6neVK07mqTHtTfcd3uWLVvGoEGD2pzzYcaMGfz0pz9tc/+Ww2x3dNjtbAzPnW2qQUhWXDPpmlZ51y29Lgclkd6oelM1Ny6/MbK+q1WrVnHSSScxYcIEzjzzTDZv3gzArbfeytixYznuuOO46KKLqKmp4Wc/+xk333wz5eXlLF++vEPHbznMdsv0rl27uOyyyzj22GMZP348TzzxBBDUSM4991xOOeUUTj311EjOvTvyK1xJr1U1oYobn76RmvdrmvOWv7VctYg+7iv/8xVWb9n7eN/bdm/jpXdeap6e97gDj2NwadvDuZYfVM4t0zs+3re78+Uvf5n777+f4cOHs3jxYr7xjW9wxx13cNNNN/GXv/yF0tJS3n//ffbff3+uvPLKvdY6Fi9ezNNPP92crg4nsUgdZnvZsmVp6R/96EeYGS+//DKvvfYaZ5xxBuvXr2/e76WXXmLo0KEdPqeeohqEZM3cE+ampR1n/jORjdIuBWLbrm0kPBjvO+EJtu3altXj7969m1deeYXTTz+d8vJyvv/971NbWwvAcccdx8UXX8zdd9/d4eadGTNmsHr16ubXgAEDAFoNs52afvrpp7nkkksAOOqooxg5cmRzgDj99NPzMjiAahCSRVUTqrh95e1p3xjvf/1+1SL6sI5806/eVM2pd51KfVM9JUUlLPrsoqz+vrg7xxxzTPM3/VQPP/wwTz31FA8++CA/+MEPePnll7v8PvkwPHe2qQYhWTX50Mlpad3yKu2Jl8V5vPJxvnfy93i88vGsf5koLS2lrq6uOUA0NDSwdu1aEokEmzZt4uSTT2bevHls27aNHTt2sO+++/LBBx9ktQwnnngiixYtAmD9+vW89dZbHHnkkVl9jygoQEhWVY6rbDW/9pYdmScdEkmKl8WZe+LcSGqasViMe+65hzlz5jBu3DjKy8tZsWIFTU1NXHLJJc0dx1dffTX7778/n/70p7nvvvva7KRevHhx2m2ubd0Sm+qqq64ikUhw7LHHMmPGDO688860iYbylQbrk6w76c6TeOrNp5rTUw+bypOXPZnDEklP0mB9+UuD9UnOjT1gbFr66bee1tAbIr2QAoRkXeW4SmIpv1oJEuqHEOmFFCAk6+JlcU4YeUJaXupT1lL4CqXpupB05ZooQEgk1MzUd/Xv35+tW7cqSOQRd2fr1q3079+/U/vpOQiJROW4ShauWkiC8AGosJlJz0MUvhEjRlBbW0tdXV2uiyIp+vfvz4gRIzq1jwKERCLZzJR6N5OamfqGfv36pT1RLL2XmpgkMmpmEundFCAkMrqbSaR3U4CQyGS6m0lPVYv0HgoQEqmWzUwi0nsoQEikKsdVUmRFzemHNzysfgiRXkIBQiIVL4tz1hFnNacbEg3qhxDpJRQgJHLFsfS7qdUPIdI7KEBI5A4adNBe0yKSnxQgJHLjDx6flt6v/345KomIdIYChERu64db0yYR+tGKH6mjWqQXUICQyE0bNY2Y7flVa/ImdVSL9AIKEBK5eFmcTx/56VwXQ0Q6SQFCekTqra6gfgiR3kABQnrE1g+3pqVvrr5Z/RAieS7SAGFm083sdTPbaGbXZVh/rZmtM7OXzOxxMxuZsu5SM9sQvi6NspwSvWmjpqU9D9GYaGRZzbLcFUhE2hVZgDCzIuA24CxgLDDTzFoOzPMiUOHuxwH3APPDfYcC3wYmAROBb5vZkKjKKtGLl8W5Nn5tc9pxhg0clsMSiUh7oqxBTAQ2uvsb7l4P/Bo4L3UDd3/C3T8Mk88CyemOzgT+4O7vuvt7wB+A6RGWVXrA9l3b09KPbng0RyURkY6IMkAcCmxKSdeGeW25Akj+xejQvmZWZWYrzWylpjfsfR5c/6D6IUTyWF50UpvZJUAF8MPO7OfuC929wt0rhg8fHk3hJGsqx1WmPQ+R8IT6IUTyWJQB4m2gLCU9IsxLY2anAd8AznX33Z3ZV3qXeFmcayerH0Kkt4gyQLwAjDGz0WZWAlwEPJC6gZmNBxYQBIe/pqxaApxhZkPCzukzwjzp5XbU70hLv7j5xRyVRETaU9z+Jl3j7o1m9iWCP+xFwB3uvtbMrgdWuvsDBE1Kg4DfmhnAW+5+rru/a2bfIwgyANe7+7tRlVVERFqLLEAAuPsjwCMt8r6VsnzaXva9A7gjutJJLmhkV5HeIy86qaXvaDmyq56oFslfChDSo6aNmkZRbM8c1XqiWiR/KUBIj9IT1SK9hwKE9LiWT1TrTiaR/KQAITm3ZceWXBdBRDJQgJAeVzmuMm1k10c3PqqOapE8pAAhPS5eFueK8Vc0pxuaGtRRLZKHFCAkJz558CeblxMk1FEtkocUICQnWnZMq6NaJP8oQEheUEe1SP5RgJCcqBxXSbGpo1oknylASE7Ey+JcPv7y5rQ6qkXyjwKE5MyEQyY0L6ujWiT/KEBIzqijWiS/KUBI3lBHtUh+UYCQnKkcV0mR7RnZ9eEND6ujWiSPKEBIzsTL4px1xFnN6YZEA3etuSuHJRKRVAoQklMj9huR6yKISBsUICSnWk5B2jItIrmjACE5tfXDrc3LhqWlRSS3FCAkp1KffXCc93e/n8PSiEgqBQjJqa0fbsWw5vTN1TfrTiaRPKEAITk1bdQ0imJ7bnVtTDRqyA2RPKEAITkVL4tzbfza5rTjGnJDJE8oQEjO7V+6f/OyOqpF8ocChOScOqpF8pMChOScOqpF8pMChOScOqpF8pMChOScOqpF8pMChOSF7bu2p6U1N4RI7ilASF7S3BAiuacAIXmhclwlxVbcnH5046PqqBbJMQUIyQvxsjiXjb+sOd3Q1KCOapEcU4CQvFFxSEXzcoKEOqpFckwBQvJGy45pdVSL5JYChIiIZNRugDCzmJlN6crBzWy6mb1uZhvN7LoM66ea2Z/MrNHMLmyxrsnMVoevB7ry/tK7tJxNbr/+++WoJCICHQgQ7p4Abuvsgc2sKNzvLGAsMNPMxrbY7C3g88CvMhziI3cvD1/ndvb9pffRkBsi+aWjTUyPm9kFZmbtb9psIrDR3d9w93rg18B5qRu4e427vwQkOnFcKVAackMkv3Q0QMwCfgvUm9l2M/vAzLa3s8+hwKaUdG2Y11H9zWylmT1rZp/JtIGZVYXbrKyrq+vEoSUfacgNkfxS3P4m4O77Rl2QDEa6+9tmdjjwRzN72d3/3KJcC4GFABUVFZ6DMkqWacgNkfzRoQABYGbnAlPD5DJ3f6idXd4GylLSI8K8DnH3t8Ofb5jZMmA88Oe97iQFR0NuiOROh5qYzOwm4BpgXfi6xsxubGe3F4AxZjbazEqAi4AO3Y1kZkPMrDRcPgA4PnxfKXCV4yrpF+vXnH54w8PqqBbJkY72Qfw9cLq73+HudwDTgbP3toO7NwJfApYArwK/cfe1ZnZ9WBvBzD5lZrXA54AFZrY23P1oYKWZrQGeAG5ydwWIPiBeFufsMXt+tRoSDdy15q4clkik7+pwExOwP/BuuDy4Izu4+yPAIy3yvpWy/AJB01PL/VYAx3aibFJADhp0UK6LICJ0PEDcALxoZk8ARtAX0erBN5Fs0ANzIvmhQ09SEzynMBn4HXAvEHf3xRGXTfqorR9uTUvrgTmR3Ojok9Sz3X2zuz8QvnRriURm2qhpFMf2VG71wJxIbnS0k3qpmX3NzMrMbGjyFWnJpM/SA3Mi+aGjAWIG8EXgKWBV+FoZVaFEWj4w9+iGR3NUEpG+q6N9ENe5++gWr8N7oHwiADy4/kH1Q4j0sI72QXy9B8oi0qxyXCVFtmfgvoQn1A8h0sPUByF5KV4W56vxrzan1Q8h0vM6+hzEjPDnF1PyHFAzk0Rm+24N3CeSSx0dzXV01AURaanlQH0auE+kZ+21icnMZqcsf67FuhuiKpQIaMgNkVxrrw/iopTluS3WTc9yWUTSaGRXkdxqL0BYG8uZ0iJZpZFdRXKrvQDhbSxnSotETv0QIj2nvU7qceHc0wYMSJmH2oD+kZZMBPVDiOTSXmsQ7l7k7vu5+77uXhwuJ9P99rZvbzJnDgwaBMXF0K8flJYGP5Pp1OXetG7ffeHjH4eDD4YDDwzOs7oabrwx+NkbVI6rTBu4T/0QIj3H3AujpaiiosJXruz88FBf/Sr8+McRFKgXiMWCF4A7mAXpRCL4efTRcPvtEI/ntpzn//p8fv/675vTV064ktvPuT2HJRIpHGa2yt0rMq3r6JPUBeu++3JdgtxJJKCxMXg1NQU/6+v3/FyzBqZMgaKioFYyZEhQC8m1dXWafVakJ/T5APG5z7W/TV+XDCTvvw/z5wcBo7y855qpWvZDPP3W02pmEukBfT5AzJsHs2fDPvsEf/iKi6GkJPiZTKcu95Z1sQivbCKxp3bRE4GiclwlsZRf1QQJ3e4q0gP6fICAIEjs2BF8S25ogN27g5/JdOpyb1nX1BQEvkMPhalT4Ywzgo7rkpKg87q0tO2g05ngkgwUl1wS3fWJl8U596hzo3sDEclIAaKAzZsHtbXw5JOwZAns2hUEle3bg+W2gk5TEyxYACNH7gkk1s5jkYsWQVlZdLWJs444Ky09/uDx0byRiDRTgJCMqqqgpmZPIEkkghpJSUnb+9TWwvHHw8KF2S9Py5FcNcOcSPQUIKTD5s0LahgLFgTNVJm4w6xZ0fdLPPD6A+qoFomYAoR0WlVV0Ey1txrF9OnZDRLqqBbpeQoQ0mXJGsUZZ7Ret307nHBC9oJEvCzOCSNPSMvT8xAi0VKAkG5bsiRzkEgk4NJLs/c+Yw8Ym5bW8xAi0VKAkKxYsgQmTmydv2FD9m6BVTOTSM9SgJCsee65zDWJRYuy09SkZiaRnqUAIVm1ZAkccUTr/Kuuys7x1cwk0nMUICTr7srQ6rN6dXYG+lMzk0jPUYCQrIvHg1tgW/rhD7vf1KRmJpGeowAhkZg3D8aNS89zD0aD7S41M4n0DAUIicztGeb0+f3vu1+LUDOTSM9QgJDIxOPwmc+0zu9uLSJeFueEw9Kbmbbs2NK9g4pIKwoQEqlMfRHPPtv94w4dMDQt/e5H73b/oCKSRgFCIhWPB/NRpNqypfsjvmqWOZHoRRogzGy6mb1uZhvN7LoM66ea2Z/MrNHMLmyx7lIz2xC+sjhgg/S0m25qnXfDDd07pvohRKIXWYAwsyLgNuAsYCww08zGttjsLeDzwK9a7DsU+DYwCZgIfNvMhkRVVolWplrEm292rxah211FohdlDWIisNHd33D3euDXwHmpG7h7jbu/BCRa7Hsm8Ad3f9fd3wP+AEyPsKwSsShqES1vd13+1nI1M4lkUZQB4lBgU0q6NszL2r5mVmVmK81sZV1dXZcLKtGLohZROa4SY89cqI5z3dJWLZki0kW9upPa3Re6e4W7VwwfPjzXxZF2ZKpF3HJL148XL4tz9PCj0/JUixDJnigDxNtAWUp6RJgX9b6Sp+JxKC9Pz3vtte49OHfNpGvS0o4z/5ksPK4tIpEGiBeAMWY22sxKgIuABzq47xLgDDMbEnZOnxHmSS83eXJ62j3z4H4dVTWhivKD0qPO/a/fr1qESBZEFiDcvRH4EsEf9leB37j7WjO73szOBTCzT5lZLfA5YIGZrQ33fRf4HkGQeQG4PsyTXq6yEszS87r74NzkQ9OjjuO65VUkCyLtg3D3R9z94+7+d+7+gzDvW+7+QLj8gruPcPd93H2Yux+Tsu8d7n5E+PrPKMspPSceh/POS89bvTq7ndWgW15FsqFXd1JL75Rp+I3udlafOPLEtDx1Vot0nwKE9LhMt7y++mr3OqtbPhOhzmqR7lOAkJzIdMvrdd14hCFTM5M6q0W6RwFCciIeh7EtBl5ZvrzrtYh4WZzzjkrv3FAtQqR7FCAkZ65Jf4Sh27e8zp4yW7UIkSxSgJCcqaqCMWPS87pzy2tbtQjd8irSNQoQklNDWozRu2ZN9zqrZ09pfYvU42883vUDivRhChCSU1dckZ52796UpPGyOGOHp3dubHhvA3OWzun6QUX6KAUIyamqqtbjM91/f3bHZwKY/8x89UWIdJIChORcFOMzjTtwXKv8qx6+qusHFemDFCAk56IYn+n2s29vlbf6ndVqahLpBAUIybkoxmeKl8WZfXzrDms1NYl0nAKE5IVM4zP94hfdO+a80+apqUmkGxQgJC9kmkzovfe6f1w1NYl0nQKE5I2WndUbNnSvmQmCpqapI6e2yp//zHwWrurmwUUKnAKE5I3KytZ5N9zQ/ePedOpNrYbgAJj10Cz1R4jshQKE5I1Mw4C/+WZ2ahE/O+dnGdddet+l3Tu4SAFTgJC8kmkY8GzUIqomVGW8q2nDexs485dndv8NRAqQAoTklahqERDc1XTG4We0yn/sjccUJEQyUICQvBNVLQJgyT8t4YghR7TKV5AQaU0BQvJOlLUIgLvOvytjp/VjbzzGpJ9Pys6biBQABQjJS5lqEbfckp1jx8viPHP5MwzrP+LUFhIAAA7KSURBVKzVuuf/93nKflymu5tEUICQPJXpwblXX+3eKK9pxy+L8+A/PphxXe0HtUy5YwqX/O6S7LyZSC+lACF5q+WDcwBXZXGUjHhZnAXnLGhz/aKXF6k2IX2aAoTkrUyjvHZ3EL+WqiZUseLyFYzYd0TG9apNSF+mACF5Kx6Hr3+9dX627mhqfp+yOJuu3cTEQya2uc2ilxcx+MbBGp5D+hQFCMlr8+bBuBYDsmbzjqZUz/3Lc8w+fjYlsZKM67fXb2fWQ7Po//3+GuxP+gRz91yXISsqKip85cqVuS6GRKC6GqZMSc876CDYvDm69zzzl2fy2BuP7XUbwygtLuWCoy/g7s/eHV1hRCJkZqvcvSLTOtUgJO/F4zBqVHreli1wSYTdAkv+aUnGoTlSOc6uxl0senkRxdcXc9KdJ6lDWwqKahDSKyxcCLNmtc5fsACqqqJ73+pN1Vz18FWsfmd1h/cpjhUTsxglRSV88uBPctOpNxEvi0dXSJFu2FsNQgFCeo3yclizJj0v6qampGSgWPPOGpzO/5+JESMWixGzGAP7DaRqQhXzTpsXQUlFOkcBQgpCdTUcfzy0/JU94wxYsqTnyjFn6RwWrFzA9t3buxQskgyjKFYEwAEDD+C7075L1YQIq0MiGShASMFoq6lp9uzgjqeeNmfpHG6pvoX6RH1Wjhcjhplh4QMg7o6ZqclKIqMAIQXlkktg0aLW+StWBB3auZCsVXzU+BEJT9CYaIz0/YosqHkkg0cikSDhCWKx4L6T1MCS8ERzujPriqyIAwcdyNwT5lI1oYrqTdXcteYu1tWt481tb7JPyT5cM+ma5lpP6vr1W9ezs2EnwwYOY+4JcwG4d929XDD2gla1pIWrFvKLP/2C+kQ9JbESxgwbQ93OOobvM5y6nXVcMPYCjv3YsSyrWca0UdMAWFazjGEDh7H1w61MGzVtrwGzelN1877xsnirdD5IlmnYwGG8uPlFACrHVfZI+RQgpOAMGwbvvpueN2YMrF+fm/K0VL2pmvnPzOfZ2md5b9d7NCYaMTOaEk3dapbKR8mRcTtzXskA1+RNWSlDjKB/x3HcnZgF6aZEE03seQ/D0sqZ7BuClOBI68CZ8AQxC2p37o4TrnOaA3Nyv+S2zWUJ+57Sjhnulzx+W59DcawYPCxbLOX9wvNLkACHA/bpehOlAoQUnLaamiZOhOee6/nydMYlv7uEe9bdQ5M34e5Z+yMpsuCcBZ0OEnoOQgpOVRVcfHHr/Oefh0l5PqXD3Z+9m13/ZxcN32yg8VuNrLh8BVMPm8qgfoMojhVTZEUUx4rTlmP6ryodcO+6e7N6vOKsHq0FM5sO/AQoAv7D3W9qsb4UuAuYAGwFZrh7jZmNAl4FXg83fdbdr4yyrNL73H031NXBYy0eeE4GiXyvSSTFy+I8edmT7W7Xsp8jG/0Mba1TzaZ3umDsBVk9XmRNTGZWBKwHTgdqgReAme6+LmWbq4Dj3P1KM7sION/dZ4QB4iF3/0RH309NTH3XpElBUGipNzQ35bOFqxZyw/IbqNtZ13yX1oDiAQwbOIz9++/Plg+2sH33duoT9bg7RbEiBpUMoqSohKEDhnLOx8/hsY2PsbZuLcWxYoYNCDqV6xP1aQGppKiEEfuNoL6png/qP8DdKY4Vs7N+JwNLBrKrYVfGu8SStav6pvrmPp7kMVMDIEBJrIQmb6Ix0UhRrIjSotJ298tmwG1vXctzako00ejBjQ7tHRO6d5t0TvogzCwOfMfdzwzTcwHc/caUbZaE21SbWTGwBRgOjEQBQjqhrAxqa1vnH300rFvXOl9EArnqgzgU2JSSrg3zMm7j7o3ANiA5D+RoM3vRzJ40sxMzvYGZVZnZSjNbWVdXl93SS6/ym9+0njsCglnoBg2KZvRXkUKXrz1fm4HD3H08cC3wKzPbr+VG7r7Q3SvcvWL48OE9XkjJH/E4PPMMjMgw78/OncEdT2PGZG/KUpG+IMoA8TZQlpIeEeZl3CZsYhoMbHX33e6+FcDdVwF/Bj4eYVmlAMTjsGlT0KyUycaNwbDhczSVg0iHRBkgXgDGmNloMysBLgIeaLHNA8Cl4fKFwB/d3c1seNjJjZkdDowB3oiwrFJA1q1rO0gAzJ8fPGinZieRvYssQIR9Cl8ClhDcsvobd19rZteb2bnhZr8AhpnZRoKmpOvC/KnAS2a2GrgHuNLdWzw3K9K2desyPyeR9O67QbOTAoVI2/QktRS06mr4h3/IfIdTquJimDEjeLZCpC/Rk9TSZyX7JfZWmwBobAwGADSD0tJg7gl1aEtfpwAhfcLddwejvY4Z0/629fXBxERTpgQ1iwEDop3eVCRfKUBInxGPB6O9LlgQzETXEU1NsGvXntpFcTH06wdDhuhuKCl8ChDS51RVBdOUrlgBU6dCUVHH921qCpqj3n8/uBvKLAgY/frtCR4DB8Lo0er8lt5PAUL6rHgcnnwy+IM/ezYMHgyxLvyPaGwMXsng8dFHUFMT3CVVVJQePEpL09PJ5YED4aST1O8h+UV3MYm0sHAh3HBDMFLsRx+1ngM7amZ7ajXuQToWg0RiTzqKdbEYlJTAEUfA5MlQWRlsv2wZTJuWu9n6JFqaMEikG+bMCfotdu4M/pgm/6j2NcnAZbbnM4jFgp/J5WTQSST21MaSy5nW9WQALNR1AAccAN/9btB82vnrqgAhklVz5sBttwUd2Kn/cSFoahLJhQULOh8k9ByESJbNmwc7dgR9Dg0NwSvZF7FiRfAcRbJ/obg4+OZdXBw04aSmi4szj0Ir0hX3ZndCuWhnlBPpi+JxePHFzu2T7PfYsiUIMj3dfAGq+RSCC7I7oZwChEg+qKrqWvtxNlVXB7fuvvgifPBB0OfS1BTUepJ3aOVrO3xfXgfd64PYGwUIEQGCms999+W6FJJP1AchIiIZKUCIiEhGChAiIpKRAoSIiGSkACEiIhkpQIiISEYFM9SGmdUBb3bjEAcAf8tScXqLvnbOfe18QefcV3TnnEe6+/BMKwomQHSXma1sazySQtXXzrmvnS/onPuKqM5ZTUwiIpKRAoSIiGSkALFHX5wgsq+dc187X9A59xWRnLP6IEREJCPVIEREJCMFCBERyajPBwgzm25mr5vZRjO7LtflyRYzKzOzJ8xsnZmtNbNrwvyhZvYHM9sQ/hwS5puZ3Rp+Di+Z2SdzewZdY2ZFZvaimT0Upkeb2XPheS02s5IwvzRMbwzXj8plubvDzPY3s3vM7DUze9XM4oV8nc3sX8Pf6VfM7L/NrH8hXmczu8PM/mpmr6Tkdfq6mtml4fYbzOzSzpShTwcIMysCbgPOAsYCM81sbG5LlTWNwFfdfSwwGfhieG7XAY+7+xjg8TANwWcwJnxVAbf3fJGz4hrg1ZT0POBmdz8CeA+4Isy/AngvzL853K63+gnwP+5+FDCO4PwL8jqb2aHA1UCFu38CKAIuojCv853A9BZ5nbquZjYU+DYwCZgIfDsZVDrE3fvsC4gDS1LSc4G5uS5XROd6P3A68DpwcJh3MPB6uLwAmJmyffN2veUFjAj/05wCPAQYwdOlxS2vN7AEiIfLxeF2lutz6MI5Dwb+0rLshXqdgUOBTcDQ8Lo9BJxZqNcZGAW80tXrCswEFqTkp23X3qtP1yDY88uWVBvmFZSwWj0eeA440N03h6u2AAeGy4XwWdwCzAbCiRgZBrzv7o1hOvWcms83XL8t3L63GQ3UAf8ZNq39h5ntQ4FeZ3d/G/i/wFvAZoLrtorCv85Jnb2u3brefT1AFDwzGwTcC3zF3benrvPgK0VB3OdsZucAf3X3VbkuSw8rBj4J3O7u44Gd7Gl2AAruOg8BziMIjIcA+9C6GaZP6Inr2tcDxNtAWUp6RJhXEMysH0FwWOTuvwuz3zGzg8P1BwN/DfN7+2dxPHCumdUAvyZoZvoJsL+ZJedeTz2n5vMN1w8GtvZkgbOkFqh19+fC9D0EAaNQr/NpwF/cvc7dG4DfEVz7Qr/OSZ29rt263n09QLwAjAnvgCgh6Ox6IMdlygozM+AXwKvu/uOUVQ8AyTsZLiXom0jmV4Z3Q0wGtqVUZfOeu8919xHuPorgOv7R3S8GngAuDDdreb7Jz+HCcPte9y3b3bcAm8zsyDDrVGAdBXqdCZqWJpvZwPB3PHm+BX2dU3T2ui4BzjCzIWHt64wwr2Ny3QmT6xfw98B64M/AN3Jdniye1wkE1c+XgNXh6+8J2l8fBzYAS4Gh4fZGcEfXn4GXCe4Syfl5dPHcpwEPhcuHA88DG4HfAqVhfv8wvTFcf3iuy92N8y0HVobX+vfAkEK+zsB3gdeAV4BfAqWFeJ2B/yboZ2kgqCle0ZXrClwenv9G4LLOlEFDbYiISEZ9vYlJRETaoAAhIiIZKUCIiEhGChAiIpKRAoSIiGSkACHSDjNrMrPVKa+sjfprZqNSR+sUySfF7W8i0ud95O7luS6ESE9TDUKki8ysxszmm9nLZva8mR0R5o8ysz+G4/I/bmaHhfkHmtl9ZrYmfE0JD1VkZj8P5zh4zMwGhNtfbcF8Hi+Z2a9zdJrShylAiLRvQIsmphkp67a5+7HATwlGkwX4f8B/uftxwCLg1jD/VuBJdx9HMF7S2jB/DHCbux8DvA9cEOZfB4wPj3NlVCcn0hY9SS3SDjPb4e6DMuTXAKe4+xvhwIhb3H2Ymf2NYMz+hjB/s7sfYGZ1wAh3351yjFHAHzyYAAYzmwP0c/fvm9n/ADsIhs/4vbvviPhURdKoBiHSPd7GcmfsTlluYk/f4NkE4+t8EnghZbRSkR6hACHSPTNSflaHyysIRpQFuBhYHi4/DnwBmufOHtzWQc0sBpS5+xPAHIJhqlvVYkSipG8kIu0bYGarU9L/4+7JW12HmNlLBLWAmWHelwlmePs6wWxvl4X51wALzewKgprCFwhG68ykCLg7DCIG3Oru72ftjEQ6QH0QIl0U9kFUuPvfcl0WkSioiUlERDJSDUJERDJSDUJERDJSgBARkYwUIEREJCMFCBERyUgBQkREMvr/WaeqRKTasmEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.05404407414707001\n",
            "training error 0.1250462860889735, test error 0.2499771523853952\n",
            "training error 0.12514106741151668, test error 0.25004299706752675\n",
            "training error 0.12508967977886476, test error 0.2502343491788476\n",
            "training error 0.12536816990739272, test error 0.25014956029720475\n",
            "training error 0.1250121441129004, test error 0.25027616342108694\n",
            "training error 0.12498784675905476, test error 0.25034808720031576\n",
            "training error 0.125054849656965, test error 0.2504810603162793\n",
            "training error 0.12503301907262243, test error 0.2504415980948619\n",
            "training error 0.12508080038339286, test error 0.25045700261676174\n",
            "training error 0.12510891440793784, test error 0.25040025330383714\n",
            "training error 0.12502237778879, test error 0.2504178836535482\n",
            "training error 0.12499855936803851, test error 0.2502868079930623\n",
            "training error 0.12498515989173188, test error 0.2504008420022289\n",
            "training error 0.12511474511113632, test error 0.25044713395176893\n",
            "training error 0.12497272370312619, test error 0.2504912919678488\n",
            "training error 0.12497032958921385, test error 0.2504956320771302\n",
            "training error 0.12497650843069574, test error 0.2504832030731933\n",
            "training error 0.12501218006094064, test error 0.25048655747496357\n",
            "training error 0.12521950916011523, test error 0.25041252221938176\n",
            "training error 0.1250456618920315, test error 0.25047581732770735\n",
            "training error 0.1250154808164486, test error 0.2506174621192859\n",
            "training error 0.12496570041412224, test error 0.2505861108799366\n",
            "training error 0.12497329453250755, test error 0.2505820919115259\n",
            "training error 0.12498271228106714, test error 0.250524095865616\n",
            "training error 0.12499221264747726, test error 0.2505112691865917\n",
            "training error 0.12506199058408457, test error 0.2505032817369644\n",
            "training error 0.12496678885560834, test error 0.250440736184299\n",
            "training error 0.124970565708495, test error 0.2505008781823847\n",
            "training error 0.12496840775457921, test error 0.25049352091454435\n",
            "training error 0.12496364679255093, test error 0.25057209053416\n",
            "training error 0.1250069865602294, test error 0.250617657444988\n",
            "training error 0.12499332237473114, test error 0.2505492954298344\n",
            "training error 0.12498813354997773, test error 0.2505348857817863\n",
            "training error 0.12507064165828932, test error 0.25060024943743875\n",
            "training error 0.12513217135631208, test error 0.250687029367158\n",
            "training error 0.12496550022990824, test error 0.25052223144789004\n",
            "training error 0.12506769995122963, test error 0.2504459536777119\n",
            "training error 0.1251055243263468, test error 0.2506591381952773\n",
            "training error 0.12514254284665483, test error 0.2504780188584633\n",
            "training error 0.12497615087294746, test error 0.2506170430638991\n",
            "training error 0.12499264759272077, test error 0.2505717302978033\n",
            "training error 0.12498018117604141, test error 0.2505846515763866\n",
            "training error 0.12505692580334546, test error 0.2504507467689164\n",
            "training error 0.12517154568433025, test error 0.25048510026682175\n",
            "training error 0.1249535853276812, test error 0.2505180970115616\n",
            "training error 0.12501641262035837, test error 0.25051903834251193\n",
            "training error 0.12496725292480068, test error 0.25043804734886954\n",
            "training error 0.12496297532678782, test error 0.25044419384580424\n",
            "training error 0.12502495858705112, test error 0.25033418208181774\n",
            "training error 0.12496805474258917, test error 0.25043679619429265\n",
            "Loss: 0.18387432791810987\n",
            "training error 0.1249896171456578, test error 0.25041244203013174\n",
            "Loss: 0.17413177187708406\n",
            "training error 0.12513501331529778, test error 0.2502611318199665\n",
            "Loss: 0.11360215598164025\n",
            "training error 0.12496553383624566, test error 0.2503875558589162\n",
            "Loss: 0.16417639356427483\n",
            "training error 0.1250133532785983, test error 0.25046644731271817\n",
            "Loss: 0.19573585931909054\n",
            "training error 0.12498928354365355, test error 0.25052849757408796\n",
            "Loss: 0.22055823239506278\n",
            "training error 0.1250011751486973, test error 0.2504919972712356\n",
            "Loss: 0.20595677682040403\n",
            "training error 0.12499699278488811, test error 0.25048889536773566\n",
            "Loss: 0.20471590201631695\n",
            "training error 0.12508910965622194, test error 0.25058954589227195\n",
            "Loss: 0.2449797915661467\n",
            "training error 0.12514949325069363, test error 0.25059809617448675\n",
            "Loss: 0.24840021704632953\n",
            "training error 0.1249566708122872, test error 0.25056635165981983\n",
            "Loss: 0.23570125061520564\n",
            "training error 0.12506916193426904, test error 0.25067115465385675\n",
            "Loss: 0.2776262797776097\n",
            "training error 0.12515652386894682, test error 0.25058454872558655\n",
            "Loss: 0.2429807421979513\n",
            "training error 0.12505627859886456, test error 0.2506277415605395\n",
            "Loss: 0.26025945528864725\n",
            "training error 0.12494374979373381, test error 0.25065727833147655\n",
            "Loss: 0.27207524351378076\n",
            "training error 0.12503039390277945, test error 0.2507125896610776\n",
            "Loss: 0.29420179751011855\n",
            "training error 0.12504518524658387, test error 0.2506047634805509\n",
            "Loss: 0.2510673832255117\n",
            "training error 0.12500030007338425, test error 0.2506194480022503\n",
            "Loss: 0.2569417287644038\n",
            "training error 0.12511732178834878, test error 0.25066014507153134\n",
            "Loss: 0.2732220443423472\n",
            "training error 0.12502702123135215, test error 0.2504927460079352\n",
            "Loss: 0.20625629887369712\n",
            "training error 0.12495190887994309, test error 0.2506037516678716\n",
            "Loss: 0.2506626211624008\n",
            "training error 0.12522478928381686, test error 0.25069674950761894\n",
            "Loss: 0.2878651570181523\n",
            "training error 0.12497366722096326, test error 0.25067168776661974\n",
            "Loss: 0.2778395443731485\n",
            "training error 0.12495697714834479, test error 0.25077927586371873\n",
            "Loss: 0.32087871658241784\n",
            "training error 0.12506388282004854, test error 0.2506528525869182\n",
            "Loss: 0.2703047838873207\n",
            "training error 0.12508259050264003, test error 0.2509091799347171\n",
            "Loss: 0.37284509421284806\n",
            "training error 0.1249513489603352, test error 0.2508326708202342\n",
            "Loss: 0.3422386512828224\n",
            "training error 0.12500083790834826, test error 0.2508314276047224\n",
            "Loss: 0.3417413196267427\n",
            "training error 0.12499951618826771, test error 0.25062054919901267\n",
            "Loss: 0.2573822477285992\n",
            "training error 0.12501531387070977, test error 0.25069664571853123\n",
            "Loss: 0.2878236375885912\n",
            "training error 0.12493122080242464, test error 0.25074812784915546\n",
            "Loss: 0.30841837200050737\n",
            "training error 0.12509877997531352, test error 0.2506485008486441\n",
            "Loss: 0.26856392948018115\n",
            "training error 0.12500025115467314, test error 0.2507519097481642\n",
            "Loss: 0.30993126986842423\n",
            "training error 0.1249309299140296, test error 0.25073505093515025\n",
            "Loss: 0.30318712831265415\n",
            "training error 0.12495032928853307, test error 0.250776703411736\n",
            "Loss: 0.3198496417417207\n",
            "training error 0.12493995517757944, test error 0.2507913146080982\n",
            "Loss: 0.3256946544649919\n",
            "training error 0.12492899338440722, test error 0.2508331967532186\n",
            "Loss: 0.3424490437044403\n",
            "training error 0.1249764343756742, test error 0.2508645022577451\n",
            "Loss: 0.3549723900294177\n",
            "training error 0.12494800109502914, test error 0.25080604674409923\n",
            "Loss: 0.33158804746526993\n",
            "training error 0.12494465469700208, test error 0.25090159158261044\n",
            "Loss: 0.3698094759436277\n",
            "training error 0.1250452726714236, test error 0.2508046982835632\n",
            "Loss: 0.33104861395178986\n",
            "training error 0.12491904845034249, test error 0.25085004752958706\n",
            "Loss: 0.34918997030821686\n",
            "training error 0.1251207373161333, test error 0.25079590720739287\n",
            "Loss: 0.3275318620860945\n",
            "training error 0.12495513719278249, test error 0.2509241034954011\n",
            "Loss: 0.37881506408472543\n",
            "training error 0.12512702114959953, test error 0.2508062149591466\n",
            "Loss: 0.3316553396341071\n",
            "training error 0.12503323762789822, test error 0.25100348330693206\n",
            "Loss: 0.41056989078527284\n",
            "training error 0.12498068194694748, test error 0.25093967006999257\n",
            "Loss: 0.38504226302786826\n",
            "training error 0.12508041591894123, test error 0.2507774622250669\n",
            "Loss: 0.32015319481593085\n",
            "training error 0.12491023459441505, test error 0.25088742258969127\n",
            "Loss: 0.36414136076432513\n",
            "training error 0.12496473235627314, test error 0.25095099903350154\n",
            "Loss: 0.38957426261299055\n",
            "training error 0.1250382726599947, test error 0.25096538860149314\n",
            "Loss: 0.395330615885392\n",
            "training error 0.12493252253899817, test error 0.25103340469254065\n",
            "Loss: 0.42253953894033636\n",
            "training error 0.12490275268341199, test error 0.2509273338182059\n",
            "Loss: 0.38010731130571074\n",
            "training error 0.12492903283069066, test error 0.250839899988608\n",
            "Loss: 0.345130582927311\n",
            "training error 0.12497923035159797, test error 0.2507902276043544\n",
            "Loss: 0.32525981322710784\n",
            "training error 0.1250206055731704, test error 0.25078467216670697\n",
            "Loss: 0.32303743506398863\n",
            "training error 0.12493812749711034, test error 0.2507326554891725\n",
            "Loss: 0.30222886234521074\n",
            "training error 0.12494194649771291, test error 0.2507280689435591\n",
            "Loss: 0.30039407641790916\n",
            "training error 0.12502382866709322, test error 0.2507521286266502\n",
            "Loss: 0.3100188292649353\n",
            "training error 0.12495907745784753, test error 0.25074407494127554\n",
            "Loss: 0.3067970606761561\n",
            "training error 0.12491556009867281, test error 0.2506997909258346\n",
            "Loss: 0.2890818354972291\n",
            "training error 0.12498091396932864, test error 0.25053449138074413\n",
            "Loss: 0.22295597418826585\n",
            "training error 0.12489429462322321, test error 0.2506307962974069\n",
            "Loss: 0.2614814617153405\n",
            "training error 0.12491415832463741, test error 0.2505892881318856\n",
            "Loss: 0.24487667798800405\n",
            "training error 0.12503241264126314, test error 0.2506825147092256\n",
            "Loss: 0.2821707172433685\n",
            "training error 0.12503254209479184, test error 0.25068498560533603\n",
            "Loss: 0.2831591660223287\n",
            "training error 0.12488737939464616, test error 0.2505449673901748\n",
            "Loss: 0.2271467609584521\n",
            "training error 0.12493484465257496, test error 0.25057084857897644\n",
            "Loss: 0.23750018268307027\n",
            "training error 0.12489975533812674, test error 0.25053838507783777\n",
            "Loss: 0.2245135953774291\n",
            "training error 0.1250151890032438, test error 0.2503836108464922\n",
            "Loss: 0.16259824436688497\n",
            "training error 0.12488469712920365, test error 0.25050139626919504\n",
            "Loss: 0.20971671962708438\n",
            "training error 0.12494579248479094, test error 0.25053467824764986\n",
            "Loss: 0.22303072778231403\n",
            "training error 0.12484876319556702, test error 0.2505881245726822\n",
            "Loss: 0.24441121176748126\n",
            "training error 0.1250047901065811, test error 0.250649486046235\n",
            "Loss: 0.268958044534906\n",
            "training error 0.12491390187809545, test error 0.2506395513557923\n",
            "Loss: 0.2649838051502762\n",
            "training error 0.12499188142220011, test error 0.2505597837952651\n",
            "Loss: 0.2330738646753039\n",
            "training error 0.1249189022515944, test error 0.25066509215516625\n",
            "Loss: 0.27520105865932454\n",
            "training error 0.12486565025758586, test error 0.25053106859591345\n",
            "Loss: 0.22158673512060467\n",
            "training error 0.12502625638701256, test error 0.25059348034517526\n",
            "Loss: 0.24655371656920888\n",
            "training error 0.12484972940380741, test error 0.2505450046422828\n",
            "Loss: 0.22716166316356645\n",
            "training error 0.12495553833886952, test error 0.25043570222543843\n",
            "Loss: 0.18343670038143411\n",
            "training error 0.12486519709093585, test error 0.25062960054000816\n",
            "Loss: 0.26100311503951534\n",
            "training error 0.12482193756601014, test error 0.25052223319184375\n",
            "Loss: 0.21805225047455412\n",
            "training error 0.12488144436955471, test error 0.2504907732843229\n",
            "Loss: 0.20546713730695831\n",
            "training error 0.12483344502672736, test error 0.2506455101683698\n",
            "Loss: 0.2673675480326043\n",
            "training error 0.12508362058573455, test error 0.2507259237561792\n",
            "Loss: 0.2995359230389072\n",
            "training error 0.1248388630252623, test error 0.2505921022871058\n",
            "Loss: 0.2460024429602914\n",
            "training error 0.1248351644980355, test error 0.2505991386795575\n",
            "Loss: 0.24881725718812753\n",
            "training error 0.12479656267699965, test error 0.25065862433413433\n",
            "Loss: 0.27261369378610567\n",
            "training error 0.12501149764624797, test error 0.25058959809086945\n",
            "Loss: 0.24500067291350902\n",
            "training error 0.12500214125410533, test error 0.2506797195985294\n",
            "Loss: 0.281052570776974\n",
            "training error 0.12482644777715912, test error 0.2505675048751728\n",
            "Loss: 0.23616257891738712\n",
            "training error 0.12480388425392064, test error 0.25070016344757634\n",
            "Loss: 0.2892308578131386\n",
            "training error 0.12518945671042275, test error 0.25087138420679744\n",
            "Loss: 0.3577254212511427\n",
            "training error 0.12478586485603432, test error 0.2506780506644936\n",
            "Loss: 0.2803849361472155\n",
            "training error 0.12490333221044982, test error 0.25061342174827045\n",
            "Loss: 0.25453100685550734\n",
            "training error 0.12476481378863044, test error 0.2505841260621541\n",
            "Loss: 0.24281166137258836\n",
            "training error 0.12476572085958647, test error 0.25066998886515496\n",
            "Loss: 0.2771599216762066\n",
            "training error 0.12479153854094387, test error 0.25066363762919114\n",
            "Loss: 0.274619195092507\n",
            "training error 0.12474760210545632, test error 0.2505353467736373\n",
            "Loss: 0.2232981626183017\n",
            "training error 0.12472882403485826, test error 0.2506173407400352\n",
            "Loss: 0.2560987468378739\n",
            "training error 0.12479340548110734, test error 0.2506315207240992\n",
            "Loss: 0.26177125887696384\n",
            "training error 0.12474845482573917, test error 0.2505901063498009\n",
            "Loss: 0.24520399506779977\n",
            "training error 0.12466925115731423, test error 0.2505695620763434\n",
            "Loss: 0.23698553459592375\n",
            "training error 0.12465189372238078, test error 0.25054578349359014\n",
            "Loss: 0.22747323216094628\n",
            "training error 0.12467702698108685, test error 0.25054131690958825\n",
            "Loss: 0.22568643526399423\n",
            "training error 0.12476536216223585, test error 0.2505950792412933\n",
            "Loss: 0.24719333347131833\n",
            "training error 0.12464227720038472, test error 0.25049243552512707\n",
            "Loss: 0.2061320943993472\n",
            "training error 0.12459185928269521, test error 0.2503338352324515\n",
            "Loss: 0.14268617897783553\n",
            "training error 0.12463642288654742, test error 0.25031354423385793\n",
            "Loss: 0.13456903771114082\n",
            "training error 0.12487735768805759, test error 0.2501530830818523\n",
            "Loss: 0.07037871052546496\n",
            "training error 0.12455114152352513, test error 0.25024452244851714\n",
            "Loss: 0.10695780017115908\n",
            "training error 0.12463821315462068, test error 0.25016836924162783\n",
            "Loss: 0.07649373329039477\n",
            "training error 0.12460708784263688, test error 0.25017886717000554\n",
            "Loss: 0.08069328844075851\n",
            "training error 0.1245132364567887, test error 0.250173320915187\n",
            "Loss: 0.07847458374490301\n",
            "training error 0.12456105110467386, test error 0.25007595657209886\n",
            "Loss: 0.03952528691555379\n",
            "training error 0.1246122310304714, test error 0.2499485982392748\n",
            "Loss: 0.0\n",
            "training error 0.12452888838884313, test error 0.25007152734757604\n",
            "Loss: 0.049181755435800945\n",
            "training error 0.12445222280818649, test error 0.2499660352401659\n",
            "Loss: 0.006976234719435759\n",
            "training error 0.12447309606773999, test error 0.24983625376650073\n",
            "Loss: 0.0\n",
            "training error 0.12439301974827156, test error 0.24983535731941728\n",
            "Loss: 0.0\n",
            "training error 0.12438074362027461, test error 0.24973792461118874\n",
            "Loss: 0.0\n",
            "training error 0.12434583405860855, test error 0.24973316807590676\n",
            "Loss: 0.0\n",
            "training error 0.12436613648544247, test error 0.24972438313362444\n",
            "Loss: 0.0\n",
            "training error 0.12447707733708424, test error 0.24959381478837525\n",
            "Loss: 0.0\n",
            "training error 0.12442788018286506, test error 0.24966512283052764\n",
            "Loss: 0.028569635114084058\n",
            "training error 0.12429810932892522, test error 0.24971074890928968\n",
            "Loss: 0.046849767096013295\n",
            "training error 0.12419630193041542, test error 0.24962392927021806\n",
            "Loss: 0.012065395878635421\n",
            "training error 0.12421319582112635, test error 0.24958182463903303\n",
            "Loss: 0.0\n",
            "training error 0.12414989415596094, test error 0.2494741023300219\n",
            "Loss: 0.0\n",
            "training error 0.12409262155388251, test error 0.2494768834401276\n",
            "Loss: 0.0011147891022522671\n",
            "training error 0.12425467903060769, test error 0.24936754840431827\n",
            "Loss: 0.0\n",
            "training error 0.12402265730763098, test error 0.249469952819725\n",
            "Loss: 0.04106565431709619\n",
            "training error 0.12398732843022883, test error 0.24939479264715675\n",
            "Loss: 0.010925336120437557\n",
            "training error 0.12404530946631583, test error 0.24917648873241297\n",
            "Loss: 0.0\n",
            "training error 0.12394213321116274, test error 0.24928956864450966\n",
            "Loss: 0.045381453391502014\n",
            "training error 0.12391068756373669, test error 0.24916482416432964\n",
            "Loss: 0.0\n",
            "training error 0.1237991534226073, test error 0.24887425760707438\n",
            "Loss: 0.0\n",
            "training error 0.12407550317134673, test error 0.24899303719337612\n",
            "Loss: 0.04772674660842302\n",
            "training error 0.12366516137119696, test error 0.24869277872307347\n",
            "Loss: 0.0\n",
            "training error 0.12382576463253464, test error 0.24854915783794554\n",
            "Loss: 0.0\n",
            "training error 0.12357742887571235, test error 0.24829762961774274\n",
            "Loss: 0.0\n",
            "training error 0.12351119156811502, test error 0.24822031152414567\n",
            "Loss: 0.0\n",
            "training error 0.12361996456511994, test error 0.2480422709213404\n",
            "Loss: 0.0\n",
            "training error 0.12340594874965113, test error 0.2480769751627269\n",
            "Loss: 0.013991260948209572\n",
            "training error 0.12334777637897937, test error 0.24790366168803962\n",
            "Loss: 0.0\n",
            "training error 0.12325722215237299, test error 0.24776689742684477\n",
            "Loss: 0.0\n",
            "training error 0.12323876847661325, test error 0.2477299998309234\n",
            "Loss: 0.0\n",
            "training error 0.12314794217225317, test error 0.2474928787293041\n",
            "Loss: 0.0\n",
            "training error 0.123115716720165, test error 0.24736090303680866\n",
            "Loss: 0.0\n",
            "training error 0.12299484531786391, test error 0.2471833627387768\n",
            "Loss: 0.0\n",
            "training error 0.12281787795689628, test error 0.24704538441028806\n",
            "Loss: 0.0\n",
            "training error 0.12275938994390387, test error 0.2467695532639899\n",
            "Loss: 0.0\n",
            "training error 0.12263421105929806, test error 0.24663418501947748\n",
            "Loss: 0.0\n",
            "training error 0.1226149455743126, test error 0.24635912156868806\n",
            "Loss: 0.0\n",
            "training error 0.12243024993238905, test error 0.24626543354818203\n",
            "Loss: 0.0\n",
            "training error 0.12243890385734355, test error 0.24615208500520147\n",
            "Loss: 0.0\n",
            "training error 0.12222938500925629, test error 0.24580515204667636\n",
            "Loss: 0.0\n",
            "training error 0.12210943035210065, test error 0.2454484847772012\n",
            "Loss: 0.0\n",
            "training error 0.12193201419482086, test error 0.2451930997480165\n",
            "Loss: 0.0\n",
            "training error 0.12181199066040004, test error 0.24482308811788692\n",
            "Loss: 0.0\n",
            "training error 0.1216936380138262, test error 0.24443479272914678\n",
            "Loss: 0.0\n",
            "training error 0.12153500080479562, test error 0.24420689899832979\n",
            "Loss: 0.0\n",
            "training error 0.12139700418502361, test error 0.2439068130582376\n",
            "Loss: 0.0\n",
            "training error 0.12127826057427042, test error 0.2434799960764934\n",
            "Loss: 0.0\n",
            "training error 0.12119840264942625, test error 0.24321996492491532\n",
            "Loss: 0.0\n",
            "training error 0.12094957803638284, test error 0.24289194787649948\n",
            "Loss: 0.0\n",
            "training error 0.12071708636866534, test error 0.24249402322925373\n",
            "Loss: 0.0\n",
            "training error 0.12056985497558288, test error 0.24213238872681403\n",
            "Loss: 0.0\n",
            "training error 0.12046395464423436, test error 0.2417113929282376\n",
            "Loss: 0.0\n",
            "training error 0.12016629231817542, test error 0.2412507371717891\n",
            "Loss: 0.0\n",
            "training error 0.11990788429347722, test error 0.24075548645974523\n",
            "Loss: 0.0\n",
            "training error 0.11976764865322444, test error 0.2402788160120742\n",
            "Loss: 0.0\n",
            "training error 0.11963399187959574, test error 0.2400201343588626\n",
            "Loss: 0.0\n",
            "training error 0.11928052092053161, test error 0.23950383201809872\n",
            "Loss: 0.0\n",
            "training error 0.11919276974243466, test error 0.2390384031109071\n",
            "Loss: 0.0\n",
            "training error 0.11887832570802233, test error 0.23853091760907114\n",
            "Loss: 0.0\n",
            "training error 0.11846082621699078, test error 0.2378803944675088\n",
            "Loss: 0.0\n",
            "training error 0.11821391283885732, test error 0.23722248392416062\n",
            "Loss: 0.0\n",
            "training error 0.11792601737388855, test error 0.23661659286255646\n",
            "Loss: 0.0\n",
            "training error 0.11763609412501631, test error 0.23603017210478827\n",
            "Loss: 0.0\n",
            "training error 0.11733420089063598, test error 0.23530958565313276\n",
            "Loss: 0.0\n",
            "training error 0.11704559823283142, test error 0.2347269798887866\n",
            "Loss: 0.0\n",
            "training error 0.11665945342995579, test error 0.23414906542943312\n",
            "Loss: 0.0\n",
            "training error 0.11651618366572666, test error 0.23348287084976962\n",
            "Loss: 0.0\n",
            "training error 0.11599541270341102, test error 0.23271850304029054\n",
            "Loss: 0.0\n",
            "training error 0.11558162770117453, test error 0.23187816279875897\n",
            "Loss: 0.0\n",
            "training error 0.11525067504062605, test error 0.23114584863713983\n",
            "Loss: 0.0\n",
            "training error 0.11485200829999234, test error 0.2302610004985395\n",
            "Loss: 0.0\n",
            "training error 0.11457345861511371, test error 0.2293563564706394\n",
            "Loss: 0.0\n",
            "training error 0.11404361588779084, test error 0.22841552536661222\n",
            "Loss: 0.0\n",
            "training error 0.11358469564661464, test error 0.2276149688960753\n",
            "Loss: 0.0\n",
            "training error 0.11308483868319025, test error 0.2266035020453632\n",
            "Loss: 0.0\n",
            "training error 0.1126750911100073, test error 0.22549079389595597\n",
            "Loss: 0.0\n",
            "training error 0.11225654078247041, test error 0.22471263345255515\n",
            "Loss: 0.0\n",
            "training error 0.11161336578286855, test error 0.22363463434041717\n",
            "Loss: 0.0\n",
            "training error 0.11133082323496073, test error 0.22238737720296914\n",
            "Loss: 0.0\n",
            "training error 0.11075211645903162, test error 0.2213443956322275\n",
            "Loss: 0.0\n",
            "training error 0.11012072769649649, test error 0.22029348277601818\n",
            "Loss: 0.0\n",
            "training error 0.10953260772317502, test error 0.21905412669900348\n",
            "Loss: 0.0\n",
            "training error 0.10897911711403538, test error 0.21776972952347573\n",
            "Loss: 0.0\n",
            "training error 0.10836567995980645, test error 0.21660409345513323\n",
            "Loss: 0.0\n",
            "training error 0.10778922327610022, test error 0.21534341150796324\n",
            "Loss: 0.0\n",
            "training error 0.10713395065459087, test error 0.2142366590181462\n",
            "Loss: 0.0\n",
            "training error 0.1066199631671685, test error 0.2128080756504066\n",
            "Loss: 0.0\n",
            "training error 0.10592882332375239, test error 0.2113425707672924\n",
            "Loss: 0.0\n",
            "training error 0.10518084801419503, test error 0.2099889744993507\n",
            "Loss: 0.0\n",
            "training error 0.10452052802456353, test error 0.2085322474788588\n",
            "Loss: 0.0\n",
            "training error 0.10384000648197668, test error 0.20700373443048642\n",
            "Loss: 0.0\n",
            "training error 0.10313993990249114, test error 0.20575127533414195\n",
            "Loss: 0.0\n",
            "training error 0.10240172127896248, test error 0.20420011225751158\n",
            "Loss: 0.0\n",
            "training error 0.10176493996582535, test error 0.20283099876032146\n",
            "Loss: 0.0\n",
            "training error 0.10097802109824558, test error 0.20117384664911048\n",
            "Loss: 0.0\n",
            "training error 0.10021755126664729, test error 0.19968678995130137\n",
            "Loss: 0.0\n",
            "training error 0.09951170206914636, test error 0.1981235714589164\n",
            "Loss: 0.0\n",
            "training error 0.09867397674976992, test error 0.19642481662392114\n",
            "Loss: 0.0\n",
            "training error 0.09796401143402066, test error 0.19493300311850983\n",
            "Loss: 0.0\n",
            "training error 0.09715496235078554, test error 0.19317875762675865\n",
            "Loss: 0.0\n",
            "training error 0.09647136692328932, test error 0.19151791643433388\n",
            "Loss: 0.0\n",
            "training error 0.09555836398669264, test error 0.18997794356104492\n",
            "Loss: 0.0\n",
            "training error 0.09475135519550244, test error 0.18827246525039276\n",
            "Loss: 0.0\n",
            "training error 0.09396227092622794, test error 0.18655365479807987\n",
            "Loss: 0.0\n",
            "training error 0.09327868384843117, test error 0.18476357681000652\n",
            "Loss: 0.0\n",
            "training error 0.0923497407236094, test error 0.18304708391086078\n",
            "Loss: 0.0\n",
            "training error 0.09163340170480261, test error 0.18137076946881842\n",
            "Loss: 0.0\n",
            "training error 0.09073648229505811, test error 0.17952482948862208\n",
            "Loss: 0.0\n",
            "training error 0.08985648965235721, test error 0.17786238835232646\n",
            "Loss: 0.0\n",
            "training error 0.08907583496780266, test error 0.17628872038621904\n",
            "Loss: 0.0\n",
            "training error 0.08826011580965235, test error 0.1743445044516716\n",
            "Loss: 0.0\n",
            "training error 0.08745781562169762, test error 0.1725746369055488\n",
            "Loss: 0.0\n",
            "training error 0.08659227669609662, test error 0.1708958921421658\n",
            "Loss: 0.0\n",
            "training error 0.08577151974100553, test error 0.16916671710601985\n",
            "Loss: 0.0\n",
            "training error 0.08494707288527467, test error 0.16730511339337856\n",
            "Loss: 0.0\n",
            "training error 0.08409829659298904, test error 0.1656545396444701\n",
            "Loss: 0.0\n",
            "training error 0.08328330071096607, test error 0.16387504484945709\n",
            "Loss: 0.0\n",
            "training error 0.08250322604431255, test error 0.16212173751055325\n",
            "Loss: 0.0\n",
            "training error 0.08166417349296115, test error 0.160440213501267\n",
            "Loss: 0.0\n",
            "training error 0.08092651138707209, test error 0.15875759358074593\n",
            "Loss: 0.0\n",
            "training error 0.08009713349849966, test error 0.15707470055417838\n",
            "Loss: 0.0\n",
            "training error 0.07932548626866355, test error 0.15543856104717377\n",
            "Loss: 0.0\n",
            "training error 0.07856189288853499, test error 0.1537682111083959\n",
            "Loss: 0.0\n",
            "training error 0.0778183763023866, test error 0.15208624817733918\n",
            "Loss: 0.0\n",
            "training error 0.07699811566587444, test error 0.15057759171474688\n",
            "Loss: 0.0\n",
            "training error 0.07622094927150987, test error 0.14887549615368537\n",
            "Loss: 0.0\n",
            "training error 0.07558547321531137, test error 0.14723140739246507\n",
            "Loss: 0.0\n",
            "training error 0.07479630802622883, test error 0.14562725197973253\n",
            "Loss: 0.0\n",
            "training error 0.07401989480769479, test error 0.1440165519600934\n",
            "Loss: 0.0\n",
            "training error 0.07335869480759744, test error 0.14231750383163574\n",
            "Loss: 0.0\n",
            "training error 0.07257954976444467, test error 0.1408724857897348\n",
            "Loss: 0.0\n",
            "training error 0.07190114332420666, test error 0.1392898401992492\n",
            "Loss: 0.0\n",
            "training error 0.0712088646844969, test error 0.1377330666003428\n",
            "Loss: 0.0\n",
            "training error 0.07047862673621517, test error 0.13616605680734828\n",
            "Loss: 0.0\n",
            "training error 0.06982497854310546, test error 0.13473261532458478\n",
            "Loss: 0.0\n",
            "training error 0.06912683121540275, test error 0.1332685208902191\n",
            "Loss: 0.0\n",
            "training error 0.06842737569904829, test error 0.13173566164165007\n",
            "Loss: 0.0\n",
            "training error 0.0679003312744632, test error 0.13021507974312077\n",
            "Loss: 0.0\n",
            "training error 0.06718744591454655, test error 0.12910886913112432\n",
            "Loss: 0.0\n",
            "training error 0.06653466605315755, test error 0.12771769085622875\n",
            "Loss: 0.0\n",
            "training error 0.06592619626729264, test error 0.12617493068944016\n",
            "Loss: 0.0\n",
            "training error 0.06531457320647495, test error 0.12488518763673567\n",
            "Loss: 0.0\n",
            "training error 0.0647426291241571, test error 0.12362067927089464\n",
            "Loss: 0.0\n",
            "training error 0.06414870028210176, test error 0.12222215431161151\n",
            "Loss: 0.0\n",
            "training error 0.0636378028647337, test error 0.12099418365451696\n",
            "Loss: 0.0\n",
            "training error 0.06298897599432908, test error 0.11963397511069093\n",
            "Loss: 0.0\n",
            "training error 0.06243850985259235, test error 0.11836178399263078\n",
            "Loss: 0.0\n",
            "training error 0.061975657626852365, test error 0.11702485968892341\n",
            "Loss: 0.0\n",
            "training error 0.061447774404991475, test error 0.1159788388376248\n",
            "Loss: 0.0\n",
            "training error 0.060816937785858984, test error 0.1147779798595603\n",
            "Loss: 0.0\n",
            "training error 0.06035891581353285, test error 0.11366898058595655\n",
            "Loss: 0.0\n",
            "training error 0.05983043209597349, test error 0.11240311927886802\n",
            "Loss: 0.0\n",
            "training error 0.059339756136671654, test error 0.11135019489978325\n",
            "Loss: 0.0\n",
            "training error 0.058836030133869566, test error 0.11009283324968584\n",
            "Loss: 0.0\n",
            "training error 0.0583422208350588, test error 0.1090544508072294\n",
            "Loss: 0.0\n",
            "training error 0.058083914612018984, test error 0.10816355451373615\n",
            "Loss: 0.0\n",
            "training error 0.057426871888515245, test error 0.10694842245223436\n",
            "Loss: 0.0\n",
            "training error 0.05697927934876524, test error 0.10603910502733312\n",
            "Loss: 0.0\n",
            "training error 0.05659773583413677, test error 0.10507586746205393\n",
            "Loss: 0.0\n",
            "training error 0.05609579019712977, test error 0.10389777425760578\n",
            "Loss: 0.0\n",
            "training error 0.05571523124347835, test error 0.10306806592721998\n",
            "Loss: 0.0\n",
            "training error 0.05526394680951885, test error 0.10203410214434666\n",
            "Loss: 0.0\n",
            "training error 0.054869675790562344, test error 0.1011130426544783\n",
            "Loss: 0.0\n",
            "training error 0.05451178693675964, test error 0.10028956612647842\n",
            "Loss: 0.0\n",
            "training error 0.05406047358400543, test error 0.09950787758794045\n",
            "Loss: 0.0\n",
            "training error 0.053703295114525915, test error 0.09858238550809753\n",
            "Loss: 0.0\n",
            "training error 0.05327338313290136, test error 0.09777735643166753\n",
            "Loss: 0.0\n",
            "training error 0.05288745625988378, test error 0.0969790102154784\n",
            "Loss: 0.0\n",
            "training error 0.05254774985419016, test error 0.09599450028263488\n",
            "Loss: 0.0\n",
            "training error 0.05217151255159232, test error 0.0951406830039641\n",
            "Loss: 0.0\n",
            "training error 0.05189119440209598, test error 0.09423625858433382\n",
            "Loss: 0.0\n",
            "training error 0.051514135075957056, test error 0.09360827674846195\n",
            "Loss: 0.0\n",
            "training error 0.05125139703014389, test error 0.09288011748959817\n",
            "Loss: 0.0\n",
            "training error 0.050863407975289494, test error 0.09218551607985037\n",
            "Loss: 0.0\n",
            "training error 0.05056547194812343, test error 0.09149193688297765\n",
            "Loss: 0.0\n",
            "training error 0.050301285490700495, test error 0.09062159723996402\n",
            "Loss: 0.0\n",
            "training error 0.04994984045811208, test error 0.09001335112941812\n",
            "Loss: 0.0\n",
            "training error 0.04962160859235035, test error 0.08928269919005927\n",
            "Loss: 0.0\n",
            "training error 0.0493476996775615, test error 0.08871289847476675\n",
            "Loss: 0.0\n",
            "training error 0.04908349994337969, test error 0.08806280181368493\n",
            "Loss: 0.0\n",
            "training error 0.04878931473517406, test error 0.0874535144151855\n",
            "Loss: 0.0\n",
            "training error 0.04858193303803755, test error 0.08685763886382077\n",
            "Loss: 0.0\n",
            "training error 0.04830801676233177, test error 0.0862915834750326\n",
            "Loss: 0.0\n",
            "training error 0.04802888880824698, test error 0.08564649275435603\n",
            "Loss: 0.0\n",
            "training error 0.047757458140368574, test error 0.08489544564443946\n",
            "Loss: 0.0\n",
            "training error 0.04752821224719605, test error 0.08439531591825887\n",
            "Loss: 0.0\n",
            "training error 0.04726443179370886, test error 0.08379032914255506\n",
            "Loss: 0.0\n",
            "training error 0.04707568074449263, test error 0.08297829268774057\n",
            "Loss: 0.0\n",
            "training error 0.04677027505145103, test error 0.08255893134762236\n",
            "Loss: 0.0\n",
            "training error 0.04661196925695446, test error 0.08208972128844141\n",
            "Loss: 0.0\n",
            "training error 0.046322587424012907, test error 0.08137703218277782\n",
            "Loss: 0.0\n",
            "training error 0.04614399278397951, test error 0.08075579651345546\n",
            "Loss: 0.0\n",
            "training error 0.045909473890437195, test error 0.08020795604939057\n",
            "Loss: 0.0\n",
            "training error 0.045692946054558876, test error 0.07974233816361039\n",
            "Loss: 0.0\n",
            "training error 0.04551398886300939, test error 0.07918812848282762\n",
            "Loss: 0.0\n",
            "training error 0.04529866212379066, test error 0.07892754944528159\n",
            "Loss: 0.0\n",
            "training error 0.04511929728565731, test error 0.0784483607313481\n",
            "Loss: 0.0\n",
            "training error 0.04489819963812783, test error 0.07811547633115666\n",
            "Loss: 0.0\n",
            "training error 0.04472682800439819, test error 0.07759162027157819\n",
            "Loss: 0.0\n",
            "training error 0.04456040116882808, test error 0.0772342332893986\n",
            "Loss: 0.0\n",
            "training error 0.044456722846038965, test error 0.07672257401257125\n",
            "Loss: 0.0\n",
            "training error 0.04420244218324232, test error 0.07645064734632713\n",
            "Loss: 0.0\n",
            "training error 0.04402199144676499, test error 0.07591538889918502\n",
            "Loss: 0.0\n",
            "training error 0.043876006138398635, test error 0.07548617578509324\n",
            "Loss: 0.0\n",
            "training error 0.0437320308619783, test error 0.07521486532523503\n",
            "Loss: 0.0\n",
            "training error 0.043603193565753436, test error 0.07463205105252944\n",
            "Loss: 0.0\n",
            "training error 0.04343289461193815, test error 0.07426172752476769\n",
            "Loss: 0.0\n",
            "training error 0.04328045342004529, test error 0.07415564969948968\n",
            "Loss: 0.0\n",
            "training error 0.043105949141021296, test error 0.07358594191694084\n",
            "Loss: 0.0\n",
            "training error 0.042912318767939356, test error 0.07328561651846718\n",
            "Loss: 0.0\n",
            "training error 0.04285736756838089, test error 0.07281748301715174\n",
            "Loss: 0.0\n",
            "training error 0.042612692725532926, test error 0.07266853901813955\n",
            "Loss: 0.0\n",
            "training error 0.04250296000378126, test error 0.07224360118297445\n",
            "Loss: 0.0\n",
            "training error 0.04244527609337842, test error 0.07188995177740114\n",
            "Loss: 0.0\n",
            "training error 0.0422560768669871, test error 0.07180786268220102\n",
            "Loss: 0.0\n",
            "training error 0.04212139740480069, test error 0.07118728979248261\n",
            "Loss: 0.0\n",
            "training error 0.041982117972064595, test error 0.07103136883395603\n",
            "Loss: 0.0\n",
            "training error 0.04183953603768027, test error 0.07063194214480446\n",
            "Loss: 0.0\n",
            "training error 0.04174630552899637, test error 0.07028321024785766\n",
            "Loss: 0.0\n",
            "training error 0.04161073389892809, test error 0.07000563615120352\n",
            "Loss: 0.0\n",
            "training error 0.041474345552426795, test error 0.0697212870462981\n",
            "Loss: 0.0\n",
            "training error 0.041367576136154646, test error 0.06932370548582034\n",
            "Loss: 0.0\n",
            "training error 0.041245751822817804, test error 0.0689696072838728\n",
            "Loss: 0.0\n",
            "training error 0.04111454763184487, test error 0.0687528050546085\n",
            "Loss: 0.0\n",
            "training error 0.04102399394891541, test error 0.06847232376582785\n",
            "Loss: 0.0\n",
            "training error 0.040924141552051825, test error 0.06823207152731041\n",
            "Loss: 0.0\n",
            "training error 0.0408179281333607, test error 0.06809647011281779\n",
            "Loss: 0.0\n",
            "training error 0.04069402718827313, test error 0.0678279453022273\n",
            "Loss: 0.0\n",
            "training error 0.04062410548233282, test error 0.06758938051569613\n",
            "Loss: 0.0\n",
            "training error 0.04052992336779024, test error 0.06744605622016661\n",
            "Loss: 0.0\n",
            "training error 0.04040752337256911, test error 0.06713384493694263\n",
            "Loss: 0.0\n",
            "training error 0.04033955011304747, test error 0.06688054148201404\n",
            "Loss: 0.0\n",
            "training error 0.04024359068505693, test error 0.06661828038302793\n",
            "Loss: 0.0\n",
            "training error 0.04015494339876511, test error 0.06652575132908996\n",
            "Loss: 0.0\n",
            "training error 0.040097418653162104, test error 0.06618661972146442\n",
            "Loss: 0.0\n",
            "training error 0.039967261416679244, test error 0.06609528639068482\n",
            "Loss: 0.0\n",
            "training error 0.03990111305099296, test error 0.06580551646965765\n",
            "Loss: 0.0\n",
            "training error 0.03979678086175916, test error 0.06565436384535686\n",
            "Loss: 0.0\n",
            "training error 0.03972314363825645, test error 0.06541258754915981\n",
            "Loss: 0.0\n",
            "training error 0.03961402121509168, test error 0.06504303386362303\n",
            "Loss: 0.0\n",
            "training error 0.03954748678637098, test error 0.06478058691273984\n",
            "Loss: 0.0\n",
            "training error 0.03943965947377391, test error 0.06470245636257153\n",
            "Loss: 0.0\n",
            "training error 0.03938461586632601, test error 0.06459488865938634\n",
            "Loss: 0.0\n",
            "training error 0.03936673312097478, test error 0.06448317337078494\n",
            "Loss: 0.0\n",
            "training error 0.03924538125376803, test error 0.06422771841675513\n",
            "Loss: 0.0\n",
            "training error 0.0391836437550793, test error 0.06411506674769142\n",
            "Loss: 0.0\n",
            "training error 0.039093246732844616, test error 0.0639787000319457\n",
            "Loss: 0.0\n",
            "training error 0.039016170128415406, test error 0.06382906050577596\n",
            "Loss: 0.0\n",
            "training error 0.03904245987728098, test error 0.06358940543681982\n",
            "Loss: 0.0\n",
            "training error 0.03889488888809429, test error 0.06342441662804911\n",
            "Loss: 0.0\n",
            "training error 0.03885664909786617, test error 0.06335975424479554\n",
            "Loss: 0.0\n",
            "training error 0.038810302704570955, test error 0.06332147790392374\n",
            "Loss: 0.0\n",
            "training error 0.03868905228870776, test error 0.06316856837636775\n",
            "Loss: 0.0\n",
            "training error 0.038654194251791396, test error 0.06307725912602258\n",
            "Loss: 0.0\n",
            "training error 0.038737868924678044, test error 0.06310422312473384\n",
            "Loss: 0.04274757509261651\n",
            "training error 0.03860416397393708, test error 0.06261087213544393\n",
            "Loss: 0.0\n",
            "training error 0.038495902472053305, test error 0.06274001550667307\n",
            "Loss: 0.2062634919854922\n",
            "training error 0.03843219493706661, test error 0.0626816174520242\n",
            "Loss: 0.11299206378603266\n",
            "training error 0.03833693454707037, test error 0.0624739531670161\n",
            "Loss: 0.0\n",
            "training error 0.03826069613906867, test error 0.06230853048780666\n",
            "Loss: 0.0\n",
            "training error 0.03822222706392639, test error 0.062193208891436966\n",
            "Loss: 0.0\n",
            "training error 0.03818080113206178, test error 0.06201288854590842\n",
            "Loss: 0.0\n",
            "training error 0.03813648489215626, test error 0.06183354535165829\n",
            "Loss: 0.0\n",
            "training error 0.03809667565295762, test error 0.06166632268759721\n",
            "Loss: 0.0\n",
            "training error 0.03800481232896491, test error 0.061420838011394946\n",
            "Loss: 0.0\n",
            "training error 0.037988641278214885, test error 0.06147551869762782\n",
            "Loss: 0.08902627838247579\n",
            "training error 0.03788030763895931, test error 0.06121771844712001\n",
            "Loss: 0.0\n",
            "training error 0.03782925064241818, test error 0.06113981742522083\n",
            "Loss: 0.0\n",
            "training error 0.03783354967357745, test error 0.061012406126372765\n",
            "Loss: 0.0\n",
            "training error 0.03777620326412554, test error 0.060955100547903235\n",
            "Loss: 0.0\n",
            "training error 0.037695413980782355, test error 0.06087021411342526\n",
            "Loss: 0.0\n",
            "training error 0.03766385273827922, test error 0.06088130705673563\n",
            "Loss: 0.018223926877758423\n",
            "training error 0.03765317254123068, test error 0.060830757117718987\n",
            "Loss: 0.0\n",
            "training error 0.03755619297516384, test error 0.06075395787425705\n",
            "Loss: 0.0\n",
            "training error 0.037542113094251255, test error 0.06056903395237934\n",
            "Loss: 0.0\n",
            "training error 0.037538782258668975, test error 0.06057575628606252\n",
            "Loss: 0.011098631172590245\n",
            "training error 0.037476108281347315, test error 0.06040027358534477\n",
            "Loss: 0.0\n",
            "training error 0.037396924704414594, test error 0.06049507056629222\n",
            "Loss: 0.15694793304785737\n",
            "training error 0.03737797558900942, test error 0.060291498684207384\n",
            "Loss: 0.0\n",
            "training error 0.0373182052384953, test error 0.06020568487374939\n",
            "Loss: 0.0\n",
            "training error 0.03732036855198636, test error 0.06008288196718358\n",
            "Loss: 0.0\n",
            "training error 0.0372895708203468, test error 0.05987423323073625\n",
            "Loss: 0.0\n",
            "training error 0.03723968495241447, test error 0.059758402379179926\n",
            "Loss: 0.0\n",
            "training error 0.03718985448957723, test error 0.05975034947691137\n",
            "Loss: 0.0\n",
            "training error 0.03714021100774355, test error 0.05976850574212445\n",
            "Loss: 0.03038687701750753\n",
            "training error 0.03714424340879071, test error 0.059581276122895856\n",
            "Loss: 0.0\n",
            "training error 0.03720876263244156, test error 0.05960081552767011\n",
            "Loss: 0.03279453889835082\n",
            "training error 0.03704744081602771, test error 0.05943769727718744\n",
            "Loss: 0.0\n",
            "training error 0.03704677773905578, test error 0.05951849427816646\n",
            "Loss: 0.13593561776497065\n",
            "training error 0.03701158816912961, test error 0.05939274144147934\n",
            "Loss: 0.0\n",
            "training error 0.036946055639546546, test error 0.05922257607369123\n",
            "Loss: 0.0\n",
            "training error 0.03692738383908918, test error 0.05908912281636658\n",
            "Loss: 0.0\n",
            "training error 0.03687032428069662, test error 0.059101706550000466\n",
            "Loss: 0.02129619299475749\n",
            "training error 0.036840116824613044, test error 0.059013932632963985\n",
            "Loss: 0.0\n",
            "training error 0.03683393823112183, test error 0.05898320933993239\n",
            "Loss: 0.0\n",
            "training error 0.036802177978240175, test error 0.05876223117424576\n",
            "Loss: 0.0\n",
            "training error 0.036789016893912, test error 0.05882731865697756\n",
            "Loss: 0.1107641446404406\n",
            "training error 0.03677806045797618, test error 0.0588132415482299\n",
            "Loss: 0.08680809588881289\n",
            "training error 0.036713720105250126, test error 0.05853438069177417\n",
            "Loss: 0.0\n",
            "training error 0.03670597947744228, test error 0.058439633463735756\n",
            "Loss: 0.0\n",
            "training error 0.03666425818373742, test error 0.058396213051051055\n",
            "Loss: 0.0\n",
            "training error 0.03661421439658708, test error 0.0582780220686298\n",
            "Loss: 0.0\n",
            "training error 0.03661168319658035, test error 0.05826785389355509\n",
            "Loss: 0.0\n",
            "training error 0.03658124763164316, test error 0.05806287867606859\n",
            "Loss: 0.0\n",
            "training error 0.03656624424523386, test error 0.05813734183168002\n",
            "Loss: 0.1282457179342833\n",
            "training error 0.03653140720232302, test error 0.05814936043105631\n",
            "Loss: 0.14894500059186022\n",
            "training error 0.03648055399833012, test error 0.058069160545191296\n",
            "Loss: 0.010819079704527113\n",
            "training error 0.03646925936099654, test error 0.058104714689603494\n",
            "Loss: 0.07205294413372698\n",
            "training error 0.03645737794313773, test error 0.05813555562324632\n",
            "Loss: 0.12516938332183258\n",
            "training error 0.036403775365942435, test error 0.05809321417623359\n",
            "Loss: 0.052245945872297916\n",
            "training error 0.03639248919263033, test error 0.058030682093146164\n",
            "Loss: 0.0\n",
            "training error 0.036408058699586174, test error 0.05801814990229143\n",
            "Loss: 0.0\n",
            "training error 0.03637121135632552, test error 0.057908000522751135\n",
            "Loss: 0.0\n",
            "training error 0.0363490017080295, test error 0.05816267046806622\n",
            "Loss: 0.4397836965809754\n",
            "training error 0.03631719429741606, test error 0.05805930704671795\n",
            "Loss: 0.2612877712940165\n",
            "training error 0.03628726107858653, test error 0.057827256010099255\n",
            "Loss: 0.0\n",
            "training error 0.036273887931805714, test error 0.057916353194362606\n",
            "Loss: 0.15407472256299126\n",
            "training error 0.03626085311531495, test error 0.05792001381507553\n",
            "Loss: 0.1604049913073391\n",
            "training error 0.03621414549552035, test error 0.057640403089261535\n",
            "Loss: 0.0\n",
            "training error 0.03619404995138185, test error 0.05763309245262503\n",
            "Loss: 0.0\n",
            "training error 0.036168773530434066, test error 0.05750676345214696\n",
            "Loss: 0.0\n",
            "training error 0.03613563840302663, test error 0.05752851334429692\n",
            "Loss: 0.03782145063346842\n",
            "training error 0.03613869636017327, test error 0.05754834122186201\n",
            "Loss: 0.07230066033823146\n",
            "training error 0.036122777110881944, test error 0.05739414241344667\n",
            "Loss: 0.0\n",
            "training error 0.036109680922246805, test error 0.057364315948305836\n",
            "Loss: 0.0\n",
            "training error 0.036065869936513756, test error 0.05721375242654733\n",
            "Loss: 0.0\n",
            "training error 0.03605394857855548, test error 0.057094408528074035\n",
            "Loss: 0.0\n",
            "training error 0.03604968200157659, test error 0.05711570572221581\n",
            "Loss: 0.03730171603635313\n",
            "training error 0.03600471915720364, test error 0.057031215605851907\n",
            "Loss: 0.0\n",
            "training error 0.03609773173255427, test error 0.05699774685576489\n",
            "Loss: 0.0\n",
            "training error 0.03598349694155164, test error 0.057086882056742755\n",
            "Loss: 0.15638372724351424\n",
            "training error 0.03595025334005003, test error 0.05696970231463042\n",
            "Loss: 0.0\n",
            "training error 0.03594965213877392, test error 0.05704416180650254\n",
            "Loss: 0.1307001596408064\n",
            "training error 0.03596479536026247, test error 0.05691028094826564\n",
            "Loss: 0.0\n",
            "training error 0.03593195177143351, test error 0.056968821669611626\n",
            "Loss: 0.1028649312049712\n",
            "training error 0.035885653051456426, test error 0.056807338652006815\n",
            "Loss: 0.0\n",
            "training error 0.03589094339177804, test error 0.05685997953298274\n",
            "Loss: 0.09266563480185841\n",
            "training error 0.03588504567936762, test error 0.056768291911152555\n",
            "Loss: 0.0\n",
            "training error 0.03587598981542623, test error 0.0567530063382433\n",
            "Loss: 0.0\n",
            "training error 0.035821660853722494, test error 0.05666520730995102\n",
            "Loss: 0.0\n",
            "training error 0.035803789195561334, test error 0.056561530011707245\n",
            "Loss: 0.0\n",
            "training error 0.03582324760611016, test error 0.05644340876841291\n",
            "Loss: 0.0\n",
            "training error 0.0357782688316597, test error 0.05655513561234849\n",
            "Loss: 0.19794489095086298\n",
            "training error 0.035775404404154866, test error 0.05661069018998619\n",
            "Loss: 0.2963701612346492\n",
            "training error 0.03577454642350746, test error 0.05663144631983684\n",
            "Loss: 0.3331435069689803\n",
            "training error 0.03578478275202452, test error 0.056582220709357214\n",
            "Loss: 0.2459311795179664\n",
            "training error 0.03574228319463309, test error 0.056547608283702484\n",
            "Loss: 0.18460882778554843\n",
            "training error 0.0357214212834244, test error 0.05653450912337036\n",
            "Loss: 0.16140122814203295\n",
            "training error 0.0357026957229212, test error 0.056486215291391195\n",
            "Loss: 0.07583971966313996\n",
            "training error 0.035709149077656925, test error 0.05648217337830464\n",
            "Loss: 0.06867871862732411\n",
            "training error 0.0356793766614795, test error 0.056403151992765486\n",
            "Loss: 0.0\n",
            "training error 0.03567238451108744, test error 0.05633246534915512\n",
            "Loss: 0.0\n",
            "training error 0.035704103661542454, test error 0.05639001069773787\n",
            "Loss: 0.10215308033489912\n",
            "training error 0.03573564502358921, test error 0.056180896288798673\n",
            "Loss: 0.0\n",
            "training error 0.03563035584776243, test error 0.0563776971699954\n",
            "Loss: 0.3502985786931312\n",
            "training error 0.03563445290820776, test error 0.05635866470113838\n",
            "Loss: 0.31642146010966155\n",
            "training error 0.03564256928800745, test error 0.056457921735424646\n",
            "Loss: 0.49309545579678815\n",
            "training error 0.03558745027040812, test error 0.05636737861239064\n",
            "Loss: 0.33193191264402344\n",
            "training error 0.035666205125155295, test error 0.056180116576839946\n",
            "Loss: 0.0\n",
            "training error 0.03560413025245964, test error 0.056183282426205954\n",
            "Loss: 0.005635177637408617\n",
            "training error 0.03557762466589112, test error 0.05643458910109981\n",
            "Loss: 0.4529583414299543\n",
            "training error 0.03560509007734236, test error 0.056233603345738646\n",
            "Loss: 0.09520587025757532\n",
            "training error 0.03555689073900397, test error 0.056417263448635645\n",
            "Loss: 0.42211886739562665\n",
            "training error 0.0355664992098127, test error 0.05632420522213921\n",
            "Loss: 0.2564762305222068\n",
            "training error 0.03552354015153397, test error 0.05638978448033596\n",
            "Loss: 0.3732066009675128\n",
            "training error 0.035505450001219935, test error 0.05611551432057204\n",
            "Loss: 0.0\n",
            "training error 0.03546812298209324, test error 0.05605946566751212\n",
            "Loss: 0.0\n",
            "training error 0.035484477316876456, test error 0.05604666880056124\n",
            "Loss: 0.0\n",
            "training error 0.03546154641627415, test error 0.05614805120097593\n",
            "Loss: 0.18088925280366208\n",
            "training error 0.03546157215729599, test error 0.05605328012125154\n",
            "Loss: 0.011796099271887961\n",
            "training error 0.03546530533075224, test error 0.05596936649433788\n",
            "Loss: 0.0\n",
            "training error 0.03544210740345928, test error 0.05592863510379326\n",
            "Loss: 0.0\n",
            "training error 0.035415577239738406, test error 0.056121107698434175\n",
            "Loss: 0.3441396241544492\n",
            "training error 0.03542448870607283, test error 0.056128409838873516\n",
            "Loss: 0.3571957990920138\n",
            "training error 0.03546754191184522, test error 0.05586576511640377\n",
            "Loss: 0.0\n",
            "training error 0.035382942406123497, test error 0.056022764719898845\n",
            "Loss: 0.28103007838153893\n",
            "training error 0.035418687889120344, test error 0.05600004457741799\n",
            "Loss: 0.24036090928751808\n",
            "training error 0.03539966832926666, test error 0.0560259061322613\n",
            "Loss: 0.2866532222799645\n",
            "training error 0.035375677906856084, test error 0.0560278318738095\n",
            "Loss: 0.29010030931830855\n",
            "training error 0.03538266253769225, test error 0.056086974706876576\n",
            "Loss: 0.39596627740063894\n",
            "training error 0.0354001081355765, test error 0.05611353781764202\n",
            "Loss: 0.44351437901544166\n",
            "training error 0.03536342902117767, test error 0.05589831481915116\n",
            "Loss: 0.05826413131471142\n",
            "training error 0.035341305662519425, test error 0.05594868103565949\n",
            "Loss: 0.1484199116989604\n",
            "training error 0.03545238327737804, test error 0.05582046359593721\n",
            "Loss: 0.0\n",
            "training error 0.035308027473823, test error 0.0559807621741393\n",
            "Loss: 0.28716812415321424\n",
            "training error 0.035296744061250857, test error 0.056004728448561525\n",
            "Loss: 0.3301026912964078\n",
            "training error 0.03534550663115434, test error 0.0559044046592435\n",
            "Loss: 0.15037686521901517\n",
            "training error 0.03527051954843169, test error 0.0557767413206231\n",
            "Loss: 0.0\n",
            "training error 0.03527753116144745, test error 0.055795614904284016\n",
            "Loss: 0.03383773095029241\n",
            "training error 0.03531702962696762, test error 0.05545397155737586\n",
            "Loss: 0.0\n",
            "training error 0.03528010368360281, test error 0.055421679346982534\n",
            "Loss: 0.0\n",
            "training error 0.03528347467227062, test error 0.05558008561805318\n",
            "Loss: 0.28582004900807867\n",
            "training error 0.0352587774318683, test error 0.05546533428095598\n",
            "Loss: 0.07876869573029133\n",
            "training error 0.035257063703232776, test error 0.055555364896543655\n",
            "Loss: 0.2412152629373443\n",
            "training error 0.035219857736691264, test error 0.05552437707273909\n",
            "Loss: 0.18530244295484888\n",
            "training error 0.03530918745638846, test error 0.055593677640748455\n",
            "Loss: 0.3103447888850086\n",
            "training error 0.035217799303450426, test error 0.055539403344445966\n",
            "Loss: 0.2124150672634606\n",
            "training error 0.03521604675838793, test error 0.05547695973495475\n",
            "Loss: 0.09974506118104465\n",
            "training error 0.03520567255440446, test error 0.05548876603671377\n",
            "Loss: 0.12104773893844012\n",
            "training error 0.03520411093554477, test error 0.05554328370352535\n",
            "Loss: 0.21941658566764222\n",
            "training error 0.03521668067244829, test error 0.05561406916557893\n",
            "Loss: 0.3471381972961929\n",
            "training error 0.03520270492982179, test error 0.05550610708681384\n",
            "Loss: 0.1523370291663717\n",
            "training error 0.035194917463083765, test error 0.05545367130138949\n",
            "Loss: 0.05772462109396859\n",
            "training error 0.03517044415797308, test error 0.05563172988924094\n",
            "Loss: 0.37900428989769974\n",
            "training error 0.03519924374755929, test error 0.05555982582463694\n",
            "Loss: 0.24926432991954073\n",
            "training error 0.03519475311714146, test error 0.05573956399244071\n",
            "Loss: 0.5735745455636154\n",
            "training error 0.03515994152509282, test error 0.05572134454288221\n",
            "Loss: 0.5407003169707991\n",
            "training error 0.03521037091657973, test error 0.0557667893182572\n",
            "Loss: 0.6226985095742377\n",
            "training error 0.035173283530996455, test error 0.0557412931149813\n",
            "Loss: 0.576694484477347\n",
            "training error 0.03514503666301494, test error 0.05563094814034527\n",
            "Loss: 0.37759374278889535\n",
            "training error 0.0351399519067364, test error 0.055733385686689556\n",
            "Loss: 0.5624267315241349\n",
            "training error 0.03521559758028631, test error 0.055856255314034584\n",
            "Loss: 0.7841263061179937\n",
            "training error 0.035115490874051546, test error 0.05573435394192682\n",
            "Loss: 0.5641738009898756\n",
            "training error 0.03510207511772675, test error 0.055672810488350336\n",
            "Loss: 0.4531279894922191\n",
            "training error 0.03512776260453569, test error 0.055613948467176995\n",
            "Loss: 0.34692041536796836\n",
            "training error 0.03514245877877276, test error 0.05547675907652713\n",
            "Loss: 0.09938300353504737\n",
            "training error 0.035081957263572286, test error 0.05548969025798622\n",
            "Loss: 0.12271535580485882\n",
            "training error 0.035112851027144266, test error 0.0556469404016166\n",
            "Loss: 0.4064493485008924\n",
            "training error 0.03507853811014309, test error 0.055572641099527594\n",
            "Loss: 0.2723875463966463\n",
            "training error 0.03515856878475032, test error 0.05537498510224114\n",
            "Loss: 0.0\n",
            "training error 0.03510285143436997, test error 0.055430098743903075\n",
            "Loss: 0.09952804783635028\n",
            "training error 0.03508515043042136, test error 0.05557201490050581\n",
            "Loss: 0.35581011516461203\n",
            "training error 0.035070036964437404, test error 0.0553343036275664\n",
            "Loss: 0.0\n",
            "training error 0.03507800126133919, test error 0.055348842772964396\n",
            "Loss: 0.026275103226836016\n",
            "training error 0.03505729626620453, test error 0.05512071252600128\n",
            "Loss: 0.0\n",
            "training error 0.03507091331566368, test error 0.05516918800298288\n",
            "Loss: 0.08794421327324997\n",
            "training error 0.035075905618240755, test error 0.0552694206770471\n",
            "Loss: 0.26978633662557705\n",
            "training error 0.035038203288056964, test error 0.05526466599726106\n",
            "Loss: 0.26116039627006593\n",
            "training error 0.03505469367679525, test error 0.05538808672116621\n",
            "Loss: 0.48507028104691763\n",
            "training error 0.03504457442811768, test error 0.05532385422555891\n",
            "Loss: 0.3685396836294652\n",
            "training error 0.035010766973753556, test error 0.05522032012720484\n",
            "Loss: 0.18070811613071225\n",
            "training error 0.034994887008397584, test error 0.05518923252458129\n",
            "Loss: 0.12430898556996528\n",
            "training error 0.03500858819178281, test error 0.05535625366722869\n",
            "Loss: 0.4273187526672606\n",
            "training error 0.03503502203461031, test error 0.05534301784526587\n",
            "Loss: 0.40330632366141206\n",
            "training error 0.035036608733138304, test error 0.05520839332306361\n",
            "Loss: 0.15907050733601213\n",
            "training error 0.0350754734823751, test error 0.055414711476858156\n",
            "Loss: 0.5333729144342936\n",
            "training error 0.03497997528147044, test error 0.055269417404103856\n",
            "Loss: 0.26978039885174887\n",
            "training error 0.03507951628510464, test error 0.0552079349898123\n",
            "Loss: 0.15823899912374984\n",
            "training error 0.03506788545741113, test error 0.055451407700316764\n",
            "Loss: 0.5999472052533594\n",
            "training error 0.03496905787573507, test error 0.055281397393285345\n",
            "Loss: 0.29151449594970735\n",
            "training error 0.03496043830086115, test error 0.055325990984308036\n",
            "Loss: 0.37241619148142835\n",
            "training error 0.03496882044649821, test error 0.05531196145639765\n",
            "Loss: 0.34696382109746793\n",
            "training error 0.03495851662218858, test error 0.055288401832100716\n",
            "Loss: 0.30422194927239854\n",
            "training error 0.03507405536465529, test error 0.05534709720878581\n",
            "Loss: 0.41070710520612064\n",
            "training error 0.03497125534105107, test error 0.05528474816325935\n",
            "Loss: 0.2975934630393251\n",
            "training error 0.034959036759661985, test error 0.055302967381300416\n",
            "Loss: 0.33064676951184424\n",
            "training error 0.03495101259134576, test error 0.05526248380607729\n",
            "Loss: 0.25720146489240925\n",
            "training error 0.03495013568228172, test error 0.055274447294633273\n",
            "Loss: 0.27890562655459306\n",
            "training error 0.034968492315848754, test error 0.05509722533127937\n",
            "Loss: 0.0\n",
            "training error 0.034936361062433315, test error 0.05513656032878025\n",
            "Loss: 0.07139197530250385\n",
            "training error 0.03500702492036933, test error 0.055194656516257094\n",
            "Loss: 0.17683501191194662\n",
            "training error 0.03496326130453573, test error 0.0548990009753634\n",
            "Loss: 0.0\n",
            "training error 0.03493827876444593, test error 0.05498062542813099\n",
            "Loss: 0.1486811259174381\n",
            "training error 0.03492675689112011, test error 0.05501111428151807\n",
            "Loss: 0.20421738859144334\n",
            "training error 0.03490668011930271, test error 0.05500127390570924\n",
            "Loss: 0.18629288061495775\n",
            "training error 0.03491743690999259, test error 0.055170945713854114\n",
            "Loss: 0.4953546215034965\n",
            "training error 0.03492740187976705, test error 0.05507318086745318\n",
            "Loss: 0.3172733364819269\n",
            "training error 0.03490231909995116, test error 0.05517434346994317\n",
            "Loss: 0.5015437251824295\n",
            "training error 0.034949249741059715, test error 0.055082636340657735\n",
            "Loss: 0.3344967340603189\n",
            "training error 0.03491501299292741, test error 0.055113573017178015\n",
            "Loss: 0.39084871856029935\n",
            "training error 0.03489485418935452, test error 0.0552046707811715\n",
            "Loss: 0.5567857344895399\n",
            "training error 0.03490091333352812, test error 0.055198562813197384\n",
            "Loss: 0.5456599073058177\n",
            "training error 0.03495348067478931, test error 0.05535344830932861\n",
            "Loss: 0.8277879850111525\n",
            "training error 0.03491217423341895, test error 0.05528164084232538\n",
            "Loss: 0.6969887614780124\n",
            "training error 0.0349205923028427, test error 0.0550915491447811\n",
            "Loss: 0.3507316453793319\n",
            "training error 0.03492604005569791, test error 0.055206095696764136\n",
            "Loss: 0.5593812563885248\n",
            "training error 0.03492140793144803, test error 0.05537099373100754\n",
            "Loss: 0.8597474403148953\n",
            "training error 0.03491720764336824, test error 0.05530223132529914\n",
            "Loss: 0.7344948774508753\n",
            "training error 0.03486376852414545, test error 0.055265489860087634\n",
            "Loss: 0.6675693149474693\n",
            "training error 0.03486927976040936, test error 0.05514354232713455\n",
            "Loss: 0.4454386189666648\n",
            "training error 0.03495573279368386, test error 0.054936508966285304\n",
            "Loss: 0.06832180960585355\n",
            "training error 0.03483706781483924, test error 0.05519906701933798\n",
            "Loss: 0.5465783322892159\n",
            "training error 0.03486398493325158, test error 0.05533502235611098\n",
            "Loss: 0.7942246179365897\n",
            "training error 0.034896593900881775, test error 0.05526762300186103\n",
            "Loss: 0.671454889794898\n",
            "training error 0.03487684235107855, test error 0.05523220155964897\n",
            "Loss: 0.6069337845238776\n",
            "training error 0.034837700168935434, test error 0.05519359939117948\n",
            "Loss: 0.5366189012224298\n",
            "training error 0.0348627507091917, test error 0.05517850189419916\n",
            "Loss: 0.5091184062915532\n",
            "training error 0.0348350763725678, test error 0.05502956261143299\n",
            "Loss: 0.23782151541917873\n",
            "training error 0.03486480198303847, test error 0.05505240419624154\n",
            "Loss: 0.27942807364926825\n",
            "training error 0.03481581958082596, test error 0.05502127899472733\n",
            "Loss: 0.222732685825755\n",
            "training error 0.03480756196439185, test error 0.05502451388499955\n",
            "Loss: 0.22862512505916754\n",
            "training error 0.03482496588790771, test error 0.05510112617996279\n",
            "Loss: 0.3681764713534541\n",
            "training error 0.03483204020527651, test error 0.05495811466483351\n",
            "Loss: 0.10767716792632331\n",
            "training error 0.03481008399888561, test error 0.05468652678823084\n",
            "Loss: 0.0\n",
            "training error 0.034818325890712744, test error 0.05473449291384674\n",
            "Loss: 0.08771104773510618\n",
            "training error 0.03487806490791807, test error 0.05464418431054716\n",
            "Loss: 0.0\n",
            "training error 0.03480118828111049, test error 0.05483335790594736\n",
            "Loss: 0.34619163555467125\n",
            "training error 0.03485623545506674, test error 0.05499378063805934\n",
            "Loss: 0.6397685900578098\n",
            "training error 0.034776663704682895, test error 0.05491978166886763\n",
            "Loss: 0.504348928980658\n",
            "training error 0.034825695834977195, test error 0.055063906975308705\n",
            "Loss: 0.7681012536965071\n",
            "training error 0.03479447982325414, test error 0.05484920433873254\n",
            "Loss: 0.37519093892999233\n",
            "training error 0.03478153635030137, test error 0.05502697705405942\n",
            "Loss: 0.7005187255368694\n",
            "training error 0.034797866824759716, test error 0.05510961174399693\n",
            "Loss: 0.8517419361678158\n",
            "training error 0.03479833082533567, test error 0.05497637632328353\n",
            "Loss: 0.607918330061441\n",
            "training error 0.03477941535815925, test error 0.05497294305634147\n",
            "Loss: 0.601635379761456\n",
            "training error 0.034760671348633684, test error 0.05506067960826204\n",
            "Loss: 0.7621951045108544\n",
            "training error 0.0347580626190617, test error 0.054973233508220636\n",
            "Loss: 0.6021669127742157\n",
            "training error 0.03474940141553272, test error 0.05503406529914495\n",
            "Loss: 0.7134903622717959\n",
            "training error 0.03474976121336588, test error 0.055069736370761874\n",
            "Loss: 0.7787691692793253\n",
            "training error 0.03478566431783742, test error 0.05496460640667319\n",
            "Loss: 0.5863791365336102\n",
            "training error 0.03481071387569324, test error 0.05506965175913609\n",
            "Loss: 0.7786143282347613\n",
            "training error 0.03480798287676254, test error 0.0551188096568366\n",
            "Loss: 0.8685743090099063\n",
            "training error 0.03479590268310171, test error 0.05486744409869586\n",
            "Loss: 0.4085700810902315\n",
            "training error 0.0347440417657422, test error 0.05504277394265651\n",
            "Loss: 0.729427362011914\n",
            "training error 0.03475406440114817, test error 0.05506570483246467\n",
            "Loss: 0.7713913698884634\n",
            "training error 0.03475168583619535, test error 0.05481140243125922\n",
            "Loss: 0.30601265774550246\n",
            "training error 0.03475315505629482, test error 0.05486608494259506\n",
            "Loss: 0.406082797003271\n",
            "training error 0.0347354138710338, test error 0.054797597155931924\n",
            "Loss: 0.28074871520251676\n",
            "training error 0.03474704045300211, test error 0.05487030615776888\n",
            "Loss: 0.41380770904484443\n",
            "training error 0.03478869542766304, test error 0.05487582054393802\n",
            "Loss: 0.4238991510504553\n",
            "training error 0.034749703688429494, test error 0.054812868520623184\n",
            "Loss: 0.30869563194022565\n",
            "training error 0.03475899642056716, test error 0.05481258584994045\n",
            "Loss: 0.3081783386796433\n",
            "training error 0.03480810962814842, test error 0.05476934313402714\n",
            "Loss: 0.22904326427253263\n",
            "training error 0.03473590658223107, test error 0.05484753558928347\n",
            "Loss: 0.3721370925415357\n",
            "training error 0.03477530648948179, test error 0.05473042437106112\n",
            "Loss: 0.1578211141808783\n",
            "training error 0.03475707255427674, test error 0.0547713605199138\n",
            "Loss: 0.23273512263242147\n",
            "training error 0.034733887888348206, test error 0.05475009653002719\n",
            "Loss: 0.19382157647027665\n",
            "training error 0.03474911459343659, test error 0.05467133703834513\n",
            "Loss: 0.0496900596844041\n",
            "training error 0.034730112855165425, test error 0.05475949950893396\n",
            "Loss: 0.21102922450346728\n",
            "training error 0.034731420137012026, test error 0.05478807615022504\n",
            "Loss: 0.2633250756569616\n",
            "training error 0.03473278762124916, test error 0.05486787560103731\n",
            "Loss: 0.40935973939859593\n",
            "training error 0.03479240828130775, test error 0.05470314301080203\n",
            "Loss: 0.10789565440267879\n",
            "training error 0.034752047139644165, test error 0.05478088582686967\n",
            "Loss: 0.2501666335535724\n",
            "training error 0.03475832209197372, test error 0.05502011017631805\n",
            "Loss: 0.6879521956709489\n",
            "training error 0.034723642886877064, test error 0.054937886789815156\n",
            "Loss: 0.5374816789264436\n",
            "training error 0.03473898337247594, test error 0.05472571225855219\n",
            "Loss: 0.1491978497504709\n",
            "training error 0.03479110987781344, test error 0.054907060627932144\n",
            "Loss: 0.48106915804806416\n",
            "training error 0.03470931916429681, test error 0.054717396251583846\n",
            "Loss: 0.1339793830952063\n",
            "training error 0.03474146151098752, test error 0.0548571208290461\n",
            "Loss: 0.3896782817523592\n",
            "training error 0.03474531178272795, test error 0.054653662102036205\n",
            "Loss: 0.01734455662323331\n",
            "training error 0.03476555288295192, test error 0.05464237298312367\n",
            "Loss: 0.0\n",
            "training error 0.03471743455181817, test error 0.05478034785488737\n",
            "Loss: 0.2525052706007447\n",
            "training error 0.03471373906013844, test error 0.05484286568666024\n",
            "Loss: 0.366918002624983\n",
            "training error 0.034723339728214044, test error 0.054755694071328104\n",
            "Loss: 0.2073868355597197\n",
            "training error 0.03471433285517846, test error 0.054787384833924424\n",
            "Loss: 0.2653835162787388\n",
            "training error 0.03472897836218946, test error 0.054843968210718436\n",
            "Loss: 0.36893571158966854\n",
            "training error 0.03471654049230892, test error 0.054752619499564115\n",
            "Loss: 0.20176011842403252\n",
            "training error 0.03471064358973926, test error 0.054682582611449655\n",
            "Loss: 0.07358689992911227\n",
            "training error 0.034713752305248737, test error 0.05467814472006251\n",
            "Loss: 0.0654651966705222\n",
            "training error 0.03474286098563382, test error 0.0547685821080185\n",
            "Loss: 0.23097299404220006\n",
            "training error 0.03478115171418704, test error 0.05475699696837011\n",
            "Loss: 0.20977124343015685\n",
            "training error 0.03476122475077715, test error 0.05494183191283766\n",
            "Loss: 0.5480342696801976\n",
            "training error 0.03469592545028611, test error 0.054872476170941195\n",
            "Loss: 0.4211076043285855\n",
            "training error 0.034748073738004925, test error 0.054753683378575314\n",
            "Loss: 0.2037071038002436\n",
            "training error 0.03481255514170437, test error 0.05507315482398963\n",
            "Loss: 0.7883659097290829\n",
            "training error 0.03473319167971433, test error 0.0547842790870517\n",
            "Loss: 0.25969974615096536\n",
            "training error 0.03469171936941166, test error 0.05486075796432416\n",
            "Loss: 0.3996623303090141\n",
            "training error 0.03470163135952777, test error 0.05499287075353718\n",
            "Loss: 0.641439511643771\n",
            "training error 0.03471817207772934, test error 0.054917087668319056\n",
            "Loss: 0.5027502837042563\n",
            "training error 0.03470549347264445, test error 0.05507328066872445\n",
            "Loss: 0.7885962158595605\n",
            "training error 0.03469321031196647, test error 0.05498315762366316\n",
            "Loss: 0.6236636916276295\n",
            "training error 0.03471010404181499, test error 0.05502057893583957\n",
            "Loss: 0.6921477455466896\n",
            "training error 0.03471328512049001, test error 0.054909950863119866\n",
            "Loss: 0.4896893480062392\n",
            "training error 0.03470800851031062, test error 0.054725815445299454\n",
            "Loss: 0.15270651258421086\n",
            "training error 0.034665762492805986, test error 0.054740691689573066\n",
            "Loss: 0.17993125313164882\n",
            "training error 0.03468972333067386, test error 0.0546885947728218\n",
            "Loss: 0.08458964568103244\n",
            "training error 0.03471094648982954, test error 0.05463729697176876\n",
            "Loss: 0.0\n",
            "training error 0.03469292251985477, test error 0.05480162761093522\n",
            "Loss: 0.3007664146551292\n",
            "training error 0.03467918288536444, test error 0.05495252477770989\n",
            "Loss: 0.576946195021355\n",
            "training error 0.034758284678452024, test error 0.05507239684795521\n",
            "Loss: 0.7963422429394074\n",
            "training error 0.03482891729260811, test error 0.05481237300252348\n",
            "Loss: 0.3204331847623809\n",
            "training error 0.03469231725194692, test error 0.0550332365368152\n",
            "Loss: 0.7246690209638862\n",
            "training error 0.03467862928172959, test error 0.054939336969829194\n",
            "Loss: 0.5528091885960373\n",
            "training error 0.03467525332539977, test error 0.055017635110060414\n",
            "Loss: 0.6961144847414058\n",
            "training error 0.03468539288920048, test error 0.05490054734734216\n",
            "Loss: 0.48181442011934195\n",
            "training error 0.0347026683837174, test error 0.05489928840200505\n",
            "Loss: 0.4795102334064216\n",
            "training error 0.03470795970922757, test error 0.05480357233895985\n",
            "Loss: 0.30432575622656444\n",
            "training error 0.034723892415729575, test error 0.05473248260048103\n",
            "Loss: 0.1742136488952939\n",
            "training error 0.03469597546107844, test error 0.05456936018354596\n",
            "Loss: 0.0\n",
            "training error 0.03468925749687569, test error 0.05463665988458845\n",
            "Loss: 0.12332873395641819\n",
            "training error 0.034671656651602926, test error 0.0546111127371208\n",
            "Loss: 0.07651281494671647\n",
            "training error 0.03468902751142015, test error 0.05472163520010841\n",
            "Loss: 0.27904856507436904\n",
            "training error 0.034666426053879346, test error 0.05476819862917607\n",
            "Loss: 0.36437745460329296\n",
            "training error 0.03469245266680909, test error 0.05481543566694364\n",
            "Loss: 0.450940752411233\n",
            "training error 0.03466937467065634, test error 0.05484456020358925\n",
            "Loss: 0.5043123450919174\n",
            "training error 0.03469304407205654, test error 0.05485961781083941\n",
            "Loss: 0.5319058649710273\n",
            "training error 0.034714888020268446, test error 0.05493081202452695\n",
            "Loss: 0.6623714109259105\n",
            "training error 0.03466457041670212, test error 0.05471335908868396\n",
            "Loss: 0.2638823410310298\n",
            "training error 0.034718618398690775, test error 0.054786519558780014\n",
            "Loss: 0.39795111121632143\n",
            "training error 0.03467359145449445, test error 0.05456962827840323\n",
            "Loss: 0.0004912919198130083\n",
            "training error 0.03472076220029676, test error 0.054730135949873346\n",
            "Loss: 0.29462644565854124\n",
            "training error 0.03464680260517024, test error 0.054511227702190294\n",
            "Loss: 0.0\n",
            "training error 0.03465600965787336, test error 0.05454943910089429\n",
            "Loss: 0.07009821703660712\n",
            "training error 0.03463521914781406, test error 0.05457551570351662\n",
            "Loss: 0.11793533926174327\n",
            "training error 0.03465088879207392, test error 0.05440539159766849\n",
            "Loss: 0.0\n",
            "training error 0.03468346531218962, test error 0.05435646326583195\n",
            "Loss: 0.0\n",
            "training error 0.03466143559383221, test error 0.05453488382195327\n",
            "Loss: 0.3282416577560454\n",
            "training error 0.03464682348782025, test error 0.054628099269169936\n",
            "Loss: 0.4997308268743339\n",
            "training error 0.034659947258585094, test error 0.054566211678304916\n",
            "Loss: 0.3858757539966895\n",
            "training error 0.03472313801103125, test error 0.05451968682208169\n",
            "Loss: 0.3002836211978943\n",
            "training error 0.03472517074360162, test error 0.054608357290539096\n",
            "Loss: 0.4634113582321353\n",
            "training error 0.03464318642868082, test error 0.054667655947381516\n",
            "Loss: 0.5725035494448472\n",
            "training error 0.03470556407181702, test error 0.05458972822149753\n",
            "Loss: 0.429139317848537\n",
            "training error 0.03471876916189584, test error 0.05449205596879699\n",
            "Loss: 0.24945092969335114\n",
            "training error 0.03465035634356412, test error 0.054826795809022257\n",
            "Loss: 0.8652743665277463\n",
            "training error 0.03470289498020769, test error 0.054918203569790255\n",
            "Loss: 1.033437921100755\n",
            "training error 0.03471523781969481, test error 0.05488466638918643\n",
            "Loss: 0.9717393141847497\n",
            "training error 0.03464752566053003, test error 0.05462069788050059\n",
            "Loss: 0.4861144356953506\n",
            "training error 0.034668916342949926, test error 0.054677640499478555\n",
            "Loss: 0.5908722060813387\n",
            "training error 0.03466746292192442, test error 0.05468837545798461\n",
            "Loss: 0.610621391111188\n",
            "training error 0.03464029609801351, test error 0.05459365908140986\n",
            "Loss: 0.43637095080653054\n",
            "training error 0.034647614791070806, test error 0.05458013473877238\n",
            "Loss: 0.4114901145178651\n",
            "training error 0.03465863482039055, test error 0.05478728092013772\n",
            "Loss: 0.7925785241008843\n",
            "training error 0.034736451523754694, test error 0.05462169955314944\n",
            "Loss: 0.48795722050631873\n",
            "training error 0.03475278077702715, test error 0.0547911591000954\n",
            "Loss: 0.7997132413445796\n",
            "training error 0.03467002102756133, test error 0.05462320102796129\n",
            "Loss: 0.49071949516812374\n",
            "training error 0.034646572491251394, test error 0.05478001947916622\n",
            "Loss: 0.7792195957688763\n",
            "training error 0.03463106098349425, test error 0.054688953961833854\n",
            "Loss: 0.6116856690543848\n",
            "training error 0.03464416047813845, test error 0.05473853616298034\n",
            "Loss: 0.702902422624252\n",
            "training error 0.034642798967671334, test error 0.05446460518362316\n",
            "Loss: 0.198949510865587\n",
            "training error 0.034644041393144996, test error 0.05452097913722897\n",
            "Loss: 0.30266110322971773\n",
            "training error 0.034678601117969554, test error 0.05439845172973162\n",
            "Loss: 0.07724649724603072\n",
            "training error 0.0346350488740019, test error 0.05450374717081337\n",
            "Loss: 0.27095932320160276\n",
            "training error 0.03464314203203411, test error 0.054616179091206134\n",
            "Loss: 0.4778011845694152\n",
            "training error 0.03468624204867333, test error 0.054358710572751684\n",
            "Loss: 0.004134387678500495\n",
            "training error 0.03464400740322969, test error 0.05446323282981951\n",
            "Loss: 0.1964247811072628\n",
            "training error 0.03465674339407626, test error 0.054588720802162316\n",
            "Loss: 0.42728596081482717\n",
            "training error 0.03462635376687286, test error 0.054545642773436716\n",
            "Loss: 0.34803498284938783\n",
            "training error 0.03463177765769805, test error 0.05462541751056607\n",
            "Loss: 0.4947971751193547\n",
            "training error 0.034637821319610955, test error 0.054542677257176105\n",
            "Loss: 0.34257929996930336\n",
            "training error 0.03462856340875324, test error 0.054612034166323556\n",
            "Loss: 0.47017573465317763\n",
            "training error 0.03466810138542389, test error 0.05473748349629904\n",
            "Loss: 0.7009658237028704\n",
            "training error 0.03466417387034893, test error 0.05466406903334139\n",
            "Loss: 0.5659046763309084\n",
            "training error 0.03463599409273527, test error 0.05458919032419264\n",
            "Loss: 0.42814974407465733\n",
            "training error 0.034667262690524045, test error 0.0545322913707838\n",
            "Loss: 0.3234723055691946\n",
            "training error 0.03466156272079761, test error 0.05473175165222357\n",
            "Loss: 0.6904209064453992\n",
            "training error 0.03465110257189457, test error 0.05461649361838032\n",
            "Loss: 0.47837982261040857\n",
            "training error 0.03466942484636076, test error 0.05477311196248346\n",
            "Loss: 0.7665117846499259\n",
            "training error 0.034663331690571335, test error 0.054691704145557664\n",
            "Loss: 0.6167452030243492\n",
            "training error 0.03464642554680906, test error 0.05454934784701705\n",
            "Loss: 0.3548512349705213\n",
            "training error 0.03465355202466173, test error 0.05463967714857629\n",
            "Loss: 0.5210307399126979\n",
            "training error 0.03463487732512336, test error 0.05473620061779017\n",
            "Loss: 0.6986057023267156\n",
            "training error 0.03470806464462158, test error 0.05457883506556292\n",
            "Loss: 0.4090990958029339\n",
            "training error 0.03462385308476696, test error 0.054760029722082736\n",
            "Loss: 0.7424442872177472\n",
            "training error 0.03462019967671616, test error 0.054782236932257394\n",
            "Loss: 0.783299061131304\n",
            "training error 0.03461965956287515, test error 0.05475606763177134\n",
            "Loss: 0.7351551994564387\n",
            "training error 0.03463552585276463, test error 0.05479243096948633\n",
            "Loss: 0.8020531091625038\n",
            "training error 0.0346617156213265, test error 0.054734394606604675\n",
            "Loss: 0.695283169775851\n",
            "training error 0.034723646100183185, test error 0.054666840636674735\n",
            "Loss: 0.5710036161199028\n",
            "training error 0.03463619381696611, test error 0.054865277356654314\n",
            "Loss: 0.9360691631719931\n",
            "training error 0.03462791652520568, test error 0.05497606334423214\n",
            "Loss: 1.1398829893880746\n",
            "training error 0.034778464587564485, test error 0.054670190869340704\n",
            "Loss: 0.577167064704831\n",
            "training error 0.03460340579464068, test error 0.054958636153799145\n",
            "Loss: 1.1078220542463324\n",
            "training error 0.034684457109350864, test error 0.05484174304188037\n",
            "Loss: 0.8927729048064714\n",
            "training error 0.03474658342607726, test error 0.05501349570704231\n",
            "Loss: 1.2087475927142766\n",
            "training error 0.03464873237687346, test error 0.054995710438050106\n",
            "Loss: 1.1760278977164251\n",
            "training error 0.034623021732277916, test error 0.054976670271506566\n",
            "Loss: 1.1409995581233456\n",
            "training error 0.03462577426637853, test error 0.05503470960352998\n",
            "Loss: 1.2477749598627241\n",
            "training error 0.03461398496936004, test error 0.0549000479651117\n",
            "Loss: 1.0000369167164846\n",
            "training error 0.03464289027044489, test error 0.05477195904761077\n",
            "Loss: 0.7643907583663623\n",
            "training error 0.03464511031607473, test error 0.05485129000136745\n",
            "Loss: 0.9103365189812695\n",
            "training error 0.03461275759576266, test error 0.05459205755831077\n",
            "Loss: 0.43342461654769515\n",
            "training error 0.034628315223780765, test error 0.054650633673338375\n",
            "Loss: 0.5411875420734846\n",
            "training error 0.034596945080899, test error 0.05479842326367843\n",
            "Loss: 0.8130771784857771\n",
            "training error 0.03459849687489591, test error 0.05481513960315105\n",
            "Loss: 0.8438303556946414\n",
            "training error 0.034595751878476036, test error 0.05482587822710217\n",
            "Loss: 0.863586284071749\n",
            "training error 0.034705427463186944, test error 0.05498651607880284\n",
            "Loss: 1.159112964891773\n",
            "training error 0.034593525716048956, test error 0.05481193059724323\n",
            "Loss: 0.8379267230537168\n",
            "training error 0.034609714517047176, test error 0.05472616374293159\n",
            "Loss: 0.6801407871067866\n",
            "training error 0.03460005248225062, test error 0.05472205197284974\n",
            "Loss: 0.672576332330288\n",
            "training error 0.03463325211631519, test error 0.054580014500246736\n",
            "Loss: 0.4112689107852896\n",
            "training error 0.03458832134543992, test error 0.05472545891164668\n",
            "Loss: 0.6788441036168047\n",
            "training error 0.034620925850374545, test error 0.05468115718894712\n",
            "Loss: 0.5973418865154123\n",
            "training error 0.034606935624875525, test error 0.054789925238728186\n",
            "Loss: 0.7974432971777023\n",
            "training error 0.03459688330882927, test error 0.05464416975193636\n",
            "Loss: 0.5292958165754369\n",
            "training error 0.03459545789989557, test error 0.054408987236963115\n",
            "Loss: 0.09662875024503137\n",
            "training error 0.03458335166998607, test error 0.05446650157029794\n",
            "Loss: 0.20243830789330453\n",
            "training error 0.034644221641366484, test error 0.054352434013853985\n",
            "Loss: 0.0\n",
            "training error 0.03461508099503531, test error 0.054652243146415556\n",
            "Loss: 0.551602035863108\n",
            "training error 0.03459557857389395, test error 0.05468806852449632\n",
            "Loss: 0.6175151430325654\n",
            "training error 0.03465217536505895, test error 0.05469490392343147\n",
            "Loss: 0.6300912108005985\n",
            "training error 0.034612479781382655, test error 0.054715518567026715\n",
            "Loss: 0.6680189392809677\n",
            "training error 0.03461710067941616, test error 0.054669227418044766\n",
            "Loss: 0.5828504462376705\n",
            "training error 0.03458686593043822, test error 0.05458367290452084\n",
            "Loss: 0.42544348723723147\n",
            "training error 0.034577304251768366, test error 0.054596712759912215\n",
            "Loss: 0.44943478703449724\n",
            "training error 0.03457472821145623, test error 0.05467060615402648\n",
            "Loss: 0.5853871053712156\n",
            "training error 0.03462956804765856, test error 0.05464771696548726\n",
            "Loss: 0.5432745690064467\n",
            "training error 0.03458281462957705, test error 0.05468536689607143\n",
            "Loss: 0.6125445681652186\n",
            "training error 0.034585166103188875, test error 0.05464809327599631\n",
            "Loss: 0.5439669216413812\n",
            "training error 0.03459183227833413, test error 0.05443214866481122\n",
            "Loss: 0.14666252285393533\n",
            "training error 0.03459076086878425, test error 0.05452838104532977\n",
            "Loss: 0.32371509145465716\n",
            "training error 0.03458728191517765, test error 0.05455668874158985\n",
            "Loss: 0.37579683677790854\n",
            "training error 0.03459131029522219, test error 0.054621683739586535\n",
            "Loss: 0.4953774943435185\n",
            "training error 0.03456437641979729, test error 0.05457859457370634\n",
            "Loss: 0.41610015072133244\n",
            "training error 0.034580976727938424, test error 0.054611538605835674\n",
            "Loss: 0.4767120308092343\n",
            "training error 0.03459243799008897, test error 0.05465768122938839\n",
            "Loss: 0.5616072602316224\n",
            "training error 0.034577575777235074, test error 0.05445650203022227\n",
            "Loss: 0.19146891626189433\n",
            "training error 0.034588219325625766, test error 0.05428937090494946\n",
            "Loss: 0.0\n",
            "training error 0.03457340766370422, test error 0.054390381171756555\n",
            "Loss: 0.1860590114111771\n",
            "training error 0.03456781231720057, test error 0.05437589823454067\n",
            "Loss: 0.15938171348992558\n",
            "training error 0.03457685096993218, test error 0.05430165599803221\n",
            "Loss: 0.022628910370436905\n",
            "training error 0.034590318821817574, test error 0.05425455618476244\n",
            "Loss: 0.0\n",
            "training error 0.03455532610931425, test error 0.05434171782663533\n",
            "Loss: 0.16065312851525348\n",
            "training error 0.03455460176041847, test error 0.05434883747004482\n",
            "Loss: 0.1737757930620809\n",
            "training error 0.034568505080159864, test error 0.054392976551374296\n",
            "Loss: 0.2551313223178475\n",
            "training error 0.034561296447908736, test error 0.0544387459396184\n",
            "Loss: 0.33949177324150703\n",
            "training error 0.03456304072954459, test error 0.05444658345010398\n",
            "Loss: 0.35393758394706154\n",
            "training error 0.03460272847491827, test error 0.054415531903655355\n",
            "Loss: 0.2967045170265914\n",
            "training error 0.03455214567645522, test error 0.054496732046908074\n",
            "Loss: 0.4463696308212528\n",
            "training error 0.03456771060576863, test error 0.054446895982571755\n",
            "Loss: 0.35451363228244936\n",
            "training error 0.03456283360290044, test error 0.054332659808218\n",
            "Loss: 0.14395772253592565\n",
            "training error 0.034622936210232075, test error 0.0544910164696065\n",
            "Loss: 0.4358348892189001\n",
            "training error 0.03458484180741319, test error 0.05433450557106712\n",
            "Loss: 0.14735976464801404\n",
            "training error 0.03456478801781242, test error 0.05452068191930554\n",
            "Loss: 0.49051315365444825\n",
            "training error 0.034612521568474705, test error 0.05452561228920311\n",
            "Loss: 0.49960062988552156\n",
            "training error 0.03457700218004551, test error 0.05443052924464175\n",
            "Loss: 0.32434706364574684\n",
            "training error 0.03457763532604476, test error 0.05452069394722105\n",
            "Loss: 0.4905353230653775\n",
            "training error 0.03462043413892309, test error 0.05446087674077468\n",
            "Loss: 0.3802824509514524\n",
            "training error 0.03457907426963935, test error 0.054502007836316944\n",
            "Loss: 0.4560937715752722\n",
            "training error 0.034609320031235846, test error 0.05452999338833096\n",
            "Loss: 0.5076757104611307\n",
            "training error 0.03457530595161701, test error 0.05452408119241073\n",
            "Loss: 0.4967785686614601\n",
            "training error 0.034628817359328316, test error 0.054573567393943645\n",
            "Loss: 0.5879897129649869\n",
            "training error 0.03459566917418517, test error 0.05444550701230484\n",
            "Loss: 0.35195353343620006\n",
            "training error 0.034586659646104106, test error 0.05435382015095209\n",
            "Loss: 0.18295968701984577\n",
            "training error 0.034587797519650174, test error 0.05440907487470453\n",
            "Loss: 0.2848031590487743\n",
            "training error 0.03461374607435455, test error 0.054662225559201943\n",
            "Loss: 0.7514011782737695\n",
            "training error 0.034561624556264076, test error 0.054583388074483256\n",
            "Loss: 0.6060908296825707\n",
            "training error 0.034621434795657706, test error 0.05459941012611079\n",
            "Loss: 0.6356220852198335\n",
            "training error 0.03461383968098439, test error 0.05452667590629112\n",
            "Loss: 0.5015610497337564\n",
            "training error 0.03457479892353629, test error 0.05468440835608518\n",
            "Loss: 0.7922876925928524\n",
            "training error 0.03461499358283825, test error 0.05467710912809232\n",
            "Loss: 0.7788340243552794\n",
            "training error 0.03458874204238977, test error 0.054712361669589926\n",
            "Loss: 0.8438102106456125\n",
            "training error 0.034562939043483724, test error 0.054722507534707136\n",
            "Loss: 0.8625106956015038\n",
            "training error 0.034563207373419655, test error 0.05471098835975383\n",
            "Loss: 0.8412789765287698\n",
            "training error 0.03458847538475122, test error 0.05476295281327418\n",
            "Loss: 0.9370579436322579\n",
            "training error 0.03457540706607099, test error 0.054767450127691464\n",
            "Loss: 0.9453472279496333\n",
            "training error 0.034555037340819696, test error 0.05468218233706498\n",
            "Loss: 0.7881847763094418\n",
            "training error 0.03459367277262751, test error 0.054800666308596785\n",
            "Loss: 1.006570069386581\n",
            "training error 0.0346699161082168, test error 0.054402486471056014\n",
            "Loss: 0.2726596560661321\n",
            "training error 0.034650796315906784, test error 0.05460811370045464\n",
            "Loss: 0.6516641929355638\n",
            "training error 0.0346010459386082, test error 0.05471546004022213\n",
            "Loss: 0.8495210132953623\n",
            "training error 0.03456375227530183, test error 0.05460192465564295\n",
            "Loss: 0.6402567734543174\n",
            "training error 0.03459237630057053, test error 0.0546939000847283\n",
            "Loss: 0.8097824972886913\n",
            "training error 0.0345892123984202, test error 0.05448078165538165\n",
            "Loss: 0.4169704565434307\n",
            "training error 0.034552550109185155, test error 0.05458959225100871\n",
            "Loss: 0.6175261393813969\n",
            "training error 0.034601730159365844, test error 0.05463358565592599\n",
            "Loss: 0.6986131632388259\n",
            "training error 0.03455853355511025, test error 0.054723143625111825\n",
            "Loss: 0.8636831140109713\n",
            "training error 0.03459241704633974, test error 0.05446381397715784\n",
            "Loss: 0.38569625688722464\n",
            "training error 0.03457157284337272, test error 0.05453680691687761\n",
            "Loss: 0.5202341553656398\n",
            "training error 0.03457267478823445, test error 0.05468338780735951\n",
            "Loss: 0.7904066547640642\n",
            "training error 0.03456065887535978, test error 0.0547390365258743\n",
            "Loss: 0.8929763234298393\n",
            "training error 0.034643899697750044, test error 0.0548672697519502\n",
            "Loss: 1.129331083459939\n",
            "training error 0.034582912901083944, test error 0.05473406521579226\n",
            "Loss: 0.8838133877583809\n",
            "training error 0.03458930820903833, test error 0.05449867991463449\n",
            "Loss: 0.4499598688830764\n",
            "training error 0.0346092505866964, test error 0.054609923613219535\n",
            "Loss: 0.6550001574925801\n",
            "training error 0.03459706958160823, test error 0.0546837877199046\n",
            "Loss: 0.7911437588401293\n",
            "training error 0.034566076903878855, test error 0.05476449829366481\n",
            "Loss: 0.9399065161749265\n",
            "training error 0.034572601216581386, test error 0.05465793265429711\n",
            "Loss: 0.7434886540422125\n",
            "training error 0.034604285998887666, test error 0.054600717627421434\n",
            "Loss: 0.6380320234860237\n",
            "training error 0.034562180964818785, test error 0.05462512395251662\n",
            "Loss: 0.6830168631224653\n",
            "training error 0.03456718618661262, test error 0.05464408990338426\n",
            "Loss: 0.7179742053280869\n",
            "training error 0.03463398670297879, test error 0.054813365952249726\n",
            "Loss: 1.0299775848949588\n",
            "training error 0.03471735999945905, test error 0.05490391577647524\n",
            "Loss: 1.196875686350518\n",
            "training error 0.03456349697942904, test error 0.05450889012802696\n",
            "Loss: 0.4687789582102475\n",
            "training error 0.03460162175082636, test error 0.05445305801700618\n",
            "Loss: 0.3658712672309994\n",
            "training error 0.03455560677848282, test error 0.054564361254960364\n",
            "Loss: 0.5710212966131278\n",
            "training error 0.03456198914854276, test error 0.05459837267440273\n",
            "Loss: 0.6337098924363849\n",
            "training error 0.034551279313818406, test error 0.05448119605188612\n",
            "Loss: 0.41773425692004373\n",
            "training error 0.03454753356325149, test error 0.054560322233431664\n",
            "Loss: 0.5635767208710618\n",
            "training error 0.03455996698803049, test error 0.054524513936809994\n",
            "Loss: 0.49757618720209784\n",
            "training error 0.03457667611782274, test error 0.05452869499764926\n",
            "Loss: 0.5052825645706971\n",
            "training error 0.03454561039027256, test error 0.054643027709957874\n",
            "Loss: 0.7160164095205257\n",
            "training error 0.034572702644322925, test error 0.054716415393349\n",
            "Loss: 0.8512818849973014\n",
            "training error 0.03458905731184886, test error 0.05477888114940613\n",
            "Loss: 0.9664164662191999\n",
            "training error 0.034700855030192615, test error 0.05464907501888202\n",
            "Loss: 0.7271625866333897\n",
            "training error 0.03464982389167528, test error 0.05436447488151362\n",
            "Loss: 0.20259809402340956\n",
            "training error 0.03459554003763092, test error 0.054482150565185426\n",
            "Loss: 0.41949358068273934\n",
            "training error 0.03454389925703633, test error 0.05468031309566197\n",
            "Loss: 0.7847394593914592\n",
            "training error 0.034587658306498056, test error 0.054531474770146394\n",
            "Loss: 0.5104061388704784\n",
            "training error 0.034630628868317206, test error 0.05464802328462102\n",
            "Loss: 0.7252240687743106\n",
            "training error 0.034583238491298095, test error 0.054573644896283316\n",
            "Loss: 0.5881325624233824\n",
            "training error 0.03458728567163784, test error 0.05478777883205455\n",
            "Loss: 0.9828163472137552\n",
            "training error 0.03456337000314153, test error 0.05468626104626875\n",
            "Loss: 0.7957025029126763\n",
            "training error 0.034569350350885236, test error 0.054616932358912317\n",
            "Loss: 0.6679184194518406\n",
            "training error 0.034593877808851696, test error 0.05451563713651311\n",
            "Loss: 0.4812147957888868\n",
            "training error 0.0345917156760223, test error 0.05466411226451103\n",
            "Loss: 0.7548786840203103\n",
            "training error 0.034590876488508865, test error 0.054720387782091796\n",
            "Loss: 0.8586036456421864\n",
            "training error 0.03456717258906839, test error 0.054639918047431894\n",
            "Loss: 0.7102847940680279\n",
            "training error 0.0346601904613681, test error 0.054578675729699544\n",
            "Loss: 0.5974052093124183\n",
            "training error 0.03456107623276684, test error 0.054711755950769644\n",
            "Loss: 0.8426937720220629\n",
            "training error 0.03458576187799063, test error 0.054704668191383525\n",
            "Loss: 0.82962987493298\n",
            "training error 0.03457902675492332, test error 0.054696177622716786\n",
            "Loss: 0.8139803714372285\n",
            "training error 0.03457488874641847, test error 0.05477520119404628\n",
            "Loss: 0.959633708016705\n",
            "training error 0.03462267046103986, test error 0.05468436601854316\n",
            "Loss: 0.7922096575945003\n",
            "training error 0.03457645821801993, test error 0.05446492802300074\n",
            "Loss: 0.3877496251593948\n",
            "training error 0.034603045874500714, test error 0.05455727819924319\n",
            "Loss: 0.557966069153415\n",
            "training error 0.03461788120078259, test error 0.054715117981129456\n",
            "Loss: 0.8488905425722892\n",
            "training error 0.03457145640978556, test error 0.05459379738279653\n",
            "Loss: 0.6252768834359657\n",
            "training error 0.034590302694335065, test error 0.05466323908720737\n",
            "Loss: 0.7532692757695303\n",
            "training error 0.03463323282501509, test error 0.054585452515426514\n",
            "Loss: 0.6098959312047736\n",
            "training error 0.034571709831604035, test error 0.05466908113318124\n",
            "Loss: 0.7640371197713769\n",
            "training error 0.034641156537063456, test error 0.054925671703505013\n",
            "Loss: 1.2369754098754626\n",
            "training error 0.034572051652570004, test error 0.05478449921906671\n",
            "Loss: 0.9767714853284648\n",
            "training error 0.034561829273927036, test error 0.05477027954634162\n",
            "Loss: 0.9505623082104009\n",
            "training error 0.03457618321821425, test error 0.05479447960749763\n",
            "Loss: 0.9951669697499721\n",
            "training error 0.03458939347733306, test error 0.05479566811659367\n",
            "Loss: 0.9973575859481665\n",
            "training error 0.03460425957542298, test error 0.054818457108357595\n",
            "Loss: 1.0393614163477949\n",
            "training error 0.03459847883119886, test error 0.054824401679892866\n",
            "Loss: 1.0503182316888404\n",
            "training error 0.03462021921510971, test error 0.054893233378373145\n",
            "Loss: 1.1771862835550762\n",
            "training error 0.034549442497258206, test error 0.05469651928662706\n",
            "Loss: 0.8146101137746387\n",
            "training error 0.034548855397451984, test error 0.05473634291922348\n",
            "Loss: 0.8880115668448818\n",
            "training error 0.03463822849537251, test error 0.05482258315601772\n",
            "Loss: 1.0469663954505215\n",
            "training error 0.03456499426267919, test error 0.054621811979851755\n",
            "Loss: 0.676912357072168\n",
            "training error 0.034567300394877425, test error 0.05454948204635459\n",
            "Loss: 0.5435964872476218\n",
            "training error 0.03456455359339086, test error 0.05450143570323776\n",
            "Loss: 0.45503923695289217\n",
            "training error 0.03463571722472997, test error 0.054496629083893\n",
            "Loss: 0.44617985318355924\n",
            "training error 0.03455489169294345, test error 0.054436102153710295\n",
            "Loss: 0.3346188444148579\n",
            "training error 0.03455775963532656, test error 0.05455206442689961\n",
            "Loss: 0.5483562359703376\n",
            "training error 0.03461936882413229, test error 0.05445000007480024\n",
            "Loss: 0.3602349807677374\n",
            "training error 0.034588278038364, test error 0.05462144355792599\n",
            "Loss: 0.6762332953459849\n",
            "training error 0.034566078518663716, test error 0.05457725938956982\n",
            "Loss: 0.594794663342979\n",
            "training error 0.03455692886591423, test error 0.054410046344161266\n",
            "Loss: 0.2865937357764192\n",
            "training error 0.034592368832351394, test error 0.05430963935716699\n",
            "Loss: 0.10152727490198021\n",
            "training error 0.03459201788644916, test error 0.05448115045094545\n",
            "Loss: 0.4176502069454857\n",
            "training error 0.034542748121123346, test error 0.05457562452656188\n",
            "Loss: 0.5917813440516451\n",
            "training error 0.0345521272623459, test error 0.054611107978545646\n",
            "Loss: 0.6571831360466351\n",
            "training error 0.03457074592225986, test error 0.05453134294417697\n",
            "Loss: 0.5101631620981983\n",
            "training error 0.034547096087372574, test error 0.05462211890657853\n",
            "Loss: 0.677478073112181\n",
            "training error 0.034569134023381326, test error 0.05457722358893239\n",
            "Loss: 0.5947286769264482\n",
            "training error 0.03475730317613484, test error 0.054281682023833204\n",
            "Loss: 0.04999734764834507\n",
            "training error 0.03457029245205879, test error 0.05457717790202942\n",
            "Loss: 0.5946444685093422\n",
            "training error 0.03460446758118106, test error 0.05472715618831401\n",
            "Loss: 0.8710789227399651\n",
            "training error 0.03456794525614931, test error 0.05458092197169254\n",
            "Loss: 0.601545399834591\n",
            "training error 0.03455601146346691, test error 0.05472256190613408\n",
            "Loss: 0.8626109110133839\n",
            "training error 0.03459914392175401, test error 0.05480860865379797\n",
            "Loss: 1.021209107579324\n",
            "training error 0.03456074600189631, test error 0.05446275109812059\n",
            "Loss: 0.38373719738697964\n",
            "training error 0.0345887137695185, test error 0.054340657831689426\n",
            "Loss: 0.1586993848659901\n",
            "training error 0.034572453334733785, test error 0.05450741902800847\n",
            "Loss: 0.46606748083040106\n",
            "training error 0.03458146467720285, test error 0.0545397007104534\n",
            "Loss: 0.5255678891186832\n",
            "training error 0.03458924303742975, test error 0.054628130448106604\n",
            "Loss: 0.6885583250777483\n",
            "training error 0.0345834054715322, test error 0.05441089724965591\n",
            "Loss: 0.2881620934489959\n",
            "training error 0.03459632324308425, test error 0.05445610252735177\n",
            "Loss: 0.3714827965838996\n",
            "training error 0.034588144859949994, test error 0.05446573436550143\n",
            "Loss: 0.38923584596255445\n",
            "training error 0.034568680537784696, test error 0.05468976923285538\n",
            "Loss: 0.8021686632378566\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.05468976923285538\n",
            "training error 0.12507470042622232, test error 0.25003941199157803\n",
            "training error 0.12503202013398776, test error 0.25004745833144143\n",
            "training error 0.125146766676595, test error 0.2502523822737064\n",
            "training error 0.12504414773692535, test error 0.2502718752800295\n",
            "training error 0.12498135436050015, test error 0.2502989236400842\n",
            "training error 0.1250065698820888, test error 0.25023094162813797\n",
            "training error 0.12501463545036406, test error 0.2503183377049009\n",
            "training error 0.12501701069224308, test error 0.25035254801460594\n",
            "training error 0.12501252704452775, test error 0.25043558777027636\n",
            "training error 0.12496589285047659, test error 0.25042595140083646\n",
            "training error 0.12497545031904293, test error 0.25038625524966646\n",
            "training error 0.12496986688923745, test error 0.2503669286575839\n",
            "training error 0.12498802300373377, test error 0.25037485066416304\n",
            "training error 0.12498792042568209, test error 0.25035992767161735\n",
            "training error 0.12504734011334656, test error 0.2503842364474767\n",
            "training error 0.12499072796855414, test error 0.2502673845775383\n",
            "training error 0.1250813632441599, test error 0.2501700061851263\n",
            "training error 0.12504575593879358, test error 0.25024034760515046\n",
            "training error 0.12513371001757456, test error 0.2504620045532049\n",
            "training error 0.12496663481655229, test error 0.25041203216293123\n",
            "training error 0.1249841567101568, test error 0.2503633013683052\n",
            "training error 0.12497330521372173, test error 0.25038267490199145\n",
            "training error 0.12496744029254539, test error 0.25036631095006\n",
            "training error 0.12499489260223193, test error 0.2504783264694169\n",
            "training error 0.12507819645247448, test error 0.25044026262723185\n",
            "training error 0.12502595537515548, test error 0.2505323075909768\n",
            "training error 0.1250287211080019, test error 0.25053476468912067\n",
            "training error 0.12498228730606246, test error 0.2504141852801208\n",
            "training error 0.1249724892609578, test error 0.25038105022716645\n",
            "training error 0.12504595886902983, test error 0.250282618566318\n",
            "training error 0.12497658339710332, test error 0.2503339572887076\n",
            "training error 0.12507072280454962, test error 0.2503399872484294\n",
            "training error 0.1251797153041159, test error 0.25044817496126465\n",
            "training error 0.1251588889037184, test error 0.25037478955856496\n",
            "training error 0.12495831853791056, test error 0.2504551262864889\n",
            "training error 0.12499441060877164, test error 0.2505070712223657\n",
            "training error 0.12496196481849477, test error 0.25047215998943984\n",
            "training error 0.12497035490230822, test error 0.2504876019626042\n",
            "training error 0.12499406693923938, test error 0.2505097665535655\n",
            "training error 0.12518709754410956, test error 0.25040023837618486\n",
            "training error 0.12494984105380368, test error 0.25045449310995505\n",
            "training error 0.12496719518822956, test error 0.25040676764815917\n",
            "training error 0.12499465986571517, test error 0.2504783605108607\n",
            "training error 0.12510878832260944, test error 0.25053083790247543\n",
            "training error 0.1250621189254511, test error 0.25047059739363775\n",
            "training error 0.12500587937511412, test error 0.25046818411650507\n",
            "training error 0.12496954502241206, test error 0.2505077464751007\n",
            "training error 0.12501376920914456, test error 0.25051806763429074\n",
            "training error 0.1250702488481015, test error 0.25054408394033223\n",
            "training error 0.1249600421277122, test error 0.25056238393992536\n",
            "Loss: 0.20915580635141318\n",
            "training error 0.12496367643256855, test error 0.250549387354623\n",
            "Loss: 0.2039579916554013\n",
            "training error 0.12496192067129133, test error 0.25056018988961926\n",
            "Loss: 0.20827832456220552\n",
            "training error 0.12497493041313984, test error 0.25061884232011694\n",
            "Loss: 0.23173559876970007\n",
            "training error 0.12500122411136227, test error 0.2505541428058341\n",
            "Loss: 0.20585987231220848\n",
            "training error 0.12500313022175633, test error 0.25066427479812364\n",
            "Loss: 0.2499057254888415\n",
            "training error 0.12504304388850665, test error 0.2506498078901472\n",
            "Loss: 0.24411987442591876\n",
            "training error 0.12501798946122925, test error 0.25061473261719486\n",
            "Loss: 0.2300919767145304\n",
            "training error 0.12495996821312717, test error 0.2507011594941651\n",
            "Loss: 0.2646572783531198\n",
            "training error 0.1250807522205661, test error 0.2505410088606103\n",
            "Loss: 0.20060712230804967\n",
            "training error 0.1250166505627273, test error 0.2505605585185717\n",
            "Loss: 0.20842575290138665\n",
            "training error 0.12499105741927469, test error 0.25068933263555065\n",
            "Loss: 0.2599272805818842\n",
            "training error 0.12497985148086115, test error 0.25078365853574225\n",
            "Loss: 0.29765169348154874\n",
            "training error 0.125033604842177, test error 0.2509192261935004\n",
            "Loss: 0.3518702091460657\n",
            "training error 0.12496909480295705, test error 0.25082086703585993\n",
            "Loss: 0.3125327475606898\n",
            "training error 0.12494383068777946, test error 0.2507946252991689\n",
            "Loss: 0.3020377074060221\n",
            "training error 0.1249464076486289, test error 0.2507015348750474\n",
            "Loss: 0.26480740703858086\n",
            "training error 0.12494672608077313, test error 0.2507641797799188\n",
            "Loss: 0.289861419273052\n",
            "training error 0.12499388091531698, test error 0.2507916257117113\n",
            "Loss: 0.30083806154472814\n",
            "training error 0.12498678498678586, test error 0.2507388202851115\n",
            "Loss: 0.27971922024718765\n",
            "training error 0.12513872219097077, test error 0.2507730726499805\n",
            "Loss: 0.29341800660895867\n",
            "training error 0.1249431274094943, test error 0.2506188893245047\n",
            "Loss: 0.2317543975612235\n",
            "training error 0.12495379278907767, test error 0.2506783206336347\n",
            "Loss: 0.2555231741139252\n",
            "training error 0.12499700386906232, test error 0.2506658712323065\n",
            "Loss: 0.2505441985080292\n",
            "training error 0.1249328530746471, test error 0.25070578711926306\n",
            "Loss: 0.26650803662402467\n",
            "training error 0.1250033069733318, test error 0.25077986660814783\n",
            "Loss: 0.29613516152195984\n",
            "training error 0.12515899687763968, test error 0.2506414055377816\n",
            "Loss: 0.24075946324166875\n",
            "training error 0.12501438927728678, test error 0.2506827238709442\n",
            "Loss: 0.2572841914169288\n",
            "training error 0.12507028150585447, test error 0.25064288971134735\n",
            "Loss: 0.2413530390919405\n",
            "training error 0.1249374901221679, test error 0.25062041790307477\n",
            "Loss: 0.23236573261351534\n",
            "training error 0.1250128981180789, test error 0.2505562266210036\n",
            "Loss: 0.20669326699702673\n",
            "training error 0.12504548126979723, test error 0.2506751194834875\n",
            "Loss: 0.2542429158851389\n",
            "training error 0.12504955648290653, test error 0.25065644676975396\n",
            "Loss: 0.24677500769227922\n",
            "training error 0.12502482665448533, test error 0.2504860078811483\n",
            "Loss: 0.17861019829357172\n",
            "training error 0.1249497125265062, test error 0.25054034600696945\n",
            "Loss: 0.20034202264413992\n",
            "training error 0.12499960610316191, test error 0.2505926764621393\n",
            "Loss: 0.22127090531627314\n",
            "training error 0.1249263889786457, test error 0.25059080495871977\n",
            "Loss: 0.2205224219453461\n",
            "training error 0.12498589657808598, test error 0.2505033243393214\n",
            "Loss: 0.18553568977317525\n",
            "training error 0.12504458365330637, test error 0.2504905695026059\n",
            "Loss: 0.18043455926981533\n",
            "training error 0.1250999884160614, test error 0.2506302123908673\n",
            "Loss: 0.23628291019544\n",
            "training error 0.12492722772591706, test error 0.2506099375842827\n",
            "Loss: 0.22817426587287137\n",
            "training error 0.12497778666674052, test error 0.25055037068233554\n",
            "Loss: 0.20435126074234145\n",
            "training error 0.1250757457666215, test error 0.25060978275696416\n",
            "Loss: 0.22811234470720976\n",
            "training error 0.12492647958483091, test error 0.2506019365476325\n",
            "Loss: 0.2249743556721473\n",
            "training error 0.12498608668604724, test error 0.2506906520229736\n",
            "Loss: 0.26045495236466376\n",
            "training error 0.12492111377283342, test error 0.2507058882782651\n",
            "Loss: 0.2665484938468632\n",
            "training error 0.12493920708862233, test error 0.2507161818920976\n",
            "Loss: 0.27066529037524933\n",
            "training error 0.12491205757563296, test error 0.2506517702292962\n",
            "Loss: 0.2449046863615134\n",
            "training error 0.12517987023756869, test error 0.2508576993049017\n",
            "Loss: 0.32726333293058296\n",
            "training error 0.12494472963413886, test error 0.2506284423371186\n",
            "Loss: 0.23557500029651735\n",
            "training error 0.12494911048344307, test error 0.2507132003363957\n",
            "Loss: 0.2694728560793447\n",
            "training error 0.12500281709195454, test error 0.250700660455275\n",
            "Loss: 0.264457694261111\n",
            "training error 0.12501790669689503, test error 0.250787586214133\n",
            "Loss: 0.29922251720067017\n",
            "training error 0.1250633849364093, test error 0.2507414880461877\n",
            "Loss: 0.28078615647733063\n",
            "training error 0.12490787403718762, test error 0.25064569846942003\n",
            "Loss: 0.24247636523093696\n",
            "training error 0.12494626167329077, test error 0.2505353654479017\n",
            "Loss: 0.19835011303754868\n",
            "training error 0.1250869114201165, test error 0.25065107063420683\n",
            "Loss: 0.2446248924347083\n",
            "training error 0.12502517710038988, test error 0.25045282322456913\n",
            "Loss: 0.1653384279295178\n",
            "training error 0.1249263777253732, test error 0.25059622871757226\n",
            "Loss: 0.22269158352243856\n",
            "training error 0.12497779527612499, test error 0.2507037353027153\n",
            "Loss: 0.26568743937041006\n",
            "training error 0.12490821749479178, test error 0.2505883489469318\n",
            "Loss: 0.21954017207985732\n",
            "training error 0.12499707020638684, test error 0.2505383069318428\n",
            "Loss: 0.19952652115562408\n",
            "training error 0.12494143788750181, test error 0.25060567995591804\n",
            "Loss: 0.22647148296728314\n",
            "training error 0.12488537936642305, test error 0.250607703221079\n",
            "Loss: 0.22728066146633363\n",
            "training error 0.12488869247512815, test error 0.25061578897449244\n",
            "Loss: 0.2305144530310388\n",
            "training error 0.1249590791821196, test error 0.25068959611378766\n",
            "Loss: 0.26003265526457486\n",
            "training error 0.12489406106542296, test error 0.25059381006272974\n",
            "Loss: 0.221724274079782\n",
            "training error 0.12490718315776532, test error 0.25058604124960326\n",
            "Loss: 0.2186172386470231\n",
            "training error 0.12494631700434669, test error 0.2506328374564587\n",
            "Loss: 0.23733277092359284\n",
            "training error 0.12491699297096878, test error 0.2505203873873194\n",
            "Loss: 0.19235983316006067\n",
            "training error 0.1248988636813399, test error 0.25055185643982314\n",
            "Loss: 0.20494547006149055\n",
            "training error 0.12486730238299928, test error 0.25055417956766957\n",
            "Loss: 0.20587457472858173\n",
            "training error 0.12488040203732766, test error 0.2505217709549237\n",
            "Loss: 0.19291317296885602\n",
            "training error 0.12486275124958363, test error 0.2505170676458972\n",
            "Loss: 0.19103214589837325\n",
            "training error 0.12487464662146164, test error 0.2505775570807204\n",
            "Loss: 0.2152241060143334\n",
            "training error 0.12489361632892877, test error 0.2506423165882601\n",
            "Loss: 0.24112382599201787\n",
            "training error 0.12485655639620694, test error 0.2506859465871915\n",
            "Loss: 0.25857307472600954\n",
            "training error 0.12487514557520335, test error 0.2506061705802594\n",
            "Loss: 0.22666770177033335\n",
            "training error 0.12484587149157246, test error 0.2505903880494475\n",
            "Loss: 0.2203556845222554\n",
            "training error 0.12485745141797057, test error 0.2506090824342132\n",
            "Loss: 0.22783225976166221\n",
            "training error 0.12491625400237392, test error 0.25061572003559757\n",
            "Loss: 0.23048688181963595\n",
            "training error 0.12484514736192129, test error 0.25074408997250547\n",
            "Loss: 0.2818267629549398\n",
            "training error 0.12486962671426435, test error 0.2506728476660139\n",
            "Loss: 0.25333433213208245\n",
            "training error 0.12483687114253704, test error 0.25074182817301993\n",
            "Loss: 0.28092218576547534\n",
            "training error 0.12491080256498428, test error 0.25068040147627624\n",
            "Loss: 0.2563553799749707\n",
            "training error 0.12483458734903234, test error 0.25059443304042067\n",
            "Loss: 0.22197342587788604\n",
            "training error 0.12490462478017114, test error 0.25054737173655206\n",
            "Loss: 0.20315187151020986\n",
            "training error 0.12486780736702394, test error 0.2506189056108071\n",
            "Loss: 0.23176091105532315\n",
            "training error 0.12482135591297507, test error 0.25058015448343846\n",
            "Loss: 0.21626290333727205\n",
            "training error 0.12494768855690062, test error 0.2505350223887521\n",
            "Loss: 0.19821291100730054\n",
            "training error 0.1248358063957788, test error 0.250471455895526\n",
            "Loss: 0.17279032153638685\n",
            "training error 0.12479798978188873, test error 0.25050959856668714\n",
            "Loss: 0.18804498513416146\n",
            "training error 0.12478923701056174, test error 0.2504636980368021\n",
            "Loss: 0.16968766717397887\n",
            "training error 0.1249227606972498, test error 0.2504458239213474\n",
            "Loss: 0.1625391479416205\n",
            "training error 0.12484244316235579, test error 0.2505442203528875\n",
            "Loss: 0.20189151673675187\n",
            "training error 0.12481191012929516, test error 0.25067036240495205\n",
            "Loss: 0.252340384401184\n",
            "training error 0.12479394302384243, test error 0.25059156257114135\n",
            "Loss: 0.22082541914709708\n",
            "training error 0.12480987777346295, test error 0.25060442900510943\n",
            "Loss: 0.22597118151534978\n",
            "training error 0.12475537043057185, test error 0.2505490611007686\n",
            "Loss: 0.2038275106836851\n",
            "training error 0.12490660163037848, test error 0.25055433048925424\n",
            "Loss: 0.20593493384697403\n",
            "training error 0.1247196913972419, test error 0.25052399763877725\n",
            "Loss: 0.19380370611954412\n",
            "training error 0.12473018722536398, test error 0.25046779402020897\n",
            "Loss: 0.17132580228806926\n",
            "training error 0.12471479941936188, test error 0.2504541139099411\n",
            "Loss: 0.16585462070157764\n",
            "training error 0.12473188641578468, test error 0.2505124972239626\n",
            "Loss: 0.1892042652861914\n",
            "training error 0.12467873140860983, test error 0.2504473550698663\n",
            "Loss: 0.1631515108114323\n",
            "training error 0.1247028220485027, test error 0.25046210122123924\n",
            "Loss: 0.1690490416268675\n",
            "training error 0.12474979339449546, test error 0.2504060332737373\n",
            "Loss: 0.14662539766794325\n",
            "training error 0.12470709202091818, test error 0.25041844807330715\n",
            "Loss: 0.1515905347521329\n",
            "training error 0.12470472078905075, test error 0.25033337905789776\n",
            "Loss: 0.11756829212574704\n",
            "training error 0.12473358786147568, test error 0.2503820153187443\n",
            "Loss: 0.1370197299847442\n",
            "training error 0.12481928241795469, test error 0.2502400603159367\n",
            "Loss: 0.08024667901771476\n",
            "training error 0.12469042106251024, test error 0.25045892576464174\n",
            "Loss: 0.1677790591980033\n",
            "training error 0.12471991922497222, test error 0.2503286207691509\n",
            "Loss: 0.11566527663351422\n",
            "training error 0.12468535247308501, test error 0.2502990021778712\n",
            "Loss: 0.10381970755151126\n",
            "training error 0.12463265671517318, test error 0.25026955399007134\n",
            "Loss: 0.09204228911763312\n",
            "training error 0.12465823424692411, test error 0.25004374313221095\n",
            "Loss: 0.0017321831780048313\n",
            "training error 0.12452332578700377, test error 0.25007173608160466\n",
            "Loss: 0.012927598001111207\n",
            "training error 0.12452830082572877, test error 0.2500316057799928\n",
            "Loss: 0.0\n",
            "training error 0.12449856401648704, test error 0.2500635045257605\n",
            "Loss: 0.012757885415393133\n",
            "training error 0.1245079723167334, test error 0.24998726435222524\n",
            "Loss: 0.0\n",
            "training error 0.1247605615460689, test error 0.24983840887344524\n",
            "Loss: 0.0\n",
            "training error 0.1244351108339117, test error 0.24993357177025663\n",
            "Loss: 0.0380897786054879\n",
            "training error 0.12443652490354727, test error 0.24996444367688087\n",
            "Loss: 0.05044652821954987\n",
            "training error 0.1244511263186631, test error 0.24983949177490056\n",
            "Loss: 0.000433440742830804\n",
            "training error 0.12451632676574197, test error 0.2498548746854723\n",
            "Loss: 0.006590584730870042\n",
            "training error 0.1244134265649538, test error 0.2496580716032884\n",
            "Loss: 0.0\n",
            "training error 0.12431754570130264, test error 0.24968902280129707\n",
            "Loss: 0.012397435344224661\n",
            "training error 0.12431743610102677, test error 0.2496130168885326\n",
            "Loss: 0.0\n",
            "training error 0.12430524747212131, test error 0.2495794302006416\n",
            "Loss: 0.0\n",
            "training error 0.12423937299401117, test error 0.2496193685901852\n",
            "Loss: 0.016002276113646907\n",
            "training error 0.1242757982975953, test error 0.24963695931023405\n",
            "Loss: 0.023050421080861305\n",
            "training error 0.12420821685437283, test error 0.24960634838607376\n",
            "Loss: 0.010785418257630575\n",
            "training error 0.12419125647394737, test error 0.24955468016625384\n",
            "Loss: 0.0\n",
            "training error 0.12419661228048236, test error 0.24953882080657672\n",
            "Loss: 0.0\n",
            "training error 0.1240857239433903, test error 0.24940251681414063\n",
            "Loss: 0.0\n",
            "training error 0.1240259634782602, test error 0.24919844742460126\n",
            "Loss: 0.0\n",
            "training error 0.123969195142341, test error 0.24913590201206964\n",
            "Loss: 0.0\n",
            "training error 0.12398423285538951, test error 0.2490457432385471\n",
            "Loss: 0.0\n",
            "training error 0.12389554972640905, test error 0.24898761567932198\n",
            "Loss: 0.0\n",
            "training error 0.12383460015167849, test error 0.24887092856773424\n",
            "Loss: 0.0\n",
            "training error 0.1238184778042689, test error 0.24886328692170034\n",
            "Loss: 0.0\n",
            "training error 0.12374311173040395, test error 0.248794242343393\n",
            "Loss: 0.0\n",
            "training error 0.12381938815086878, test error 0.24863379619513956\n",
            "Loss: 0.0\n",
            "training error 0.12366676402435753, test error 0.24852118727708686\n",
            "Loss: 0.0\n",
            "training error 0.12361626414014455, test error 0.24850327276699452\n",
            "Loss: 0.0\n",
            "training error 0.12351669926658881, test error 0.24833558640510783\n",
            "Loss: 0.0\n",
            "training error 0.1234659795968725, test error 0.24820478584807792\n",
            "Loss: 0.0\n",
            "training error 0.12341256367640761, test error 0.24797961917448524\n",
            "Loss: 0.0\n",
            "training error 0.12333853961442785, test error 0.24773004019303674\n",
            "Loss: 0.0\n",
            "training error 0.12339286475315699, test error 0.24765195410709043\n",
            "Loss: 0.0\n",
            "training error 0.12315522684362513, test error 0.24748132647995813\n",
            "Loss: 0.0\n",
            "training error 0.12311530776329611, test error 0.24730297248595312\n",
            "Loss: 0.0\n",
            "training error 0.12301539636523381, test error 0.24705320867257588\n",
            "Loss: 0.0\n",
            "training error 0.12299536219862939, test error 0.24688208029389697\n",
            "Loss: 0.0\n",
            "training error 0.12289104497572337, test error 0.2467935351605569\n",
            "Loss: 0.0\n",
            "training error 0.12275060194523663, test error 0.24646664699690102\n",
            "Loss: 0.0\n",
            "training error 0.12306248774749197, test error 0.24636231964433566\n",
            "Loss: 0.0\n",
            "training error 0.12272648331355383, test error 0.24590620928035983\n",
            "Loss: 0.0\n",
            "training error 0.12247852505504817, test error 0.2459106122609819\n",
            "Loss: 0.0017905121773686972\n",
            "training error 0.12230162812482172, test error 0.24566499764932595\n",
            "Loss: 0.0\n",
            "training error 0.12218769947657013, test error 0.24537330387993506\n",
            "Loss: 0.0\n",
            "training error 0.12215716446058024, test error 0.2452453613726379\n",
            "Loss: 0.0\n",
            "training error 0.1219682753936075, test error 0.24488831554781373\n",
            "Loss: 0.0\n",
            "training error 0.121800632555837, test error 0.244587754838772\n",
            "Loss: 0.0\n",
            "training error 0.12166035730452553, test error 0.2442769918022106\n",
            "Loss: 0.0\n",
            "training error 0.12158124911145107, test error 0.24400645227582557\n",
            "Loss: 0.0\n",
            "training error 0.12139545297286121, test error 0.24364540161987952\n",
            "Loss: 0.0\n",
            "training error 0.1213784878888561, test error 0.24333435799996464\n",
            "Loss: 0.0\n",
            "training error 0.12107169020962358, test error 0.24295288034210466\n",
            "Loss: 0.0\n",
            "training error 0.12092151500370077, test error 0.24264931056216857\n",
            "Loss: 0.0\n",
            "training error 0.12067778152629634, test error 0.24226392028727328\n",
            "Loss: 0.0\n",
            "training error 0.12051059632424425, test error 0.24202188099273755\n",
            "Loss: 0.0\n",
            "training error 0.12046026506474425, test error 0.24168990780961527\n",
            "Loss: 0.0\n",
            "training error 0.12015792184106043, test error 0.24110411074376883\n",
            "Loss: 0.0\n",
            "training error 0.11992393923033312, test error 0.24082008563834723\n",
            "Loss: 0.0\n",
            "training error 0.11965438024405052, test error 0.24035386370908463\n",
            "Loss: 0.0\n",
            "training error 0.11941411425303337, test error 0.2398287202656416\n",
            "Loss: 0.0\n",
            "training error 0.11923520405904094, test error 0.2393056130526661\n",
            "Loss: 0.0\n",
            "training error 0.11893246278588769, test error 0.23880462522500046\n",
            "Loss: 0.0\n",
            "training error 0.1186836367113177, test error 0.23833286780242516\n",
            "Loss: 0.0\n",
            "training error 0.11843246668379535, test error 0.23783119061526942\n",
            "Loss: 0.0\n",
            "training error 0.11825549171483284, test error 0.2372125569633196\n",
            "Loss: 0.0\n",
            "training error 0.11783245842448592, test error 0.23666647472486416\n",
            "Loss: 0.0\n",
            "training error 0.11757801887505086, test error 0.2360165813849173\n",
            "Loss: 0.0\n",
            "training error 0.11749747338733695, test error 0.23515210958273872\n",
            "Loss: 0.0\n",
            "training error 0.11691912665996454, test error 0.23466360815808807\n",
            "Loss: 0.0\n",
            "training error 0.11679973472866831, test error 0.23384770203048336\n",
            "Loss: 0.0\n",
            "training error 0.11633771728388405, test error 0.2332504060453746\n",
            "Loss: 0.0\n",
            "training error 0.11593363201671919, test error 0.2324276051991502\n",
            "Loss: 0.0\n",
            "training error 0.11554684339896312, test error 0.2317919014724768\n",
            "Loss: 0.0\n",
            "training error 0.11515384206008462, test error 0.23084865028018853\n",
            "Loss: 0.0\n",
            "training error 0.1147084325306257, test error 0.23004987523203668\n",
            "Loss: 0.0\n",
            "training error 0.11422662329759, test error 0.22914011705080453\n",
            "Loss: 0.0\n",
            "training error 0.11389342169188835, test error 0.2283220393848954\n",
            "Loss: 0.0\n",
            "training error 0.11346906998748536, test error 0.22732879247771617\n",
            "Loss: 0.0\n",
            "training error 0.11298210556990275, test error 0.22652402061583204\n",
            "Loss: 0.0\n",
            "training error 0.11256047638242095, test error 0.2254252466491787\n",
            "Loss: 0.0\n",
            "training error 0.11196253459301192, test error 0.22434053831562734\n",
            "Loss: 0.0\n",
            "training error 0.11143392410134799, test error 0.22318812329389037\n",
            "Loss: 0.0\n",
            "training error 0.11095366626690985, test error 0.22199343689759135\n",
            "Loss: 0.0\n",
            "training error 0.11037160837325924, test error 0.22098882121343855\n",
            "Loss: 0.0\n",
            "training error 0.10983624229908964, test error 0.2198543663029736\n",
            "Loss: 0.0\n",
            "training error 0.10927899558878204, test error 0.21872821618795965\n",
            "Loss: 0.0\n",
            "training error 0.10869810853052189, test error 0.21767430893784523\n",
            "Loss: 0.0\n",
            "training error 0.10809233432589256, test error 0.2163359259405134\n",
            "Loss: 0.0\n",
            "training error 0.10755074370214524, test error 0.21509012252482335\n",
            "Loss: 0.0\n",
            "training error 0.106849602308994, test error 0.21375206366574978\n",
            "Loss: 0.0\n",
            "training error 0.10625096890252685, test error 0.21226077677413413\n",
            "Loss: 0.0\n",
            "training error 0.10562446835612171, test error 0.21101466647754125\n",
            "Loss: 0.0\n",
            "training error 0.10488911608493758, test error 0.20963685626413836\n",
            "Loss: 0.0\n",
            "training error 0.10435900490479089, test error 0.20819643878535254\n",
            "Loss: 0.0\n",
            "training error 0.10353897163700775, test error 0.20675864022047422\n",
            "Loss: 0.0\n",
            "training error 0.10284570787858735, test error 0.2051629868224797\n",
            "Loss: 0.0\n",
            "training error 0.10215924935595874, test error 0.20354053855069706\n",
            "Loss: 0.0\n",
            "training error 0.10155289873446983, test error 0.2021934690303442\n",
            "Loss: 0.0\n",
            "training error 0.1005912674745, test error 0.20056117622787503\n",
            "Loss: 0.0\n",
            "training error 0.09979522643218267, test error 0.1989515867837331\n",
            "Loss: 0.0\n",
            "training error 0.09906471961095699, test error 0.19726384119880663\n",
            "Loss: 0.0\n",
            "training error 0.09828149738707338, test error 0.19556471712165435\n",
            "Loss: 0.0\n",
            "training error 0.09746208865768545, test error 0.19396381156051581\n",
            "Loss: 0.0\n",
            "training error 0.09671688624763282, test error 0.19214220794826728\n",
            "Loss: 0.0\n",
            "training error 0.09586720277929382, test error 0.19060124593364827\n",
            "Loss: 0.0\n",
            "training error 0.0950660230565032, test error 0.18884324433442154\n",
            "Loss: 0.0\n",
            "training error 0.09434955226223851, test error 0.1870284232453828\n",
            "Loss: 0.0\n",
            "training error 0.09358139531689937, test error 0.1853264028379015\n",
            "Loss: 0.0\n",
            "training error 0.09258882918679599, test error 0.18360781181109148\n",
            "Loss: 0.0\n",
            "training error 0.09179499120356426, test error 0.1818903104038831\n",
            "Loss: 0.0\n",
            "training error 0.09097776227619594, test error 0.180163673395611\n",
            "Loss: 0.0\n",
            "training error 0.09013970526694291, test error 0.1784800398071052\n",
            "Loss: 0.0\n",
            "training error 0.08926573935681267, test error 0.17672879205044648\n",
            "Loss: 0.0\n",
            "training error 0.0883680124022491, test error 0.17488902577504248\n",
            "Loss: 0.0\n",
            "training error 0.08763357113100394, test error 0.17327282064530222\n",
            "Loss: 0.0\n",
            "training error 0.08672232818703594, test error 0.17138259792960328\n",
            "Loss: 0.0\n",
            "training error 0.08602361132565639, test error 0.16935187999561727\n",
            "Loss: 0.0\n",
            "training error 0.08511578595597535, test error 0.1676073760405685\n",
            "Loss: 0.0\n",
            "training error 0.08420024747897563, test error 0.16584666350122249\n",
            "Loss: 0.0\n",
            "training error 0.0834198101254326, test error 0.16429839919622732\n",
            "Loss: 0.0\n",
            "training error 0.08251371941586351, test error 0.162342735149673\n",
            "Loss: 0.0\n",
            "training error 0.08173279815120646, test error 0.16059416743561677\n",
            "Loss: 0.0\n",
            "training error 0.08089291791247248, test error 0.1587573693561209\n",
            "Loss: 0.0\n",
            "training error 0.08015551847412321, test error 0.15702717371262231\n",
            "Loss: 0.0\n",
            "training error 0.07945993036068454, test error 0.15531253716418364\n",
            "Loss: 0.0\n",
            "training error 0.07865693329694404, test error 0.1536995384443285\n",
            "Loss: 0.0\n",
            "training error 0.07778940345522997, test error 0.15184549473312753\n",
            "Loss: 0.0\n",
            "training error 0.07702660599434578, test error 0.1502923866768188\n",
            "Loss: 0.0\n",
            "training error 0.07617592231433665, test error 0.14861464040926797\n",
            "Loss: 0.0\n",
            "training error 0.07545362937534016, test error 0.14697182126996397\n",
            "Loss: 0.0\n",
            "training error 0.07461865241521867, test error 0.14511654204320404\n",
            "Loss: 0.0\n",
            "training error 0.07391857631299845, test error 0.14351831279034272\n",
            "Loss: 0.0\n",
            "training error 0.07324511298569146, test error 0.14192276014721233\n",
            "Loss: 0.0\n",
            "training error 0.07244564648515993, test error 0.14026802904644692\n",
            "Loss: 0.0\n",
            "training error 0.07178457253299414, test error 0.13865905017631738\n",
            "Loss: 0.0\n",
            "training error 0.07103481809995793, test error 0.13721352902976797\n",
            "Loss: 0.0\n",
            "training error 0.07028182505692747, test error 0.13569496510749302\n",
            "Loss: 0.0\n",
            "training error 0.06974750751582569, test error 0.13427681181334677\n",
            "Loss: 0.0\n",
            "training error 0.0689650339993317, test error 0.13280091002490482\n",
            "Loss: 0.0\n",
            "training error 0.06828700313537538, test error 0.13124093347772742\n",
            "Loss: 0.0\n",
            "training error 0.06772341876263886, test error 0.12975056537074517\n",
            "Loss: 0.0\n",
            "training error 0.06696322878699354, test error 0.12836606741259968\n",
            "Loss: 0.0\n",
            "training error 0.06635201973340964, test error 0.12703372727762266\n",
            "Loss: 0.0\n",
            "training error 0.0657624813028533, test error 0.1257258657993704\n",
            "Loss: 0.0\n",
            "training error 0.06510658310238442, test error 0.12428359313813311\n",
            "Loss: 0.0\n",
            "training error 0.06458791817032468, test error 0.12283880843551141\n",
            "Loss: 0.0\n",
            "training error 0.06390129431759585, test error 0.1216311291691274\n",
            "Loss: 0.0\n",
            "training error 0.06334444043325661, test error 0.12032865822401206\n",
            "Loss: 0.0\n",
            "training error 0.0627322686979098, test error 0.11905811907417925\n",
            "Loss: 0.0\n",
            "training error 0.06219355526418155, test error 0.11788991120381033\n",
            "Loss: 0.0\n",
            "training error 0.06183120368317278, test error 0.11651357733166802\n",
            "Loss: 0.0\n",
            "training error 0.061081215030889785, test error 0.11549561207290554\n",
            "Loss: 0.0\n",
            "training error 0.060553707961436054, test error 0.11439707687134294\n",
            "Loss: 0.0\n",
            "training error 0.06007819311046721, test error 0.11339089933169864\n",
            "Loss: 0.0\n",
            "training error 0.05951765208472716, test error 0.11217231342354063\n",
            "Loss: 0.0\n",
            "training error 0.05902018874651675, test error 0.11094505797081483\n",
            "Loss: 0.0\n",
            "training error 0.058529273631359804, test error 0.10981436058718896\n",
            "Loss: 0.0\n",
            "training error 0.058041722373125874, test error 0.10868459954609153\n",
            "Loss: 0.0\n",
            "training error 0.05758703632224598, test error 0.10746795483209788\n",
            "Loss: 0.0\n",
            "training error 0.05721608307993719, test error 0.10642177398725892\n",
            "Loss: 0.0\n",
            "training error 0.05673666134312509, test error 0.10512732422967495\n",
            "Loss: 0.0\n",
            "training error 0.056208020172010445, test error 0.10410006919231654\n",
            "Loss: 0.0\n",
            "training error 0.05577546415950227, test error 0.10326814472795598\n",
            "Loss: 0.0\n",
            "training error 0.05538439931059319, test error 0.1021661244217452\n",
            "Loss: 0.0\n",
            "training error 0.05491662015008953, test error 0.1012728041152399\n",
            "Loss: 0.0\n",
            "training error 0.05450626401260683, test error 0.10035090225956085\n",
            "Loss: 0.0\n",
            "training error 0.05412734579570693, test error 0.0994889017900507\n",
            "Loss: 0.0\n",
            "training error 0.05370504824445794, test error 0.09853655217483916\n",
            "Loss: 0.0\n",
            "training error 0.05331318349086487, test error 0.09781887834301942\n",
            "Loss: 0.0\n",
            "training error 0.05292621410661966, test error 0.09687726505949726\n",
            "Loss: 0.0\n",
            "training error 0.05253983612931619, test error 0.09588911533067807\n",
            "Loss: 0.0\n",
            "training error 0.05223072992006751, test error 0.09520496090589076\n",
            "Loss: 0.0\n",
            "training error 0.05186182746642583, test error 0.0942683864995792\n",
            "Loss: 0.0\n",
            "training error 0.05152535565030034, test error 0.0935112597424287\n",
            "Loss: 0.0\n",
            "training error 0.051200162558284176, test error 0.09283897789436832\n",
            "Loss: 0.0\n",
            "training error 0.05080766957732592, test error 0.09201995331732547\n",
            "Loss: 0.0\n",
            "training error 0.05051030677557306, test error 0.09124172320783568\n",
            "Loss: 0.0\n",
            "training error 0.05021637775133095, test error 0.09062987315890475\n",
            "Loss: 0.0\n",
            "training error 0.04991473840566795, test error 0.08997299883189176\n",
            "Loss: 0.0\n",
            "training error 0.04964463527462512, test error 0.08914080685419086\n",
            "Loss: 0.0\n",
            "training error 0.049328839184019715, test error 0.08820932165935841\n",
            "Loss: 0.0\n",
            "training error 0.04901771906704053, test error 0.08746652532739314\n",
            "Loss: 0.0\n",
            "training error 0.04879696615479378, test error 0.08683138224626565\n",
            "Loss: 0.0\n",
            "training error 0.048467014093632174, test error 0.08625351770740622\n",
            "Loss: 0.0\n",
            "training error 0.04826997211890384, test error 0.08557529280753075\n",
            "Loss: 0.0\n",
            "training error 0.04790078924289503, test error 0.08521587838439242\n",
            "Loss: 0.0\n",
            "training error 0.047639874542553355, test error 0.08455758775431517\n",
            "Loss: 0.0\n",
            "training error 0.04738479831147698, test error 0.08403981567238317\n",
            "Loss: 0.0\n",
            "training error 0.047222301067529115, test error 0.08349973101246677\n",
            "Loss: 0.0\n",
            "training error 0.04691605745996265, test error 0.0828539657017192\n",
            "Loss: 0.0\n",
            "training error 0.04666814015985422, test error 0.08217112223504418\n",
            "Loss: 0.0\n",
            "training error 0.04646686597696404, test error 0.08169582216312378\n",
            "Loss: 0.0\n",
            "training error 0.04625012489077138, test error 0.08120481504628731\n",
            "Loss: 0.0\n",
            "training error 0.04604869440884181, test error 0.08066364365593437\n",
            "Loss: 0.0\n",
            "training error 0.04576843275720391, test error 0.07992383143330531\n",
            "Loss: 0.0\n",
            "training error 0.04552136514586785, test error 0.07942413846401353\n",
            "Loss: 0.0\n",
            "training error 0.04538529149672957, test error 0.07895500805281852\n",
            "Loss: 0.0\n",
            "training error 0.04514767071029938, test error 0.07851122675168579\n",
            "Loss: 0.0\n",
            "training error 0.04497001349855333, test error 0.07809869002941011\n",
            "Loss: 0.0\n",
            "training error 0.04476101321498229, test error 0.07757246847237949\n",
            "Loss: 0.0\n",
            "training error 0.04453372391040399, test error 0.07719699292072256\n",
            "Loss: 0.0\n",
            "training error 0.044363671727743366, test error 0.07678296550473873\n",
            "Loss: 0.0\n",
            "training error 0.044151399801228564, test error 0.07603415776454141\n",
            "Loss: 0.0\n",
            "training error 0.043970929522550835, test error 0.07569984064512404\n",
            "Loss: 0.0\n",
            "training error 0.043808338596411825, test error 0.07534440543899361\n",
            "Loss: 0.0\n",
            "training error 0.04361255589986667, test error 0.07493624722982738\n",
            "Loss: 0.0\n",
            "training error 0.0434524679289055, test error 0.07456673682260896\n",
            "Loss: 0.0\n",
            "training error 0.04332683815866244, test error 0.07418484709270991\n",
            "Loss: 0.0\n",
            "training error 0.04315633942703884, test error 0.07383676769002144\n",
            "Loss: 0.0\n",
            "training error 0.0429874159429219, test error 0.07332760366183186\n",
            "Loss: 0.0\n",
            "training error 0.04284233407603827, test error 0.0728737059057881\n",
            "Loss: 0.0\n",
            "training error 0.042711573182536676, test error 0.07236408725055833\n",
            "Loss: 0.0\n",
            "training error 0.04251683915950709, test error 0.07213799806215278\n",
            "Loss: 0.0\n",
            "training error 0.04240112294232493, test error 0.07171283021993426\n",
            "Loss: 0.0\n",
            "training error 0.04224502313809657, test error 0.07151663812866521\n",
            "Loss: 0.0\n",
            "training error 0.04213143816939826, test error 0.07114392439658897\n",
            "Loss: 0.0\n",
            "training error 0.04199515147015932, test error 0.07071269486866358\n",
            "Loss: 0.0\n",
            "training error 0.04184694297530602, test error 0.07056309603275321\n",
            "Loss: 0.0\n",
            "training error 0.04173434991214116, test error 0.07019125438987421\n",
            "Loss: 0.0\n",
            "training error 0.04162357166276675, test error 0.06975086487177422\n",
            "Loss: 0.0\n",
            "training error 0.0414643298823376, test error 0.06969644192811865\n",
            "Loss: 0.0\n",
            "training error 0.04133016285758213, test error 0.06935885427687358\n",
            "Loss: 0.0\n",
            "training error 0.04120876758621413, test error 0.069298994826647\n",
            "Loss: 0.0\n",
            "training error 0.04105931470984478, test error 0.06901506172255506\n",
            "Loss: 0.0\n",
            "training error 0.041003111512145377, test error 0.06868559912085555\n",
            "Loss: 0.0\n",
            "training error 0.04084590398977366, test error 0.0682102026485206\n",
            "Loss: 0.0\n",
            "training error 0.04075371695311973, test error 0.06789215443318183\n",
            "Loss: 0.0\n",
            "training error 0.040623725671395064, test error 0.06748363289763204\n",
            "Loss: 0.0\n",
            "training error 0.04050994977364495, test error 0.06721368814705368\n",
            "Loss: 0.0\n",
            "training error 0.04043852200911889, test error 0.06692508657361465\n",
            "Loss: 0.0\n",
            "training error 0.04032362592312865, test error 0.06679531576967307\n",
            "Loss: 0.0\n",
            "training error 0.040218048666912816, test error 0.0665797394798461\n",
            "Loss: 0.0\n",
            "training error 0.04012518026253216, test error 0.06641749051890748\n",
            "Loss: 0.0\n",
            "training error 0.040097833967198494, test error 0.06609365116597964\n",
            "Loss: 0.0\n",
            "training error 0.039910408901548206, test error 0.0658692770209611\n",
            "Loss: 0.0\n",
            "training error 0.03986067417658785, test error 0.06570142435298304\n",
            "Loss: 0.0\n",
            "training error 0.03973820835171441, test error 0.06538374655511663\n",
            "Loss: 0.0\n",
            "training error 0.03965151287293493, test error 0.06519239057512238\n",
            "Loss: 0.0\n",
            "training error 0.03960386110470301, test error 0.06500637291461372\n",
            "Loss: 0.0\n",
            "training error 0.03951973217818869, test error 0.06491788649945619\n",
            "Loss: 0.0\n",
            "training error 0.039420646313403834, test error 0.06467887227636826\n",
            "Loss: 0.0\n",
            "training error 0.03936131003106436, test error 0.0643837611088686\n",
            "Loss: 0.0\n",
            "training error 0.03930327775198343, test error 0.06433301646374441\n",
            "Loss: 0.0\n",
            "training error 0.03916064215609565, test error 0.06410490266928708\n",
            "Loss: 0.0\n",
            "training error 0.03913797905532059, test error 0.0639361640225389\n",
            "Loss: 0.0\n",
            "training error 0.039033782216054025, test error 0.06379460616577627\n",
            "Loss: 0.0\n",
            "training error 0.03901375261140035, test error 0.06352184001547136\n",
            "Loss: 0.0\n",
            "training error 0.03886112425293657, test error 0.06320807054571813\n",
            "Loss: 0.0\n",
            "training error 0.038902805079712, test error 0.06305613801621474\n",
            "Loss: 0.0\n",
            "training error 0.03873759081539442, test error 0.06272717671433552\n",
            "Loss: 0.0\n",
            "training error 0.03868217010805915, test error 0.06254224072946919\n",
            "Loss: 0.0\n",
            "training error 0.03860797249943634, test error 0.06250639214729649\n",
            "Loss: 0.0\n",
            "training error 0.03859748459031334, test error 0.06267072278779812\n",
            "Loss: 0.26290213665569606\n",
            "training error 0.038507929968279946, test error 0.062310382282457014\n",
            "Loss: 0.0\n",
            "training error 0.038431851276960104, test error 0.06220273575849002\n",
            "Loss: 0.0\n",
            "training error 0.03832749469916513, test error 0.06216992676990465\n",
            "Loss: 0.0\n",
            "training error 0.03828048298484153, test error 0.061892217870225676\n",
            "Loss: 0.0\n",
            "training error 0.03824864391437128, test error 0.06186833356670304\n",
            "Loss: 0.0\n",
            "training error 0.0381747956250334, test error 0.06164515191759515\n",
            "Loss: 0.0\n",
            "training error 0.038087165477788676, test error 0.061615688409714826\n",
            "Loss: 0.0\n",
            "training error 0.03805642085912237, test error 0.06147870550529362\n",
            "Loss: 0.0\n",
            "training error 0.0379917300793778, test error 0.06134803936571482\n",
            "Loss: 0.0\n",
            "training error 0.037942137665581556, test error 0.06106247308731126\n",
            "Loss: 0.0\n",
            "training error 0.03793906888158375, test error 0.061023999002420753\n",
            "Loss: 0.0\n",
            "training error 0.03780628851581076, test error 0.060934983586154755\n",
            "Loss: 0.0\n",
            "training error 0.03781433517004124, test error 0.060618919769089494\n",
            "Loss: 0.0\n",
            "training error 0.0377088750474069, test error 0.06049152882511361\n",
            "Loss: 0.0\n",
            "training error 0.03766621036194526, test error 0.06047813053101602\n",
            "Loss: 0.0\n",
            "training error 0.03763035731627485, test error 0.06031231705472706\n",
            "Loss: 0.0\n",
            "training error 0.037564185064461575, test error 0.060152505422014314\n",
            "Loss: 0.0\n",
            "training error 0.03754098225737443, test error 0.06021511316319282\n",
            "Loss: 0.10408168494273617\n",
            "training error 0.03747595783841327, test error 0.06008524864780748\n",
            "Loss: 0.0\n",
            "training error 0.03743666080783045, test error 0.05997288314633383\n",
            "Loss: 0.0\n",
            "training error 0.037433975003166364, test error 0.059938760267367906\n",
            "Loss: 0.0\n",
            "training error 0.03735662177756111, test error 0.05983586349988088\n",
            "Loss: 0.0\n",
            "training error 0.03731187198583894, test error 0.05978077815937956\n",
            "Loss: 0.0\n",
            "training error 0.03730014669177301, test error 0.059779354027775365\n",
            "Loss: 0.0\n",
            "training error 0.037245075145322465, test error 0.059678989217747926\n",
            "Loss: 0.0\n",
            "training error 0.03718842393317882, test error 0.05932852026544321\n",
            "Loss: 0.0\n",
            "training error 0.03714281622494621, test error 0.05926235594291257\n",
            "Loss: 0.0\n",
            "training error 0.037100181267306596, test error 0.059162283353893355\n",
            "Loss: 0.0\n",
            "training error 0.03707324382287998, test error 0.059237941368152774\n",
            "Loss: 0.1278821742001668\n",
            "training error 0.037025608962167625, test error 0.059150521185103215\n",
            "Loss: 0.0\n",
            "training error 0.03697979521953349, test error 0.05893913272131358\n",
            "Loss: 0.0\n",
            "training error 0.03700758498829016, test error 0.05880456515265553\n",
            "Loss: 0.0\n",
            "training error 0.0369306715012952, test error 0.05862925059058904\n",
            "Loss: 0.0\n",
            "training error 0.036886959188190176, test error 0.05852435054016872\n",
            "Loss: 0.0\n",
            "training error 0.036887628920740025, test error 0.05858781627714609\n",
            "Loss: 0.10844329991122059\n",
            "training error 0.03684267046851546, test error 0.058378253057861\n",
            "Loss: 0.0\n",
            "training error 0.03682595862115588, test error 0.05835557912002749\n",
            "Loss: 0.0\n",
            "training error 0.03672820361390082, test error 0.058390090854179635\n",
            "Loss: 0.0591404192582079\n",
            "training error 0.03673668835080378, test error 0.058286761314869366\n",
            "Loss: 0.0\n",
            "training error 0.036695751666101305, test error 0.05829221881573473\n",
            "Loss: 0.009363191129940773\n",
            "training error 0.036673478916364244, test error 0.05838860961759823\n",
            "Loss: 0.1747365961520364\n",
            "training error 0.03663636478034513, test error 0.05826830907121123\n",
            "Loss: 0.0\n",
            "training error 0.03658108584357576, test error 0.05813544347813953\n",
            "Loss: 0.0\n",
            "training error 0.0365528308789975, test error 0.05816047396596221\n",
            "Loss: 0.04305546896203882\n",
            "training error 0.03657252112622908, test error 0.05810182332852779\n",
            "Loss: 0.0\n",
            "training error 0.036533962467053205, test error 0.05790242060402306\n",
            "Loss: 0.0\n",
            "training error 0.03648337643397589, test error 0.05776655939218252\n",
            "Loss: 0.0\n",
            "training error 0.036486991917826846, test error 0.05775289268264101\n",
            "Loss: 0.0\n",
            "training error 0.03651475432420103, test error 0.057741069104823045\n",
            "Loss: 0.0\n",
            "training error 0.03638471243111498, test error 0.05764212285844962\n",
            "Loss: 0.0\n",
            "training error 0.036406842115092944, test error 0.05760130760578648\n",
            "Loss: 0.0\n",
            "training error 0.0363369425805058, test error 0.05759424823966243\n",
            "Loss: 0.0\n",
            "training error 0.036317968515563924, test error 0.05758424940129704\n",
            "Loss: 0.0\n",
            "training error 0.03628515063515976, test error 0.05742542699795994\n",
            "Loss: 0.0\n",
            "training error 0.03626322873446389, test error 0.0574256434348337\n",
            "Loss: 0.00037690076517549187\n",
            "training error 0.03626258563384163, test error 0.057341323039073196\n",
            "Loss: 0.0\n",
            "training error 0.03625106780094092, test error 0.057014386959512296\n",
            "Loss: 0.0\n",
            "training error 0.036219750480812066, test error 0.05705480932292278\n",
            "Loss: 0.07089853204802488\n",
            "training error 0.03617299774698673, test error 0.057034144659203964\n",
            "Loss: 0.034653884300639426\n",
            "training error 0.0361354948856573, test error 0.05695657645233224\n",
            "Loss: 0.0\n",
            "training error 0.03618967511764398, test error 0.05691960513423084\n",
            "Loss: 0.0\n",
            "training error 0.03610091297760647, test error 0.056807348686010485\n",
            "Loss: 0.0\n",
            "training error 0.036164456017573184, test error 0.056606394372802\n",
            "Loss: 0.0\n",
            "training error 0.036131857712466534, test error 0.05658709542350373\n",
            "Loss: 0.0\n",
            "training error 0.03604835374820529, test error 0.05658933585041208\n",
            "Loss: 0.003959254122487543\n",
            "training error 0.03600870027897479, test error 0.05660206823809046\n",
            "Loss: 0.026459768741760037\n",
            "training error 0.036010655068067804, test error 0.05653297258741083\n",
            "Loss: 0.0\n",
            "training error 0.0359438674438504, test error 0.0565782819159996\n",
            "Loss: 0.08014672944840484\n",
            "training error 0.03596083118262459, test error 0.05652195890731939\n",
            "Loss: 0.0\n",
            "training error 0.035927846403813725, test error 0.05643716145710321\n",
            "Loss: 0.0\n",
            "training error 0.03591288870944795, test error 0.05656426468813518\n",
            "Loss: 0.22521194856437443\n",
            "training error 0.03586023809965201, test error 0.05647014255680326\n",
            "Loss: 0.05843862244050424\n",
            "training error 0.03584780301185019, test error 0.05641161217428368\n",
            "Loss: 0.0\n",
            "training error 0.03584058166788446, test error 0.05641937112245698\n",
            "Loss: 0.013754168466828531\n",
            "training error 0.035844945920219314, test error 0.056426838816403056\n",
            "Loss: 0.026992035030537487\n",
            "training error 0.03583086722027736, test error 0.05628305759431175\n",
            "Loss: 0.0\n",
            "training error 0.03578876889132762, test error 0.05639149233391467\n",
            "Loss: 0.19265964614878506\n",
            "training error 0.03579945561910091, test error 0.05636521583910195\n",
            "Loss: 0.14597331470938002\n",
            "training error 0.03574681057563817, test error 0.05622717051436191\n",
            "Loss: 0.0\n",
            "training error 0.03575856699772269, test error 0.05616223522653504\n",
            "Loss: 0.0\n",
            "training error 0.03571788857923095, test error 0.056184165639691634\n",
            "Loss: 0.03904832681274062\n",
            "training error 0.035718754000882105, test error 0.05616760114983823\n",
            "Loss: 0.009554326464300544\n",
            "training error 0.035720978005160345, test error 0.05588057959348902\n",
            "Loss: 0.0\n",
            "training error 0.03572697684713816, test error 0.05592941630539036\n",
            "Loss: 0.08739478412107538\n",
            "training error 0.03568113110816214, test error 0.055810653555715364\n",
            "Loss: 0.0\n",
            "training error 0.03567416803614719, test error 0.05600558586130201\n",
            "Loss: 0.34927436460145955\n",
            "training error 0.03567584088274893, test error 0.055797539424038436\n",
            "Loss: 0.0\n",
            "training error 0.03561031580830183, test error 0.05590905331958323\n",
            "Loss: 0.19985450379331748\n",
            "training error 0.035594598573508565, test error 0.05588759927101136\n",
            "Loss: 0.1614046925770385\n",
            "training error 0.03564311150646265, test error 0.05593411305334572\n",
            "Loss: 0.24476640138084704\n",
            "training error 0.03556861813850774, test error 0.05582520996576553\n",
            "Loss: 0.04959097123764522\n",
            "training error 0.035554081442184596, test error 0.05583996753546357\n",
            "Loss: 0.07603939503979529\n",
            "training error 0.035552436282140755, test error 0.055787829007521404\n",
            "Loss: 0.0\n",
            "training error 0.03554114817907453, test error 0.05562786567152525\n",
            "Loss: 0.0\n",
            "training error 0.03553615385982008, test error 0.055799398739267564\n",
            "Loss: 0.3083581684675707\n",
            "training error 0.03551064976868363, test error 0.05564695072025253\n",
            "Loss: 0.034308432467966554\n",
            "training error 0.035481340857193844, test error 0.05565116325182557\n",
            "Loss: 0.041881132808319066\n",
            "training error 0.03555489660841181, test error 0.05573632707832403\n",
            "Loss: 0.1949767539873415\n",
            "training error 0.035464780321039974, test error 0.05540944236826854\n",
            "Loss: 0.0\n",
            "training error 0.03543689152309921, test error 0.05548282350009899\n",
            "Loss: 0.1324343445702647\n",
            "training error 0.0354585218095178, test error 0.05553969553100298\n",
            "Loss: 0.23507394618544897\n",
            "training error 0.03539843416582622, test error 0.05545226768305764\n",
            "Loss: 0.07728883915574247\n",
            "training error 0.03546520839684775, test error 0.05551782000302984\n",
            "Loss: 0.19559416252739048\n",
            "training error 0.035438371282436446, test error 0.05547303156304785\n",
            "Loss: 0.11476237995082794\n",
            "training error 0.03540656600320653, test error 0.05553454408845599\n",
            "Loss: 0.2257768980167496\n",
            "training error 0.03539741666031112, test error 0.05557219034744297\n",
            "Loss: 0.29371885407682985\n",
            "training error 0.035355503201925834, test error 0.055713350762483474\n",
            "Loss: 0.5484776262411506\n",
            "training error 0.03534545811587472, test error 0.05567264321054343\n",
            "Loss: 0.4750108122828278\n",
            "training error 0.03532726289185209, test error 0.05559183324284211\n",
            "Loss: 0.32916930179760495\n",
            "training error 0.035409602682327894, test error 0.055437013573255006\n",
            "Loss: 0.04975903710278118\n",
            "training error 0.03535230465331636, test error 0.055365834856855485\n",
            "Loss: 0.0\n",
            "training error 0.035311737026234545, test error 0.05552179485054517\n",
            "Loss: 0.2816899521029592\n",
            "training error 0.03532763688542307, test error 0.05546747414665919\n",
            "Loss: 0.18357763423324513\n",
            "training error 0.035303104901878074, test error 0.055545089721328114\n",
            "Loss: 0.32376440260690753\n",
            "training error 0.03526798086388926, test error 0.05550303776271791\n",
            "Loss: 0.247811500029127\n",
            "training error 0.03528285075790108, test error 0.05544159129209286\n",
            "Loss: 0.13682885019838942\n",
            "training error 0.035269421243916244, test error 0.05552948678047062\n",
            "Loss: 0.29558286990207794\n",
            "training error 0.03525358205947649, test error 0.05541355318942266\n",
            "Loss: 0.0861873259755841\n",
            "training error 0.03531886807040487, test error 0.05501677082969229\n",
            "Loss: 0.0\n",
            "training error 0.03519265817445054, test error 0.05517058516923316\n",
            "Loss: 0.2795771856858309\n",
            "training error 0.03520505595860965, test error 0.05526284087346428\n",
            "Loss: 0.4472636980707545\n",
            "training error 0.03521626103746146, test error 0.05526288954843775\n",
            "Loss: 0.44735217104496794\n",
            "training error 0.03516803822669811, test error 0.05521910648618946\n",
            "Loss: 0.3677708695835946\n",
            "training error 0.0351886602885824, test error 0.055233191152497384\n",
            "Loss: 0.39337154751419856\n",
            "training error 0.03519306889942023, test error 0.0552680724717269\n",
            "Loss: 0.456772795358229\n",
            "training error 0.03515706557353969, test error 0.05519661496516495\n",
            "Loss: 0.32688966066254377\n",
            "training error 0.03516544020744694, test error 0.055094340348597455\n",
            "Loss: 0.14099249689751847\n",
            "training error 0.03518466532467787, test error 0.0551959765064613\n",
            "Loss: 0.32572918051434474\n",
            "training error 0.03519184108740085, test error 0.05513962444406161\n",
            "Loss: 0.2233021177299399\n",
            "training error 0.03514697397002855, test error 0.05509595278510951\n",
            "Loss: 0.14392330597217118\n",
            "training error 0.03511257284672556, test error 0.05497670038467161\n",
            "Loss: 0.0\n",
            "training error 0.035116908020815525, test error 0.05505254723649459\n",
            "Loss: 0.13796181162615273\n",
            "training error 0.035109946378990106, test error 0.05499661824110871\n",
            "Loss: 0.036229632367423825\n",
            "training error 0.03513642547532072, test error 0.05508518126748602\n",
            "Loss: 0.19732155996152745\n",
            "training error 0.03509841588738946, test error 0.05493963187601361\n",
            "Loss: 0.0\n",
            "training error 0.0351170315661107, test error 0.05499786377997157\n",
            "Loss: 0.10599252665066672\n",
            "training error 0.03510292996467504, test error 0.055045486737689844\n",
            "Loss: 0.1926748652323118\n",
            "training error 0.035111909236670694, test error 0.054802073211297216\n",
            "Loss: 0.0\n",
            "training error 0.03506675157280791, test error 0.05472918441414471\n",
            "Loss: 0.0\n",
            "training error 0.03507788246140474, test error 0.054853804802197134\n",
            "Loss: 0.22770371856704053\n",
            "training error 0.03505179100901088, test error 0.05488700183258159\n",
            "Loss: 0.2883606253706317\n",
            "training error 0.03503477616333814, test error 0.05500122529620254\n",
            "Loss: 0.4970673050766816\n",
            "training error 0.035099082891037775, test error 0.05478687356545301\n",
            "Loss: 0.10540838846009315\n",
            "training error 0.03505451435413434, test error 0.054845503139385424\n",
            "Loss: 0.2125350971074358\n",
            "training error 0.034992460954493494, test error 0.05502313094173604\n",
            "Loss: 0.5370928339932579\n",
            "training error 0.03499518921703666, test error 0.05498060408137972\n",
            "Loss: 0.45938866059553884\n",
            "training error 0.03501912605783773, test error 0.05496849961786453\n",
            "Loss: 0.4372716426922718\n",
            "training error 0.03506356910672431, test error 0.05487182744430091\n",
            "Loss: 0.2606343063269456\n",
            "training error 0.0350425480806433, test error 0.05483312982264816\n",
            "Loss: 0.18992683632350627\n",
            "training error 0.034969134906345165, test error 0.05501853235087135\n",
            "Loss: 0.5286903867177895\n",
            "training error 0.03496457096117323, test error 0.05507075900307894\n",
            "Loss: 0.6241178131752845\n",
            "training error 0.03500584565679917, test error 0.05500982969902659\n",
            "Loss: 0.5127890866382989\n",
            "training error 0.035003376322377835, test error 0.05511702266911293\n",
            "Loss: 0.708649798311245\n",
            "training error 0.03495305516987853, test error 0.05502836427265976\n",
            "Loss: 0.5466550647842627\n",
            "training error 0.034943525123950575, test error 0.055039637790987375\n",
            "Loss: 0.5672537973403946\n",
            "training error 0.03498460733214258, test error 0.05501926122610904\n",
            "Loss: 0.5300221720266673\n",
            "training error 0.03495824277709473, test error 0.054988967015082935\n",
            "Loss: 0.4746692349230086\n",
            "training error 0.0350161088430999, test error 0.05510394260325775\n",
            "Loss: 0.6847501805931966\n",
            "training error 0.03499734160054092, test error 0.05487301535825415\n",
            "Loss: 0.2628048373990888\n",
            "training error 0.0349061380276235, test error 0.054955794132936164\n",
            "Loss: 0.4140564512649325\n",
            "training error 0.03491134546979473, test error 0.05486307552523802\n",
            "Loss: 0.24464298623587677\n",
            "training error 0.034933493664793314, test error 0.05497659849120442\n",
            "Loss: 0.45206973154849894\n",
            "training error 0.03489605847714724, test error 0.05482070607186467\n",
            "Loss: 0.16722642352460237\n",
            "training error 0.0349209381708861, test error 0.054903785799961385\n",
            "Loss: 0.31902793305933486\n",
            "training error 0.03489699755038763, test error 0.05480034767666273\n",
            "Loss: 0.13002799745656102\n",
            "training error 0.03488159691331878, test error 0.054830948384259\n",
            "Loss: 0.18594095856465653\n",
            "training error 0.03488784893211373, test error 0.05475821121005534\n",
            "Loss: 0.05303714320128439\n",
            "training error 0.034896801062399674, test error 0.05485202157753169\n",
            "Loss: 0.2244454484420011\n",
            "training error 0.034871672326845, test error 0.05479499884347199\n",
            "Loss: 0.12025472338350607\n",
            "training error 0.03487513356991754, test error 0.05470402353158358\n",
            "Loss: 0.0\n",
            "training error 0.03486746778409782, test error 0.054701168638888326\n",
            "Loss: 0.0\n",
            "training error 0.03484712988393186, test error 0.054737275868258106\n",
            "Loss: 0.06600814985899817\n",
            "training error 0.03483366899160235, test error 0.05479863164939053\n",
            "Loss: 0.17817354350435632\n",
            "training error 0.03486845910741846, test error 0.05453611691717677\n",
            "Loss: 0.0\n",
            "training error 0.03487053825532283, test error 0.054568350933967204\n",
            "Loss: 0.05910581576498153\n",
            "training error 0.0348372952281866, test error 0.054684922707028016\n",
            "Loss: 0.2728573251323363\n",
            "training error 0.034832800010948774, test error 0.05468132778098195\n",
            "Loss: 0.26626549892745377\n",
            "training error 0.03481811759117737, test error 0.05466654112664328\n",
            "Loss: 0.23915199108250906\n",
            "training error 0.034841453300079255, test error 0.05447846513415158\n",
            "Loss: 0.0\n",
            "training error 0.03485580113609975, test error 0.05442852098843218\n",
            "Loss: 0.0\n",
            "training error 0.03483743853191034, test error 0.05447016955684235\n",
            "Loss: 0.07651975040627867\n",
            "training error 0.03480030896125921, test error 0.054610835984639734\n",
            "Loss: 0.33496224570626065\n",
            "training error 0.03479627045991612, test error 0.054528182821401594\n",
            "Loss: 0.18310589955328194\n",
            "training error 0.03485777322517727, test error 0.05462138963955926\n",
            "Loss: 0.3543521808503147\n",
            "training error 0.03477834464016906, test error 0.05448090145296816\n",
            "Loss: 0.09623716313569286\n",
            "training error 0.034895554471757115, test error 0.05434156823427405\n",
            "Loss: 0.0\n",
            "training error 0.03479680209641366, test error 0.054526452908009185\n",
            "Loss: 0.3402269749339437\n",
            "training error 0.0347556336470784, test error 0.05456001637880428\n",
            "Loss: 0.4019908729694155\n",
            "training error 0.03476347468742468, test error 0.054591806681808054\n",
            "Loss: 0.46049176655187374\n",
            "training error 0.0347943438508479, test error 0.05456252385730859\n",
            "Loss: 0.40660516472761365\n",
            "training error 0.034757183507759824, test error 0.054568117697370334\n",
            "Loss: 0.41689901572143917\n",
            "training error 0.034768372880091464, test error 0.05453473672387157\n",
            "Loss: 0.3554709513805987\n",
            "training error 0.034829877663412646, test error 0.054287315202186846\n",
            "Loss: 0.0\n",
            "training error 0.03477223406714653, test error 0.05429624536318665\n",
            "Loss: 0.016449811464314124\n",
            "training error 0.03478302749288101, test error 0.054252990686345676\n",
            "Loss: 0.0\n",
            "training error 0.0347705932962308, test error 0.054305366744070514\n",
            "Loss: 0.09654040645914463\n",
            "training error 0.03478980967006459, test error 0.054170913720105254\n",
            "Loss: 0.0\n",
            "training error 0.03474503874033217, test error 0.05426889857618533\n",
            "Loss: 0.18088093656007942\n",
            "training error 0.034770200174105724, test error 0.05451015989878346\n",
            "Loss: 0.6262515349677367\n",
            "training error 0.03472838587046212, test error 0.054422208940456\n",
            "Loss: 0.4638932650262495\n",
            "training error 0.034719134607400774, test error 0.05440924390198241\n",
            "Loss: 0.4399596859461896\n",
            "training error 0.03472934544906942, test error 0.05439853515312678\n",
            "Loss: 0.4201912380463524\n",
            "training error 0.034711822214937536, test error 0.05431515249514056\n",
            "Loss: 0.26626609213307173\n",
            "training error 0.034734299592577345, test error 0.05426799000162583\n",
            "Loss: 0.17920369964987692\n",
            "training error 0.034734286821943856, test error 0.054275189567982746\n",
            "Loss: 0.1924941647029943\n",
            "training error 0.0347206115063323, test error 0.05444473002216878\n",
            "Loss: 0.5054673869418158\n",
            "training error 0.034717527542856244, test error 0.054529142992925106\n",
            "Loss: 0.6612944996105874\n",
            "training error 0.03469736627673728, test error 0.0543892352440771\n",
            "Loss: 0.4030235212569844\n",
            "training error 0.034733020956268734, test error 0.054552131380807715\n",
            "Loss: 0.7037312729708978\n",
            "training error 0.034713306680547173, test error 0.05449638094620255\n",
            "Loss: 0.6008154630341878\n",
            "training error 0.03470653820907912, test error 0.05448291729892368\n",
            "Loss: 0.5759614475593144\n",
            "training error 0.034686861312765584, test error 0.05447966891135457\n",
            "Loss: 0.5699648945273861\n",
            "training error 0.03470888760249401, test error 0.0544582134683681\n",
            "Loss: 0.5303579514041257\n",
            "training error 0.03474758382605176, test error 0.054431900364890325\n",
            "Loss: 0.48178372278073756\n",
            "training error 0.034713549751510614, test error 0.054245267504277474\n",
            "Loss: 0.13725776263697753\n",
            "training error 0.03472054967576691, test error 0.05434452726794339\n",
            "Loss: 0.3204921902096425\n",
            "training error 0.034740772298861126, test error 0.05423164742792535\n",
            "Loss: 0.1121149776684538\n",
            "training error 0.034647182248892176, test error 0.054392456499917946\n",
            "Loss: 0.40896998887147706\n",
            "training error 0.03467194694385597, test error 0.05451405914187544\n",
            "Loss: 0.6334495732214895\n",
            "training error 0.03471022000121418, test error 0.054498510618874216\n",
            "Loss: 0.6047468581785731\n",
            "training error 0.03465708888181004, test error 0.054323514190486755\n",
            "Loss: 0.28170185788256585\n",
            "training error 0.03468213646372512, test error 0.05429407488703143\n",
            "Loss: 0.22735663563391295\n",
            "training error 0.03465733860254345, test error 0.05435598774561779\n",
            "Loss: 0.3416483363540612\n",
            "training error 0.03467043273479718, test error 0.054408020970113366\n",
            "Loss: 0.43770214258009865\n",
            "training error 0.03466927996415947, test error 0.05442049063675078\n",
            "Loss: 0.4607212607397715\n",
            "training error 0.034654632345337576, test error 0.05433744422434738\n",
            "Loss: 0.30741682723420816\n",
            "training error 0.03466085806129057, test error 0.05429016448276872\n",
            "Loss: 0.2201379937573522\n",
            "training error 0.034673474978234205, test error 0.054320791293313096\n",
            "Loss: 0.27667536490567635\n",
            "training error 0.03464671562468973, test error 0.05419199768749418\n",
            "Loss: 0.038921195787589014\n",
            "training error 0.034647608534885715, test error 0.05404430834371706\n",
            "Loss: 0.0\n",
            "training error 0.03469588780294232, test error 0.05424420051073991\n",
            "Loss: 0.3698671944352583\n",
            "training error 0.03466552387360702, test error 0.05418317505208655\n",
            "Loss: 0.2569497374012242\n",
            "training error 0.034638317548619776, test error 0.05428422244818224\n",
            "Loss: 0.4439211303054247\n",
            "training error 0.03471864335877656, test error 0.05421599842448699\n",
            "Loss: 0.31768392645159516\n",
            "training error 0.034622777757737, test error 0.054263896487506666\n",
            "Loss: 0.40631132217114896\n",
            "training error 0.0346366018956697, test error 0.05419268615978541\n",
            "Loss: 0.27454845961703\n",
            "training error 0.03462733900042668, test error 0.053951382705384694\n",
            "Loss: 0.0\n",
            "training error 0.03462258770367835, test error 0.0538454334329329\n",
            "Loss: 0.0\n",
            "training error 0.03463931001964098, test error 0.05389014511817898\n",
            "Loss: 0.08303709784742708\n",
            "training error 0.034637885250037886, test error 0.053988457942352026\n",
            "Loss: 0.26562049982803426\n",
            "training error 0.03462492440672418, test error 0.05398082075948246\n",
            "Loss: 0.25143697045022506\n",
            "training error 0.03461992509983181, test error 0.05389463719994113\n",
            "Loss: 0.09137964702152779\n",
            "training error 0.03465342369020565, test error 0.05409357506059512\n",
            "Loss: 0.46084061700661927\n",
            "training error 0.034647657061865324, test error 0.054150734100110016\n",
            "Loss: 0.5669945392070774\n",
            "training error 0.034629667867712446, test error 0.053885294850424835\n",
            "Loss: 0.07402933721682992\n",
            "training error 0.034596146756664885, test error 0.05392443292928495\n",
            "Loss: 0.14671531328731557\n",
            "training error 0.034581517385987984, test error 0.05408440725657053\n",
            "Loss: 0.443814467452075\n",
            "training error 0.03458191060931191, test error 0.05410215415016984\n",
            "Loss: 0.4767734250977762\n",
            "training error 0.034698670380021134, test error 0.05432848564799782\n",
            "Loss: 0.8971089733479154\n",
            "training error 0.034588752874092306, test error 0.054085464664944446\n",
            "Loss: 0.44577825213445443\n",
            "training error 0.034586174431798684, test error 0.0540705067354894\n",
            "Loss: 0.417998868626146\n",
            "training error 0.03459032922491007, test error 0.054172849782950463\n",
            "Loss: 0.6080670711386738\n",
            "training error 0.034661616373163534, test error 0.05423084208812065\n",
            "Loss: 0.7157685074033138\n",
            "training error 0.034602278413213984, test error 0.05418890798849194\n",
            "Loss: 0.637889851860618\n",
            "training error 0.03456940678594313, test error 0.054118656913465804\n",
            "Loss: 0.5074218241240658\n",
            "training error 0.034575705635814726, test error 0.05412300653258334\n",
            "Loss: 0.5154997962755203\n",
            "training error 0.03457633023675428, test error 0.054133647125578055\n",
            "Loss: 0.5352611619407588\n",
            "training error 0.03464649934548199, test error 0.0543673119296029\n",
            "Loss: 0.9692158896260405\n",
            "training error 0.03461703360270439, test error 0.05411970850440822\n",
            "Loss: 0.5093748048605518\n",
            "training error 0.0345729260685923, test error 0.05403021066122329\n",
            "Loss: 0.343162300885802\n",
            "training error 0.034570132743964595, test error 0.05400944503493124\n",
            "Loss: 0.3045970503749773\n",
            "training error 0.03460688847967531, test error 0.05410860360860628\n",
            "Loss: 0.4887511510166753\n",
            "training error 0.03461115936160041, test error 0.054090426978214715\n",
            "Loss: 0.4549941000790092\n",
            "training error 0.03455827828212133, test error 0.05407261812382799\n",
            "Loss: 0.4219200708599713\n",
            "training error 0.0345741002934141, test error 0.054108252731131\n",
            "Loss: 0.48809951270138985\n",
            "training error 0.03459797383735252, test error 0.05417998312288539\n",
            "Loss: 0.6213148796901935\n",
            "training error 0.03457459759267121, test error 0.05412866644586588\n",
            "Loss: 0.5260112044334297\n",
            "training error 0.03457201664963848, test error 0.053937764504700225\n",
            "Loss: 0.17147428459709513\n",
            "training error 0.03455844284344105, test error 0.05399014837834645\n",
            "Loss: 0.26875992296320206\n",
            "training error 0.03459838759120425, test error 0.05411848940213233\n",
            "Loss: 0.5071107274854381\n",
            "training error 0.03453589747432127, test error 0.05397528874943199\n",
            "Loss: 0.24116310004418384\n",
            "training error 0.03456001419360196, test error 0.053989874589575625\n",
            "Loss: 0.26825145130018324\n",
            "training error 0.03455269759079472, test error 0.053979914567208534\n",
            "Loss: 0.2497540194251302\n",
            "training error 0.03454934481261528, test error 0.05396614131547763\n",
            "Loss: 0.22417478112619627\n",
            "training error 0.03455315953222137, test error 0.05397906537402721\n",
            "Loss: 0.2481769252739907\n",
            "training error 0.03457090588744717, test error 0.0540981342766213\n",
            "Loss: 0.4693078457677302\n",
            "training error 0.034547832303660535, test error 0.054047722788304706\n",
            "Loss: 0.37568525773641337\n",
            "training error 0.034565844881005794, test error 0.05393967925283189\n",
            "Loss: 0.17503029298924222\n",
            "training error 0.0345429051796402, test error 0.05397433538767084\n",
            "Loss: 0.23939254737077587\n",
            "training error 0.03466573502160459, test error 0.05396384120069335\n",
            "Loss: 0.21990308223245325\n",
            "training error 0.03453657341384114, test error 0.05392298728026837\n",
            "Loss: 0.14403050062186118\n",
            "training error 0.03454290522207815, test error 0.053958707916150137\n",
            "Loss: 0.21036971196140009\n",
            "training error 0.034641264477673774, test error 0.05389576948930252\n",
            "Loss: 0.09348249825551491\n",
            "training error 0.034539536144212214, test error 0.05411832121941989\n",
            "Loss: 0.5067983839834422\n",
            "training error 0.034584762253081255, test error 0.05426433248092864\n",
            "Loss: 0.7779657833333342\n",
            "training error 0.03454953965605467, test error 0.05408804529846058\n",
            "Loss: 0.4505709213574516\n",
            "training error 0.034541713987152584, test error 0.05403730390732343\n",
            "Loss: 0.35633564846220267\n",
            "training error 0.03465799250678655, test error 0.05419405286333574\n",
            "Loss: 0.6474447472635925\n",
            "training error 0.03454204899530917, test error 0.05419942897878424\n",
            "Loss: 0.6574290952495732\n",
            "training error 0.03457640289556804, test error 0.054198249225555384\n",
            "Loss: 0.6552380956537984\n",
            "training error 0.034543774829467064, test error 0.0540031209783728\n",
            "Loss: 0.29285221677397555\n",
            "training error 0.03454292325421992, test error 0.05408756471216018\n",
            "Loss: 0.44967839200118487\n",
            "training error 0.0345496661033406, test error 0.05398539636636243\n",
            "Loss: 0.2599346397756497\n",
            "training error 0.03453060473720366, test error 0.054168233974907394\n",
            "Loss: 0.5994947415114638\n",
            "training error 0.03455342629851113, test error 0.05425715903110571\n",
            "Loss: 0.764643483993166\n",
            "training error 0.03458761089437058, test error 0.05414610677845739\n",
            "Loss: 0.5584008268760376\n",
            "training error 0.03453535179692313, test error 0.054230707721172194\n",
            "Loss: 0.7155189654461047\n",
            "training error 0.03453089769971352, test error 0.054152586363525976\n",
            "Loss: 0.5704345030032032\n",
            "training error 0.03451750628956375, test error 0.054194275346653234\n",
            "Loss: 0.6478579360956083\n",
            "training error 0.03453781802766918, test error 0.05421946735195146\n",
            "Loss: 0.6946437147440365\n",
            "training error 0.03455718491036148, test error 0.054117732478863874\n",
            "Loss: 0.5057049940365488\n",
            "training error 0.034569449759197186, test error 0.05425621018416508\n",
            "Loss: 0.7628813160986292\n",
            "training error 0.03453170438011488, test error 0.054387829700976116\n",
            "Loss: 1.0073208319862248\n",
            "training error 0.03452126905174221, test error 0.054278768367787174\n",
            "Loss: 0.8047756461910804\n",
            "training error 0.03452345686577772, test error 0.05432318221353774\n",
            "Loss: 0.8872596061463511\n",
            "training error 0.034534016153604755, test error 0.05422668620678858\n",
            "Loss: 0.7080503388101622\n",
            "training error 0.034527490269540884, test error 0.05431998826020327\n",
            "Loss: 0.8813278991642637\n",
            "training error 0.03455912114501983, test error 0.054380284667605104\n",
            "Loss: 0.9933084396811287\n",
            "training error 0.034510975519253646, test error 0.05431366739583383\n",
            "Loss: 0.8695889939935864\n",
            "training error 0.0345578116477976, test error 0.054254537589878124\n",
            "Loss: 0.7597750280063709\n",
            "training error 0.03453647121711776, test error 0.05427797698178926\n",
            "Loss: 0.8033059096740569\n",
            "training error 0.03458238346522672, test error 0.054469457334553356\n",
            "Loss: 1.1589170368508706\n",
            "training error 0.034508707690090006, test error 0.054302703610680854\n",
            "Loss: 0.8492274062897209\n",
            "training error 0.034532046900854325, test error 0.05432374203435735\n",
            "Loss: 0.8882992872927709\n",
            "training error 0.03454750930876711, test error 0.05431296255670389\n",
            "Loss: 0.8682799895246784\n",
            "training error 0.03452194035729848, test error 0.05432843694770032\n",
            "Loss: 0.8970185287282106\n",
            "training error 0.03450909352440277, test error 0.05424265798854329\n",
            "Loss: 0.7377126160664105\n",
            "training error 0.034531807744254156, test error 0.05429852375628236\n",
            "Loss: 0.8414647156918198\n",
            "training error 0.03452780959029602, test error 0.054349259494811876\n",
            "Loss: 0.9356894907467161\n",
            "training error 0.034517261215483444, test error 0.05432315006461534\n",
            "Loss: 0.8871999002059372\n",
            "training error 0.03450112690154684, test error 0.054391017660653025\n",
            "Loss: 1.0132414077410612\n",
            "training error 0.03449576190900702, test error 0.05435160594947204\n",
            "Loss: 0.94004725056136\n",
            "training error 0.0345441834855701, test error 0.054482064662991\n",
            "Loss: 1.182330960063771\n",
            "training error 0.034575589921493774, test error 0.054322067793182376\n",
            "Loss: 0.8851899406532748\n",
            "training error 0.0345141292230937, test error 0.05446091610788465\n",
            "Loss: 1.1430545465259678\n",
            "training error 0.034546534011690944, test error 0.05423126902410349\n",
            "Loss: 0.716561399122484\n",
            "training error 0.034507924847685646, test error 0.05426009559078887\n",
            "Loss: 0.7700971677987223\n",
            "training error 0.03454930294284046, test error 0.05434424306708884\n",
            "Loss: 0.9263731431881306\n",
            "training error 0.034511120398556985, test error 0.05445190196676165\n",
            "Loss: 1.1263137747495877\n",
            "training error 0.03449365945518137, test error 0.054410245824152235\n",
            "Loss: 1.0489513320063315\n",
            "training error 0.03452460401750175, test error 0.05434479325970116\n",
            "Loss: 0.927394943138915\n",
            "training error 0.03448641823194135, test error 0.05449605751035509\n",
            "Loss: 1.2083180242806968\n",
            "training error 0.03449937557365835, test error 0.054532819050516525\n",
            "Loss: 1.2765903694317737\n",
            "training error 0.03448803975720368, test error 0.05429518501703519\n",
            "Loss: 0.835264116988621\n",
            "training error 0.03448674015135315, test error 0.054083478740106515\n",
            "Loss: 0.44209005666211443\n",
            "training error 0.034548482883089225, test error 0.054180906432920214\n",
            "Loss: 0.6230296212679898\n",
            "training error 0.034477521933393145, test error 0.054127011902173894\n",
            "Loss: 0.5229384393232062\n",
            "training error 0.03448299248249802, test error 0.05408246697439066\n",
            "Loss: 0.4402110380502311\n",
            "training error 0.03450777845031703, test error 0.05399008502284902\n",
            "Loss: 0.26864226117946366\n",
            "training error 0.03449707749810497, test error 0.05419246348666322\n",
            "Loss: 0.6444930082373013\n",
            "training error 0.03450301160473077, test error 0.05412515618928249\n",
            "Loss: 0.5194920692726868\n",
            "training error 0.03454811962722409, test error 0.05431904514204364\n",
            "Loss: 0.8795763705767845\n",
            "training error 0.03446783349887446, test error 0.05422751625876947\n",
            "Loss: 0.7095918845420046\n",
            "training error 0.034490900997307866, test error 0.054309441991505385\n",
            "Loss: 0.8617417095368562\n",
            "training error 0.03447808087508143, test error 0.054107503362366964\n",
            "Loss: 0.4867078092341437\n",
            "training error 0.03452812431883365, test error 0.05417917807188471\n",
            "Loss: 0.6198197649713411\n",
            "training error 0.03449900564482341, test error 0.05404037965817916\n",
            "Loss: 0.36204783361819537\n",
            "training error 0.03448448458598385, test error 0.053872277500453154\n",
            "Loss: 0.049853935252808235\n",
            "training error 0.03447225527443428, test error 0.053973654694590034\n",
            "Loss: 0.2381283861645045\n",
            "training error 0.03446205212001959, test error 0.054045299775493244\n",
            "Loss: 0.3711853165956569\n",
            "training error 0.03451817165417052, test error 0.05379803443783573\n",
            "Loss: 0.0\n",
            "training error 0.03445287348881489, test error 0.05391976763458926\n",
            "Loss: 0.2262781494260624\n",
            "training error 0.03449578186627189, test error 0.05361883298204937\n",
            "Loss: 0.0\n",
            "training error 0.03453228679142947, test error 0.05368651490705319\n",
            "Loss: 0.12622789650509336\n",
            "training error 0.034456471180769416, test error 0.05371323065982063\n",
            "Loss: 0.1760532121295233\n",
            "training error 0.03445882851778086, test error 0.05386007567285042\n",
            "Loss: 0.44992156185461596\n",
            "training error 0.034500068893115826, test error 0.05379298488548884\n",
            "Loss: 0.32479614671541146\n",
            "training error 0.03449566346245302, test error 0.053922282295063906\n",
            "Loss: 0.5659379291528488\n",
            "training error 0.03450799044527939, test error 0.053883680654761096\n",
            "Loss: 0.4939452389804755\n",
            "training error 0.03447688129325901, test error 0.053850590470485855\n",
            "Loss: 0.43223150439337665\n",
            "training error 0.03447730009164239, test error 0.05387667690454981\n",
            "Loss: 0.4808831303485528\n",
            "training error 0.0344819311396996, test error 0.05397465326609272\n",
            "Loss: 0.6636106462116986\n",
            "training error 0.03446988852638313, test error 0.054033416831325294\n",
            "Loss: 0.7732056559580869\n",
            "training error 0.03449696282817702, test error 0.05404845999186104\n",
            "Loss: 0.8012613962625759\n",
            "training error 0.034466823732742354, test error 0.05407560462985576\n",
            "Loss: 0.8518865898467176\n",
            "training error 0.03444932004623519, test error 0.054088172550907844\n",
            "Loss: 0.8753259680523096\n",
            "training error 0.03445559734293413, test error 0.05410111085623802\n",
            "Loss: 0.899456118991071\n",
            "training error 0.03446164578773713, test error 0.05403761079147697\n",
            "Loss: 0.7810274601981693\n",
            "training error 0.03444997306563173, test error 0.05412938464915338\n",
            "Loss: 0.9521872049601088\n",
            "training error 0.03450061464136619, test error 0.05405265988184109\n",
            "Loss: 0.8090942597295259\n",
            "training error 0.034462894368910264, test error 0.054053385591201646\n",
            "Loss: 0.8104477195498783\n",
            "training error 0.0344611778090109, test error 0.054061821954080234\n",
            "Loss: 0.8261816742247463\n",
            "training error 0.03447314770648806, test error 0.05420787483486708\n",
            "Loss: 1.0985726843680377\n",
            "training error 0.034478930368902566, test error 0.054127385480153264\n",
            "Loss: 0.948458722095169\n",
            "training error 0.03456292333743184, test error 0.05403100904416382\n",
            "Loss: 0.7687150935426823\n",
            "training error 0.03445694713028881, test error 0.054156094349465195\n",
            "Loss: 1.002001232655858\n",
            "training error 0.03455385422810297, test error 0.05424927077881232\n",
            "Loss: 1.1757767965110633\n",
            "training error 0.034492947585718685, test error 0.054106606255671125\n",
            "Loss: 0.9097051287652036\n",
            "training error 0.03448156608049323, test error 0.054084097129093664\n",
            "Loss: 0.8677252397493618\n",
            "training error 0.03450468420095585, test error 0.05415621589234192\n",
            "Loss: 1.0022279121077782\n",
            "training error 0.03450130748831275, test error 0.054093630264649264\n",
            "Loss: 0.8855046933954114\n",
            "training error 0.034460028214037375, test error 0.05396124111397436\n",
            "Loss: 0.6385967632671408\n",
            "training error 0.03451307606878713, test error 0.05384083913277492\n",
            "Loss: 0.4140451001607426\n",
            "training error 0.03447058830509054, test error 0.053938960730700515\n",
            "Loss: 0.5970434842517358\n",
            "training error 0.03445541785615855, test error 0.05406379779693043\n",
            "Loss: 0.8298666534387822\n",
            "training error 0.034455089525932445, test error 0.05405819059309611\n",
            "Loss: 0.8194091266287673\n",
            "training error 0.03445875654274893, test error 0.0541147118806208\n",
            "Loss: 0.9248222517961979\n",
            "training error 0.03444394739327406, test error 0.054030975031406754\n",
            "Loss: 0.7686516591947523\n",
            "training error 0.03453902638648858, test error 0.05433125141734323\n",
            "Loss: 1.3286720274802821\n",
            "training error 0.034483024945523695, test error 0.054303504633626806\n",
            "Loss: 1.2769238222820833\n",
            "training error 0.03449775972038907, test error 0.05419185733914924\n",
            "Loss: 1.068699793021799\n",
            "training error 0.03447652946160722, test error 0.05412511833601967\n",
            "Loss: 0.9442304612258035\n",
            "training error 0.03445940106679684, test error 0.05423769341957889\n",
            "Loss: 1.1541848322896175\n",
            "training error 0.03447717922384395, test error 0.05415491154512507\n",
            "Loss: 0.9997952832266455\n",
            "training error 0.03451767082122617, test error 0.054257835436406714\n",
            "Loss: 1.1917500229280842\n",
            "training error 0.03449927111371273, test error 0.0541718810304246\n",
            "Loss: 1.0314436507045555\n",
            "training error 0.034478079064981695, test error 0.05413259460737924\n",
            "Loss: 0.9581738295234343\n",
            "training error 0.03446142567000936, test error 0.054097200964622325\n",
            "Loss: 0.8921641072141551\n",
            "training error 0.03452087762814719, test error 0.0539716030704865\n",
            "Loss: 0.657921981545595\n",
            "training error 0.03447963669116478, test error 0.054159736364799156\n",
            "Loss: 1.0087936507884754\n",
            "training error 0.03443952524850009, test error 0.054219971465576514\n",
            "Loss: 1.1211330983805556\n",
            "training error 0.03446371974830289, test error 0.05417025691028533\n",
            "Loss: 1.0284146400958871\n",
            "training error 0.03443919916003044, test error 0.054313536813370415\n",
            "Loss: 1.2956340014964818\n",
            "training error 0.03444369766049887, test error 0.054277249604714396\n",
            "Loss: 1.2279577641785888\n",
            "training error 0.03444137626390819, test error 0.05422272695859384\n",
            "Loss: 1.1262721379009522\n",
            "training error 0.0345315483346918, test error 0.05401583170446691\n",
            "Loss: 0.7404091069092233\n",
            "training error 0.03445593508646903, test error 0.05425742357191319\n",
            "Loss: 1.1909818889150436\n",
            "training error 0.03443228760582618, test error 0.05417207751593974\n",
            "Loss: 1.0318100994021862\n",
            "training error 0.03452361214193596, test error 0.05404572609588927\n",
            "Loss: 0.7961626355851736\n",
            "training error 0.03445638234123748, test error 0.05420507012583832\n",
            "Loss: 1.0933418561817954\n",
            "training error 0.03447009076082633, test error 0.05426241049072265\n",
            "Loss: 1.200282574014122\n",
            "training error 0.03444087743363348, test error 0.05405235985574412\n",
            "Loss: 0.8085347061542514\n",
            "training error 0.034463172500249384, test error 0.054183214321038724\n",
            "Loss: 1.05258042296128\n",
            "training error 0.034480709952093944, test error 0.05395274148343897\n",
            "Loss: 0.6227448133781399\n",
            "training error 0.03446821572830153, test error 0.05423125477246281\n",
            "Loss: 1.1421766501678032\n",
            "training error 0.03447308264305022, test error 0.054231501897163165\n",
            "Loss: 1.1426375417736168\n",
            "training error 0.03442942819162458, test error 0.054178946140873215\n",
            "Loss: 1.0446201971821445\n",
            "training error 0.03447143276406119, test error 0.05405527891659414\n",
            "Loss: 0.8139788023564076\n",
            "training error 0.034456821558013474, test error 0.0542278917049267\n",
            "Loss: 1.1359044742380586\n",
            "training error 0.03451694200256589, test error 0.05427816971706884\n",
            "Loss: 1.2296737887603992\n",
            "training error 0.03450768220493182, test error 0.0542853586032586\n",
            "Loss: 1.2430811790185237\n",
            "training error 0.03445996562051608, test error 0.05426125592894741\n",
            "Loss: 1.198129297430084\n",
            "training error 0.034480957167256125, test error 0.0542361789796536\n",
            "Loss: 1.1513603770729297\n",
            "training error 0.03448816228952133, test error 0.054203536899032595\n",
            "Loss: 1.0904823631259886\n",
            "training error 0.03445062674330602, test error 0.05426023283468717\n",
            "Loss: 1.196221209910564\n",
            "training error 0.0345074002391902, test error 0.054389578195354796\n",
            "Loss: 1.4374524219194607\n",
            "training error 0.034450768280717775, test error 0.05420808942413675\n",
            "Loss: 1.0989728968637813\n",
            "training error 0.0344809433719325, test error 0.05404827861121705\n",
            "Loss: 0.8009231183965682\n",
            "training error 0.034432907410598, test error 0.0540908320065938\n",
            "Loss: 0.8802858963798066\n",
            "training error 0.03442988935244605, test error 0.054162795595138216\n",
            "Loss: 1.0144991653789903\n",
            "training error 0.03448144539492273, test error 0.05417012410826077\n",
            "Loss: 1.028166962149224\n",
            "training error 0.034445143870454255, test error 0.054133138499924474\n",
            "Loss: 0.9591881980111072\n",
            "training error 0.03443713774482458, test error 0.05420168369194513\n",
            "Loss: 1.087026101614108\n",
            "training error 0.03440941784861829, test error 0.05420734129149659\n",
            "Loss: 1.0975776172604146\n",
            "training error 0.03443472719294723, test error 0.05424548802153214\n",
            "Loss: 1.1687218923480902\n",
            "training error 0.034490809329140365, test error 0.05444777967139542\n",
            "Loss: 1.5459991261346007\n",
            "training error 0.03441805872831136, test error 0.05427925779766008\n",
            "Loss: 1.2317030768495218\n",
            "training error 0.03445428559419075, test error 0.05437932620493993\n",
            "Loss: 1.4183322922100228\n",
            "training error 0.03442733745175028, test error 0.054435921392918675\n",
            "Loss: 1.523883242932289\n",
            "training error 0.034436508646405184, test error 0.05424846881071492\n",
            "Loss: 1.1742811128998998\n",
            "training error 0.034475192406176226, test error 0.054389031107583745\n",
            "Loss: 1.436432094283413\n",
            "training error 0.03450323300587505, test error 0.054465715741595346\n",
            "Loss: 1.5794501902521851\n",
            "training error 0.034445225358827394, test error 0.054283784615548024\n",
            "Loss: 1.2401456662088695\n",
            "training error 0.034414651417530924, test error 0.0543630277496335\n",
            "Loss: 1.3879354066383298\n",
            "training error 0.03444799494208998, test error 0.054228452121343806\n",
            "Loss: 1.136949660016895\n",
            "training error 0.03440666567361746, test error 0.05428583438883684\n",
            "Loss: 1.2439685268994527\n",
            "training error 0.034496012336173214, test error 0.05422368055918726\n",
            "Loss: 1.128050618595866\n",
            "training error 0.03443374520343477, test error 0.05441914804170151\n",
            "Loss: 1.49260066872412\n",
            "training error 0.034419917731877206, test error 0.05447475125025651\n",
            "Loss: 1.5963015616055731\n",
            "training error 0.03442569855516034, test error 0.05440839995723816\n",
            "Loss: 1.4725553155047644\n",
            "training error 0.03442700839492504, test error 0.05444680707689714\n",
            "Loss: 1.5441852214966412\n",
            "training error 0.034409098430405106, test error 0.05446620964301694\n",
            "Loss: 1.5803713244770812\n",
            "training error 0.03442708051391255, test error 0.05439394994949451\n",
            "Loss: 1.4456058148535789\n",
            "training error 0.03442703580059977, test error 0.05429877439987333\n",
            "Loss: 1.2681018590083637\n",
            "training error 0.03447474905300591, test error 0.0542406527769117\n",
            "Loss: 1.1597040820908955\n",
            "training error 0.03441734727222321, test error 0.05435795627610199\n",
            "Loss: 1.378477025600433\n",
            "training error 0.03444693687407009, test error 0.05443139161935027\n",
            "Loss: 1.5154351411805767\n",
            "training error 0.034528393612529136, test error 0.05448269505706455\n",
            "Loss: 1.6111168911572271\n",
            "training error 0.0344338096845065, test error 0.05433088429980127\n",
            "Loss: 1.3279873472633819\n",
            "training error 0.03442996927598661, test error 0.05427594853768244\n",
            "Loss: 1.2255312528959061\n",
            "training error 0.0344197456884461, test error 0.054306780219609045\n",
            "Loss: 1.283032843683829\n",
            "training error 0.034420616122734414, test error 0.05427154493376072\n",
            "Loss: 1.2173184595976938\n",
            "training error 0.03451015616012773, test error 0.05453379238386566\n",
            "Loss: 1.7064142409115801\n",
            "training error 0.03445298567354132, test error 0.054137137729848864\n",
            "Loss: 0.9666468271941175\n",
            "training error 0.03445179147306302, test error 0.05395095939272028\n",
            "Loss: 0.6194211850565567\n",
            "training error 0.034447871701082135, test error 0.05409765514738714\n",
            "Loss: 0.8930111654949169\n",
            "training error 0.03444399115398702, test error 0.054124221424231414\n",
            "Loss: 0.9425577060792056\n",
            "training error 0.03443879723375999, test error 0.05397169525695624\n",
            "Loss: 0.6580939108186135\n",
            "training error 0.03442617230429023, test error 0.05402915931941961\n",
            "Loss: 0.7652653266579046\n",
            "training error 0.03448713112499054, test error 0.054246596679238646\n",
            "Loss: 1.1707895570935767\n",
            "training error 0.034421344223387905, test error 0.05418495362287823\n",
            "Loss: 1.0558242493237113\n",
            "training error 0.034413965681871786, test error 0.054087112821061614\n",
            "Loss: 0.8733495545660608\n",
            "training error 0.03445008305830151, test error 0.054109953420106736\n",
            "Loss: 0.9159476451525705\n",
            "training error 0.03444901289048346, test error 0.054173303127830856\n",
            "Loss: 1.0340958856137616\n",
            "training error 0.03448099116147849, test error 0.054015780067019147\n",
            "Loss: 0.7403128022250405\n",
            "training error 0.034409108508536373, test error 0.05415630420498042\n",
            "Loss: 1.002392616622183\n",
            "training error 0.034412589613360606, test error 0.05419254742725758\n",
            "Loss: 1.069986818624491\n",
            "training error 0.03447165946947431, test error 0.05406029216202319\n",
            "Loss: 0.82332858703138\n",
            "training error 0.0344222165655061, test error 0.054266561846703774\n",
            "Loss: 1.2080249207796934\n",
            "training error 0.03446644238961825, test error 0.0542666545895861\n",
            "Loss: 1.2081978877712674\n",
            "training error 0.03442431703831426, test error 0.05408501392121421\n",
            "Loss: 0.8694350720406474\n",
            "training error 0.03443744623007397, test error 0.05414476672903995\n",
            "Loss: 0.980875035394102\n",
            "training error 0.034432460880469376, test error 0.05420896787955384\n",
            "Loss: 1.1006112305764537\n",
            "training error 0.03444601309097479, test error 0.054214734961812776\n",
            "Loss: 1.1113669332618592\n",
            "training error 0.0344303953747027, test error 0.05429680498650526\n",
            "Loss: 1.2644288708835827\n",
            "training error 0.034442383564316635, test error 0.054233929989760364\n",
            "Loss: 1.1471659741585816\n",
            "training error 0.034414767479850526, test error 0.05420640936097721\n",
            "Loss: 1.0958395516078356\n",
            "training error 0.03444695798721901, test error 0.054231761133498924\n",
            "Loss: 1.1431210217774712\n",
            "training error 0.034489730191572525, test error 0.05435891542794937\n",
            "Loss: 1.3802658594747275\n",
            "training error 0.03446076704893628, test error 0.054397352108074726\n",
            "Loss: 1.4519508962195893\n",
            "training error 0.034458505186320744, test error 0.054439167508439995\n",
            "Loss: 1.5299373014426854\n",
            "training error 0.03447011950886247, test error 0.05440449527124402\n",
            "Loss: 1.4652730122971303\n",
            "training error 0.03443730232444496, test error 0.05407924523375567\n",
            "Loss: 0.8586763756317506\n",
            "training error 0.03446988709319519, test error 0.05417797492005885\n",
            "Loss: 1.0428088544871317\n",
            "training error 0.034448497807713495, test error 0.053866203407679315\n",
            "Loss: 0.4613498874784572\n",
            "training error 0.034460399618875696, test error 0.05401295592710986\n",
            "Loss: 0.7350457351289919\n",
            "training error 0.0344241602087834, test error 0.0539540822160196\n",
            "Loss: 0.6252453015574977\n",
            "training error 0.03445966663290341, test error 0.054054073673888516\n",
            "Loss: 0.8117310050087312\n",
            "training error 0.034469279267147594, test error 0.05389924971770918\n",
            "Loss: 0.5229817958807281\n",
            "training error 0.034431888933952874, test error 0.053952512588313115\n",
            "Loss: 0.6223179202267559\n",
            "training error 0.03442906851409683, test error 0.054090796306503076\n",
            "Loss: 0.8802193151270288\n",
            "training error 0.03443457457004093, test error 0.053865689251253464\n",
            "Loss: 0.46039097733949763\n",
            "training error 0.03442951443162189, test error 0.053982680595029926\n",
            "Loss: 0.6785817458995425\n",
            "training error 0.03443899825966059, test error 0.054060843361874035\n",
            "Loss: 0.8243565837634748\n",
            "training error 0.03441721325544751, test error 0.05399043978482748\n",
            "Loss: 0.69305276170879\n",
            "training error 0.03449569595030865, test error 0.053890091411341864\n",
            "Loss: 0.5059014047980259\n",
            "training error 0.03445828737386723, test error 0.053796207701762674\n",
            "Loss: 0.33080675174836927\n",
            "training error 0.03442618759710658, test error 0.05382439395805894\n",
            "Loss: 0.3833745804918687\n",
            "training error 0.03459192467116875, test error 0.054037391007317415\n",
            "Loss: 0.780617559147867\n",
            "training error 0.03440690076515341, test error 0.05382482823197948\n",
            "Loss: 0.3841845084526119\n",
            "training error 0.034424931827463805, test error 0.053929428638688236\n",
            "Loss: 0.5792659768310227\n",
            "training error 0.0344364445165012, test error 0.05400241027347668\n",
            "Loss: 0.715377918716964\n",
            "training error 0.03442709540764023, test error 0.05390844454044223\n",
            "Loss: 0.5401302905824412\n",
            "training error 0.03442909817622363, test error 0.053989513828115794\n",
            "Loss: 0.6913258373051789\n",
            "training error 0.034437491986971894, test error 0.05385821817617957\n",
            "Loss: 0.4464573001996097\n",
            "training error 0.03441727698511192, test error 0.05373792827570106\n",
            "Loss: 0.22211466946988612\n",
            "training error 0.03443244477506172, test error 0.05372337056387283\n",
            "Loss: 0.19496429894036993\n",
            "training error 0.03454036255522506, test error 0.05360150529200765\n",
            "Loss: 0.0\n",
            "training error 0.03442921932566308, test error 0.053717977785596394\n",
            "Loss: 0.2172933259135812\n",
            "training error 0.03445898222567725, test error 0.05373262879676937\n",
            "Loss: 0.24462653436203574\n",
            "training error 0.03441928977568513, test error 0.05382055267657166\n",
            "Loss: 0.4086590168889792\n",
            "training error 0.03446961150010471, test error 0.053810827206642556\n",
            "Loss: 0.3905149929924079\n",
            "training error 0.03442884198424624, test error 0.053914589999291315\n",
            "Loss: 0.584096856194738\n",
            "training error 0.034465378495597304, test error 0.0540560007638402\n",
            "Loss: 0.8479155004259109\n",
            "training error 0.03441470540101309, test error 0.05400735040972691\n",
            "Loss: 0.7571524633651849\n",
            "training error 0.03443636671220735, test error 0.05405872395647211\n",
            "Loss: 0.8529959410163102\n",
            "training error 0.03447504048207926, test error 0.05408690920786239\n",
            "Loss: 0.9055788885225846\n",
            "training error 0.03446675339406629, test error 0.05392830693342122\n",
            "Loss: 0.6096874325324242\n",
            "training error 0.03444460856294579, test error 0.05398005553766912\n",
            "Loss: 0.7062306246796934\n",
            "training error 0.03444786612979957, test error 0.05411439745701365\n",
            "Loss: 0.9568614952357946\n",
            "training error 0.034414153462110346, test error 0.05406390946007978\n",
            "Loss: 0.8626701163578776\n",
            "training error 0.034502347324829666, test error 0.05402723719428133\n",
            "Loss: 0.794253631412789\n",
            "training error 0.034440650911084496, test error 0.05393710824325163\n",
            "Loss: 0.6261073255605432\n",
            "training error 0.03446201765866823, test error 0.05395469367376006\n",
            "Loss: 0.6589150432032342\n",
            "training error 0.034443209024753015, test error 0.05413897667439267\n",
            "Loss: 1.0027169562813798\n",
            "training error 0.0344769643174262, test error 0.053965665607302635\n",
            "Loss: 0.6793844936091586\n",
            "training error 0.034442772501161296, test error 0.05402161129573176\n",
            "Loss: 0.7837578467908335\n",
            "training error 0.03444610291823274, test error 0.05406692122654443\n",
            "Loss: 0.8682889258451176\n",
            "training error 0.03444068924014995, test error 0.054176949384587954\n",
            "Loss: 1.07355957532429\n",
            "training error 0.034431159678545466, test error 0.05393396334565577\n",
            "Loss: 0.6202401440724037\n",
            "training error 0.03440682177283239, test error 0.0541149967246187\n",
            "Loss: 0.9579795004145408\n",
            "training error 0.0344127835068125, test error 0.05415511301696516\n",
            "Loss: 1.0328212275785953\n",
            "training error 0.034421938681902665, test error 0.05414682199698331\n",
            "Loss: 1.0173533411140578\n",
            "training error 0.034408029372158665, test error 0.0540938489570782\n",
            "Loss: 0.9185258182366152\n",
            "training error 0.03445105140765816, test error 0.05394119285777497\n",
            "Loss: 0.633727661036354\n",
            "training error 0.034426752038361964, test error 0.05407068770241176\n",
            "Loss: 0.8753157357207053\n",
            "training error 0.03445175719197878, test error 0.05409155344207613\n",
            "Loss: 0.9142432612644358\n",
            "training error 0.03441388131663347, test error 0.054030808896915986\n",
            "Loss: 0.800917068596485\n",
            "training error 0.034440299615671785, test error 0.05403330775773637\n",
            "Loss: 0.8055789914413136\n",
            "training error 0.03443826387646917, test error 0.0541922424979058\n",
            "Loss: 1.1020907018934656\n",
            "training error 0.034422092162039106, test error 0.054185168177459865\n",
            "Loss: 1.0888927135022985\n",
            "training error 0.03445872175934671, test error 0.05420966994212429\n",
            "Loss: 1.1346036772726942\n",
            "training error 0.03441195817018266, test error 0.054068180842175205\n",
            "Loss: 0.8706388890110883\n",
            "training error 0.03440931014077175, test error 0.05407621490091771\n",
            "Loss: 0.8856273836414852\n",
            "training error 0.03445190583450063, test error 0.053913500353027846\n",
            "Loss: 0.5820639911519709\n",
            "training error 0.03456963657087798, test error 0.053777684702638384\n",
            "Loss: 0.3286836995919362\n",
            "training error 0.03446281160245855, test error 0.05365858489106987\n",
            "Loss: 0.10648879868442318\n",
            "training error 0.0344114352713409, test error 0.05382818815220422\n",
            "Loss: 0.4229039071975027\n",
            "training error 0.03440354686573939, test error 0.05386129680362225\n",
            "Loss: 0.4846720445616626\n",
            "training error 0.034433609497361455, test error 0.053811683179485315\n",
            "Loss: 0.39211191240371424\n",
            "training error 0.03448608659240461, test error 0.05379148819512208\n",
            "Loss: 0.35443576086053863\n",
            "training error 0.03441331367121847, test error 0.053532134945591975\n",
            "Loss: 0.0\n",
            "training error 0.03442524223367948, test error 0.05363072168399882\n",
            "Loss: 0.1841636589070328\n",
            "training error 0.034476136873489015, test error 0.05356556516051275\n",
            "Loss: 0.062448872914844245\n",
            "training error 0.0344056027800669, test error 0.05345690090423233\n",
            "Loss: 0.0\n",
            "training error 0.03443543770944386, test error 0.05334392880204905\n",
            "Loss: 0.0\n",
            "training error 0.034393552417198596, test error 0.0534357075196614\n",
            "Loss: 0.17205091502152658\n",
            "training error 0.03441268610248724, test error 0.053546329594538936\n",
            "Loss: 0.37942610721637404\n",
            "training error 0.034409733605593476, test error 0.05362845011797829\n",
            "Loss: 0.533371505096758\n",
            "training error 0.034453746735226394, test error 0.053782033658028985\n",
            "Loss: 0.8212834446552852\n",
            "training error 0.03442218547005737, test error 0.053769802871188946\n",
            "Loss: 0.7983552743560551\n",
            "training error 0.03439252324676839, test error 0.053772598576911156\n",
            "Loss: 0.8035961814002013\n",
            "training error 0.03439825588104353, test error 0.05380250547262961\n",
            "Loss: 0.8596604728576818\n",
            "training error 0.03439735408843701, test error 0.05373381722526088\n",
            "Loss: 0.730895590121694\n",
            "training error 0.03439197484640612, test error 0.05365996527630674\n",
            "Loss: 0.5924506899940729\n",
            "training error 0.03442636024397688, test error 0.05348960151181059\n",
            "Loss: 0.2730820789411892\n",
            "training error 0.03443566646278537, test error 0.053439129193206585\n",
            "Loss: 0.17846527860894135\n",
            "training error 0.03444466407561866, test error 0.05341395680388067\n",
            "Loss: 0.13127642339858525\n",
            "training error 0.03439593061718163, test error 0.053382377432234965\n",
            "Loss: 0.07207686244594669\n",
            "training error 0.03444779626445706, test error 0.053340656418664374\n",
            "Loss: 0.0\n",
            "training error 0.034412485781496804, test error 0.0533637307258937\n",
            "Loss: 0.043258386338962396\n",
            "training error 0.034402489706690326, test error 0.053269746948341384\n",
            "Loss: 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.053269746948341384\n",
            "training error 0.12505642480558482, test error 0.25004921939662383\n",
            "training error 0.12512972866509603, test error 0.25022814623673334\n",
            "training error 0.12514795225043998, test error 0.2501782945516562\n",
            "training error 0.12498712222517248, test error 0.25032785595957424\n",
            "training error 0.12499514138841818, test error 0.2504165286681553\n",
            "training error 0.1250398227131689, test error 0.25040075593740774\n",
            "training error 0.1249977786629477, test error 0.2504709668026573\n",
            "training error 0.12500177005955362, test error 0.2504131540994818\n",
            "training error 0.1250919784047905, test error 0.25039855233537955\n",
            "training error 0.12500242731147562, test error 0.2503787087387688\n",
            "training error 0.12504071425687288, test error 0.25032790917485365\n",
            "training error 0.12503886637654302, test error 0.25041979213551135\n",
            "training error 0.12510306197970097, test error 0.2504311173946403\n",
            "training error 0.12513162195139396, test error 0.2504387351721267\n",
            "training error 0.12496668172700448, test error 0.2504693964415096\n",
            "training error 0.12505103587542146, test error 0.25052225739592326\n",
            "training error 0.12503924328830723, test error 0.2504900103067796\n",
            "training error 0.12513696335952487, test error 0.25046352140052064\n",
            "training error 0.12499624849864276, test error 0.25055265083132894\n",
            "training error 0.12496632652699585, test error 0.250530278442868\n",
            "training error 0.12503880244952145, test error 0.250441924651229\n",
            "training error 0.12497642280051843, test error 0.2505500793280728\n",
            "training error 0.12508621439420886, test error 0.2505539045323897\n",
            "training error 0.1250026168062119, test error 0.25040092472198283\n",
            "training error 0.12497085996839764, test error 0.25039121837063755\n",
            "training error 0.1250071845259625, test error 0.2503896703466779\n",
            "training error 0.12503691640006717, test error 0.2505471862625725\n",
            "training error 0.12508487822959152, test error 0.2505843765453044\n",
            "training error 0.1249793059728286, test error 0.25065180567209994\n",
            "training error 0.12503633580736107, test error 0.25062272160229093\n",
            "training error 0.1250078081407498, test error 0.25050834315939524\n",
            "training error 0.12499452072925715, test error 0.2505988360942542\n",
            "training error 0.12496607565170419, test error 0.2506304341458292\n",
            "training error 0.12513340117130528, test error 0.25055747762638597\n",
            "training error 0.12496798033525766, test error 0.2505736651234147\n",
            "training error 0.12497464891931455, test error 0.2506098655656461\n",
            "training error 0.12498159518302766, test error 0.2506445953981132\n",
            "training error 0.12526481869799744, test error 0.2505907531245734\n",
            "training error 0.12497536261463585, test error 0.250663050455241\n",
            "training error 0.12495844886207555, test error 0.25070000707165613\n",
            "training error 0.12501476545931026, test error 0.25076890080750797\n",
            "training error 0.12496226554446559, test error 0.2506921274497588\n",
            "training error 0.1250158689647486, test error 0.25061440516432054\n",
            "training error 0.12495342728173904, test error 0.250650684305116\n",
            "training error 0.1250579859549811, test error 0.2507802720638346\n",
            "training error 0.12503442079728272, test error 0.25062341048202424\n",
            "training error 0.12497844782676538, test error 0.25065060825206853\n",
            "training error 0.12495219385553386, test error 0.25065567297198227\n",
            "training error 0.12496007156688452, test error 0.25061574150438054\n",
            "training error 0.12509850092890643, test error 0.2505722031099523\n",
            "Loss: 0.2091523079297941\n",
            "training error 0.12495499359834829, test error 0.2506048646798531\n",
            "Loss: 0.2222143642639951\n",
            "training error 0.12495776236974167, test error 0.2505903837171835\n",
            "Loss: 0.21642311936247438\n",
            "training error 0.12495236160706175, test error 0.2506132035595708\n",
            "Loss: 0.2255492595849118\n",
            "training error 0.12499949995409193, test error 0.25061728399115973\n",
            "Loss: 0.22718111094552906\n",
            "training error 0.12501633889505082, test error 0.25058027508180136\n",
            "Loss: 0.21238046111839992\n",
            "training error 0.12498004554072593, test error 0.25065827789160683\n",
            "Loss: 0.24357544344775217\n",
            "training error 0.12496116196863623, test error 0.25072280815459747\n",
            "Loss: 0.2693824678193346\n",
            "training error 0.1250472580383297, test error 0.2505686402523491\n",
            "Loss: 0.20772744541199817\n",
            "training error 0.12494833965209565, test error 0.2506898775404189\n",
            "Loss: 0.25621281495737946\n",
            "training error 0.125004191234353, test error 0.2506639265555325\n",
            "Loss: 0.24583446426746924\n",
            "training error 0.12517220250479233, test error 0.2507235069327286\n",
            "Loss: 0.26966192405313283\n",
            "training error 0.12493504719788824, test error 0.25059444907646805\n",
            "Loss: 0.21804894298804722\n",
            "training error 0.12499432308981356, test error 0.2506513875761636\n",
            "Loss: 0.2408198597831479\n",
            "training error 0.12493962813128363, test error 0.25061384162486106\n",
            "Loss: 0.22580443546260653\n",
            "training error 0.12499830339451946, test error 0.2506805833606813\n",
            "Loss: 0.2524958748445272\n",
            "training error 0.1250812668874248, test error 0.25075990057135283\n",
            "Loss: 0.2842165140302777\n",
            "training error 0.12498659643671453, test error 0.2506257336198385\n",
            "Loss: 0.23056029713102166\n",
            "training error 0.1249542181016347, test error 0.25050387009209607\n",
            "Loss: 0.1818244810239067\n",
            "training error 0.12502304352180563, test error 0.2505426414529108\n",
            "Loss: 0.19732997266601604\n",
            "training error 0.12515809433198746, test error 0.2504457163457426\n",
            "Loss: 0.15856756124874583\n",
            "training error 0.12498653015817855, test error 0.25052686346758163\n",
            "Loss: 0.19102002082245217\n",
            "training error 0.12514929930801894, test error 0.25068459875264704\n",
            "Loss: 0.2541017154768177\n",
            "training error 0.12502961908332702, test error 0.25057996550515743\n",
            "Loss: 0.2122566548355076\n",
            "training error 0.12514160728227142, test error 0.25068058768160256\n",
            "Loss: 0.2524976028728476\n",
            "training error 0.12499346938369317, test error 0.25065138274338117\n",
            "Loss: 0.24081792705066274\n",
            "training error 0.1251492874728804, test error 0.2504980669409454\n",
            "Loss: 0.17950367747783247\n",
            "training error 0.12491936784940197, test error 0.2505690913378754\n",
            "Loss: 0.20790784410607\n",
            "training error 0.1250049245186375, test error 0.25051225933242827\n",
            "Loss: 0.1851795166254666\n",
            "training error 0.12498137959376616, test error 0.2506299478851426\n",
            "Loss: 0.23224567144024544\n",
            "training error 0.1249363826960208, test error 0.25071691813918395\n",
            "Loss: 0.2670269254074542\n",
            "training error 0.12537550894026786, test error 0.25081052874738474\n",
            "Loss: 0.304463798206589\n",
            "training error 0.1251255626087476, test error 0.2505436562910075\n",
            "Loss: 0.19773582800088896\n",
            "training error 0.1249798408253086, test error 0.2505779228412899\n",
            "Loss: 0.21143975011872485\n",
            "training error 0.12498988038239453, test error 0.2505094084906777\n",
            "Loss: 0.18403940438780353\n",
            "training error 0.12498256861061445, test error 0.250598804745368\n",
            "Loss: 0.21979086760210986\n",
            "training error 0.1252487181152265, test error 0.2505009068393453\n",
            "Loss: 0.1806394132368716\n",
            "training error 0.1250151672256445, test error 0.250554949531675\n",
            "Loss: 0.2022522350885625\n",
            "training error 0.1252818716323907, test error 0.2506477171393936\n",
            "Loss: 0.23935197406892428\n",
            "training error 0.12491656309633742, test error 0.25066564072380987\n",
            "Loss: 0.24651999661244872\n",
            "training error 0.12504422237690005, test error 0.2505846028204998\n",
            "Loss: 0.21411121585097614\n",
            "training error 0.12497402137009339, test error 0.2505874023459997\n",
            "Loss: 0.2152308056287966\n",
            "training error 0.12505441315122837, test error 0.250659800213091\n",
            "Loss: 0.24418425218062367\n",
            "training error 0.12494087170169568, test error 0.2505806466522294\n",
            "Loss: 0.212529060033817\n",
            "training error 0.12499097757781896, test error 0.25055008195449646\n",
            "Loss: 0.20030558746841898\n",
            "training error 0.12505130711656368, test error 0.25056246367438545\n",
            "Loss: 0.20525730054270497\n",
            "training error 0.1250305147844799, test error 0.2506639118946782\n",
            "Loss: 0.245828601080067\n",
            "training error 0.1250594681299414, test error 0.2507196735449173\n",
            "Loss: 0.26812887075244873\n",
            "training error 0.12503597574566336, test error 0.2506639047448262\n",
            "Loss: 0.24582574170222138\n",
            "training error 0.12525132695246044, test error 0.2506874017734886\n",
            "Loss: 0.25522270311608697\n",
            "training error 0.12501323544420445, test error 0.25094817548317777\n",
            "Loss: 0.3595116548346544\n",
            "training error 0.1249523037324089, test error 0.25079972612112095\n",
            "Loss: 0.3001435982516121\n",
            "training error 0.12490845112054269, test error 0.2508346080189521\n",
            "Loss: 0.3140936109392367\n",
            "training error 0.12499520179590402, test error 0.2507095921612698\n",
            "Loss: 0.2640971110565715\n",
            "training error 0.12500054176749525, test error 0.25077255031053874\n",
            "Loss: 0.2892754137206799\n",
            "training error 0.12498342134642729, test error 0.25064280321092075\n",
            "Loss: 0.23738678958056258\n",
            "training error 0.12508409783068497, test error 0.2505959244402913\n",
            "Loss: 0.2186389723537907\n",
            "training error 0.12499612663820324, test error 0.25059727564210704\n",
            "Loss: 0.2191793466924885\n",
            "training error 0.12508466084548583, test error 0.2506330395095873\n",
            "Loss: 0.23348207779743113\n",
            "training error 0.1249510287306867, test error 0.2507124394619054\n",
            "Loss: 0.2652358071270733\n",
            "training error 0.12512192901027766, test error 0.25078955700998923\n",
            "Loss: 0.296076754469321\n",
            "training error 0.1249632240084835, test error 0.25065636833058846\n",
            "Loss: 0.24281176939071614\n",
            "training error 0.1250005478195321, test error 0.2506420490569003\n",
            "Loss: 0.23708518735110484\n",
            "training error 0.12490699068795724, test error 0.2507476467150117\n",
            "Loss: 0.27931593630774465\n",
            "training error 0.12497786520254991, test error 0.25090296764883335\n",
            "Loss: 0.34143208055983365\n",
            "training error 0.12496121696359044, test error 0.2508118142373189\n",
            "Loss: 0.3049778929665292\n",
            "training error 0.1248959765879036, test error 0.25077085599972565\n",
            "Loss: 0.28859782279790913\n",
            "training error 0.12491975256305882, test error 0.2507707890808633\n",
            "Loss: 0.2885710605218694\n",
            "training error 0.12493984435572035, test error 0.25085169498683174\n",
            "Loss: 0.32092705273958355\n",
            "training error 0.12489075595622386, test error 0.2507283565665338\n",
            "Loss: 0.27160139573669806\n",
            "training error 0.12514082652872252, test error 0.2508321670388211\n",
            "Loss: 0.3131174110787338\n",
            "training error 0.12488897080660208, test error 0.25068607512419455\n",
            "Loss: 0.2546921478529196\n",
            "training error 0.12490379626711526, test error 0.250679540993744\n",
            "Loss: 0.2520790101409487\n",
            "training error 0.1249693879232865, test error 0.2506092866279356\n",
            "Loss: 0.2239827953325335\n",
            "training error 0.12492065570399558, test error 0.2505548690628797\n",
            "Loss: 0.20222005390619913\n",
            "training error 0.12519176924898523, test error 0.2505843239297019\n",
            "Loss: 0.2139996814904288\n",
            "training error 0.1249146008728464, test error 0.250542302927236\n",
            "Loss: 0.19719458905009812\n",
            "training error 0.12489974376406623, test error 0.25055298660874015\n",
            "Loss: 0.2014672204664114\n",
            "training error 0.12500941657248724, test error 0.2504607749633482\n",
            "Loss: 0.16458982264269117\n",
            "training error 0.12505037604065078, test error 0.25057768338737435\n",
            "Loss: 0.2113439874060541\n",
            "training error 0.12495809028232858, test error 0.25058822953936555\n",
            "Loss: 0.21556161784561656\n",
            "training error 0.12495414324020877, test error 0.2505990379722388\n",
            "Loss: 0.21988413998719203\n",
            "training error 0.12490305054071124, test error 0.25044825220211026\n",
            "Loss: 0.15958170413381545\n",
            "training error 0.124894368794267, test error 0.25052120152901114\n",
            "Loss: 0.18875569119001234\n",
            "training error 0.1248771534962739, test error 0.25054928473507043\n",
            "Loss: 0.1999867624675078\n",
            "training error 0.12487559555637698, test error 0.2506386820343402\n",
            "Loss: 0.2357386434313824\n",
            "training error 0.12486465317955862, test error 0.2505862261099203\n",
            "Loss: 0.2147604038086115\n",
            "training error 0.12489728208918839, test error 0.25058854267448955\n",
            "Loss: 0.21568684724035947\n",
            "training error 0.12487770603612083, test error 0.2505845754613493\n",
            "Loss: 0.2141002743449194\n",
            "training error 0.12485942642955024, test error 0.2505931320441547\n",
            "Loss: 0.21752223375994806\n",
            "training error 0.12493514713591015, test error 0.250704017702837\n",
            "Loss: 0.261867766591406\n",
            "training error 0.12492128485280465, test error 0.2507410546992313\n",
            "Loss: 0.2766796490214496\n",
            "training error 0.12487851127699451, test error 0.2506701243620066\n",
            "Loss: 0.24831309886950148\n",
            "training error 0.12489483291176387, test error 0.2506640875660266\n",
            "Loss: 0.24589885578785164\n",
            "training error 0.12490252787024872, test error 0.2505890292913119\n",
            "Loss: 0.21588145565525352\n",
            "training error 0.12490359231137103, test error 0.25054091924820593\n",
            "Loss: 0.1966412263827877\n",
            "training error 0.12493250037242634, test error 0.25057985067072946\n",
            "Loss: 0.21221073010588842\n",
            "training error 0.12484104702841732, test error 0.250456088135107\n",
            "Loss: 0.16271546036614737\n",
            "training error 0.12485700319947604, test error 0.2504322252899216\n",
            "Loss: 0.15317220114583474\n",
            "training error 0.12491910186380804, test error 0.250484659009416\n",
            "Loss: 0.17414156054673668\n",
            "training error 0.12503653427389247, test error 0.2505066405473866\n",
            "Loss: 0.1829324450068448\n",
            "training error 0.1248145004592841, test error 0.25048860947139684\n",
            "Loss: 0.17572143429731568\n",
            "training error 0.1248933681203758, test error 0.2505298317724132\n",
            "Loss: 0.19220710904401805\n",
            "training error 0.12505885717689577, test error 0.2504760748485655\n",
            "Loss: 0.1707085720849877\n",
            "training error 0.12506347450515015, test error 0.2507307882737526\n",
            "Loss: 0.2725738871624639\n",
            "training error 0.12478845339333862, test error 0.250685806735662\n",
            "Loss: 0.2545848135716122\n",
            "training error 0.12493445076626178, test error 0.25072261261196066\n",
            "Loss: 0.26930426616078584\n",
            "training error 0.12480535371911737, test error 0.2505471080590909\n",
            "Loss: 0.19911626345745503\n",
            "training error 0.12479676783304977, test error 0.2505335448648303\n",
            "Loss: 0.19369205365853404\n",
            "training error 0.12481692262548137, test error 0.25053763364218495\n",
            "Loss: 0.19532724266833057\n",
            "training error 0.12479582196729898, test error 0.25050802090746244\n",
            "Loss: 0.1834844803537905\n",
            "training error 0.12481075207862219, test error 0.25046665865451817\n",
            "Loss: 0.1669428358551217\n",
            "training error 0.12478138794589098, test error 0.25058800721413504\n",
            "Loss: 0.21547270525832385\n",
            "training error 0.12476974221738577, test error 0.25050168074854084\n",
            "Loss: 0.18094891598094964\n",
            "training error 0.12474695394103627, test error 0.25060945062341927\n",
            "Loss: 0.22404838061373944\n",
            "training error 0.12472665806781956, test error 0.25059634491213734\n",
            "Loss: 0.21880712798614255\n",
            "training error 0.12479551415240774, test error 0.2505090370610212\n",
            "Loss: 0.18389086176988645\n",
            "training error 0.12489833435542234, test error 0.2506990063294525\n",
            "Loss: 0.25986361181076223\n",
            "training error 0.12487482926212859, test error 0.25054417339916746\n",
            "Loss: 0.19794263055008532\n",
            "training error 0.12472190216296394, test error 0.2505703127190634\n",
            "Loss: 0.208396300415159\n",
            "training error 0.12468383084870108, test error 0.2505922821369886\n",
            "Loss: 0.21718233781140572\n",
            "training error 0.12521105762134643, test error 0.25044229955623243\n",
            "Loss: 0.15720111446742013\n",
            "training error 0.12469426782875184, test error 0.2506207182997077\n",
            "Loss: 0.22855456396262408\n",
            "training error 0.12467448061474723, test error 0.2505114438018604\n",
            "Loss: 0.18485336860956103\n",
            "training error 0.12485513372626282, test error 0.2504801665220783\n",
            "Loss: 0.17234491933000307\n",
            "training error 0.12493345075808057, test error 0.25062341728419774\n",
            "Loss: 0.22963394525263858\n",
            "training error 0.12479402413107506, test error 0.2505301388466728\n",
            "Loss: 0.19232991457018844\n",
            "training error 0.12464945939995736, test error 0.2503740687817239\n",
            "Loss: 0.12991417685044304\n",
            "training error 0.12461414493528325, test error 0.2504311091934852\n",
            "Loss: 0.15272585044772136\n",
            "training error 0.12464615114260828, test error 0.2503829865910014\n",
            "Loss: 0.13348059841296056\n",
            "training error 0.12458871631425815, test error 0.2502445176946265\n",
            "Loss: 0.0781039422854235\n",
            "training error 0.12462664585508874, test error 0.250237490574047\n",
            "Loss: 0.07529364733769306\n",
            "training error 0.12460161667072939, test error 0.25015265124733077\n",
            "Loss: 0.04136459652084312\n",
            "training error 0.1245333950723583, test error 0.25019587449672104\n",
            "Loss: 0.05865049307136605\n",
            "training error 0.1248253074303218, test error 0.2502091610830281\n",
            "Loss: 0.0639640814677156\n",
            "training error 0.12449665304637841, test error 0.25005382824750105\n",
            "Loss: 0.001843177470561841\n",
            "training error 0.12453921706311645, test error 0.25008886099655614\n",
            "Loss: 0.01585351877040342\n",
            "training error 0.12445847589363848, test error 0.24998852850963418\n",
            "Loss: 0.0\n",
            "training error 0.12444476459397671, test error 0.24997521670204176\n",
            "Loss: 0.0\n",
            "training error 0.12446446562781889, test error 0.24992376618867654\n",
            "Loss: 0.0\n",
            "training error 0.12442919200759904, test error 0.24999129334647252\n",
            "Loss: 0.0270191021949584\n",
            "training error 0.12444159121887424, test error 0.24979929062939454\n",
            "Loss: 0.0\n",
            "training error 0.12438771506668862, test error 0.24986646501489246\n",
            "Loss: 0.02689134357773959\n",
            "training error 0.12436826944139283, test error 0.24973849122324596\n",
            "Loss: 0.0\n",
            "training error 0.12432974186421582, test error 0.24967860270735276\n",
            "Loss: 0.0\n",
            "training error 0.1243429837219945, test error 0.24967489583442265\n",
            "Loss: 0.0\n",
            "training error 0.12429205665773045, test error 0.2496221618171006\n",
            "Loss: 0.0\n",
            "training error 0.12427991961211599, test error 0.24952231570173486\n",
            "Loss: 0.0\n",
            "training error 0.12422949836826719, test error 0.24956623790224716\n",
            "Loss: 0.017602513983083767\n",
            "training error 0.1242329616571047, test error 0.2495406851771805\n",
            "Loss: 0.007361856751764151\n",
            "training error 0.1241808745249075, test error 0.24947915566956003\n",
            "Loss: 0.0\n",
            "training error 0.12415768395469098, test error 0.2494765540059105\n",
            "Loss: 0.0\n",
            "training error 0.12405897659480775, test error 0.24945985660178127\n",
            "Loss: 0.0\n",
            "training error 0.12401842813036774, test error 0.2493557920059887\n",
            "Loss: 0.0\n",
            "training error 0.12404699211669117, test error 0.24929172133774555\n",
            "Loss: 0.0\n",
            "training error 0.12397482773989274, test error 0.24937274520135985\n",
            "Loss: 0.03250162627925768\n",
            "training error 0.12397588406513517, test error 0.2491615725422898\n",
            "Loss: 0.0\n",
            "training error 0.12393659815844027, test error 0.24914646817571418\n",
            "Loss: 0.0\n",
            "training error 0.12384318140878955, test error 0.24900374291775484\n",
            "Loss: 0.0\n",
            "training error 0.12381139595697933, test error 0.24876587022501753\n",
            "Loss: 0.0\n",
            "training error 0.12370832394959973, test error 0.24875760657449433\n",
            "Loss: 0.0\n",
            "training error 0.12367951939982898, test error 0.24862259669239137\n",
            "Loss: 0.0\n",
            "training error 0.12364241515447402, test error 0.24854587160685768\n",
            "Loss: 0.0\n",
            "training error 0.12357522011409586, test error 0.24831320347872743\n",
            "Loss: 0.0\n",
            "training error 0.1235380623814191, test error 0.24825843920891855\n",
            "Loss: 0.0\n",
            "training error 0.12346280112048065, test error 0.2480605819384105\n",
            "Loss: 0.0\n",
            "training error 0.12346259012777279, test error 0.24789466132396065\n",
            "Loss: 0.0\n",
            "training error 0.12327453184159616, test error 0.2478242583066372\n",
            "Loss: 0.0\n",
            "training error 0.12335874802333205, test error 0.247760140093259\n",
            "Loss: 0.0\n",
            "training error 0.12324567117533272, test error 0.24751332980002497\n",
            "Loss: 0.0\n",
            "training error 0.12305416872076375, test error 0.2474340758363461\n",
            "Loss: 0.0\n",
            "training error 0.12296782245522607, test error 0.24727626982380194\n",
            "Loss: 0.0\n",
            "training error 0.1230389774856034, test error 0.2470902822747799\n",
            "Loss: 0.0\n",
            "training error 0.12289580787823626, test error 0.24680993251907443\n",
            "Loss: 0.0\n",
            "training error 0.1227172921799123, test error 0.24660636581106668\n",
            "Loss: 0.0\n",
            "training error 0.12261470917949657, test error 0.24639450470352992\n",
            "Loss: 0.0\n",
            "training error 0.12256640806734116, test error 0.2462324865159718\n",
            "Loss: 0.0\n",
            "training error 0.12239918937937182, test error 0.24591776991293635\n",
            "Loss: 0.0\n",
            "training error 0.12235769472954339, test error 0.24569889904168934\n",
            "Loss: 0.0\n",
            "training error 0.1221803008142429, test error 0.24556299274455215\n",
            "Loss: 0.0\n",
            "training error 0.12209025097476577, test error 0.2452822183264668\n",
            "Loss: 0.0\n",
            "training error 0.12198261726999811, test error 0.2451748747964189\n",
            "Loss: 0.0\n",
            "training error 0.12186464549715807, test error 0.24486808694711418\n",
            "Loss: 0.0\n",
            "training error 0.12164460935693593, test error 0.24459679623641986\n",
            "Loss: 0.0\n",
            "training error 0.1215325278469179, test error 0.24426511862348252\n",
            "Loss: 0.0\n",
            "training error 0.12133829916142749, test error 0.2438634704192806\n",
            "Loss: 0.0\n",
            "training error 0.12120843196726154, test error 0.24352876610132992\n",
            "Loss: 0.0\n",
            "training error 0.12105971300306266, test error 0.2432294798991989\n",
            "Loss: 0.0\n",
            "training error 0.12085098799170692, test error 0.2429522280678655\n",
            "Loss: 0.0\n",
            "training error 0.1209315378416809, test error 0.2426697209917898\n",
            "Loss: 0.0\n",
            "training error 0.12050236600731526, test error 0.2421608442790938\n",
            "Loss: 0.0\n",
            "training error 0.12032082278985168, test error 0.24182136226007986\n",
            "Loss: 0.0\n",
            "training error 0.12012415317567647, test error 0.241383186031561\n",
            "Loss: 0.0\n",
            "training error 0.12000341577754176, test error 0.24088876471290183\n",
            "Loss: 0.0\n",
            "training error 0.11970427227678328, test error 0.24049787720304103\n",
            "Loss: 0.0\n",
            "training error 0.11950452414160541, test error 0.24001192745428143\n",
            "Loss: 0.0\n",
            "training error 0.11922320327753687, test error 0.23953055381892469\n",
            "Loss: 0.0\n",
            "training error 0.11908392877664188, test error 0.2389760013534127\n",
            "Loss: 0.0\n",
            "training error 0.1187504100003098, test error 0.23856452612045226\n",
            "Loss: 0.0\n",
            "training error 0.11846980627084465, test error 0.2379430737506697\n",
            "Loss: 0.0\n",
            "training error 0.11818989148072513, test error 0.2374480938233102\n",
            "Loss: 0.0\n",
            "training error 0.11790492146517173, test error 0.23686031966407445\n",
            "Loss: 0.0\n",
            "training error 0.11761806175560063, test error 0.23624221813333876\n",
            "Loss: 0.0\n",
            "training error 0.11744189478928323, test error 0.23571234675028938\n",
            "Loss: 0.0\n",
            "training error 0.1171386183820435, test error 0.2348271639554026\n",
            "Loss: 0.0\n",
            "training error 0.11682152777598147, test error 0.23416464189380012\n",
            "Loss: 0.0\n",
            "training error 0.11695362933289495, test error 0.23366774470776505\n",
            "Loss: 0.0\n",
            "training error 0.1160928085741509, test error 0.23287174425149368\n",
            "Loss: 0.0\n",
            "training error 0.11573504550976473, test error 0.23214754623579492\n",
            "Loss: 0.0\n",
            "training error 0.11528266489591209, test error 0.23127278435926998\n",
            "Loss: 0.0\n",
            "training error 0.11504338617061753, test error 0.23039164634809808\n",
            "Loss: 0.0\n",
            "training error 0.11451115845849713, test error 0.22973093956080778\n",
            "Loss: 0.0\n",
            "training error 0.1141924041762443, test error 0.22874157353716124\n",
            "Loss: 0.0\n",
            "training error 0.11385925338012261, test error 0.2280368417059432\n",
            "Loss: 0.0\n",
            "training error 0.11321913019569639, test error 0.22696970862975324\n",
            "Loss: 0.0\n",
            "training error 0.11282211628696201, test error 0.22589939191624356\n",
            "Loss: 0.0\n",
            "training error 0.11272939453732805, test error 0.22514114633394763\n",
            "Loss: 0.0\n",
            "training error 0.11183717066800916, test error 0.22405466612902306\n",
            "Loss: 0.0\n",
            "training error 0.11162266352839573, test error 0.2230390047184325\n",
            "Loss: 0.0\n",
            "training error 0.11093266478684777, test error 0.2217631497318775\n",
            "Loss: 0.0\n",
            "training error 0.11043594097975726, test error 0.22087688600440414\n",
            "Loss: 0.0\n",
            "training error 0.10983003059287208, test error 0.21963494449802\n",
            "Loss: 0.0\n",
            "training error 0.10918546837666902, test error 0.21852814235629583\n",
            "Loss: 0.0\n",
            "training error 0.10865128459908037, test error 0.21737741485068254\n",
            "Loss: 0.0\n",
            "training error 0.10807368292821148, test error 0.21616120836738395\n",
            "Loss: 0.0\n",
            "training error 0.10762727682447193, test error 0.21486450702522444\n",
            "Loss: 0.0\n",
            "training error 0.10688407395289719, test error 0.213630554923704\n",
            "Loss: 0.0\n",
            "training error 0.10632481377391619, test error 0.21242034464396536\n",
            "Loss: 0.0\n",
            "training error 0.10576918646835676, test error 0.21117704535446677\n",
            "Loss: 0.0\n",
            "training error 0.10503406079726714, test error 0.2098943446262335\n",
            "Loss: 0.0\n",
            "training error 0.104379725016578, test error 0.20867882981170374\n",
            "Loss: 0.0\n",
            "training error 0.10369574715402285, test error 0.2072669545607959\n",
            "Loss: 0.0\n",
            "training error 0.10308170677476537, test error 0.2059145225233124\n",
            "Loss: 0.0\n",
            "training error 0.10238490764557014, test error 0.20442723964564738\n",
            "Loss: 0.0\n",
            "training error 0.10161000473298618, test error 0.20286277642740266\n",
            "Loss: 0.0\n",
            "training error 0.10110896619249102, test error 0.20132642859172345\n",
            "Loss: 0.0\n",
            "training error 0.10020557183537149, test error 0.19962288258256008\n",
            "Loss: 0.0\n",
            "training error 0.09946701597995158, test error 0.1981044275834155\n",
            "Loss: 0.0\n",
            "training error 0.09876950830030724, test error 0.19646798294461004\n",
            "Loss: 0.0\n",
            "training error 0.09792127683640033, test error 0.19490797079676556\n",
            "Loss: 0.0\n",
            "training error 0.09718452988516431, test error 0.19340685959263412\n",
            "Loss: 0.0\n",
            "training error 0.0965007451035865, test error 0.19187075641176812\n",
            "Loss: 0.0\n",
            "training error 0.09569813761152099, test error 0.19022208824443396\n",
            "Loss: 0.0\n",
            "training error 0.09498029753940763, test error 0.18871854165483154\n",
            "Loss: 0.0\n",
            "training error 0.09415027592331947, test error 0.18684064532129965\n",
            "Loss: 0.0\n",
            "training error 0.09334131536977547, test error 0.18537814272078262\n",
            "Loss: 0.0\n",
            "training error 0.09249190411238532, test error 0.1835309142532172\n",
            "Loss: 0.0\n",
            "training error 0.09168234221064857, test error 0.1817987703262929\n",
            "Loss: 0.0\n",
            "training error 0.0908854935937598, test error 0.18005575753558473\n",
            "Loss: 0.0\n",
            "training error 0.09009432921748417, test error 0.1784009481208492\n",
            "Loss: 0.0\n",
            "training error 0.08926090975283933, test error 0.17660765914576318\n",
            "Loss: 0.0\n",
            "training error 0.08844518811948612, test error 0.1747721933329704\n",
            "Loss: 0.0\n",
            "training error 0.08769842686073276, test error 0.17311298157782684\n",
            "Loss: 0.0\n",
            "training error 0.08683094883832958, test error 0.17145602393502757\n",
            "Loss: 0.0\n",
            "training error 0.08611252620365481, test error 0.16959782766705622\n",
            "Loss: 0.0\n",
            "training error 0.08538397805492734, test error 0.16788003737224677\n",
            "Loss: 0.0\n",
            "training error 0.08442149736551188, test error 0.1662054743138401\n",
            "Loss: 0.0\n",
            "training error 0.08364473377277787, test error 0.1644545450865106\n",
            "Loss: 0.0\n",
            "training error 0.0828859426397942, test error 0.16273032712059476\n",
            "Loss: 0.0\n",
            "training error 0.0821022106806493, test error 0.16102403407295288\n",
            "Loss: 0.0\n",
            "training error 0.0812818220054974, test error 0.15938342574798345\n",
            "Loss: 0.0\n",
            "training error 0.080514563536919, test error 0.1579111373858343\n",
            "Loss: 0.0\n",
            "training error 0.0797845220625977, test error 0.1561085067497665\n",
            "Loss: 0.0\n",
            "training error 0.07901137693260878, test error 0.15462657199117358\n",
            "Loss: 0.0\n",
            "training error 0.07824117222820369, test error 0.1530444611701733\n",
            "Loss: 0.0\n",
            "training error 0.07752719632090077, test error 0.15123455606941788\n",
            "Loss: 0.0\n",
            "training error 0.0767185826498304, test error 0.14991126823861706\n",
            "Loss: 0.0\n",
            "training error 0.07595784049360298, test error 0.14810360428942726\n",
            "Loss: 0.0\n",
            "training error 0.07523255552065684, test error 0.14646209565320173\n",
            "Loss: 0.0\n",
            "training error 0.07451972787564445, test error 0.14488938031112708\n",
            "Loss: 0.0\n",
            "training error 0.07378416482423551, test error 0.1433130484091688\n",
            "Loss: 0.0\n",
            "training error 0.0730538189774236, test error 0.14173799875391327\n",
            "Loss: 0.0\n",
            "training error 0.07235421974130324, test error 0.14017630655204572\n",
            "Loss: 0.0\n",
            "training error 0.0716546136537585, test error 0.1386528521663233\n",
            "Loss: 0.0\n",
            "training error 0.07099825400658867, test error 0.13708131266169893\n",
            "Loss: 0.0\n",
            "training error 0.07032665548450419, test error 0.1355843758995524\n",
            "Loss: 0.0\n",
            "training error 0.06968962878280423, test error 0.13406438911943352\n",
            "Loss: 0.0\n",
            "training error 0.06898267797992905, test error 0.1326962861492037\n",
            "Loss: 0.0\n",
            "training error 0.06834386201237776, test error 0.1312927795554544\n",
            "Loss: 0.0\n",
            "training error 0.06774574244665611, test error 0.12978485419523908\n",
            "Loss: 0.0\n",
            "training error 0.06706226109097634, test error 0.12857736125013206\n",
            "Loss: 0.0\n",
            "training error 0.06656076286154125, test error 0.12722870993221982\n",
            "Loss: 0.0\n",
            "training error 0.06580895250765327, test error 0.12600047162549072\n",
            "Loss: 0.0\n",
            "training error 0.06542293676344854, test error 0.12485308639524943\n",
            "Loss: 0.0\n",
            "training error 0.06466442612939753, test error 0.12334533655801398\n",
            "Loss: 0.0\n",
            "training error 0.0640655123633138, test error 0.12218145327105542\n",
            "Loss: 0.0\n",
            "training error 0.06351821472494605, test error 0.12083458365398496\n",
            "Loss: 0.0\n",
            "training error 0.06294906345664217, test error 0.11949461225866653\n",
            "Loss: 0.0\n",
            "training error 0.062443004488462836, test error 0.11838977246465077\n",
            "Loss: 0.0\n",
            "training error 0.06192601642785561, test error 0.11724924190516375\n",
            "Loss: 0.0\n",
            "training error 0.06136814579845392, test error 0.11580403897961583\n",
            "Loss: 0.0\n",
            "training error 0.06085034745766905, test error 0.11473950510789406\n",
            "Loss: 0.0\n",
            "training error 0.06034997854855233, test error 0.1136390425692231\n",
            "Loss: 0.0\n",
            "training error 0.05986238911131166, test error 0.11237234968210348\n",
            "Loss: 0.0\n",
            "training error 0.05935335803961927, test error 0.11114229044788031\n",
            "Loss: 0.0\n",
            "training error 0.058914365559112336, test error 0.1101334282425756\n",
            "Loss: 0.0\n",
            "training error 0.058466148468721446, test error 0.10904356383221517\n",
            "Loss: 0.0\n",
            "training error 0.05794603035128096, test error 0.10806197201822978\n",
            "Loss: 0.0\n",
            "training error 0.0575263738092389, test error 0.10687835473435912\n",
            "Loss: 0.0\n",
            "training error 0.05710574956311506, test error 0.1060734203455646\n",
            "Loss: 0.0\n",
            "training error 0.05660665718424031, test error 0.1050331488391101\n",
            "Loss: 0.0\n",
            "training error 0.056452549772168285, test error 0.1039655587332653\n",
            "Loss: 0.0\n",
            "training error 0.05580282680868954, test error 0.10313275061600964\n",
            "Loss: 0.0\n",
            "training error 0.0554215494991958, test error 0.10207903509740825\n",
            "Loss: 0.0\n",
            "training error 0.05503984596963, test error 0.10113953048669957\n",
            "Loss: 0.0\n",
            "training error 0.05458109163900509, test error 0.10045347794577626\n",
            "Loss: 0.0\n",
            "training error 0.05421837051121874, test error 0.0996231468550614\n",
            "Loss: 0.0\n",
            "training error 0.05386344560220665, test error 0.0985740731587751\n",
            "Loss: 0.0\n",
            "training error 0.05350077553254936, test error 0.09779719164781209\n",
            "Loss: 0.0\n",
            "training error 0.05310234593432032, test error 0.0969721983040103\n",
            "Loss: 0.0\n",
            "training error 0.05269855172803429, test error 0.0962049813399936\n",
            "Loss: 0.0\n",
            "training error 0.05235988536491471, test error 0.09538299833694003\n",
            "Loss: 0.0\n",
            "training error 0.05209027333430133, test error 0.09445760132122186\n",
            "Loss: 0.0\n",
            "training error 0.05171578882821356, test error 0.0936692488247548\n",
            "Loss: 0.0\n",
            "training error 0.051477651117376434, test error 0.09314565444873375\n",
            "Loss: 0.0\n",
            "training error 0.051123614833670415, test error 0.09228551928508723\n",
            "Loss: 0.0\n",
            "training error 0.05085154875189403, test error 0.09159165108104267\n",
            "Loss: 0.0\n",
            "training error 0.0504416396124938, test error 0.09097204920755608\n",
            "Loss: 0.0\n",
            "training error 0.05014582564792293, test error 0.09036079784760034\n",
            "Loss: 0.0\n",
            "training error 0.049874288626388874, test error 0.08948041530061829\n",
            "Loss: 0.0\n",
            "training error 0.049585868458975176, test error 0.08877117928482219\n",
            "Loss: 0.0\n",
            "training error 0.049282489462192124, test error 0.08808331081104448\n",
            "Loss: 0.0\n",
            "training error 0.04901598369769925, test error 0.0874048614541607\n",
            "Loss: 0.0\n",
            "training error 0.048733833014285, test error 0.086679088318352\n",
            "Loss: 0.0\n",
            "training error 0.048544585782159165, test error 0.08611511197241035\n",
            "Loss: 0.0\n",
            "training error 0.04820565396509888, test error 0.08540324832759703\n",
            "Loss: 0.0\n",
            "training error 0.04796042394050475, test error 0.08504667446788866\n",
            "Loss: 0.0\n",
            "training error 0.047716473624984004, test error 0.08443823908036581\n",
            "Loss: 0.0\n",
            "training error 0.0474647888599993, test error 0.08391532785707534\n",
            "Loss: 0.0\n",
            "training error 0.04723684822107026, test error 0.08344917907101214\n",
            "Loss: 0.0\n",
            "training error 0.046992293365664094, test error 0.08285080185747247\n",
            "Loss: 0.0\n",
            "training error 0.046830935537393564, test error 0.08233622493321534\n",
            "Loss: 0.0\n",
            "training error 0.046560943484578794, test error 0.08192638556328329\n",
            "Loss: 0.0\n",
            "training error 0.046360559071026486, test error 0.08123789241821738\n",
            "Loss: 0.0\n",
            "training error 0.046137283100805844, test error 0.08079915757274217\n",
            "Loss: 0.0\n",
            "training error 0.04590268497550973, test error 0.08022352424800774\n",
            "Loss: 0.0\n",
            "training error 0.045702915371180854, test error 0.07982209193030819\n",
            "Loss: 0.0\n",
            "training error 0.045510455659427504, test error 0.07933300020903332\n",
            "Loss: 0.0\n",
            "training error 0.045392114044249636, test error 0.0788405092154163\n",
            "Loss: 0.0\n",
            "training error 0.04528236219808316, test error 0.07864300744043538\n",
            "Loss: 0.0\n",
            "training error 0.04500674823765279, test error 0.07800677168922268\n",
            "Loss: 0.0\n",
            "training error 0.044856412404742856, test error 0.07771196213482974\n",
            "Loss: 0.0\n",
            "training error 0.044559045627509675, test error 0.07718416484359239\n",
            "Loss: 0.0\n",
            "training error 0.0443933632787859, test error 0.07679597412426466\n",
            "Loss: 0.0\n",
            "training error 0.044263130649527985, test error 0.07612651681280119\n",
            "Loss: 0.0\n",
            "training error 0.04413413207239676, test error 0.07550231148642512\n",
            "Loss: 0.0\n",
            "training error 0.04390582227628022, test error 0.07512367862913712\n",
            "Loss: 0.0\n",
            "training error 0.043722578333697165, test error 0.07480468893830813\n",
            "Loss: 0.0\n",
            "training error 0.04355532161609149, test error 0.07448241452763242\n",
            "Loss: 0.0\n",
            "training error 0.04346423188990241, test error 0.07437373863986525\n",
            "Loss: 0.0\n",
            "training error 0.04327063149268181, test error 0.07403041283983472\n",
            "Loss: 0.0\n",
            "training error 0.04313781613564199, test error 0.0736308527169502\n",
            "Loss: 0.0\n",
            "training error 0.04308245228108797, test error 0.07345005728756876\n",
            "Loss: 0.0\n",
            "training error 0.04290742273029922, test error 0.07302789712913187\n",
            "Loss: 0.0\n",
            "training error 0.0426957775677961, test error 0.07250937377607249\n",
            "Loss: 0.0\n",
            "training error 0.04256390296314536, test error 0.07226601199962615\n",
            "Loss: 0.0\n",
            "training error 0.042447420936564895, test error 0.0720076224117265\n",
            "Loss: 0.0\n",
            "training error 0.0423216100884226, test error 0.07159292133060204\n",
            "Loss: 0.0\n",
            "training error 0.042167043497122775, test error 0.07130274372387602\n",
            "Loss: 0.0\n",
            "training error 0.042027417614935864, test error 0.07099330570753767\n",
            "Loss: 0.0\n",
            "training error 0.04195284898092959, test error 0.07058799672820983\n",
            "Loss: 0.0\n",
            "training error 0.04178017165666502, test error 0.07055328263335876\n",
            "Loss: 0.0\n",
            "training error 0.041724328515228344, test error 0.0704030272776046\n",
            "Loss: 0.0\n",
            "training error 0.04155084143083635, test error 0.06977443097630724\n",
            "Loss: 0.0\n",
            "training error 0.04144526094539948, test error 0.06944787825960576\n",
            "Loss: 0.0\n",
            "training error 0.04139994546572915, test error 0.06915355651852162\n",
            "Loss: 0.0\n",
            "training error 0.04121420968296959, test error 0.06885391610410786\n",
            "Loss: 0.0\n",
            "training error 0.04118366697824617, test error 0.06880390744839592\n",
            "Loss: 0.0\n",
            "training error 0.04101487730757116, test error 0.06839334161118978\n",
            "Loss: 0.0\n",
            "training error 0.04093355839276782, test error 0.06807668019533462\n",
            "Loss: 0.0\n",
            "training error 0.040805590518369364, test error 0.06787952173651751\n",
            "Loss: 0.0\n",
            "training error 0.04080398171440478, test error 0.06757045351307096\n",
            "Loss: 0.0\n",
            "training error 0.040617332075339976, test error 0.0672250836695781\n",
            "Loss: 0.0\n",
            "training error 0.040512148762014345, test error 0.06703086078909207\n",
            "Loss: 0.0\n",
            "training error 0.04038095286198435, test error 0.06684867421987457\n",
            "Loss: 0.0\n",
            "training error 0.04034304389427138, test error 0.06684835222138191\n",
            "Loss: 0.0\n",
            "training error 0.04034167648239378, test error 0.0666954794662477\n",
            "Loss: 0.0\n",
            "training error 0.040131798380446795, test error 0.06630147731486392\n",
            "Loss: 0.0\n",
            "training error 0.040094672678725934, test error 0.0660915809160436\n",
            "Loss: 0.0\n",
            "training error 0.039978800544059834, test error 0.06606279875591789\n",
            "Loss: 0.0\n",
            "training error 0.039998999705588095, test error 0.065680189963958\n",
            "Loss: 0.0\n",
            "training error 0.03982497920553173, test error 0.06577904666890133\n",
            "Loss: 0.15051220923323338\n",
            "training error 0.03972613755950179, test error 0.06548718930803733\n",
            "Loss: 0.0\n",
            "training error 0.03968646076902038, test error 0.0653377427503943\n",
            "Loss: 0.0\n",
            "training error 0.0395589985965995, test error 0.06513582388706095\n",
            "Loss: 0.0\n",
            "training error 0.03949217105572179, test error 0.06487967324572053\n",
            "Loss: 0.0\n",
            "training error 0.03944199033770788, test error 0.06472925996506199\n",
            "Loss: 0.0\n",
            "training error 0.039352866346989465, test error 0.06460769628902861\n",
            "Loss: 0.0\n",
            "training error 0.03927833817378044, test error 0.06441432373419788\n",
            "Loss: 0.0\n",
            "training error 0.03922829006520841, test error 0.0641445512195769\n",
            "Loss: 0.0\n",
            "training error 0.03912775829355177, test error 0.0640402333985573\n",
            "Loss: 0.0\n",
            "training error 0.039070722038361115, test error 0.06388571945938311\n",
            "Loss: 0.0\n",
            "training error 0.03904970332291732, test error 0.0636877993549397\n",
            "Loss: 0.0\n",
            "training error 0.038962542511471444, test error 0.0635822368015033\n",
            "Loss: 0.0\n",
            "training error 0.03890310063256364, test error 0.06358910357705612\n",
            "Loss: 0.010799833252583824\n",
            "training error 0.03882688476731071, test error 0.06348613543957676\n",
            "Loss: 0.0\n",
            "training error 0.038792808398342114, test error 0.06325548222644267\n",
            "Loss: 0.0\n",
            "training error 0.03875661756644361, test error 0.06316377702943285\n",
            "Loss: 0.0\n",
            "training error 0.03862503347853626, test error 0.06289935782921895\n",
            "Loss: 0.0\n",
            "training error 0.0385954569728684, test error 0.06259421651675835\n",
            "Loss: 0.0\n",
            "training error 0.0385179093958577, test error 0.06257756749649403\n",
            "Loss: 0.0\n",
            "training error 0.038455115471242515, test error 0.06239988310408815\n",
            "Loss: 0.0\n",
            "training error 0.03840650028128355, test error 0.062237229961253054\n",
            "Loss: 0.0\n",
            "training error 0.038363554022179086, test error 0.06212249628709109\n",
            "Loss: 0.0\n",
            "training error 0.03831922089569559, test error 0.062198021154547076\n",
            "Loss: 0.12157410273236913\n",
            "training error 0.0382678320593665, test error 0.062073092509476895\n",
            "Loss: 0.0\n",
            "training error 0.038195759679925965, test error 0.06195804929331659\n",
            "Loss: 0.0\n",
            "training error 0.03817369233399978, test error 0.061780818960080905\n",
            "Loss: 0.0\n",
            "training error 0.03810072280355508, test error 0.06180028204689068\n",
            "Loss: 0.03150344579010422\n",
            "training error 0.03803971284923325, test error 0.06171305011278873\n",
            "Loss: 0.0\n",
            "training error 0.038005584632083526, test error 0.061565915082641906\n",
            "Loss: 0.0\n",
            "training error 0.037942748353237805, test error 0.061606623367861595\n",
            "Loss: 0.06612146536773\n",
            "training error 0.03793854767782257, test error 0.06144502965479009\n",
            "Loss: 0.0\n",
            "training error 0.037911519178495764, test error 0.061416995739152984\n",
            "Loss: 0.0\n",
            "training error 0.03779249238301902, test error 0.06129545464328076\n",
            "Loss: 0.0\n",
            "training error 0.03779052768744544, test error 0.06115630526215355\n",
            "Loss: 0.0\n",
            "training error 0.0377692172504266, test error 0.061165538847479814\n",
            "Loss: 0.015098337426833197\n",
            "training error 0.0376978214968706, test error 0.060826391411381596\n",
            "Loss: 0.0\n",
            "training error 0.03766849352984405, test error 0.06090578440850821\n",
            "Loss: 0.13052393095238646\n",
            "training error 0.037620059480872145, test error 0.06070002347773756\n",
            "Loss: 0.0\n",
            "training error 0.03758903633552311, test error 0.06075983668655426\n",
            "Loss: 0.09853902089287381\n",
            "training error 0.037562122981064504, test error 0.06064711601071623\n",
            "Loss: 0.0\n",
            "training error 0.037482361878843344, test error 0.060472666024722316\n",
            "Loss: 0.0\n",
            "training error 0.03742954715881466, test error 0.06030746569529118\n",
            "Loss: 0.0\n",
            "training error 0.037380557716177605, test error 0.06025072098584873\n",
            "Loss: 0.0\n",
            "training error 0.037346279718664825, test error 0.0602448854816652\n",
            "Loss: 0.0\n",
            "training error 0.0373272381292707, test error 0.06004307826437545\n",
            "Loss: 0.0\n",
            "training error 0.037325166929295564, test error 0.06008443534395775\n",
            "Loss: 0.06887901283174624\n",
            "training error 0.03728232448862136, test error 0.059824589918682516\n",
            "Loss: 0.0\n",
            "training error 0.03722034628625445, test error 0.059926456566093654\n",
            "Loss: 0.17027554647612853\n",
            "training error 0.03716824376977918, test error 0.05979306644085249\n",
            "Loss: 0.0\n",
            "training error 0.037242125074386435, test error 0.059814610469192626\n",
            "Loss: 0.036030980885470854\n",
            "training error 0.0371107702595926, test error 0.05968314651898343\n",
            "Loss: 0.0\n",
            "training error 0.037070742630377324, test error 0.05954839539968065\n",
            "Loss: 0.0\n",
            "training error 0.03704278996887672, test error 0.0594314243328332\n",
            "Loss: 0.0\n",
            "training error 0.037035566803451145, test error 0.05940834306686894\n",
            "Loss: 0.0\n",
            "training error 0.036990548017025365, test error 0.0594117342406628\n",
            "Loss: 0.005708245035629389\n",
            "training error 0.036953631374351016, test error 0.05924026808364528\n",
            "Loss: 0.0\n",
            "training error 0.036905249407901276, test error 0.05927857261268067\n",
            "Loss: 0.06465961460759839\n",
            "training error 0.036884180106687744, test error 0.05920780610318693\n",
            "Loss: 0.0\n",
            "training error 0.036844796724293284, test error 0.059094946192485\n",
            "Loss: 0.0\n",
            "training error 0.036846051226371995, test error 0.05904908341220704\n",
            "Loss: 0.0\n",
            "training error 0.03687931124299799, test error 0.058899594914141706\n",
            "Loss: 0.0\n",
            "training error 0.03680059830153798, test error 0.058801703711159795\n",
            "Loss: 0.0\n",
            "training error 0.03674267608463987, test error 0.058868523864767035\n",
            "Loss: 0.11363642444013333\n",
            "training error 0.03670131535376465, test error 0.058763367625886444\n",
            "Loss: 0.0\n",
            "training error 0.03673550119864942, test error 0.05860377940909126\n",
            "Loss: 0.0\n",
            "training error 0.036669351893245015, test error 0.058604880499205446\n",
            "Loss: 0.0018788721909857031\n",
            "training error 0.03663822328881227, test error 0.05866643713986635\n",
            "Loss: 0.10691755959577609\n",
            "training error 0.03659063593412852, test error 0.05861293410141495\n",
            "Loss: 0.015621334350779392\n",
            "training error 0.03659801737144433, test error 0.05853025915399108\n",
            "Loss: 0.0\n",
            "training error 0.03653210422511396, test error 0.058565772967770824\n",
            "Loss: 0.06067598929693219\n",
            "training error 0.036493382709216246, test error 0.05850086397738557\n",
            "Loss: 0.0\n",
            "training error 0.03646943488545509, test error 0.05847478406991945\n",
            "Loss: 0.0\n",
            "training error 0.03646671291466103, test error 0.058461987814903534\n",
            "Loss: 0.0\n",
            "training error 0.03646298519111637, test error 0.058352405130506185\n",
            "Loss: 0.0\n",
            "training error 0.036424230671323936, test error 0.058331005508403894\n",
            "Loss: 0.0\n",
            "training error 0.03638045194783774, test error 0.05810242311116132\n",
            "Loss: 0.0\n",
            "training error 0.03636337993964092, test error 0.05808129907033609\n",
            "Loss: 0.0\n",
            "training error 0.036407550742613676, test error 0.058092650616504746\n",
            "Loss: 0.019544236011159732\n",
            "training error 0.03636713645642465, test error 0.05786800887706111\n",
            "Loss: 0.0\n",
            "training error 0.03630331934746745, test error 0.057958170348507705\n",
            "Loss: 0.15580538054824622\n",
            "training error 0.036281046758072456, test error 0.05782202409432097\n",
            "Loss: 0.0\n",
            "training error 0.036286312084710545, test error 0.05770767839475581\n",
            "Loss: 0.0\n",
            "training error 0.036255577876817595, test error 0.057664278652945836\n",
            "Loss: 0.0\n",
            "training error 0.036319323426341206, test error 0.05763696791084971\n",
            "Loss: 0.0\n",
            "training error 0.0362503121839443, test error 0.057548106677740525\n",
            "Loss: 0.0\n",
            "training error 0.036188980153995354, test error 0.05748916377636746\n",
            "Loss: 0.0\n",
            "training error 0.036190964577321, test error 0.05736967345984712\n",
            "Loss: 0.0\n",
            "training error 0.03615982714987438, test error 0.05746024633863303\n",
            "Loss: 0.15787588341302783\n",
            "training error 0.03614293161254617, test error 0.05734414419233515\n",
            "Loss: 0.0\n",
            "training error 0.036165902818581716, test error 0.057247998900348776\n",
            "Loss: 0.0\n",
            "training error 0.03609407805679998, test error 0.057133037851102283\n",
            "Loss: 0.0\n",
            "training error 0.0361034573710632, test error 0.0570970376295647\n",
            "Loss: 0.0\n",
            "training error 0.03605524995120796, test error 0.0570267146205885\n",
            "Loss: 0.0\n",
            "training error 0.036088684732827246, test error 0.05678259408224385\n",
            "Loss: 0.0\n",
            "training error 0.03607745976746704, test error 0.056904871336285454\n",
            "Loss: 0.2153428458455009\n",
            "training error 0.035983395621781676, test error 0.05694714299209166\n",
            "Loss: 0.2897875880934153\n",
            "training error 0.035971890507405964, test error 0.056871676408334765\n",
            "Loss: 0.156883156767873\n",
            "training error 0.03602266134164125, test error 0.056497935647513715\n",
            "Loss: 0.0\n",
            "training error 0.035987055640012054, test error 0.05679230311863911\n",
            "Loss: 0.5210234104161326\n",
            "training error 0.03593633059774254, test error 0.0567787330514369\n",
            "Loss: 0.49700471478295594\n",
            "training error 0.035880615460812684, test error 0.05665669117269822\n",
            "Loss: 0.2809934971340633\n",
            "training error 0.03591435898880406, test error 0.056592691759424704\n",
            "Loss: 0.16771606046310783\n",
            "training error 0.03586946597886481, test error 0.05673567852385512\n",
            "Loss: 0.42079922676230286\n",
            "training error 0.03590213856899483, test error 0.05652212633629531\n",
            "Loss: 0.04281694278622794\n",
            "training error 0.03586297526773889, test error 0.056816584648621375\n",
            "Loss: 0.5640011399632217\n",
            "training error 0.035794547272610173, test error 0.0566898159025856\n",
            "Loss: 0.339623479818818\n",
            "training error 0.035807804095987696, test error 0.05676942977164229\n",
            "Loss: 0.48053813120252364\n",
            "training error 0.035867602883253524, test error 0.05663523775819354\n",
            "Loss: 0.2430214646008455\n",
            "training error 0.03579989900655906, test error 0.05657421418591843\n",
            "Loss: 0.13501119559591324\n",
            "training error 0.035766520308569485, test error 0.0566922421320388\n",
            "Loss: 0.3439178481446703\n",
            "training error 0.03574372937360957, test error 0.05676097290441396\n",
            "Loss: 0.46556967769815216\n",
            "training error 0.03574990465049143, test error 0.05665082564139094\n",
            "Loss: 0.2706116464698649\n",
            "training error 0.035785257675160935, test error 0.056771240055376614\n",
            "Loss: 0.4837422902812305\n",
            "training error 0.03574847667846141, test error 0.05662853996929599\n",
            "Loss: 0.2311665378308847\n",
            "training error 0.035689844960405726, test error 0.05662018904108999\n",
            "Loss: 0.21638559387195944\n",
            "training error 0.03569836151568975, test error 0.05664258796827901\n",
            "Loss: 0.25603116132908443\n",
            "training error 0.03569962186344658, test error 0.05657488575654453\n",
            "Loss: 0.1361998596035363\n",
            "training error 0.035721458694415745, test error 0.05643365417409765\n",
            "Loss: 0.0\n",
            "training error 0.0356699464618455, test error 0.05651819917344587\n",
            "Loss: 0.14981308686372596\n",
            "training error 0.03565201169448713, test error 0.05645086180740145\n",
            "Loss: 0.030491793515108334\n",
            "training error 0.03562482411503198, test error 0.05653302172152541\n",
            "Loss: 0.17607852775438637\n",
            "training error 0.035591135880266446, test error 0.056551772274175446\n",
            "Loss: 0.2093043624525892\n",
            "training error 0.03560048068308864, test error 0.056515743678255376\n",
            "Loss: 0.14546196832208036\n",
            "training error 0.035587906535293284, test error 0.05637111295469212\n",
            "Loss: 0.0\n",
            "training error 0.03558334357064655, test error 0.05639211517219928\n",
            "Loss: 0.03725705668440327\n",
            "training error 0.03557893830424374, test error 0.056557622205588555\n",
            "Loss: 0.3308596213921433\n",
            "training error 0.035571932250058405, test error 0.056463451592849656\n",
            "Loss: 0.163804887499297\n",
            "training error 0.03554459641286258, test error 0.056174343766669285\n",
            "Loss: 0.0\n",
            "training error 0.035541822726496246, test error 0.056253852723845914\n",
            "Loss: 0.14153962796055186\n",
            "training error 0.035531458952146815, test error 0.056307026013979446\n",
            "Loss: 0.23619723598602427\n",
            "training error 0.03561457093367155, test error 0.05648117647162753\n",
            "Loss: 0.5462150234148311\n",
            "training error 0.035489293764086216, test error 0.05630723623085241\n",
            "Loss: 0.2365714582000633\n",
            "training error 0.035474043800038865, test error 0.0561412301998981\n",
            "Loss: 0.0\n",
            "training error 0.035482015108583606, test error 0.05610429779763834\n",
            "Loss: 0.0\n",
            "training error 0.03544056967117311, test error 0.056173728088068066\n",
            "Loss: 0.12375217791718018\n",
            "training error 0.03551728944208141, test error 0.05603943115902781\n",
            "Loss: 0.0\n",
            "training error 0.035447434317746045, test error 0.056074258577219094\n",
            "Loss: 0.0621480580208722\n",
            "training error 0.03547299985195851, test error 0.05616476560509515\n",
            "Loss: 0.2236540298056644\n",
            "training error 0.03541418398421558, test error 0.05598644256494449\n",
            "Loss: 0.0\n",
            "training error 0.03540597597191863, test error 0.055987709594093826\n",
            "Loss: 0.002263099942223512\n",
            "training error 0.03548772068764231, test error 0.05599231003430382\n",
            "Loss: 0.010480161072079497\n",
            "training error 0.03540126987891595, test error 0.05597449143342527\n",
            "Loss: 0.0\n",
            "training error 0.035409235063419316, test error 0.055958850268788426\n",
            "Loss: 0.0\n",
            "training error 0.03544251634950273, test error 0.055868053848869885\n",
            "Loss: 0.0\n",
            "training error 0.03536675448730016, test error 0.055937047810340965\n",
            "Loss: 0.12349447800297231\n",
            "training error 0.035401725766165085, test error 0.055845712109634446\n",
            "Loss: 0.0\n",
            "training error 0.03533843610685687, test error 0.05599036237342993\n",
            "Loss: 0.2590176726755766\n",
            "training error 0.035336487461066914, test error 0.05587379596250062\n",
            "Loss: 0.0502882885816458\n",
            "training error 0.03534969652277295, test error 0.05581479803887652\n",
            "Loss: 0.0\n",
            "training error 0.03531009836999117, test error 0.05579264021463322\n",
            "Loss: 0.0\n",
            "training error 0.03531465022390056, test error 0.05579050163656333\n",
            "Loss: 0.0\n",
            "training error 0.03533844748681453, test error 0.05590623253066882\n",
            "Loss: 0.20743834651173465\n",
            "training error 0.03530012848734859, test error 0.05593461090185936\n",
            "Loss: 0.25830430103461754\n",
            "training error 0.03526820753063907, test error 0.05592555179251951\n",
            "Loss: 0.2420665740486383\n",
            "training error 0.03530221658020407, test error 0.05598704662186505\n",
            "Loss: 0.3522911240018578\n",
            "training error 0.03529807170139067, test error 0.05593549021431703\n",
            "Loss: 0.25988039809750063\n",
            "training error 0.03526115256267917, test error 0.055949190286918284\n",
            "Loss: 0.284436679542166\n",
            "training error 0.03526190870620186, test error 0.05592785531626107\n",
            "Loss: 0.24619545562165612\n",
            "training error 0.035256081347432064, test error 0.05595139226130573\n",
            "Loss: 0.2883835420417924\n",
            "training error 0.03524987080607666, test error 0.055723964104156753\n",
            "Loss: 0.0\n",
            "training error 0.03522435264660351, test error 0.055857387099596825\n",
            "Loss: 0.23943557782550862\n",
            "training error 0.035202462624991646, test error 0.05576812485353091\n",
            "Loss: 0.07924911675631918\n",
            "training error 0.035212094202680684, test error 0.05581284893532023\n",
            "Loss: 0.1595091673617155\n",
            "training error 0.03521632565953526, test error 0.05574615888545021\n",
            "Loss: 0.03982986790382892\n",
            "training error 0.035196536067003764, test error 0.0556538020627801\n",
            "Loss: 0.0\n",
            "training error 0.03518971482799354, test error 0.055461332694008304\n",
            "Loss: 0.0\n",
            "training error 0.03519197071964999, test error 0.055444988910625666\n",
            "Loss: 0.0\n",
            "training error 0.03514893357714359, test error 0.05562590742691508\n",
            "Loss: 0.32630273690026357\n",
            "training error 0.035205912511662295, test error 0.055745442152254576\n",
            "Loss: 0.5418943127813103\n",
            "training error 0.03516951287414295, test error 0.055646444776154165\n",
            "Loss: 0.36334368440984477\n",
            "training error 0.03517505229492699, test error 0.05535960523119232\n",
            "Loss: 0.0\n",
            "training error 0.035161302051046395, test error 0.05541560942503986\n",
            "Loss: 0.10116436635279058\n",
            "training error 0.035162273119149635, test error 0.05542910807775006\n",
            "Loss: 0.12554794469268327\n",
            "training error 0.03511970098049294, test error 0.05540718063997728\n",
            "Loss: 0.0859388512368886\n",
            "training error 0.0351224078528293, test error 0.05549181359006461\n",
            "Loss: 0.23881738014597254\n",
            "training error 0.03512553693121256, test error 0.05552909643117956\n",
            "Loss: 0.30616403292511585\n",
            "training error 0.03512801263852981, test error 0.05559621441497088\n",
            "Loss: 0.42740403005121497\n",
            "training error 0.03510619193796739, test error 0.05537226438200292\n",
            "Loss: 0.02286712623351672\n",
            "training error 0.03510130718344326, test error 0.05535927094186908\n",
            "Loss: 0.0\n",
            "training error 0.035143502811403535, test error 0.05512127156475951\n",
            "Loss: 0.0\n",
            "training error 0.035097485460531846, test error 0.055317995226671524\n",
            "Loss: 0.35689245971202155\n",
            "training error 0.035081254881061154, test error 0.05527367577480085\n",
            "Loss: 0.2764889229057266\n",
            "training error 0.03507469823542428, test error 0.05540164768469682\n",
            "Loss: 0.5086532149533474\n",
            "training error 0.03506836359386583, test error 0.055313443996307686\n",
            "Loss: 0.34863570105128083\n",
            "training error 0.03508278215643985, test error 0.05531387245158788\n",
            "Loss: 0.349412996763121\n",
            "training error 0.03507610850751859, test error 0.055364300230353075\n",
            "Loss: 0.4408981482004526\n",
            "training error 0.03516349530024998, test error 0.05502802136396407\n",
            "Loss: 0.0\n",
            "training error 0.03504089274654737, test error 0.055294919002728815\n",
            "Loss: 0.48502132577044055\n",
            "training error 0.03504947736004808, test error 0.05527372347296382\n",
            "Loss: 0.44650362289901935\n",
            "training error 0.03509639841834036, test error 0.05543946599744485\n",
            "Loss: 0.7477002139681055\n",
            "training error 0.035077682823541366, test error 0.055443370847452766\n",
            "Loss: 0.7547963259327606\n",
            "training error 0.03506944009695807, test error 0.05541425723382261\n",
            "Loss: 0.7018894379354768\n",
            "training error 0.03504647761210938, test error 0.05542465042398005\n",
            "Loss: 0.7207765247320363\n",
            "training error 0.03507714472508677, test error 0.05533951983678437\n",
            "Loss: 0.5660724574485299\n",
            "training error 0.0350266681786632, test error 0.055483380223351204\n",
            "Loss: 0.8275036028922766\n",
            "training error 0.03503364591757331, test error 0.05539166302775669\n",
            "Loss: 0.6608299822147501\n",
            "training error 0.035012182560476904, test error 0.05536243639314262\n",
            "Loss: 0.6077177061604155\n",
            "training error 0.03503413423962078, test error 0.05529868916565981\n",
            "Loss: 0.49187267684132063\n",
            "training error 0.035015503713246125, test error 0.0554755634696099\n",
            "Loss: 0.8132985605382892\n",
            "training error 0.03503702681023821, test error 0.05552027050253025\n",
            "Loss: 0.8945426827367875\n",
            "training error 0.03501617074301692, test error 0.05521575646328942\n",
            "Loss: 0.3411627288643304\n",
            "training error 0.03501958826947934, test error 0.055318879373726274\n",
            "Loss: 0.5285634528605421\n",
            "training error 0.034997787135101356, test error 0.055069369813970044\n",
            "Loss: 0.07514071736740568\n",
            "training error 0.03503335832620506, test error 0.055007895629163965\n",
            "Loss: 0.0\n",
            "training error 0.0349758280465681, test error 0.05519699546416141\n",
            "Loss: 0.3437685314709338\n",
            "training error 0.03501617572446862, test error 0.05520437996200482\n",
            "Loss: 0.3571929640163951\n",
            "training error 0.0349471040238018, test error 0.05533835303854646\n",
            "Loss: 0.6007454122773037\n",
            "training error 0.035071637849147415, test error 0.05533614346998088\n",
            "Loss: 0.5967285915276621\n",
            "training error 0.034984129788552025, test error 0.05528783274841121\n",
            "Loss: 0.508903523840365\n",
            "training error 0.03494987808829662, test error 0.05534621145284597\n",
            "Loss: 0.6150313874262014\n",
            "training error 0.03499338943781085, test error 0.05520720468321409\n",
            "Loss: 0.36232808357870017\n",
            "training error 0.03502471825452339, test error 0.05539581613782976\n",
            "Loss: 0.70520877817426\n",
            "training error 0.03495866117839225, test error 0.055287478147820084\n",
            "Loss: 0.5082588880347849\n",
            "training error 0.03492905688515223, test error 0.0552088968759077\n",
            "Loss: 0.3654043559469855\n",
            "training error 0.03493846550937616, test error 0.05525504554069191\n",
            "Loss: 0.4492989755400023\n",
            "training error 0.034954494856837603, test error 0.05524967870500501\n",
            "Loss: 0.4395424930832226\n",
            "training error 0.03492944131542613, test error 0.055212952752134524\n",
            "Loss: 0.3727776178768183\n",
            "training error 0.03492955464982593, test error 0.05515637576882899\n",
            "Loss: 0.26992514068526763\n",
            "training error 0.03496237969928216, test error 0.05532516120957829\n",
            "Loss: 0.5767637114373114\n",
            "training error 0.034963146442320486, test error 0.05544196926710662\n",
            "Loss: 0.7891115138615179\n",
            "training error 0.034946312262305754, test error 0.05541630368658098\n",
            "Loss: 0.7424535200733828\n",
            "training error 0.034964556468159186, test error 0.05542477869035353\n",
            "Loss: 0.7578604060769489\n",
            "training error 0.03492703767693171, test error 0.055272480334914886\n",
            "Loss: 0.48099405135331175\n",
            "training error 0.03491009000294194, test error 0.05526890948657737\n",
            "Loss: 0.47450253173295653\n",
            "training error 0.03491576564221387, test error 0.05525988226372367\n",
            "Loss: 0.45809175515179046\n",
            "training error 0.034916797994908655, test error 0.055273868101780074\n",
            "Loss: 0.48351690166292816\n",
            "training error 0.035026454802673385, test error 0.05506051974800702\n",
            "Loss: 0.0956664824951936\n",
            "training error 0.03492122888875177, test error 0.055132151742860826\n",
            "Loss: 0.22588777897365464\n",
            "training error 0.03489183688940213, test error 0.05522238914249144\n",
            "Loss: 0.3899322285903839\n",
            "training error 0.03490501479137577, test error 0.055392309964142655\n",
            "Loss: 0.6988348319489024\n",
            "training error 0.034909965791667476, test error 0.05532230272012031\n",
            "Loss: 0.5715672038718145\n",
            "training error 0.03497185613724946, test error 0.05550371874791452\n",
            "Loss: 0.9013671820735647\n",
            "training error 0.03497529431019186, test error 0.055199143745932724\n",
            "Loss: 0.3476739376798932\n",
            "training error 0.034882034366866044, test error 0.05532146244237325\n",
            "Loss: 0.5700396454414447\n",
            "training error 0.03488675099860439, test error 0.055436368813295014\n",
            "Loss: 0.7789303321465102\n",
            "training error 0.034872501862781534, test error 0.0554118581611267\n",
            "Loss: 0.7343719066914556\n",
            "training error 0.03488435950463838, test error 0.05541713388155333\n",
            "Loss: 0.7439627488174505\n",
            "training error 0.03484853192062704, test error 0.05543788643415041\n",
            "Loss: 0.7816892467314762\n",
            "training error 0.034844022456844155, test error 0.055462757312447736\n",
            "Loss: 0.8269025347746872\n",
            "training error 0.03486912360228054, test error 0.055483696054873965\n",
            "Loss: 0.8649675110598887\n",
            "training error 0.03491782940452663, test error 0.05547852352316311\n",
            "Loss: 0.8555642578510714\n",
            "training error 0.0348352374900769, test error 0.05534639009146587\n",
            "Loss: 0.6153561382967077\n",
            "training error 0.034848043032651446, test error 0.05540648757876838\n",
            "Loss: 0.7246086130826113\n",
            "training error 0.03488369202024903, test error 0.055341273242780625\n",
            "Loss: 0.6060541124207397\n",
            "training error 0.03488227908829082, test error 0.05536176066747346\n",
            "Loss: 0.6432986287915554\n",
            "training error 0.03483514572865297, test error 0.055443683119014556\n",
            "Loss: 0.7922271609669629\n",
            "training error 0.034838525716769216, test error 0.05551745643915851\n",
            "Loss: 0.9263412173222463\n",
            "training error 0.03490505644869793, test error 0.055657475386250134\n",
            "Loss: 1.180884579670738\n",
            "training error 0.034860054746351524, test error 0.055611303741709465\n",
            "Loss: 1.0969481846994755\n",
            "training error 0.03486717876110321, test error 0.05527647076938356\n",
            "Loss: 0.4882483453469977\n",
            "training error 0.034845617127840414, test error 0.05514298420332023\n",
            "Loss: 0.24558033462498408\n",
            "training error 0.03480798663008637, test error 0.055155480607513\n",
            "Loss: 0.2682978082709786\n",
            "training error 0.034858875818759266, test error 0.055076058776366066\n",
            "Loss: 0.1239152060308335\n",
            "training error 0.0348087238371206, test error 0.055166100568987526\n",
            "Loss: 0.2876040575885641\n",
            "training error 0.034801717882399316, test error 0.05515824034801447\n",
            "Loss: 0.27331479805017267\n",
            "training error 0.03481461931286883, test error 0.05524434947428054\n",
            "Loss: 0.42985437347144995\n",
            "training error 0.034817804387725616, test error 0.055286675296327054\n",
            "Loss: 0.5067993675716753\n",
            "training error 0.03482669558703213, test error 0.05502565824090627\n",
            "Loss: 0.03229102211446122\n",
            "training error 0.0348391238851022, test error 0.05514293461301448\n",
            "Loss: 0.24549018337454154\n",
            "training error 0.034822833976560254, test error 0.05516036686510068\n",
            "Loss: 0.27718063778443547\n",
            "training error 0.03481970734265992, test error 0.05490373801637112\n",
            "Loss: 0.0\n",
            "training error 0.03481452937345612, test error 0.05490537429167507\n",
            "Loss: 0.0029802621152485997\n",
            "training error 0.03479360774944219, test error 0.054997942425073026\n",
            "Loss: 0.17158104731196389\n",
            "training error 0.03481609234354009, test error 0.05482601785139126\n",
            "Loss: 0.0\n",
            "training error 0.03479176208731103, test error 0.054867074663459484\n",
            "Loss: 0.07488563582989105\n",
            "training error 0.034808894576576, test error 0.05503272964746582\n",
            "Loss: 0.3770322999471887\n",
            "training error 0.034828023161780686, test error 0.05491558372409829\n",
            "Loss: 0.16336381195840577\n",
            "training error 0.034792927930416884, test error 0.05479893751235871\n",
            "Loss: 0.0\n",
            "training error 0.03480856920220742, test error 0.05467152950156225\n",
            "Loss: 0.0\n",
            "training error 0.03481478025524038, test error 0.05477829135207087\n",
            "Loss: 0.19527869712439028\n",
            "training error 0.03480466538487616, test error 0.054856356460602364\n",
            "Loss: 0.3380680232017985\n",
            "training error 0.03483094852480673, test error 0.05492323507965245\n",
            "Loss: 0.46039607888235246\n",
            "training error 0.03481187903451221, test error 0.05503444950718303\n",
            "Loss: 0.6638190095091812\n",
            "training error 0.034766187817985406, test error 0.05493510660274439\n",
            "Loss: 0.48211034808274267\n",
            "training error 0.03476728655051493, test error 0.054848762991172216\n",
            "Loss: 0.324178765850891\n",
            "training error 0.034760497498040346, test error 0.05480461518567055\n",
            "Loss: 0.24342776820338585\n",
            "training error 0.03477987280683089, test error 0.05488469287317882\n",
            "Loss: 0.3898983137292422\n",
            "training error 0.03475695699660466, test error 0.054854756854057135\n",
            "Loss: 0.33514217393471046\n",
            "training error 0.03486371975316437, test error 0.0549148142854556\n",
            "Loss: 0.44499355717935174\n",
            "training error 0.034819785594858314, test error 0.05485397113277377\n",
            "Loss: 0.3337050067463654\n",
            "training error 0.0347893690302471, test error 0.055077719893997415\n",
            "Loss: 0.7429651157346173\n",
            "training error 0.034858628401818324, test error 0.05492686043669735\n",
            "Loss: 0.46702723970399873\n",
            "training error 0.03479747988340394, test error 0.055044515836655455\n",
            "Loss: 0.6822313889765175\n",
            "training error 0.03487058079668436, test error 0.05535126968792023\n",
            "Loss: 1.2433165718156003\n",
            "training error 0.034781678562887455, test error 0.055113211952276725\n",
            "Loss: 0.8078838377877373\n",
            "training error 0.03475339485255159, test error 0.05510281424886573\n",
            "Loss: 0.7888653403983481\n",
            "training error 0.034776599298339574, test error 0.05528258813999094\n",
            "Loss: 1.117690768851154\n",
            "training error 0.03476921486431125, test error 0.055275968541617766\n",
            "Loss: 1.105582824490492\n",
            "training error 0.03480933381650919, test error 0.055190258223692244\n",
            "Loss: 0.9488096032052251\n",
            "training error 0.03476129204493941, test error 0.055183441556265735\n",
            "Loss: 0.9363411987382886\n",
            "training error 0.03476761503248857, test error 0.055052178553126274\n",
            "Loss: 0.6962473064763053\n",
            "training error 0.034823084438662326, test error 0.05480406887843256\n",
            "Loss: 0.24242851458275982\n",
            "training error 0.03475537283818667, test error 0.05499566533370985\n",
            "Loss: 0.5928786611655745\n",
            "training error 0.03474291201158784, test error 0.05501471308032294\n",
            "Loss: 0.6277190008940448\n",
            "training error 0.034789159703033465, test error 0.05490420332773985\n",
            "Loss: 0.425584995149908\n",
            "training error 0.03475315032840708, test error 0.05500520761058594\n",
            "Loss: 0.6103324931016729\n",
            "training error 0.03477632797178023, test error 0.05492725145657942\n",
            "Loss: 0.46774245635448075\n",
            "training error 0.03473782014388495, test error 0.05505987153024948\n",
            "Loss: 0.7103185739044227\n",
            "training error 0.034755592373795344, test error 0.05497115078383524\n",
            "Loss: 0.5480389610545533\n",
            "training error 0.03473273657175512, test error 0.05494943907391323\n",
            "Loss: 0.5083259511571736\n",
            "training error 0.0347723356463616, test error 0.05504548884000698\n",
            "Loss: 0.6840111148418648\n",
            "training error 0.0347837304069252, test error 0.055036374112675014\n",
            "Loss: 0.667339316165183\n",
            "training error 0.0347295195358586, test error 0.05505028142232804\n",
            "Loss: 0.6927772539361055\n",
            "training error 0.0347372042746332, test error 0.05509082226399805\n",
            "Loss: 0.7669307338910691\n",
            "training error 0.034724356741273305, test error 0.05507042316966813\n",
            "Loss: 0.7296186365052693\n",
            "training error 0.03473514202141472, test error 0.05509493478606791\n",
            "Loss: 0.7744529709811099\n",
            "training error 0.034691768453949695, test error 0.055160573723304046\n",
            "Loss: 0.8945135177310659\n",
            "training error 0.03469465964990779, test error 0.05516807653966623\n",
            "Loss: 0.9082369610489804\n",
            "training error 0.034706356611532506, test error 0.05517531699512004\n",
            "Loss: 0.9214805185638664\n",
            "training error 0.034702930645354, test error 0.055208740959581774\n",
            "Loss: 0.9826164786631342\n",
            "training error 0.03469551206795561, test error 0.055178974692051484\n",
            "Loss: 0.9281708324526239\n",
            "training error 0.034698971956201245, test error 0.05525063857055856\n",
            "Loss: 1.0592516329358759\n",
            "training error 0.034721607049655476, test error 0.05520975136067452\n",
            "Loss: 0.9844646089458564\n",
            "training error 0.03472027082485547, test error 0.05519757194931761\n",
            "Loss: 0.9621871796001935\n",
            "training error 0.03471859703112094, test error 0.055211130348318635\n",
            "Loss: 0.9869869229485628\n",
            "training error 0.03472208806213576, test error 0.05535926412354236\n",
            "Loss: 1.2579392386680155\n",
            "training error 0.03468964331029948, test error 0.05524727113303658\n",
            "Loss: 1.053092234154307\n",
            "training error 0.0347506531050494, test error 0.05538474968023495\n",
            "Loss: 1.3045550127737338\n",
            "training error 0.034690193839467635, test error 0.055228940890111676\n",
            "Loss: 1.019564284429797\n",
            "training error 0.03470619119002729, test error 0.055263278511105345\n",
            "Loss: 1.0823714187952138\n",
            "training error 0.03472076937833338, test error 0.05530247579601592\n",
            "Loss: 1.1540673915765298\n",
            "training error 0.034708973831996996, test error 0.05515978193943894\n",
            "Loss: 0.8930652614405954\n",
            "training error 0.03474338384408269, test error 0.055030789207550425\n",
            "Loss: 0.6571239350051661\n",
            "training error 0.034709344670459254, test error 0.05513736760543727\n",
            "Loss: 0.8520670779143202\n",
            "training error 0.034728787612499874, test error 0.055184010078233266\n",
            "Loss: 0.9373810854448106\n",
            "training error 0.034702749221676876, test error 0.05527963659296795\n",
            "Loss: 1.112292077704402\n",
            "training error 0.034765345102788066, test error 0.055063528148748896\n",
            "Loss: 0.7170069152271363\n",
            "training error 0.0346765554730144, test error 0.05525180794197418\n",
            "Loss: 1.0613905367241383\n",
            "training error 0.03472793255899188, test error 0.055174010547149537\n",
            "Loss: 0.919090887283347\n",
            "training error 0.0348153480128738, test error 0.055285769221425765\n",
            "Loss: 1.1235093026727316\n",
            "training error 0.034684844333848705, test error 0.05521304985580488\n",
            "Loss: 0.990497904813803\n",
            "training error 0.03470525908258831, test error 0.05534018659475813\n",
            "Loss: 1.2230444242039784\n",
            "training error 0.034729076808782816, test error 0.055185072003633553\n",
            "Loss: 0.9393234591262623\n",
            "training error 0.03467519464033258, test error 0.05525747310491901\n",
            "Loss: 1.0717527179114672\n",
            "training error 0.03471201484076406, test error 0.05530178263632056\n",
            "Loss: 1.152799529305848\n",
            "training error 0.03466367177067313, test error 0.05527215478306602\n",
            "Loss: 1.0986070574202866\n",
            "training error 0.03468329328582197, test error 0.055200647473165\n",
            "Loss: 0.9678126374489615\n",
            "training error 0.03469838925988331, test error 0.05536511544513096\n",
            "Loss: 1.2686419236705104\n",
            "training error 0.03471522597542104, test error 0.055301419385929036\n",
            "Loss: 1.152135105985641\n",
            "training error 0.03466216475157877, test error 0.05519658511913701\n",
            "Loss: 0.9603821630045362\n",
            "training error 0.03470447258350555, test error 0.055275693788842874\n",
            "Loss: 1.10508027265519\n",
            "training error 0.03469086089795023, test error 0.05497459434773757\n",
            "Loss: 0.5543376030236358\n",
            "training error 0.03468908209269707, test error 0.05506904122966265\n",
            "Loss: 0.7270909223219046\n",
            "training error 0.03482326929645775, test error 0.055269527478134205\n",
            "Loss: 1.0938014392936735\n",
            "training error 0.034709388844781704, test error 0.05504943538138178\n",
            "Loss: 0.6912297557154279\n",
            "training error 0.03468723872532109, test error 0.0550327141725598\n",
            "Loss: 0.6606448992564573\n",
            "training error 0.03469315231429517, test error 0.05520724597845427\n",
            "Loss: 0.979882000332033\n",
            "training error 0.034701996645914314, test error 0.05525466594736471\n",
            "Loss: 1.0666181303484512\n",
            "training error 0.03464731857727369, test error 0.05510086348056383\n",
            "Loss: 0.7852971792005947\n",
            "training error 0.0346451345182129, test error 0.05508933795570255\n",
            "Loss: 0.7642157772965952\n",
            "training error 0.034663851507088166, test error 0.05503334199389783\n",
            "Loss: 0.6617932507727797\n",
            "training error 0.034711469096481935, test error 0.05504342793076739\n",
            "Loss: 0.6802414942397306\n",
            "training error 0.03470925380808296, test error 0.055239930355919765\n",
            "Loss: 1.0396651777252242\n",
            "training error 0.03467436424584121, test error 0.055080727794361614\n",
            "Loss: 0.7484668830925578\n",
            "training error 0.03465568008816293, test error 0.05509322637978111\n",
            "Loss: 0.7713281154989815\n",
            "training error 0.034689031097847996, test error 0.055029214556015484\n",
            "Loss: 0.654243731086801\n",
            "training error 0.0346586354385371, test error 0.055107055570206266\n",
            "Loss: 0.7966231649538358\n",
            "training error 0.03464982575108227, test error 0.05522839550413689\n",
            "Loss: 1.0185667158968625\n",
            "training error 0.034647723130721224, test error 0.055338802284604985\n",
            "Loss: 1.220512374770255\n",
            "training error 0.03466193320824255, test error 0.05530246960690524\n",
            "Loss: 1.154056071039622\n",
            "training error 0.034718397425461935, test error 0.0550452925147733\n",
            "Loss: 0.6836520152602832\n",
            "training error 0.03463880983178961, test error 0.05520703228684878\n",
            "Loss: 0.9794911358227631\n",
            "training error 0.03465282024818055, test error 0.05521750251735846\n",
            "Loss: 0.9986422929334937\n",
            "training error 0.03466766898554624, test error 0.0552554004734873\n",
            "Loss: 1.0679616561822591\n",
            "training error 0.03465630521335881, test error 0.05502158894363107\n",
            "Loss: 0.6402956808786886\n",
            "training error 0.03463750840642972, test error 0.055121798942717216\n",
            "Loss: 0.8235903499683506\n",
            "training error 0.03464540104054498, test error 0.055235155514755524\n",
            "Loss: 1.0309314890800225\n",
            "training error 0.0346405606567888, test error 0.05517677861806863\n",
            "Loss: 0.9241539812635757\n",
            "training error 0.03463513198607613, test error 0.05516912333265401\n",
            "Loss: 0.9101516559501865\n",
            "training error 0.03470198070739851, test error 0.05521140097498162\n",
            "Loss: 0.9874819276895153\n",
            "training error 0.03468944915785299, test error 0.05520223821681838\n",
            "Loss: 0.9707222755510614\n",
            "training error 0.03464639113821089, test error 0.055172127741977654\n",
            "Loss: 0.9156470378263393\n",
            "training error 0.03470359340383353, test error 0.05501943682306007\n",
            "Loss: 0.636359225120775\n",
            "training error 0.03467053841713629, test error 0.05505993297096066\n",
            "Loss: 0.7104309554524457\n",
            "training error 0.03466623503424643, test error 0.05529725904936759\n",
            "Loss: 1.144525411141939\n",
            "training error 0.03466478421852998, test error 0.05521407242607088\n",
            "Loss: 0.992368293799295\n",
            "training error 0.03466580348549426, test error 0.055301313146718586\n",
            "Loss: 1.1519407832523587\n",
            "training error 0.034641882974358086, test error 0.05524802846601577\n",
            "Loss: 1.054477476136917\n",
            "training error 0.03467892075745547, test error 0.05509316801673028\n",
            "Loss: 0.7712213633166876\n",
            "training error 0.034644085693141397, test error 0.0553268411375101\n",
            "Loss: 1.1986341738054573\n",
            "training error 0.03473651008709954, test error 0.05522072212710418\n",
            "Loss: 1.0045312991037614\n",
            "training error 0.03470640826556791, test error 0.05542538959089961\n",
            "Loss: 1.3788897003802036\n",
            "training error 0.034660204968454435, test error 0.055067873561161856\n",
            "Loss: 0.7249551333446513\n",
            "training error 0.0346322549377822, test error 0.05517317730665238\n",
            "Loss: 0.9175668024356209\n",
            "training error 0.03462195220755485, test error 0.05534969727643832\n",
            "Loss: 1.2404404651907397\n",
            "training error 0.03465799446033242, test error 0.05542696481738591\n",
            "Loss: 1.3817709559453162\n",
            "training error 0.03467594751594806, test error 0.05535738156862274\n",
            "Loss: 1.254495846948811\n",
            "training error 0.03467143400606875, test error 0.055295463473729975\n",
            "Loss: 1.1412411137132983\n",
            "training error 0.03462189569912016, test error 0.05531354932928019\n",
            "Loss: 1.1743220531256648\n",
            "training error 0.034615474165094785, test error 0.0553022819017479\n",
            "Loss: 1.1537127384878332\n",
            "training error 0.034647461836475976, test error 0.055348036612399794\n",
            "Loss: 1.2374029353215965\n",
            "training error 0.034649943653743326, test error 0.055229283080672646\n",
            "Loss: 1.0201901870963148\n",
            "training error 0.03465287853373446, test error 0.05513661203598789\n",
            "Loss: 0.8506850616139205\n",
            "training error 0.03470153298443806, test error 0.054927018816330504\n",
            "Loss: 0.4673169327756854\n",
            "training error 0.0346789455363618, test error 0.054968703295922246\n",
            "Loss: 0.5435622472415247\n",
            "training error 0.03464278551840286, test error 0.055096025980478795\n",
            "Loss: 0.7764488807733505\n",
            "training error 0.03466831163162442, test error 0.05535822515761268\n",
            "Loss: 1.2560388602824935\n",
            "training error 0.034681530094035025, test error 0.05505322473932434\n",
            "Loss: 0.6981608914950588\n",
            "training error 0.0346659815700149, test error 0.05513708448964798\n",
            "Loss: 0.8515492292426652\n",
            "training error 0.03465966304299283, test error 0.05488645427398386\n",
            "Loss: 0.39312010178069734\n",
            "training error 0.03462202044146679, test error 0.055138479029565826\n",
            "Loss: 0.8540999899961488\n",
            "training error 0.034707441879569104, test error 0.05502238696545554\n",
            "Loss: 0.6417553470554838\n",
            "training error 0.0346758230885082, test error 0.05536782651892936\n",
            "Loss: 1.2736007638989078\n",
            "training error 0.03464394995538535, test error 0.055141007262601814\n",
            "Loss: 0.8587243951646784\n",
            "training error 0.03460995274257537, test error 0.05519669458373978\n",
            "Loss: 0.9605823853209294\n",
            "training error 0.034600968489699666, test error 0.05510742024284358\n",
            "Loss: 0.7972901897117746\n",
            "training error 0.03459862308746467, test error 0.05517040639742603\n",
            "Loss: 0.9124985168917243\n",
            "training error 0.03460231464740929, test error 0.0551714126250932\n",
            "Loss: 0.9143390135384344\n",
            "training error 0.03465530471217106, test error 0.05517667954776699\n",
            "Loss: 0.9239727712214707\n",
            "training error 0.03469081241454225, test error 0.05520378398085889\n",
            "Loss: 0.9735496411920064\n",
            "training error 0.03462768017190437, test error 0.05495476402979769\n",
            "Loss: 0.5180658577099928\n",
            "training error 0.03459887539815645, test error 0.055082926310167146\n",
            "Loss: 0.7524882006330946\n",
            "training error 0.0346410441517819, test error 0.055208449829602005\n",
            "Loss: 0.9820839711909279\n",
            "training error 0.034608628837787196, test error 0.05491261832137593\n",
            "Loss: 0.44097690701481174\n",
            "training error 0.03461250864493457, test error 0.05494271220518593\n",
            "Loss: 0.49602179799255186\n",
            "training error 0.03460652984176929, test error 0.05492595715238611\n",
            "Loss: 0.4653750373246668\n",
            "training error 0.03459282754675674, test error 0.05497963172336938\n",
            "Loss: 0.5635514949299703\n",
            "training error 0.03457894707466013, test error 0.05490835998517701\n",
            "Loss: 0.43318796048681385\n",
            "training error 0.0346033021498349, test error 0.05487660579129647\n",
            "Loss: 0.3751061870847394\n",
            "training error 0.03459841778858552, test error 0.054955162429740356\n",
            "Loss: 0.5187945732705534\n",
            "training error 0.0345858495020304, test error 0.05499603701745078\n",
            "Loss: 0.5935585099722918\n",
            "training error 0.034601151758634555, test error 0.05499312562661494\n",
            "Loss: 0.5882332687317637\n",
            "training error 0.034611516148614366, test error 0.054782697741939\n",
            "Loss: 0.20333844944575574\n",
            "training error 0.03463569018156763, test error 0.05495158860041989\n",
            "Loss: 0.5122576620974817\n",
            "training error 0.034620611177050634, test error 0.05465856565651224\n",
            "Loss: 0.0\n",
            "training error 0.034607574425160326, test error 0.05449229385653223\n",
            "Loss: 0.0\n",
            "training error 0.03457854920934639, test error 0.054654095538843914\n",
            "Loss: 0.2969258052114121\n",
            "training error 0.03457419738337729, test error 0.05475765484427974\n",
            "Loss: 0.4869697510737181\n",
            "training error 0.034634145453789004, test error 0.054909800976947615\n",
            "Loss: 0.7661764460028087\n",
            "training error 0.0345733773622867, test error 0.05481444913278777\n",
            "Loss: 0.5911941917947416\n",
            "training error 0.03456910792341459, test error 0.05471461185036125\n",
            "Loss: 0.4079806117436302\n",
            "training error 0.03459204410745518, test error 0.054875352927353384\n",
            "Loss: 0.7029600769416611\n",
            "training error 0.03458391668372407, test error 0.05474868122385484\n",
            "Loss: 0.4705020640122459\n",
            "training error 0.03458137993626129, test error 0.05485706424249714\n",
            "Loss: 0.6693981114564096\n",
            "training error 0.034578142885612284, test error 0.05481060410426919\n",
            "Loss: 0.5841380958838016\n",
            "training error 0.034564624275627794, test error 0.054919126353224546\n",
            "Loss: 0.7832896479199158\n",
            "training error 0.0346043440542576, test error 0.05483769737935409\n",
            "Loss: 0.6338575574213134\n",
            "training error 0.034556381891263206, test error 0.054954501489004534\n",
            "Loss: 0.8482073331124607\n",
            "training error 0.03459088725669004, test error 0.054825716140033405\n",
            "Loss: 0.611870523158764\n",
            "training error 0.0345960583204292, test error 0.054408760978035486\n",
            "Loss: 0.0\n",
            "training error 0.034578261027198465, test error 0.054312749993272375\n",
            "Loss: 0.0\n",
            "training error 0.03464207216672613, test error 0.054353665813043676\n",
            "Loss: 0.07533372877708278\n",
            "training error 0.034561628424162195, test error 0.054485214559984764\n",
            "Loss: 0.31753974294019827\n",
            "training error 0.034552265443552874, test error 0.054474349176378146\n",
            "Loss: 0.29753452573435535\n",
            "training error 0.03458057680650588, test error 0.05460340974519782\n",
            "Loss: 0.5351593354441686\n",
            "training error 0.034546581007105684, test error 0.05462764753145426\n",
            "Loss: 0.5797856639939702\n",
            "training error 0.03459067386438326, test error 0.05434665265304395\n",
            "Loss: 0.062421180617389105\n",
            "training error 0.034560330475583656, test error 0.05452672639544333\n",
            "Loss: 0.39397084882915046\n",
            "training error 0.03454637154852322, test error 0.054547176558162\n",
            "Loss: 0.43162344922447193\n",
            "training error 0.03455643699704836, test error 0.05442633832408501\n",
            "Loss: 0.2091375060675471\n",
            "training error 0.03460681696433025, test error 0.05452206293685821\n",
            "Loss: 0.3853845434299785\n",
            "training error 0.03465178388716107, test error 0.05436427482474965\n",
            "Loss: 0.09486691703817218\n",
            "training error 0.03459030889924488, test error 0.054436522744339584\n",
            "Loss: 0.22788894151473116\n",
            "training error 0.0345675531909422, test error 0.05467140985744344\n",
            "Loss: 0.6603603467242758\n",
            "training error 0.034602478052427006, test error 0.05458257253874564\n",
            "Loss: 0.496794114653909\n",
            "training error 0.03457334442943715, test error 0.05452205970447144\n",
            "Loss: 0.3853785919972541\n",
            "training error 0.034591458161270634, test error 0.054730863551389675\n",
            "Loss: 0.7698257925976781\n",
            "training error 0.03463943873030247, test error 0.05480971132409758\n",
            "Loss: 0.9149993894375763\n",
            "training error 0.034602229708483034, test error 0.054745827183738195\n",
            "Loss: 0.7973766574505436\n",
            "training error 0.034561102772128885, test error 0.05455340005815465\n",
            "Loss: 0.44308208461565357\n",
            "training error 0.03457962177226277, test error 0.05460209827367046\n",
            "Loss: 0.5327446694080695\n",
            "training error 0.03456531244864543, test error 0.05462163191845205\n",
            "Loss: 0.5687097876979852\n",
            "training error 0.03456933947395065, test error 0.0547638314900635\n",
            "Loss: 0.8305259756631767\n",
            "training error 0.03457492024798976, test error 0.05478353149770942\n",
            "Loss: 0.8667973993129685\n",
            "training error 0.03457654438133277, test error 0.05464364896167275\n",
            "Loss: 0.6092473101460882\n",
            "training error 0.03457572171951875, test error 0.05485456305856674\n",
            "Loss: 0.9975798783185885\n",
            "training error 0.03456098629382999, test error 0.054839805454875074\n",
            "Loss: 0.9704083510188477\n",
            "training error 0.03454146530918706, test error 0.05481364596098138\n",
            "Loss: 0.9222437968452235\n",
            "training error 0.03457433395330787, test error 0.054836728518123976\n",
            "Loss: 0.9647431310631616\n",
            "training error 0.034579568736357806, test error 0.054779406480009236\n",
            "Loss: 0.8592024649730856\n",
            "training error 0.03463630183521243, test error 0.054826820862386855\n",
            "Loss: 0.9465012712082554\n",
            "training error 0.034593350527614544, test error 0.05490188153143991\n",
            "Loss: 1.084702097096013\n",
            "training error 0.03456370358367272, test error 0.054788836990232084\n",
            "Loss: 0.8765658100882145\n",
            "training error 0.03456087297051541, test error 0.05475385350342574\n",
            "Loss: 0.8121546233766486\n",
            "training error 0.03462394737958422, test error 0.05450223391227399\n",
            "Loss: 0.34887557530245417\n",
            "training error 0.034572095267310456, test error 0.05454287599770515\n",
            "Loss: 0.42370530761428604\n",
            "training error 0.03457755401773696, test error 0.05471370049093378\n",
            "Loss: 0.7382253664398597\n",
            "training error 0.03460268841388771, test error 0.054839540426257986\n",
            "Loss: 0.969920383429046\n",
            "training error 0.034602883047354586, test error 0.05465273663811737\n",
            "Loss: 0.625979433718804\n",
            "training error 0.03456558051311959, test error 0.054765301275283555\n",
            "Loss: 0.8332321270184906\n",
            "training error 0.03460875757709684, test error 0.05484463317429603\n",
            "Loss: 0.9792970915476484\n",
            "training error 0.0345460338826807, test error 0.05490001743344491\n",
            "Loss: 1.0812699416716587\n",
            "training error 0.0345513233783821, test error 0.054992341658636416\n",
            "Loss: 1.2512562251924653\n",
            "training error 0.03454689165149122, test error 0.05507603222083118\n",
            "Loss: 1.405346309390243\n",
            "training error 0.03454349701138637, test error 0.05500621938484361\n",
            "Loss: 1.2768077323595861\n",
            "training error 0.03458212912494138, test error 0.055110547531563234\n",
            "Loss: 1.4688954957899902\n",
            "training error 0.03455787594027428, test error 0.05492660280586413\n",
            "Loss: 1.1302186184050544\n",
            "training error 0.034560781392017, test error 0.054880982756432155\n",
            "Loss: 1.0462235170013878\n",
            "training error 0.03454498930407095, test error 0.055085862926408026\n",
            "Loss: 1.4234464895101384\n",
            "training error 0.03455010569469904, test error 0.05492610602605163\n",
            "Loss: 1.129303953225036\n",
            "training error 0.034558521183874466, test error 0.05512100048261532\n",
            "Loss: 1.4881413470005933\n",
            "training error 0.03455725495246742, test error 0.05504611596370043\n",
            "Loss: 1.3502648466868328\n",
            "training error 0.03458264426724508, test error 0.05490533686269273\n",
            "Loss: 1.0910640125822324\n",
            "training error 0.03462749614297318, test error 0.054825036966279495\n",
            "Loss: 0.9432167825613291\n",
            "training error 0.034535156869603775, test error 0.0549620047624125\n",
            "Loss: 1.195400286710857\n",
            "training error 0.0345511402549473, test error 0.05500589500289141\n",
            "Loss: 1.276210484103446\n",
            "training error 0.034571365211467454, test error 0.0550311055815527\n",
            "Loss: 1.3226279066504754\n",
            "training error 0.0346011688180548, test error 0.055013765077739106\n",
            "Loss: 1.290700773858\n",
            "training error 0.034549049960499845, test error 0.05489797530955044\n",
            "Loss: 1.0775100070435695\n",
            "training error 0.03452782294707179, test error 0.0548220005219411\n",
            "Loss: 0.937626116762269\n",
            "training error 0.034590841386445345, test error 0.055072246119112336\n",
            "Loss: 1.3983753831909418\n",
            "training error 0.034532225723401436, test error 0.05492165508558672\n",
            "Loss: 1.1211089337029856\n",
            "training error 0.034566946794083696, test error 0.05509424948697668\n",
            "Loss: 1.4388877267328626\n",
            "training error 0.034622281485021875, test error 0.05524462263621123\n",
            "Loss: 1.7157530102126728\n",
            "training error 0.03462295172672568, test error 0.05478819943794756\n",
            "Loss: 0.8753919562793033\n",
            "training error 0.03459284170506593, test error 0.05461799499305815\n",
            "Loss: 0.5620135232032863\n",
            "training error 0.034543919548807554, test error 0.05472548394018587\n",
            "Loss: 0.7599209153736908\n",
            "training error 0.0346036900129366, test error 0.05447907445167783\n",
            "Loss: 0.3062346473453381\n",
            "training error 0.03453785133799734, test error 0.054576567620095676\n",
            "Loss: 0.4857379286741681\n",
            "training error 0.03456160312437142, test error 0.054823830811849966\n",
            "Loss: 0.9409960251338667\n",
            "training error 0.03454187282576853, test error 0.05481357213629873\n",
            "Loss: 0.9221078717030373\n",
            "training error 0.034563486382406965, test error 0.05477340545733599\n",
            "Loss: 0.8481534522201128\n",
            "training error 0.034563358949504804, test error 0.0547764959017415\n",
            "Loss: 0.8538435423111057\n",
            "training error 0.03456067681450509, test error 0.05456163600355112\n",
            "Loss: 0.45824601094508477\n",
            "training error 0.034588323933398266, test error 0.05473736960292194\n",
            "Loss: 0.781804658578622\n",
            "training error 0.03452706110715491, test error 0.05464279649005513\n",
            "Loss: 0.6076777493749352\n",
            "training error 0.034540541415156396, test error 0.05455398303801248\n",
            "Loss: 0.44415546031086883\n",
            "training error 0.034607404908961234, test error 0.054749625045565575\n",
            "Loss: 0.8043692362241117\n",
            "training error 0.034671079621684836, test error 0.05482741640182735\n",
            "Loss: 0.9475977714601624\n",
            "training error 0.03455460787568031, test error 0.05459512352675367\n",
            "Loss: 0.5199028469673683\n",
            "training error 0.03454660185359867, test error 0.054608527411876454\n",
            "Loss: 0.5445819234723315\n",
            "training error 0.03453058339121072, test error 0.05452968432821136\n",
            "Loss: 0.3994169600431974\n",
            "training error 0.03453424658168753, test error 0.05459432277583151\n",
            "Loss: 0.518428513735758\n",
            "training error 0.03458014921908937, test error 0.05469651824733596\n",
            "Loss: 0.7065896205055378\n",
            "training error 0.034531033565056865, test error 0.054548284695237544\n",
            "Loss: 0.43366373824624826\n",
            "training error 0.03462125882970647, test error 0.05477901263947675\n",
            "Loss: 0.8584773303913495\n",
            "training error 0.03453554061985448, test error 0.05470142552355925\n",
            "Loss: 0.7156248400882248\n",
            "training error 0.03454520850541353, test error 0.054613981825898675\n",
            "Loss: 0.5546245267706196\n",
            "training error 0.03452614824689714, test error 0.05467449908103543\n",
            "Loss: 0.6660481890677028\n",
            "training error 0.034624004314432896, test error 0.05459897649649687\n",
            "Loss: 0.5269968897909871\n",
            "training error 0.03456977981429756, test error 0.05463642261980332\n",
            "Loss: 0.5959422540214421\n",
            "training error 0.03460556957350712, test error 0.05487633220535686\n",
            "Loss: 1.0376609767583034\n",
            "training error 0.03453366833058777, test error 0.05478467448295914\n",
            "Loss: 0.8689018503854617\n",
            "training error 0.03457611723838231, test error 0.05463912533785953\n",
            "Loss: 0.6009184668932743\n",
            "training error 0.03452974229496525, test error 0.054818711860582785\n",
            "Loss: 0.9315710719363102\n",
            "training error 0.034555341428306734, test error 0.05486088946388493\n",
            "Loss: 1.0092279817914829\n",
            "training error 0.034534902148619914, test error 0.054767413840522125\n",
            "Loss: 0.8371217574254075\n",
            "training error 0.03458734149902399, test error 0.05488771771860862\n",
            "Loss: 1.058623850582907\n",
            "training error 0.03454368410799973, test error 0.05481401504982064\n",
            "Loss: 0.9229233588988972\n",
            "training error 0.03454934917655324, test error 0.0547170525407627\n",
            "Loss: 0.7443971213764833\n",
            "training error 0.034588527970720956, test error 0.054758506160519846\n",
            "Loss: 0.8207210411969257\n",
            "training error 0.03455021248382018, test error 0.05478028661037968\n",
            "Loss: 0.8608229507163889\n",
            "training error 0.034548908057957066, test error 0.05491320100477266\n",
            "Loss: 1.1055433789941782\n",
            "training error 0.03460216865635885, test error 0.05497406274032297\n",
            "Loss: 1.2176012945993575\n",
            "training error 0.034654839569779036, test error 0.054854759088405834\n",
            "Loss: 0.9979408061653983\n",
            "training error 0.034555115768270424, test error 0.05506512745968445\n",
            "Loss: 1.3852685907181472\n",
            "training error 0.034600899448430256, test error 0.055068797459789674\n",
            "Loss: 1.3920257519844714\n",
            "training error 0.03455992442582211, test error 0.054967562197923446\n",
            "Loss: 1.205632571969173\n",
            "training error 0.034580076161166956, test error 0.05511623059319176\n",
            "Loss: 1.4793590823865665\n",
            "training error 0.03455179465062371, test error 0.0549898056689532\n",
            "Loss: 1.246586990650811\n",
            "training error 0.03456110876347077, test error 0.054927236475700844\n",
            "Loss: 1.1313853238964722\n",
            "training error 0.03461223977665599, test error 0.05504899980505099\n",
            "Loss: 1.3555745416496512\n",
            "training error 0.03456342551898434, test error 0.05492650020174419\n",
            "Loss: 1.130029704899571\n",
            "training error 0.034584116687287246, test error 0.055071708839813126\n",
            "Loss: 1.3973861508297158\n",
            "training error 0.03453441462403192, test error 0.05505546911536048\n",
            "Loss: 1.3674857601209789\n",
            "training error 0.03456740873951685, test error 0.055110326488663104\n",
            "Loss: 1.4684885141877713\n",
            "training error 0.03459231592282566, test error 0.05506329343964378\n",
            "Loss: 1.381891814471503\n",
            "training error 0.03462646535608933, test error 0.055137716048978325\n",
            "Loss: 1.5189178522688307\n",
            "training error 0.03454306027769824, test error 0.05503398732166241\n",
            "Loss: 1.3279337328332197\n",
            "training error 0.034574934084450025, test error 0.054935767233184434\n",
            "Loss: 1.1470920547923535\n",
            "training error 0.03456041682067275, test error 0.055080242465429\n",
            "Loss: 1.413098162497195\n",
            "training error 0.03455021545897356, test error 0.05506507470438114\n",
            "Loss: 1.385171458270773\n",
            "training error 0.034551288259233814, test error 0.055009856714330195\n",
            "Loss: 1.2835047408650269\n",
            "training error 0.034605617892536376, test error 0.05497708378115622\n",
            "Loss: 1.2231635996448942\n",
            "training error 0.03462690732573516, test error 0.054920208807391104\n",
            "Loss: 1.1184460632061022\n",
            "training error 0.034540339624110676, test error 0.05492503926459752\n",
            "Loss: 1.1273398445134752\n",
            "training error 0.034550060101319356, test error 0.055077465769572884\n",
            "Loss: 1.4079857425654874\n",
            "training error 0.03459409537890652, test error 0.05497132701522206\n",
            "Loss: 1.2125643095429073\n",
            "training error 0.034644511770597505, test error 0.054854030024178546\n",
            "Loss: 0.9965984616378654\n",
            "training error 0.03457347987986513, test error 0.054916627620149296\n",
            "Loss: 1.1118524231450655\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8ddnJgl7URY3oAZ3sWwal4jVKPVerAoobYXKdaui/dUF/VkF2lpr7S32+mutXpeidUEpqCh1Q72yjHglVUEtghuIUWJFQ4QoIoRkPr8/zkkYQpYJZDLJzPv5eMyDc77nO+d8zhwynznf7znfY+6OiIhkr0i6AxARkfRSIhARyXJKBCIiWU6JQEQkyykRiIhkOSUCEZEsp0QgbZaZfdfM3kt3HCKZTolA6mVmJWb2vXTG4O4vufvB6YyhLbLAajN7O92xSGZQIpC0MbNoumPYVWnah+OBPYD9zOzI1tywmeW05vakdSgRSLOYWcTMJpnZB2ZWbmaPmFmPhOWPmtlaM6sws0VmdljCsvvN7E4zm2tmXwMnhmceV5vZsvA9D5tZx7B+kZmVJry/wbrh8mvM7FMz+5eZXWhmbmYHNLAfPczsvrDuejP7e1h+npn9b526teupZx+uDvc3mlD/DDNblszntZPOBZ4A5obTibEeZmYvmNkXZvaZmU0Jy6NmNiWM4yszW2pm/cwsP9y/nIR1xMzswoTP42Uz+5OZlQPXm9n+ZrYg3J91ZjbDzHZLeH8/M3vczMrCOv9tZnlhTAMT6u1hZpvMrPcufh6yi5QIpLkuA0YDJwD7AOuB2xOWPwscSPCL9XVgRp33/xj4HdANqPnC/REwAugPDALOa2T79dY1sxHAVcD3gAOAoib240GgM3BYGOufmqjf0D78GfgaOKnO8r+F0019Xs1iZp2BHxB8rjOAsWaWFy7rBswDngu3dQAwP3zrVcA44PvAt4ALgE1JbvZoYDWwJ8F+G/D7cBuHAv2A68MYosDTwEdAPtAHmOXulcAsYHzCescB8929LPlPQFLC3fXSa4cXUAJ8r57yd4DhCfN7A1uBnHrq7gY40D2cvx+YXs92xifM/wG4K5wuAkqTrHsv8PuEZQeE2z6gnrj2BuLA7vUsOw/43zpltetpYB9uBO4Np7sRJIZ9m/t5JXlcxgNlQA7QEagAzgiXjQPeaOB97wGj6inPD/cvJ6EsBlyY8Hl83ERMo2u2CxTWxFdPvaOBjwEL55cAP0r3/3W9XGcE0mz7AnPMbIOZbSD4oqsG9gybH6aGzQ9fEnxxA/RKeP+aeta5NmF6E9C1ke03VHefOuuubzs1+gFfuPv6Ruo0pu66/wacaWYdgDOB1939o3BZg59X3ZWa2bNmtjF8nd3Ats8FHnH3KnffDDzGtuahfsAHDbyvsWVN2W5/zWxPM5tlZp+Ex/khth3jfsBH7l5VdyXu/grBMSsys0MIkvWTOxmTtCB1/EhzrQEucPeX6y4ws/8ARhE0z5QA3QmaQiyhWqqGu/0U6Jsw36+RumuAHma2m7tvqLPsa4ImIwDMbK963r/dPrj722b2EXAK2zcL1Wyr3s9rh5W6n9LYcjPrS9AEdZSZjQmLOwMdzaxXuK2xDbx9DbA/sLxO+dcJ6/kynK67z3WP2X+GZQPd/QszGw38d8J2vm1mOfUlA+ABgrOatcDsMJlJmumMQBqTa2YdE145wF3A78xsXwAz621mo8L63YAtQDnBF8t/tmKsjwDnm9mhYTv6rxqq6O6fEvRl3GFmu5tZrpkdHy7+J3CYmQ0JO6KvT3L7fwOuILii59GE8sY+r+b6D+B94GBgSPg6CCglaBZ6GtjbzCaaWQcz62ZmR4fvvQf4rZkdaIFBZtbTg/b5T4Dx4RndBQQJozHdgI1AhZn1AX6esOxVgqQ81cy6hP9vhiUsfwg4gyAZTN/Jz0FamBKBNGYu8E3C63qCztEngf8xs6+AfxC0/ULwh/0RwRfL2+GyVuHuzwK3AguBVQnb3tLAW/6DoK3+XeBzYGK4nveBGwg6XVeyrUO7KTMJOoQXuPu6hPLGPq/mOhe4w93XJr4Iks257v4VcDJwOsEv7pXAieF7/0iQLP+H4Jf/X4FO4bKLCL7Mywk6zxc3EcdvgMMJ+ieeAR6vWeDu1eH2DyDoDygFzkpYvobgIgIHXmr+RyCpUNNpI5JRzOxQgmaQDg00UUiamNm9wL/c/ZfpjkUCSgSSMczsDIKzmM4EbdFxdx+d3qgkkZnlA28CQ939w/RGIzXUNCSZ5GKCZp4PCK7M+Wl6w5FEZvZbgrO0/1ISaFt0RiAikuV0RiAikuXa3X0EvXr18vz8/HSHISLSrixdunSdu9c7rlO7SwT5+fksWbIk3WGIiLQr4U2P9VLTkIhIllMiEBHJckoEIiJZTolARCTLKRGIiGQ5JQIRkSzX7i4flfZj/OPjmf32bKq9GgiehmdmRCxC3OO181qmZVrW9LK8aB5H9jmSqcOnUtivMNk/w6S0uyEmCgoKXPcRtE3Fa4qZNG8Sr3/6Ol9v/RpP2TNoRLJXTiSHRectanYyMLOl7l5Q3zI1DUmLuHbetRx777Es+ngRG7duVBIQSZGqeBWxkliLrlNNQ7LLxj8+nhlvzUh3GCJZISeSQ1F+Ucuus0XXJlnn2nnXNpkEDCMaibaJdlYt07L2uiyVfQRKBLLTitcU818v/1e9ywyje8fuTDhiAjd976ZWjkxEmkOJQHZarCRWb19A1KK8dP5LLf6rRURSQ53FslOmLZ3GX5b+ZYfy0YeMVhIQaWd0RiDNdu28a/nDy3/YofzsgWfz0JkPpSEiEdkVOiOQZmmsX+CV0ldaORoRaQlKBNIsDfULAJw54MxWjkZEWoISgTRLUX4RkXr+25w98GxdHSTSTqmPQJqt5ozAMEYdMoprjr1GncMi7ZjOCKRZJs2bVJsIHGevLnspCYi0czojkKRNWzqNRR8v2q7sxUUw9FewahVs3gzuYAaRCMTj2+ZBy7RMy3ZlWV4eHHkkTJ0KhS3820uJQJL22NuPbV8QN96ZeQ6UpicekWzyzTewaBEcf3zwb0smAzUNSdLGDBizbcaBl38OpWoWEmlNVVUQi7XsOpUIJHmfD9w2XZ0L741OXywiWSonB4qKWnidLbs6yVTFxXDJ1AVwUlhgcciPbXdGUNOmme62VC3Tskxcpj4CSbtYDHzNMcFM3CCeByVF29XZf39YubLVQxORXaSmIUlKURHw2eBgZuWp8MD8HfoHztSNxSLtks4IJCmbNwM53wQz742C0kLy8qBDB+jUCc47D27SjcUi7ZISgSTlkUeAnM3BzH7/A2WHcf1PC5k8Oa1hiUgLUNOQJGXoUKDvP4KZAY/BOcPpOaQ4rTGJSMtQIpCkDBgA9Hs5mInEieRWUt41ls6QRKSFpDQRmNkIM3vPzFaZ2aR6lu9rZvPNbJmZxcysbyrjkZ3nDqzfL5yJ0iEnj6L8onSGJCItJGWJwMyiwO3AKcAAYJyZDahT7WZgursPAm4Afp+qeGTXuAO9VwDQ9fOTuOXw+RpsTiRDpPKM4ChglbuvdvdKYBYwqk6dAcCCcHphPculjXiidBoMmQ7Axj1e4Ge/eYtidRGIZIRUJoI+wJqE+dKwLNE/gZqrz88AuplZz7orMrMJZrbEzJaUlZWlJFhp3PNrHwDbNl910GMtPt6JiKRHujuLrwZOMLM3gBOAT4DqupXcfZq7F7h7Qe/evVs7xqxXXAzRNUXbleW8P6bFxzsRkfRIZSL4BOiXMN83LKvl7v9y9zPdfSjwi7BsQwpjkmYqLg6GvV0264e1ZaNz/sKiP01o8fFORCQ9UnlD2WvAgWbWnyABjAV+nFjBzHoBX7h7HJgM3JvCeGQnxGLBsLdEttaWzfnlhLTFIyItL2VnBO5eBVwKPA+8Azzi7ivM7AYzGxlWKwLeM7P3gT2B36UqHtk5tc0/NXcVi0jGSekQE+4+F5hbp+y6hOnZwOxUxiC7prb5JyERTHu2mAmnqF1IJFOku7NY2ou9/lk7eemrwyleo2tHRTKFEoEkZ683aierqSRWEktfLCLSopQIJDlVHYJ/4xFyIxpeQiSTKBFI0/oWw+CHgmmPcIrdouElRDKIEoE0LT8GVhXOOHv1L09nNCLSwpQIpGklReDhBWbxPM45viid0YhIC1MikKaVFsJrFwfTf3tSzUIiGUaJQJLzVfioiDXDNOqoSIZRIpBG1X7pR8MhJuI5DB+OkoFIBlEikEbVDjUdCTuL4zlUVqIhqEUyiBKBNKqoCCIRgkQQjxKJGHl5aAhqkQyiRCCNKiyEQw4Bun8EDhNuKGb+fDQEtUgGUSKQJsX7vhTcUBat5gGGBzeYiUjGUCKQJlX0WFA7XVmtcYZEMo0SgTSp07phtdN5UY0zJJJplAikSR2/OKJ2ev4583VDmUiGUSKQJlV51baZUiUBkUyjRCBN+vqbbYlAN5OJZB4lAmlSYiLQzWQimUeJQJpkkW2JQDeTiWQeJQJpVHExrO/8Wu38LbfoZjKRTKNEII2avqAYzjy7dv7Zz6elMRoRSQUlAmlcfgyi25qGnopfSvEa9RaLZBIlAmlU3aeROdW6s1gkwygRSKMSbx4zjA7RDrqzWCTDKBFI0s4dfK7uLBbJQEoEkrRxA8cpCYhkoJQmAjMbYWbvmdkqM5tUz/Jvm9lCM3vDzJaZ2fdTGY/smup4dbpDEJEUSFkiMLMocDtwCjAAGGdmA+pU+yXwiLsPBcYCd6QqHtl11a5EIJKJUnlGcBSwyt1Xu3slMAsYVaeOA98Kp7sD/0phPLKLquJVTVcSkXYnlYmgD7AmYb40LEt0PTDezEqBucBlKYxHdtGKshXpDkFEUiDdncXjgPvdvS/wfeBBM9shJjObYGZLzGxJWVlZqweZzRJvHrvhxRt0M5lIBkplIvgE6Jcw3zcsS/QT4BEAdy8GOgK96q7I3ae5e4G7F/Tu3TtF4Up9YiWxoAGPoGlIN5OJZJ5UJoLXgAPNrL+Z5RF0Bj9Zp87HwHAAMzuUIBHoJ38b0rNTz9rpuMfp2blnI7VFpD1KWSJw9yrgUuB54B2Cq4NWmNkNZjYyrPZ/gYvM7J/ATOA8d/dUxSTNV/Z1ee10hAjlm8obqS0i7VFOKlfu7nMJOoETy65LmH4bGFb3fdJ2HNevKJwyOuRoeAmRTJTuzmJp4/zjQqjOY+/K4zS8hEiGUiKQBhUXw7//O2DOZ68N04PrRTKUEoE06K9/DZ5RjFUTr87Rs4pFMpQSgTTo668BHCJxIkT1rGKRDKVEIPUqLobZs4FIML7QgENy9KxikQylRCD1isWguhqIBOMLVW+NpjUeEUkdJQKpV1ERRKPUJoI9eqf0SmMRSSMlAqlXYSGMHAlY0DS0Ry8lApFMpUQgDdprL2rPCHIiahoSyVRKBNIgM2o7i3MiOiMQyVRKBNK48IwgqjMCkYzVZCIws9Pre0aAZL7gjCBIBLk6IxDJWMl8wZ8FrDSzP5jZIakOSNqYsLNYZwQimavJRODu44GhwAfA/WZWHD4xrFvKo5P0q+0s1hmBSKZKqsnH3b8EZhM8gH5v4AzgdTPTM4YzmDqLRbJDMn0EI81sDhADcoGj3P0UYDDBg2Ukk9WcEUTVNCSSqZL5mTcG+JO7L0osdPdNZvaT1IQlbUbYR5Ab1RmBSKZK5q/7euDTmhkz6wTs6e4l7j4/VYFJ+iVeNaQbykQyVzJ9BI8C8YT56rBMMlxiIvjbG48z7dni9AYkIimRTCLIcffKmplwOi91IUmbstc/Afigy4Nc/PJwJQORDJRMIigzs5E1M2Y2CliXupCkTdnn1eDfSBwilTy2NJbWcESk5SXTR3AJMMPM/hswYA1wTkqjkjbBDPjigGAmHoV4HmOOKEpnSCKSAk0mAnf/ADjGzLqG8xtTHpW0CZ9+Cny9FwD2ypX8/LQzmXCKHlMmkmmSuibQzE4FDgM6mhkA7n5DCuOSNqC0FDjgWQD8y33Y7SslAZFM1GQiMLO7gM7AicA9wA+AV1Mcl7QBm78zDfZ6JJj5t6vYsH8XYEJaYxKRlpdMZ/Gx7n4OsN7dfwMUAgelNixpC0p73xP0CgEYvLnlsbTGIyKpkUwi2Bz+u8nM9gG2Eow3JBmu19ah282PGTAmTZGISColkwieMrPdgP8CXgdKgL+lMihpG/pWFdVO/+W0vzDhCDULiWSiRvsIwgfSzHf3DcBjZvY00NHdK5JZuZmNAP4MRIF73H1qneV/Iuh7gKAfYg93362Z+yApUk1V7bSSgEjmajQRuHvczG4neB4B7r4F2JLMis0sCtwOnAyUAq+Z2ZPu/nbC+q9MqH9ZzXakbaj2rekOQURaQTJNQ/PNbIzVXDeavKOAVe6+OhyWYhYwqpH644CZzdyGpFA1SgQi2SCZRHAxwSBzW8zsSzP7ysy+TOJ9fQjuQq5RGpbtwMz2BfoDCxpYPsHMlpjZkrKysiQ2LS1hQ/T9dIcgIq0gmUdVdnP3iLvnufu3wvlvtXAcY4HZ7l7dQAzT3L3A3Qt69+7dwpuW+hSvKWZ559u2mxeRzJTMDWXH11de90E19fgE6Jcw3zcsq89Y4GdNxSKtJ1YSI57QWRwriVHYT3cWi2SiZIaY+HnCdEeCtv+lwElNvO814EAz60+QAMYCP65bycwOAXYH9JOzDSnKL8LIwcN+gqL8ovQGJCIpk0zT0OkJr5OB7wDrk3hfFXAp8DzwDvCIu68wsxsSh7UmSBCz3N13bhckFQr7FXLQxgu2mxeRzLQzD6ItBQ5NpqK7zwXm1im7rs789TsRg7SCztX19u2LSIZJpo/gNqDm13oEGEJwh7FkOPd405VEpN1L5oxgScJ0FTDT3V9OUTzShsQTHlVdvKZYzUMiGSqZRDAb2FxzaaeZRc2ss7tvSm1okm4box8HEw7Dpw9n/jnzlQxEMlBSdxYDnRLmOwHzUhOOtCVf55QEEwaV1ZXESmLpDEdEUiSZRNAx8fGU4XTn1IUkbUWnqn5B71B1lLxoni4hFclQySSCr83s8JoZMzsC+CZ1IUlb4V/uBdV5sPC3ahYSyWDJ9BFMBB41s38RPK9qL+CslEYlabdoEXz0cRz2yIX/nRxcNNyvybeJSDvUZCJw99fCu38PDovec9f4xJnuqacAi4MHJ42xGBTqhEAkIzXZNGRmPwO6uPtyd18OdDWz/5P60CSdjjmG7RJBz57pjUdEUieZPoKLwieUAeDu64GLUheStAWHH852iWDiRCjWaFAiGSmZRBBNfChN+OSxvNSFJG1GQiKorAyah0Qk8yTTWfwc8LCZ/SWcvxh4NnUhSVsQjwNWDR4hGoW8PCgqSndUIpIKySSCa4EJwCXh/DKCK4ckg1VXE5wRxKP89rdBElBnsUhmSuaqobiZvQLsD/wI6AU8lurAJL2qqqhtGpo8Od3RiEgqNZgIzOwgggfKjwPWAQ8DuPuJrROapFPtGYEn040kIu1ZY2cE7wIvAae5+yoAM7uyVaKStEs8IxCRzNbYX/mZwKfAQjO728yGE9xZLFlAiUAkezT4V+7uf3f3scAhwEKCoSb2MLM7zezfWitASY+qKiBSrUQgkgWSeWbx1+7+N3c/HegLvEFwJZFksG19BNF0hyIiKdasn3vuvt7dp7n78FQFJG2DmoZEsof+yqVeSgQi2UN/5VIvJQKR7KG/cqmX7iMQyR76K5d6BWcEumpIJBvor1zqpaYhkeyhv3KpV20iiOvyUZFMp0Qg9Xr3XWrPCPRAGpHMltJEYGYjzOw9M1tlZpMaqPMjM3vbzFaY2d9SGY8kp7gYbryR2kRw4ol6OplIJktZIgifZHY7cAowABhnZgPq1DkQmAwMc/fDCIaxkDSLxbbvI9DTyUQyWyrPCI4CVrn7anevBGYBo+rUuQi4PXwOMu7+eQrjkSQVFUE0CnT6Arp9Qk7/Yj2dTCSDpTIR9AHWJMyXhmWJDgIOMrOXzewfZjaivhWZ2QQzW2JmS8rKylIUrtQoLISTL3gZ9nkddvsIzi2CvmobEslU6e4szgEOBIoIHoBzt5ntVrdSOL5RgbsX9O7du5VDzE4lPacFEwZbvZLp/5ye3oBEJGVSmQg+AfolzPcNyxKVAk+6+1Z3/xB4nyAxSBotXgwffFid7jBEpJWkMhG8BhxoZv3NLA8YCzxZp87fCc4GMLNeBE1Fq1MYkzShuBiOPx62Lj8tKHAjhw6cM/ic9AYmIimTskTg7lXApcDzwDvAI+6+wsxuMLORYbXngXIze5vg4Tc/d/fyVMUkTYvFwnGGyg8JCpb9mMOXLaSwX2E6wxKRFGrsmcW7zN3nAnPrlF2XMO3AVeFL2oCiIjADz/kmKHhrPD/5uZKASCZLd2extEGRCJCzGYCxP+jEhAnpjUdEUkuJQLYTi0E8Duz9OgBbuqxMazwiknpKBLKdoiKwfsUwfAoAz/ilFK/RPQQimUyJQLZTWAgHnByDSBUA1V5FrCSW1phEJLWUCGQH+3oReDD8dG40l6L8orTGIyKppUQgO+jxdSG7r/wZAHN+NEeXjopkuJRePirtU1UV5PneAByff3yaoxGRVNMZgeygqgoI7yPomNMxvcGISMopEcgOqqqA3G/Ii+YRMf0XEcl0+iuXHVRXQ2WX1RimS0dFsoASgexgXadi1u/zKFuqtzB8+nAlA5EMp0QgOyjvFqud3lK1RfcRiGQ4JQLZgW3uUTsdJ07Pzj3TGI2IpJoSgeygMnfbo6MjFqF8k0YGF8lkSgSyg87lwwAwjA7RDrqzWCTDKRHIDjqUHw7A9w/8PvPPma87i0UynBKB7KAqvhWAUw44RUlAJAsoEcgOqjxIBLnR3DRHIiKtQYlAdlBzRpAbUSIQyQZKBLKDrzcHieDD1RqTUCQbKBHIdoqL4fPyIBFM/c9cinVTsUjGUyKQ7cRiQCRIBNVbcoN5EcloSgSynfJyYO+lAFiPDygqSms4ItIKlAik1rRp8P8eLobTLgGg+oRf8dYGtQ2JZDolAqn12GNAfgxyKoOCSBWPLY2lMSIRaQ1ZdVnItGnw61/DunXbytzBDCIRiMe3zWfjsupqoE/Rtg8nHmHMEQnzIpKRsiYRTJsGF1+c7ijal0gODPxOuqMQkVRLadOQmY0ws/fMbJWZTapn+XlmVmZmb4avC1MVy7RpqVpzhsmPgQeTjutZBCJZIGWJwMyiwO3AKcAAYJyZDain6sPuPiR83ZOqeI44IlVrzjAlReARcMiL5GnkUZEskMqmoaOAVe6+GsDMZgGjgLdTuM0GnXjitrOCnIS9Tne7fFtblldeCN8cRO/ecWaNvV+DzmWArVu3UlpayubNm9MdirSCjh070rdvX3Jzkx8iJpWJoA+wJmG+FDi6nnpjzOx44H3gSndfU7eCmU0AJgB8+9vf3qlgqqq2TW/dulOryBr5t3xDtw7d0h2GtJDS0lK6detGfn4+VvMrQDKSu1NeXk5paSn9+/dP+n3pvnz0KSDf3QcBLwAP1FfJ3ae5e4G7F/Tu3XunNpSYCKRhxWuK+bjiY5Z/vlwPrs8QmzdvpmfPnkoCWcDM6NmzZ7PP/lKZCD4B+iXM9w3Larl7ubtvCWfvAVLWkq9EkJxYSQwPe4srqyvVWZwhlASyx84c61QmgteAA82sv5nlAWOBJxMrmNneCbMjgXdSFYwSQXKO7rOt9S4nkqPOYpEskLJE4O5VwKXA8wRf8I+4+wozu8HMRobVLjezFWb2T+By4LxUxaNEkJwvNn9RO11zZiCyK8rLyxkyZAhDhgxhr732ok+fPrXzlZWVjb53yZIlXH755U1u49hjj22pcAGYOHEiffr0IR6Pt+h626qU3lDm7nOBuXXKrkuYngxMTmUMNZQIkjN/9fza6ep4NbGSmK4cykLFxcFItEVFULiLh79nz568+eabAFx//fV07dqVq6++unZ5VVUVOTn1fxUVFBRQUFDQ5DYWL168a0EmiMfjzJkzh379+vHiiy9y4okntti6EzW2362tbUTRCpQIkrPvbvsCELUoeVHdR5BpJk6E8Du5QRUVsGxZcElxJAKDBkH37g3XHzIEbrmleXGcd955dOzYkTfeeINhw4YxduxYrrjiCjZv3kynTp247777OPjgg4nFYtx88808/fTTXH/99Xz88cesXr2ajz/+mIkTJ9aeLXTt2pWNGzcSi8W4/vrr6dWrF8uXL+eII47goYcewsyYO3cuV111FV26dGHYsGGsXr2ap59+eofYYrEYhx12GGeddRYzZ86sTQSfffYZl1xyCatXrwbgzjvv5Nhjj2X69OncfPPNmBmDBg3iwQcf5LzzzuO0007jBz/4wQ7x/epXv2L33Xfn3Xff5f3332f06NGsWbOGzZs3c8UVVzBhwgQAnnvuOaZMmUJ1dTW9evXihRde4OCDD2bx4sX07t2beDzOQQcdRHFxMTt7EU2NrEkEH3yQ7gjah6X/CoagPnm/k7nuhOt0NpCFKiqCJADBvxUVjSeCnVVaWsrixYuJRqN8+eWXvPTSS+Tk5DBv3jymTJnCY489tsN73n33XRYuXMhXX33FwQcfzE9/+tMdrpd/4403WLFiBfvssw/Dhg3j5ZdfpqCggIsvvphFixbRv39/xo0b12BcM2fOZNy4cYwaNYopU6awdetWcnNzufzyyznhhBOYM2cO1dXVbNy4kRUrVnDjjTeyePFievXqxRdffNHgemu8/vrrLF++vPbyznvvvZcePXrwzTffcOSRRzJmzBji8TgXXXRRbbxffPEFkUiE8ePHM2PGDCZOnMi8efMYPHjwLicByJJEUFwMdz9bDOdOgn1eI/qbSiKRoGfd3TEzIhYh7vHa+WxcVh2vru0XeO6D5zjj0DOUCDJMMr/ci4th+HCorIS8PJgxY9ebh+rzwx/+kGg0CkBFRQXnnnsuK1euxMzY2sDNPqeeeiodOnSgQ4cO7LHHHnz22Wf07dt3uzpHHXVUbdmQIUMoKSmha9eu7LfffrVfvuPGjWNaPePOVFZWMnfuXP74xz/SrVs3jj76aJ5//nlOO+00FixYwPTp0wGIRqN0796d6VL0Qa0AAA+USURBVNOn88Mf/pBevXoB0KNHjyb3+6ijjtruGv9bb72VOXPmALBmzRpWrlxJWVkZxx9/fG29mvVecMEFjBo1iokTJ3Lvvfdy/vnnN7m9ZGRFIpi+oJjq/zgOosHPnDjbfvEA7NAn6lm8LMFfX/8rE46Y0HAFyUiFhTB/fsv1ETSkS5cutdO/+tWvOPHEE5kzZw4lJSUUNfBEpA4dOtROR6NRqupp802mTkOef/55NmzYwMCBAwHYtGkTnTp14rTTTkt6HQA5OTm1Hc3xeHy7TvHE/Y7FYsybN4/i4mI6d+5MUVFRo/cA9OvXjz333JMFCxbw6quvMmPGjGbF1ZB031DWOvJjEImDEbwkKft02yfdIUiaFBbC5MmpSwJ1VVRU0KdPHwDuv//+Fl//wQcfzOrVqykpKQHg4YcfrrfezJkzueeeeygpKaGkpIQPP/yQF154gU2bNjF8+HDuvPNOAKqrq6moqOCkk07i0Ucfpby8HKC2aSg/P5+lS4Nm1ieffLLBM5yKigp23313OnfuzLvvvss//vEPAI455hgWLVrEhx9+uN16AS688ELGjx+/3RnVrsqKRHDO8UVEItHg16+uiExKhAjXDLsm3WFIlrjmmmuYPHkyQ4cObdYv+GR16tSJO+64gxEjRnDEEUfQrVs3utfp+Ni0aRPPPfccp556am1Zly5dOO6443jqqaf485//zMKFCxk4cCBHHHEEb7/9Nocddhi/+MUvOOGEExg8eDBXXXUVABdddBEvvvgigwcPpri4eLuzgEQjRoygqqqKQw89lEmTJnHMMccA0Lt3b6ZNm8aZZ57J4MGDOeuss2rfM3LkSDZu3NhizUIA5t6+vhkLCgp8yZIlzX7f4o+LGXZd2EfQsbLNtMu3tWV50TyO7HMkU4dPVf9AhnjnnXc49NBD0x1G2m3cuJGuXbvi7vzsZz/jwAMP5Morr0x3WM22ZMkSrrzySl566aUG69R3zM1sqbvXey1uVvQRABz77UJ44EUAqtpX7hORFnD33XfzwAMPUFlZydChQ7m4HT6paurUqdx5550t1jdQI2vOCGD74ZdFsoXOCLJPc88IsqKPQEREGqZEICKS5ZQIRESynBKBiEiWy5qrhkQkPcrLyxk+fDgAa9euJRqN1o6P8+qrr5KXl9fo+2OxGHl5eY0ONT169GjWrl1be0OWNI8SgYjsoHhNMbGSGEX5Rbt8P0lTw1A3JRaL0bVr1wYTwYYNG1i6dCldu3Zl9erV7LfffrsUb0Pa0rDRLS0z90pE6jXxuYm8ubbxcagrtlSw7LNlxD1OxCIM2nMQ3Ts0PPzokL2GcMuI5o1DvXTpUq666io2btxIr169uP/++9l777259dZbueuuu8jJyWHAgAFMnTqVu+66i2g0ykMPPcRtt93Gd7/73e3W9fjjj3P66aez5557MmvWLKZMmQLAqlWruOSSSygrKyMajfLoo4+y//77c9NNN/HQQw8RiUQ45ZRTmDp1KkVFRdx8880UFBSwbt06CgoKKCkp4f777+fxxx9n48aNVFdX88wzzzBq1CjWr1/P1q1bufHGGxk1ahTADsNR33HHHQwaNIj333+f3NxcvvzySwYPHlw735YoEYjIdio2VxD3cMA0j1OxuaLRRNBc7s5ll13GE088Qe/evXn44Yf5xS9+wb333svUqVP58MMP6dChAxs2bGC33XbjkksuafQsYubMmVx33XXsueeejBkzpjYRnH322UyaNIkzzjiDzZs3E4/HefbZZ3niiSd45ZVX6Ny5c9LDRi9btowePXpQVVXFnDlz+Na3vsW6des45phjGDlyJG+//fYOw1F369aNoqIinnnmGUaPHs2sWbM488wz21wSACUCkaySzC/34jXFDJ8+nMrqSvKiecw4c0aLDjeyZcsWli9fzsknnwwEA7jtvXfw+PJBgwZx9tlnM3r0aEaPHt3kuj777DNWrlzJcccdh5mRm5vL8uXL2Xffffnkk08444wzAOjYsSMA8+bN4/zzz6dz585AcsNGn3zyybX13J0pU6awaNEiIpEIn3zyCZ999hkLFiyodzjqCy+8kD/84Q+MHj2a++67j7vvvrs5H1WrUSIQke0U9itk/jnzW6yPoC5357DDDqO4uHiHZc888wyLFi3iqaee4ne/+x1vvfVWo+t65JFHWL9+fe24/V9++SUzZ85k0qRJzYopcdjousNAJw4YN2PGDMrKyli6dCm5ubnk5+c3Omz0sGHDKCkpIRaLUV1dzXe+851mxdVasvLy0Xr+/4lIgsJ+hUz+7uSUDDzYoUMHysrKahPB1q1bWbFiBfF4nDVr1nDiiSdy0003UVFRwcaNG+nWrRtfffVVveuaOXMmzz33XO2w0UuXLmXWrFl069aNvn378ve//x0IzkI2bdrEySefzH333cemTZuA+oeNnj17doOxV1RUsMcee5Cbm8vChQv56KOPABocjhrgnHPO4cc//nGLjhba0rImESR++Q8frmQgki6RSITZs2dz7bXXMnjwYIYMGcLixYuprq5m/PjxDBw4kKFDh3L55Zez2267cfrppzNnzhyGDBmy3YibJSUlfPTRR7VDNwP079+f7t2788orr/Dggw9y6623MmjQII499ljWrl3LiBEjGDlyJAUFBQwZMoSbb74ZgKuvvpo777yToUOHsm7dugZjP/vss1myZAkDBw5k+vTpHHLIIQANDkdd857169c3+njMdMuaQed+/3v45S+DJ5NFo/Db3wYP3hDJdBp0Lr1mz57NE088wYMPPthq29Qw1A0oKoIOHbY9h7WBJ+GJiLSYyy67jGeffZa5c+emO5RGZU0iaK3nsIqI1LjtttvSHUJSsiYRQPDlrwQg2SjxyXSS2XamuT9rOotFslXHjh0pLy/fqS8IaV/cnfLy8tr7JpKVVWcEItmob9++lJaWUlZWlu5QpBV07NiRvn37Nus9KU0EZjYC+DMQBe5x96kN1BsDzAaOdPedew6liNQrNze39oYrkfqkrGnIzKLA7cApwABgnJkNqKdeN+AK4JVUxSIiIg1LZR/BUcAqd1/t7pXALGBUPfV+C9wENHyftoiIpEwqE0EfYE3CfGlYVsvMDgf6ufszja3IzCaY2RIzW6J2ThGRlpW2zmIziwB/BM5rqq67TwOmhe8rM7OPdnKzvYCG7x/PTNrn7KB9zg67ss/7NrQglYngE6BfwnzfsKxGN+A7QCy8vnkv4EkzG9lYh7G7997ZgMxsSUO3WGcq7XN20D5nh1Ttcyqbhl4DDjSz/maWB4wFnqxZ6O4V7t7L3fPdPR/4B9BoEhARkZaXskTg7lXApcDzwDvAI+6+wsxuMLORqdquiIg0T0r7CNx9LjC3Ttl1DdQtSmUsoWmtsI22RvucHbTP2SEl+9zuhqEWEZGWpbGGRESynBKBiEiWy5pEYGYjzOw9M1tlZs17snUbZWb9zGyhmb1tZivM7IqwvIeZvWBmK8N/dw/LzcxuDT+DZeENfe2SmUXN7A0zezqc729mr4T79nB4pRpm1iGcXxUuz09n3DvLzHYzs9lm9q6ZvWNmhZl+nM3syvD/9XIzm2lmHTPtOJvZvWb2uZktTyhr9nE1s3PD+ivN7NzmxpEViSDZcY/aoSrg/7r7AOAY4Gfhfk0C5rv7gcD8cB6C/T8wfE0A7mz9kFvMFQRXo9W4CfiTux8ArAd+Epb/BFgflv8prNce/Rl4zt0PAQYT7HvGHmcz6wNcDhS4+3cIBq4cS+Yd5/uBEXXKmnVczawH8GvgaIKhfX5dkzyS5u4Z/wIKgecT5icDk9MdVwr28wngZOA9YO+wbG/gvXD6L8C4hPq19drTi+DmxPnAScDTgBHcbZlT93gTXL5cGE7nhPUs3fvQzP3tDnxYN+5MPs5sG6KmR3jcngb+PROPM5APLN/Z4wqMA/6SUL5dvWReWXFGQBLjHrV34anwUIJRXPd090/DRWuBPcPpTPkcbgGuAeLhfE9ggwf3rsD2+1W7z+HyirB+e9IfKAPuC5vD7jGzLmTwcXb3T4CbgY+BTwmO21Iy+zjXaO5x3eXjnS2JIKOZWVfgMWCiu3+ZuMyDnwgZc42wmZ0GfO7uS9MdSyvKAQ4H7nT3ocDXbGsuADLyOO9OMFpxf2AfoAs7NqFkvNY6rtmSCJoa96jdMrNcgiQww90fD4s/M7O9w+V7A5+H5ZnwOQwDRppZCcHQ5icRtJ/vZmY1N0gm7lftPofLuwPlrRlwCygFSt295pkdswkSQyYf5+8BH7p7mbtvBR4nOPaZfJxrNPe47vLxzpZE0Oi4R+2VmRnwV+Add/9jwqIngZorB84l6DuoKT8nvPrgGKAi4RS0XXD3ye7e14PxqcYCC9z9bGAh8IOwWt19rvksfhDWb1e/nN19LbDGzA4Oi4YDb5PBx5mgSegYM+sc/j+v2eeMPc4Jmntcnwf+zcx2D8+k/i0sS166O0pasUPm+8D7wAfAL9IdTwvt03EEp43LgDfD1/cJ2kbnAyuBeUCPsL4RXD31AfAWwRUZad+PXdj/IuDpcHo/4FVgFfAo0CEs7xjOrwqX75fuuHdyX4cAS8Jj/Xdg90w/zsBvgHeB5cCDQIdMO87ATII+kK0EZ34/2ZnjClwQ7vsq4PzmxqEhJkREsly2NA2JiEgDlAhERLKcEoGISJZTIhARyXJKBCIiWU6JQCRkZtVm9mbCq8VGqTWz/MQRJkXakpQ+qlKknfnG3YekOwiR1qYzApEmmFmJmf3BzN4ys1fN7ICwPN/MFoRjw883s2+H5Xua2Rwz+2f4OjZcVdTM7g7H2P8fM+sU1r/cgmdKLDOzWWnaTcliSgQi23Sq0zR0VsKyCncfCPw3weinALcBD7j7IGAGcGtYfivworsPJhgTaEVYfiBwu7sfBmwAxoTlk4Ch4XouSdXOiTREdxaLhMxso7t3rae8BDjJ3VeHg/ytdfeeZraOYNz4rWH5p+7ey8zKgL7uviVhHfnACx48bAQzuxbIdfcbzew5YCPB0BF/d/eNKd5Vke3ojEAkOd7AdHNsSZiuZlsf3akEY8gcDryWMLqmSKtQIhBJzlkJ/xaH04sJRkAFOBt4KZyeD/wUap+t3L2hlZpZBOjn7guBawmGT97hrEQklfTLQ2SbTmb2ZsL8c+5ecwnp7ma2jOBX/biw7DKCp4b9nOAJYueH5VcA08zsJwS//H9KMMJkfaLAQ2GyMOBWd9/QYnskkgT1EYg0IewjKHD3demORSQV1DQkIpLldEYgIpLldEYgIpLllAhERLKcEoGISJZTIhARyXJKBCIiWe7/AwdXXfgwZne4AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.054916627620149296\n",
            "training error 0.12501761939688735, test error 0.25006646955019424\n",
            "training error 0.12499554038170171, test error 0.2500812775142675\n",
            "training error 0.12500582768740653, test error 0.2501415921200335\n",
            "training error 0.1249885178781035, test error 0.25016461772800686\n",
            "training error 0.12501383079051662, test error 0.2502987574596017\n",
            "training error 0.12499570564863093, test error 0.25027350129586706\n",
            "training error 0.12497216806987967, test error 0.25033894596045764\n",
            "training error 0.12500642874128842, test error 0.2503760170358871\n",
            "training error 0.12508596238076042, test error 0.2505276153972046\n",
            "training error 0.12503383575901394, test error 0.2504489623610726\n",
            "training error 0.12515090983772714, test error 0.2503477317271914\n",
            "training error 0.12503403845727704, test error 0.25052588071356346\n",
            "training error 0.12501765712040225, test error 0.25063557984345636\n",
            "training error 0.1250027405372344, test error 0.2504718176347924\n",
            "training error 0.12507654828000986, test error 0.2503744979683615\n",
            "training error 0.12505634268304255, test error 0.2504084859454278\n",
            "training error 0.12504754773919563, test error 0.2505830055714747\n",
            "training error 0.1249793418115088, test error 0.2504802539287011\n",
            "training error 0.12504521439749627, test error 0.25042579568129125\n",
            "training error 0.12496811396087808, test error 0.25046957520111945\n",
            "training error 0.12518108793408125, test error 0.2506575851459646\n",
            "training error 0.125076996257554, test error 0.25047238580979664\n",
            "training error 0.12520712557780966, test error 0.25072327176434156\n",
            "training error 0.1253206659576605, test error 0.2506702035741007\n",
            "training error 0.1250269765003687, test error 0.25068732187735365\n",
            "training error 0.1251217985237671, test error 0.2505051038485156\n",
            "training error 0.1249535775104007, test error 0.2505940521214276\n",
            "training error 0.12499789336186064, test error 0.2505068265162481\n",
            "training error 0.12495860400582341, test error 0.2505536536481931\n",
            "training error 0.12502810847711215, test error 0.2505637184441907\n",
            "training error 0.12496669346054527, test error 0.25053889206886426\n",
            "training error 0.12497095885773563, test error 0.25045497225439084\n",
            "training error 0.1249718884852343, test error 0.2504518650181602\n",
            "training error 0.12496666994953667, test error 0.2504838644291246\n",
            "training error 0.12497531496072278, test error 0.25056055790258985\n",
            "training error 0.1249795578828559, test error 0.25054184202963303\n",
            "training error 0.12505037762616822, test error 0.2505075557027175\n",
            "training error 0.1249783137635948, test error 0.2505140656962518\n",
            "training error 0.12505898943019383, test error 0.2505242522926536\n",
            "training error 0.1250016475116725, test error 0.25071984834714006\n",
            "training error 0.12500196453411572, test error 0.25056911065651594\n",
            "training error 0.12497816336384177, test error 0.25057883273153164\n",
            "training error 0.12505547468867495, test error 0.2507235462865113\n",
            "training error 0.1249663666556309, test error 0.25068656861945315\n",
            "training error 0.12498882001535004, test error 0.250642236274934\n",
            "training error 0.12496402608051047, test error 0.250732701282705\n",
            "training error 0.12498271379140177, test error 0.25070346316242825\n",
            "training error 0.12496388852819443, test error 0.25074451456142344\n",
            "training error 0.12511415075896623, test error 0.25070044598239677\n",
            "training error 0.1249569508636781, test error 0.2506164617175509\n",
            "Loss: 0.21993839011922311\n",
            "training error 0.12498478854319227, test error 0.2504934178554515\n",
            "Loss: 0.17073392767339435\n",
            "training error 0.12506521913003169, test error 0.250531610196268\n",
            "Loss: 0.18600680327531816\n",
            "training error 0.12500894506832888, test error 0.250573477563036\n",
            "Loss: 0.20274929851800128\n",
            "training error 0.12495320631397067, test error 0.25057425949625073\n",
            "Loss: 0.20306198866639757\n",
            "training error 0.12495631816489762, test error 0.25060409982605963\n",
            "Loss: 0.21499494787624585\n",
            "training error 0.12498116402599294, test error 0.25057814432427045\n",
            "Loss: 0.2046155068276878\n",
            "training error 0.12513779286485094, test error 0.25053333412114664\n",
            "Loss: 0.18669618993389925\n",
            "training error 0.12495412712050835, test error 0.25055381644896996\n",
            "Loss: 0.1948869433204381\n",
            "training error 0.12500115906170667, test error 0.25061352055885916\n",
            "Loss: 0.21876223935537453\n",
            "training error 0.12500047918781393, test error 0.2506317570527175\n",
            "Loss: 0.22605489793976474\n",
            "training error 0.12496351768221732, test error 0.25060715868254535\n",
            "Loss: 0.21621816524368676\n",
            "training error 0.1251330207018048, test error 0.2506622833928754\n",
            "Loss: 0.23826218835050206\n",
            "training error 0.12496837176492653, test error 0.25058515609637116\n",
            "Loss: 0.20741947015523454\n",
            "training error 0.12497500238257588, test error 0.2506600827228013\n",
            "Loss: 0.23738215430275655\n",
            "training error 0.12502943374244976, test error 0.25063048195492205\n",
            "Loss: 0.22554499439382258\n",
            "training error 0.12494731733239844, test error 0.25074817467152405\n",
            "Loss: 0.2726095675905871\n",
            "training error 0.12496718791011513, test error 0.2506688493062013\n",
            "Loss: 0.2408878555731908\n",
            "training error 0.12494203846519662, test error 0.2506908871272749\n",
            "Loss: 0.2497006408751279\n",
            "training error 0.1251488245879956, test error 0.25071890467550456\n",
            "Loss: 0.2609046812569016\n",
            "training error 0.12496474361094621, test error 0.25059695216318395\n",
            "Loss: 0.21213664268699972\n",
            "training error 0.12496093725676236, test error 0.25061224025885426\n",
            "Loss: 0.21825025547876908\n",
            "training error 0.12503403149778167, test error 0.25057765124293063\n",
            "Loss: 0.20441832671764804\n",
            "training error 0.1249877184118157, test error 0.2506738428769124\n",
            "Loss: 0.24288475292617573\n",
            "training error 0.12497086347130905, test error 0.2505425453154631\n",
            "Loss: 0.19037968829855867\n",
            "training error 0.12524908575801358, test error 0.25067364240259266\n",
            "Loss: 0.2428045845132898\n",
            "training error 0.1250508570467858, test error 0.2504641040013994\n",
            "Loss: 0.15901150278980758\n",
            "training error 0.1249424853663744, test error 0.2505762896872314\n",
            "Loss: 0.2038738492026404\n",
            "training error 0.1249900434073856, test error 0.25051239606522324\n",
            "Loss: 0.17832319376169803\n",
            "training error 0.12495035124597945, test error 0.25049179132792865\n",
            "Loss: 0.17008348960156905\n",
            "training error 0.12495932269596989, test error 0.2505110122731724\n",
            "Loss: 0.17776982407029518\n",
            "training error 0.1250317046189886, test error 0.25054261793525734\n",
            "Loss: 0.19040872849509505\n",
            "training error 0.12495028619464313, test error 0.25062503796660085\n",
            "Loss: 0.22336797788657936\n",
            "training error 0.12498579326574281, test error 0.25068176274539133\n",
            "Loss: 0.24605185825345988\n",
            "training error 0.12493643535856523, test error 0.25065749233071394\n",
            "Loss: 0.2363462728860899\n",
            "training error 0.1251885571548065, test error 0.2505155685866917\n",
            "Loss: 0.17959186503704316\n",
            "training error 0.1249616344332764, test error 0.2506292637685998\n",
            "Loss: 0.22505784938615658\n",
            "training error 0.12495137607196781, test error 0.2506356069413826\n",
            "Loss: 0.22759444407405294\n",
            "training error 0.1249507657657515, test error 0.2506411525635276\n",
            "Loss: 0.22981210330479396\n",
            "training error 0.12507282378174223, test error 0.25064440552711825\n",
            "Loss: 0.23111294287618822\n",
            "training error 0.1249258420227255, test error 0.25077562632731537\n",
            "Loss: 0.2835873111644016\n",
            "training error 0.12492494127374972, test error 0.2507478438343111\n",
            "Loss: 0.27247726788099946\n",
            "training error 0.12496046244672396, test error 0.25065973497297395\n",
            "Loss: 0.23724309134560695\n",
            "training error 0.12494645050640997, test error 0.25067105272068363\n",
            "Loss: 0.2417689870924722\n",
            "training error 0.1249265746371481, test error 0.2505697973988259\n",
            "Loss: 0.20127762412009975\n",
            "training error 0.12502760644464445, test error 0.2506583171397695\n",
            "Loss: 0.2366761088121283\n",
            "training error 0.12495063072724823, test error 0.2506653327922951\n",
            "Loss: 0.2394816238970643\n",
            "training error 0.12493506089464254, test error 0.2506548059673859\n",
            "Loss: 0.23527201317710134\n",
            "training error 0.12496771168815853, test error 0.2506935505125446\n",
            "Loss: 0.25076571180386864\n",
            "training error 0.12504029457542248, test error 0.25067442936431505\n",
            "Loss: 0.24311928553011342\n",
            "training error 0.12510368039590922, test error 0.25050477736054055\n",
            "Loss: 0.17527652193223275\n",
            "training error 0.12497281321515728, test error 0.2506009433364747\n",
            "Loss: 0.21373268764974807\n",
            "training error 0.12497303626993088, test error 0.25055981228862234\n",
            "Loss: 0.19728464168566884\n",
            "training error 0.12493465113155568, test error 0.25044180008586747\n",
            "Loss: 0.15009230799649664\n",
            "training error 0.12496492058235055, test error 0.2505331278913209\n",
            "Loss: 0.18661371993056175\n",
            "training error 0.12496089221536676, test error 0.250558905271742\n",
            "Loss: 0.19692193137030856\n",
            "training error 0.12502799063307019, test error 0.25039753998087017\n",
            "Loss: 0.13239297186522503\n",
            "training error 0.12492217186908662, test error 0.2504590466377259\n",
            "Loss: 0.15698909503454317\n",
            "training error 0.12492590474114333, test error 0.25040931509285314\n",
            "Loss: 0.13710176469303725\n",
            "training error 0.12492231014021281, test error 0.25044486895191215\n",
            "Loss: 0.15131952812328375\n",
            "training error 0.12496045518142579, test error 0.2503946521870044\n",
            "Loss: 0.1312381613578495\n",
            "training error 0.12515857391789867, test error 0.25056926050545764\n",
            "Loss: 0.2010629238569317\n",
            "training error 0.12520121519066005, test error 0.2506302020907497\n",
            "Loss: 0.22543307848088556\n",
            "training error 0.12492582361843074, test error 0.2505203663848804\n",
            "Loss: 0.18151047419616706\n",
            "training error 0.1249361436730334, test error 0.2505762119334702\n",
            "Loss: 0.20384275596518897\n",
            "training error 0.12599074634237986, test error 0.25076788345233625\n",
            "Loss: 0.2804909844185355\n",
            "training error 0.12489405326348609, test error 0.2505887370797638\n",
            "Loss: 0.20885148277136611\n",
            "training error 0.12495041000091588, test error 0.25053682952831824\n",
            "Loss: 0.18809398116030884\n",
            "training error 0.12511478650694666, test error 0.2505342545382915\n",
            "Loss: 0.18706425893029177\n",
            "training error 0.12493664092331158, test error 0.25048232820343286\n",
            "Loss: 0.16629924595115142\n",
            "training error 0.12490975030626444, test error 0.2505959255158383\n",
            "Loss: 0.21172609290498468\n",
            "training error 0.12504951005325238, test error 0.25060255042084356\n",
            "Loss: 0.21437535052724055\n",
            "training error 0.12491342192445243, test error 0.2506324346498707\n",
            "Loss: 0.22632586475686\n",
            "training error 0.12524510377491285, test error 0.25060264771177376\n",
            "Loss: 0.2144142565550622\n",
            "training error 0.12489419370413178, test error 0.2507044514282143\n",
            "Loss: 0.25512491905357315\n",
            "training error 0.12495789479681044, test error 0.25071649813670777\n",
            "Loss: 0.2599423216086416\n",
            "training error 0.12490618216655608, test error 0.25064456956122483\n",
            "Loss: 0.23117853907821395\n",
            "training error 0.12492177416674134, test error 0.2506429677838189\n",
            "Loss: 0.23053799842163158\n",
            "training error 0.12494357091313936, test error 0.25072356622956143\n",
            "Loss: 0.2627688072491807\n",
            "training error 0.12491832803568692, test error 0.25064437658875083\n",
            "Loss: 0.23110137060602387\n",
            "training error 0.12489676605592293, test error 0.2506607215007864\n",
            "Loss: 0.23763759757997605\n",
            "training error 0.12491895390668728, test error 0.25067817011844923\n",
            "Loss: 0.24461518945553262\n",
            "training error 0.12488086717551163, test error 0.2506873888684845\n",
            "Loss: 0.24830170930438822\n",
            "training error 0.12488965355039391, test error 0.2507254049134359\n",
            "Loss: 0.2635040853045778\n",
            "training error 0.12493733687159972, test error 0.25071929128580234\n",
            "Loss: 0.26105928427044844\n",
            "training error 0.12489004206535526, test error 0.2508125161335111\n",
            "Loss: 0.2983393114073918\n",
            "training error 0.1249286462195959, test error 0.2509515788908063\n",
            "Loss: 0.35394962875436864\n",
            "training error 0.12494224916045049, test error 0.2508386252975043\n",
            "Loss: 0.30878020099973646\n",
            "training error 0.12495050526257744, test error 0.25075547450402047\n",
            "Loss: 0.27552872444898124\n",
            "training error 0.12498819690099199, test error 0.2508644395304218\n",
            "Loss: 0.3191031495197727\n",
            "training error 0.12511902332549582, test error 0.2508349721464425\n",
            "Loss: 0.3073193289890641\n",
            "training error 0.12487580398448679, test error 0.2508093557481021\n",
            "Loss: 0.2970754932654973\n",
            "training error 0.12504681353027028, test error 0.25075846191760626\n",
            "Loss: 0.27672337225248533\n",
            "training error 0.12509330976975017, test error 0.2509975455659075\n",
            "Loss: 0.37233141147952065\n",
            "training error 0.12491460850699221, test error 0.25088595871517155\n",
            "Loss: 0.3277085354351561\n",
            "training error 0.12495218843780985, test error 0.2509455127581445\n",
            "Loss: 0.3515238206591276\n",
            "training error 0.1248882955358196, test error 0.25085957859699914\n",
            "Loss: 0.31715929297979617\n",
            "training error 0.124852444390687, test error 0.2508917706015036\n",
            "Loss: 0.3300326720307112\n",
            "training error 0.12488670366893893, test error 0.25091245576091004\n",
            "Loss: 0.3383045364848458\n",
            "training error 0.12485398937843097, test error 0.2508536001972999\n",
            "Loss: 0.31476856874155246\n",
            "training error 0.1249055364494634, test error 0.25082485860410075\n",
            "Loss: 0.3032749873546292\n",
            "training error 0.12494595216264685, test error 0.25076228247306964\n",
            "Loss: 0.27825118822486505\n",
            "training error 0.1248399707120145, test error 0.2507584652214335\n",
            "Loss: 0.2767246934321177\n",
            "training error 0.12488213433886361, test error 0.2508995473511342\n",
            "Loss: 0.3331425450354919\n",
            "training error 0.12489735138584288, test error 0.250881895142961\n",
            "Loss: 0.326083538602151\n",
            "training error 0.12482414946956301, test error 0.25077344848792493\n",
            "Loss: 0.28271640696266864\n",
            "training error 0.12540140185899695, test error 0.25070453739677073\n",
            "Loss: 0.2551592973357053\n",
            "training error 0.12481267429529734, test error 0.25061450804839935\n",
            "Loss: 0.21915713017859328\n",
            "training error 0.12494953724895921, test error 0.25062788925791774\n",
            "Loss: 0.22450819125545696\n",
            "training error 0.12483874900395785, test error 0.2507196890562787\n",
            "Loss: 0.26121835016883477\n",
            "training error 0.1248275116240045, test error 0.2507896555668521\n",
            "Loss: 0.2891975153481052\n",
            "training error 0.12479208819638223, test error 0.2507555154495841\n",
            "Loss: 0.27554509832095064\n",
            "training error 0.12482415281992931, test error 0.2506597626375411\n",
            "Loss: 0.23725415423108132\n",
            "training error 0.1248301873974459, test error 0.2507096182597312\n",
            "Loss: 0.2571911023072415\n",
            "training error 0.12493857754943034, test error 0.25081461626215434\n",
            "Loss: 0.2991791395727139\n",
            "training error 0.1252593218429816, test error 0.2505798373973402\n",
            "Loss: 0.20529255604295749\n",
            "training error 0.12477156616284399, test error 0.25065073555306444\n",
            "Loss: 0.23364428022722983\n",
            "training error 0.12490054143312612, test error 0.2507312706464865\n",
            "Loss: 0.2658497548623906\n",
            "training error 0.12484113839004915, test error 0.2505650081333207\n",
            "Loss: 0.1993624271271699\n",
            "training error 0.12499737742990702, test error 0.2506772112967143\n",
            "Loss: 0.2442317627063817\n",
            "training error 0.12475767921644552, test error 0.25058190361241817\n",
            "Loss: 0.206118822387924\n",
            "training error 0.1247320737297212, test error 0.2506171297009809\n",
            "Loss: 0.22020551246919595\n",
            "training error 0.12481670985111883, test error 0.2507061217120097\n",
            "Loss: 0.25579285498211757\n",
            "training error 0.1247858842129747, test error 0.2506927549985462\n",
            "Loss: 0.2504475907859627\n",
            "training error 0.12491221868041667, test error 0.25073889639291785\n",
            "Loss: 0.26889924264261733\n",
            "training error 0.12471523400971256, test error 0.2506414964708812\n",
            "Loss: 0.22994962968097\n",
            "training error 0.12480738991146455, test error 0.250713636181047\n",
            "Loss: 0.2587978436360716\n",
            "training error 0.1248488680071805, test error 0.25053264629566585\n",
            "Loss: 0.18642113287326012\n",
            "training error 0.12468750233620557, test error 0.25059361320868223\n",
            "Loss: 0.21080141589402412\n",
            "training error 0.12486555402930741, test error 0.25049303936050193\n",
            "Loss: 0.17058256993629506\n",
            "training error 0.12483096014045401, test error 0.2506605000321149\n",
            "Loss: 0.2375490336585795\n",
            "training error 0.12476478376621918, test error 0.2504582393805928\n",
            "Loss: 0.15666627801129707\n",
            "training error 0.1246455112366186, test error 0.2504870820593639\n",
            "Loss: 0.1682002828792717\n",
            "training error 0.12473895476397401, test error 0.2505536788218188\n",
            "Loss: 0.1948319070929072\n",
            "training error 0.12469089813865693, test error 0.2503883836126847\n",
            "Loss: 0.1287313981236604\n",
            "training error 0.12461122974734273, test error 0.25043089439675603\n",
            "Loss: 0.1457311918776183\n",
            "training error 0.12473005128086094, test error 0.2504717222979143\n",
            "Loss: 0.16205801139554943\n",
            "training error 0.1245733323781148, test error 0.2504272472387669\n",
            "Loss: 0.14427271645880424\n",
            "training error 0.12479210637601997, test error 0.25043485593622106\n",
            "Loss: 0.14731538646082232\n",
            "training error 0.12468162371045338, test error 0.2505383646209645\n",
            "Loss: 0.18870785500315623\n",
            "training error 0.12455943268946801, test error 0.2503884866899358\n",
            "Loss: 0.12877261806463203\n",
            "training error 0.12456470893170407, test error 0.25027823252299286\n",
            "Loss: 0.0846826738424955\n",
            "training error 0.12468731266624448, test error 0.25026633599443177\n",
            "Loss: 0.07992532729279578\n",
            "training error 0.12454744807417875, test error 0.25020548755740984\n",
            "Loss: 0.055592422073091896\n",
            "training error 0.12456170111509328, test error 0.2502053383745023\n",
            "Loss: 0.05553276477165969\n",
            "training error 0.12445529298541691, test error 0.25017212585016246\n",
            "Loss: 0.042251286291317136\n",
            "training error 0.12443572299998798, test error 0.2500844664157337\n",
            "Loss: 0.007196832734845948\n",
            "training error 0.1245617159450779, test error 0.24992049987160686\n",
            "Loss: 0.0\n",
            "training error 0.12441491505868751, test error 0.2499060942366764\n",
            "Loss: 0.0\n",
            "training error 0.12443791741993729, test error 0.249905960459109\n",
            "Loss: 0.0\n",
            "training error 0.12449586730201873, test error 0.24976744889233995\n",
            "Loss: 0.0\n",
            "training error 0.12433187168106298, test error 0.24981415002741839\n",
            "Loss: 0.01869784685135656\n",
            "training error 0.12441701095388692, test error 0.2498364797105045\n",
            "Loss: 0.027638036289623358\n",
            "training error 0.12427519148328235, test error 0.24974577906660395\n",
            "Loss: 0.0\n",
            "training error 0.12423391187680992, test error 0.2497206015415295\n",
            "Loss: 0.0\n",
            "training error 0.12422066070658466, test error 0.24968332841879715\n",
            "Loss: 0.0\n",
            "training error 0.12434393142159872, test error 0.24968856997904515\n",
            "Loss: 0.002099283232559479\n",
            "training error 0.12414238565183063, test error 0.249623654466214\n",
            "Loss: 0.0\n",
            "training error 0.12414461732895832, test error 0.24944843605190115\n",
            "Loss: 0.0\n",
            "training error 0.12411953293736824, test error 0.24941158158866097\n",
            "Loss: 0.0\n",
            "training error 0.12406252272455241, test error 0.24929388012935044\n",
            "Loss: 0.0\n",
            "training error 0.12413815171619036, test error 0.24916940196287732\n",
            "Loss: 0.0\n",
            "training error 0.12403731763931727, test error 0.24912662618999354\n",
            "Loss: 0.0\n",
            "training error 0.1239953931519491, test error 0.2490091627416779\n",
            "Loss: 0.0\n",
            "training error 0.12391792406209967, test error 0.24896771490955705\n",
            "Loss: 0.0\n",
            "training error 0.12388336670961013, test error 0.2489198970346223\n",
            "Loss: 0.0\n",
            "training error 0.1238085659306719, test error 0.24880379440637249\n",
            "Loss: 0.0\n",
            "training error 0.12379497360869816, test error 0.2487702741381196\n",
            "Loss: 0.0\n",
            "training error 0.12370784069094715, test error 0.24864178043197174\n",
            "Loss: 0.0\n",
            "training error 0.12363455798513438, test error 0.2485822718980672\n",
            "Loss: 0.0\n",
            "training error 0.12358490099220318, test error 0.24850120800515166\n",
            "Loss: 0.0\n",
            "training error 0.12350748480919958, test error 0.2484058476221206\n",
            "Loss: 0.0\n",
            "training error 0.12347502671030916, test error 0.24826056668543794\n",
            "Loss: 0.0\n",
            "training error 0.12344313907059283, test error 0.24819649382652803\n",
            "Loss: 0.0\n",
            "training error 0.12331393302835984, test error 0.24800570286117685\n",
            "Loss: 0.0\n",
            "training error 0.12329121071552565, test error 0.2478648143570514\n",
            "Loss: 0.0\n",
            "training error 0.12316923948333683, test error 0.24769638709717404\n",
            "Loss: 0.0\n",
            "training error 0.12314658225418226, test error 0.24747395192665583\n",
            "Loss: 0.0\n",
            "training error 0.12301083422706911, test error 0.24729698911713238\n",
            "Loss: 0.0\n",
            "training error 0.12293890870871803, test error 0.24706584657611166\n",
            "Loss: 0.0\n",
            "training error 0.1228707438876419, test error 0.24688752548117554\n",
            "Loss: 0.0\n",
            "training error 0.12282165234765668, test error 0.2466395588063952\n",
            "Loss: 0.0\n",
            "training error 0.12285542593820425, test error 0.24658579304043657\n",
            "Loss: 0.0\n",
            "training error 0.12259370652823105, test error 0.24622365301327137\n",
            "Loss: 0.0\n",
            "training error 0.12246777194733495, test error 0.24599300472949653\n",
            "Loss: 0.0\n",
            "training error 0.12235177496303401, test error 0.24579520853452227\n",
            "Loss: 0.0\n",
            "training error 0.12227186488199385, test error 0.24561257806942285\n",
            "Loss: 0.0\n",
            "training error 0.1221643231286886, test error 0.2454442677083963\n",
            "Loss: 0.0\n",
            "training error 0.12199034693770085, test error 0.24510476957846744\n",
            "Loss: 0.0\n",
            "training error 0.12197732270689073, test error 0.2448607124868956\n",
            "Loss: 0.0\n",
            "training error 0.12177525695522688, test error 0.24459377256161402\n",
            "Loss: 0.0\n",
            "training error 0.12158526167686029, test error 0.24429244864432026\n",
            "Loss: 0.0\n",
            "training error 0.12154144992209145, test error 0.2440400037424191\n",
            "Loss: 0.0\n",
            "training error 0.12135222100473578, test error 0.24368659096496414\n",
            "Loss: 0.0\n",
            "training error 0.12115794175838777, test error 0.24332606589276348\n",
            "Loss: 0.0\n",
            "training error 0.12096091754095364, test error 0.2430163155517432\n",
            "Loss: 0.0\n",
            "training error 0.1207872521209531, test error 0.24267362479850627\n",
            "Loss: 0.0\n",
            "training error 0.12063794542649547, test error 0.2423028848697986\n",
            "Loss: 0.0\n",
            "training error 0.12042446556176124, test error 0.24188367745879702\n",
            "Loss: 0.0\n",
            "training error 0.12026456294739467, test error 0.2415738631829216\n",
            "Loss: 0.0\n",
            "training error 0.1200497899351038, test error 0.24112244037815808\n",
            "Loss: 0.0\n",
            "training error 0.12000569103416425, test error 0.2406372384833278\n",
            "Loss: 0.0\n",
            "training error 0.11970213864191276, test error 0.24043694404162408\n",
            "Loss: 0.0\n",
            "training error 0.11944896209871973, test error 0.2399577614158243\n",
            "Loss: 0.0\n",
            "training error 0.11911800968418867, test error 0.23934294153310706\n",
            "Loss: 0.0\n",
            "training error 0.11894835686588784, test error 0.238788656631021\n",
            "Loss: 0.0\n",
            "training error 0.11862313874410073, test error 0.23824019684887088\n",
            "Loss: 0.0\n",
            "training error 0.11844373041734252, test error 0.23770625638928178\n",
            "Loss: 0.0\n",
            "training error 0.11807063180285514, test error 0.23713234355720936\n",
            "Loss: 0.0\n",
            "training error 0.11793957737207943, test error 0.23669886754894684\n",
            "Loss: 0.0\n",
            "training error 0.11758331466631379, test error 0.2358512070500046\n",
            "Loss: 0.0\n",
            "training error 0.11720759244003581, test error 0.23521457716620664\n",
            "Loss: 0.0\n",
            "training error 0.1168239821946267, test error 0.23460874433465412\n",
            "Loss: 0.0\n",
            "training error 0.11656052471961326, test error 0.23402475310571572\n",
            "Loss: 0.0\n",
            "training error 0.11618048301966068, test error 0.23328518061744935\n",
            "Loss: 0.0\n",
            "training error 0.11583482814534359, test error 0.2325893195037823\n",
            "Loss: 0.0\n",
            "training error 0.11544957022490283, test error 0.23180290777079524\n",
            "Loss: 0.0\n",
            "training error 0.11513580568795673, test error 0.23092242308973257\n",
            "Loss: 0.0\n",
            "training error 0.11466353988788523, test error 0.23013606241868526\n",
            "Loss: 0.0\n",
            "training error 0.11422119710878204, test error 0.22932292463543985\n",
            "Loss: 0.0\n",
            "training error 0.11386147926551382, test error 0.22842790638313265\n",
            "Loss: 0.0\n",
            "training error 0.11339183477465158, test error 0.22750652745718158\n",
            "Loss: 0.0\n",
            "training error 0.11306006559805139, test error 0.22645744344898414\n",
            "Loss: 0.0\n",
            "training error 0.1125434913210389, test error 0.22562405281362177\n",
            "Loss: 0.0\n",
            "training error 0.11199365438294882, test error 0.22454486942411991\n",
            "Loss: 0.0\n",
            "training error 0.11171565887691866, test error 0.2236399164793339\n",
            "Loss: 0.0\n",
            "training error 0.11100628193500559, test error 0.22237329269933337\n",
            "Loss: 0.0\n",
            "training error 0.11049542882559071, test error 0.22123108387357984\n",
            "Loss: 0.0\n",
            "training error 0.11007569618210589, test error 0.22023224345364328\n",
            "Loss: 0.0\n",
            "training error 0.10944131894793767, test error 0.21905599254987054\n",
            "Loss: 0.0\n",
            "training error 0.10890480194731002, test error 0.21801935126049055\n",
            "Loss: 0.0\n",
            "training error 0.108423124197178, test error 0.21690729498610628\n",
            "Loss: 0.0\n",
            "training error 0.10782756852531022, test error 0.21564044810744268\n",
            "Loss: 0.0\n",
            "training error 0.10714999027565265, test error 0.21445657039282542\n",
            "Loss: 0.0\n",
            "training error 0.10664158767222083, test error 0.21302910189107338\n",
            "Loss: 0.0\n",
            "training error 0.10592729320787239, test error 0.21182176648234233\n",
            "Loss: 0.0\n",
            "training error 0.10528326115113455, test error 0.21049989444869416\n",
            "Loss: 0.0\n",
            "training error 0.10465517637563954, test error 0.20910050803211042\n",
            "Loss: 0.0\n",
            "training error 0.10401699919478595, test error 0.20760907063222642\n",
            "Loss: 0.0\n",
            "training error 0.10328453944133785, test error 0.20613180427133346\n",
            "Loss: 0.0\n",
            "training error 0.10257294222465484, test error 0.20474631364231177\n",
            "Loss: 0.0\n",
            "training error 0.10188535242072437, test error 0.20321347920198782\n",
            "Loss: 0.0\n",
            "training error 0.10118101113953312, test error 0.2016928907729807\n",
            "Loss: 0.0\n",
            "training error 0.1006404176537023, test error 0.2002573407288759\n",
            "Loss: 0.0\n",
            "training error 0.09983478493841261, test error 0.19873288273377018\n",
            "Loss: 0.0\n",
            "training error 0.09908746155288571, test error 0.1971205554165537\n",
            "Loss: 0.0\n",
            "training error 0.09828434277405312, test error 0.1955747567264165\n",
            "Loss: 0.0\n",
            "training error 0.09755340862942745, test error 0.19404150121909086\n",
            "Loss: 0.0\n",
            "training error 0.09675640381077996, test error 0.19242494453934098\n",
            "Loss: 0.0\n",
            "training error 0.0960451771416896, test error 0.19089881725685132\n",
            "Loss: 0.0\n",
            "training error 0.09523755929725322, test error 0.18901948366516286\n",
            "Loss: 0.0\n",
            "training error 0.09444982199567259, test error 0.1875056497905418\n",
            "Loss: 0.0\n",
            "training error 0.09387303646709745, test error 0.18585000926932888\n",
            "Loss: 0.0\n",
            "training error 0.09295587052504432, test error 0.18430524843944646\n",
            "Loss: 0.0\n",
            "training error 0.09210313515268505, test error 0.18267910812021976\n",
            "Loss: 0.0\n",
            "training error 0.09126849064960285, test error 0.18099731600765723\n",
            "Loss: 0.0\n",
            "training error 0.09045142945884846, test error 0.17930555556037137\n",
            "Loss: 0.0\n",
            "training error 0.08969654659434025, test error 0.17758093041673742\n",
            "Loss: 0.0\n",
            "training error 0.08894094720532199, test error 0.1758897582920109\n",
            "Loss: 0.0\n",
            "training error 0.08815532707713081, test error 0.17402622762810901\n",
            "Loss: 0.0\n",
            "training error 0.08730926901911135, test error 0.1724750767430458\n",
            "Loss: 0.0\n",
            "training error 0.08653075565142006, test error 0.17067237206352656\n",
            "Loss: 0.0\n",
            "training error 0.08570227474473519, test error 0.16901455464597984\n",
            "Loss: 0.0\n",
            "training error 0.08493538918123121, test error 0.16746847869883735\n",
            "Loss: 0.0\n",
            "training error 0.08412391845533396, test error 0.1657481269471271\n",
            "Loss: 0.0\n",
            "training error 0.08335693219786076, test error 0.16418315941208328\n",
            "Loss: 0.0\n",
            "training error 0.08283985768170424, test error 0.16246619152510425\n",
            "Loss: 0.0\n",
            "training error 0.08182181278390376, test error 0.16092749507213333\n",
            "Loss: 0.0\n",
            "training error 0.08109896557892642, test error 0.1591388214667457\n",
            "Loss: 0.0\n",
            "training error 0.08028940137044698, test error 0.15752959205316697\n",
            "Loss: 0.0\n",
            "training error 0.0795074541341759, test error 0.15583633357647564\n",
            "Loss: 0.0\n",
            "training error 0.07877398064839683, test error 0.15423269244819365\n",
            "Loss: 0.0\n",
            "training error 0.07818867433661589, test error 0.15266921540679035\n",
            "Loss: 0.0\n",
            "training error 0.07728153298982302, test error 0.1509633869021933\n",
            "Loss: 0.0\n",
            "training error 0.07661756980935414, test error 0.14938978089237737\n",
            "Loss: 0.0\n",
            "training error 0.07583571688673357, test error 0.14779540671065775\n",
            "Loss: 0.0\n",
            "training error 0.07536766245552251, test error 0.1461726124777926\n",
            "Loss: 0.0\n",
            "training error 0.07441357871517647, test error 0.1445772723926868\n",
            "Loss: 0.0\n",
            "training error 0.0736917769736263, test error 0.14312095957639728\n",
            "Loss: 0.0\n",
            "training error 0.07343641057938555, test error 0.14178965117993478\n",
            "Loss: 0.0\n",
            "training error 0.07242136398719665, test error 0.14014700353168008\n",
            "Loss: 0.0\n",
            "training error 0.07169440193810955, test error 0.138439781397378\n",
            "Loss: 0.0\n",
            "training error 0.07103560584915929, test error 0.13699500810008525\n",
            "Loss: 0.0\n",
            "training error 0.07038179700948892, test error 0.13546297166140647\n",
            "Loss: 0.0\n",
            "training error 0.06965139709126149, test error 0.1341990931212123\n",
            "Loss: 0.0\n",
            "training error 0.06906601194085298, test error 0.1327778906815752\n",
            "Loss: 0.0\n",
            "training error 0.06838946421740918, test error 0.13141614320695433\n",
            "Loss: 0.0\n",
            "training error 0.06776788734285845, test error 0.12996430301131326\n",
            "Loss: 0.0\n",
            "training error 0.06721923305334619, test error 0.1287111852138146\n",
            "Loss: 0.0\n",
            "training error 0.06655130088151887, test error 0.1272652397143428\n",
            "Loss: 0.0\n",
            "training error 0.06594034791132504, test error 0.12602193676429696\n",
            "Loss: 0.0\n",
            "training error 0.06541581788037143, test error 0.12479834550054326\n",
            "Loss: 0.0\n",
            "training error 0.06476921574184243, test error 0.12336683934720136\n",
            "Loss: 0.0\n",
            "training error 0.06428927042911703, test error 0.12224616387475776\n",
            "Loss: 0.0\n",
            "training error 0.06367831337719493, test error 0.12073824453686156\n",
            "Loss: 0.0\n",
            "training error 0.06313239656350235, test error 0.11950145462983773\n",
            "Loss: 0.0\n",
            "training error 0.06252891292051688, test error 0.1184659659767173\n",
            "Loss: 0.0\n",
            "training error 0.06198855453260071, test error 0.11722724392612524\n",
            "Loss: 0.0\n",
            "training error 0.06148537773606746, test error 0.11605782790527124\n",
            "Loss: 0.0\n",
            "training error 0.060970600176733794, test error 0.11495936298199419\n",
            "Loss: 0.0\n",
            "training error 0.06050175800817034, test error 0.11382387809915018\n",
            "Loss: 0.0\n",
            "training error 0.0600950734108943, test error 0.11279460906286605\n",
            "Loss: 0.0\n",
            "training error 0.059543565214522594, test error 0.11170802641248173\n",
            "Loss: 0.0\n",
            "training error 0.059019199275046064, test error 0.11057677021865067\n",
            "Loss: 0.0\n",
            "training error 0.05859701155719621, test error 0.1094993630136718\n",
            "Loss: 0.0\n",
            "training error 0.05814888481418518, test error 0.10819103705309574\n",
            "Loss: 0.0\n",
            "training error 0.05769762308603867, test error 0.10738155349736038\n",
            "Loss: 0.0\n",
            "training error 0.057220128863468764, test error 0.10624267141396439\n",
            "Loss: 0.0\n",
            "training error 0.0568053597260948, test error 0.10530791335028035\n",
            "Loss: 0.0\n",
            "training error 0.056357491084695586, test error 0.10425046058475973\n",
            "Loss: 0.0\n",
            "training error 0.055890460553269475, test error 0.10332431696577815\n",
            "Loss: 0.0\n",
            "training error 0.05550258839495414, test error 0.10243043169551369\n",
            "Loss: 0.0\n",
            "training error 0.05509438137727293, test error 0.10156967241287838\n",
            "Loss: 0.0\n",
            "training error 0.05468694819990015, test error 0.10068332411028229\n",
            "Loss: 0.0\n",
            "training error 0.0543124773303788, test error 0.09968156478256873\n",
            "Loss: 0.0\n",
            "training error 0.05393788371375065, test error 0.09878750393548014\n",
            "Loss: 0.0\n",
            "training error 0.053601153153476445, test error 0.09786834489876135\n",
            "Loss: 0.0\n",
            "training error 0.05326202479660091, test error 0.0970763526385979\n",
            "Loss: 0.0\n",
            "training error 0.0528500624960014, test error 0.09646066991558773\n",
            "Loss: 0.0\n",
            "training error 0.05252736208326057, test error 0.09561951309009778\n",
            "Loss: 0.0\n",
            "training error 0.052265800499870736, test error 0.09499652876749758\n",
            "Loss: 0.0\n",
            "training error 0.05187223929309882, test error 0.09410205032842403\n",
            "Loss: 0.0\n",
            "training error 0.051526371777853935, test error 0.09333200918627683\n",
            "Loss: 0.0\n",
            "training error 0.0512694084256112, test error 0.09261998747000852\n",
            "Loss: 0.0\n",
            "training error 0.05092429198806933, test error 0.09190059098124286\n",
            "Loss: 0.0\n",
            "training error 0.05060093457098535, test error 0.09116284217789981\n",
            "Loss: 0.0\n",
            "training error 0.05031364420136398, test error 0.09046388126199563\n",
            "Loss: 0.0\n",
            "training error 0.05007915862617732, test error 0.08986301482317231\n",
            "Loss: 0.0\n",
            "training error 0.04978077776476395, test error 0.08941408833672453\n",
            "Loss: 0.0\n",
            "training error 0.04951733670278306, test error 0.08856400522152914\n",
            "Loss: 0.0\n",
            "training error 0.04921588355809137, test error 0.08799247791531635\n",
            "Loss: 0.0\n",
            "training error 0.04893616646350239, test error 0.08746660827608728\n",
            "Loss: 0.0\n",
            "training error 0.048669964628320385, test error 0.0868446726274133\n",
            "Loss: 0.0\n",
            "training error 0.04852837245980896, test error 0.08618838895237514\n",
            "Loss: 0.0\n",
            "training error 0.048211124655311674, test error 0.08549298223835167\n",
            "Loss: 0.0\n",
            "training error 0.047942586455485624, test error 0.08502053175009416\n",
            "Loss: 0.0\n",
            "training error 0.047716136441402025, test error 0.08443022538069059\n",
            "Loss: 0.0\n",
            "training error 0.04750967286508064, test error 0.08377542275754749\n",
            "Loss: 0.0\n",
            "training error 0.047215507414097094, test error 0.08322239493957238\n",
            "Loss: 0.0\n",
            "training error 0.046983376532511015, test error 0.08276998970393508\n",
            "Loss: 0.0\n",
            "training error 0.046760791660310316, test error 0.08226658996810889\n",
            "Loss: 0.0\n",
            "training error 0.04655834525485699, test error 0.08166151624813645\n",
            "Loss: 0.0\n",
            "training error 0.04634438826229042, test error 0.08112253991739164\n",
            "Loss: 0.0\n",
            "training error 0.04614481705802721, test error 0.0806115111561467\n",
            "Loss: 0.0\n",
            "training error 0.04593994225800289, test error 0.08015139309394725\n",
            "Loss: 0.0\n",
            "training error 0.045754565375456066, test error 0.07979197844190691\n",
            "Loss: 0.0\n",
            "training error 0.04551673486691973, test error 0.0792880525412657\n",
            "Loss: 0.0\n",
            "training error 0.04533381121875981, test error 0.07890429481667954\n",
            "Loss: 0.0\n",
            "training error 0.04512496478477084, test error 0.07828670707599669\n",
            "Loss: 0.0\n",
            "training error 0.04495829141046232, test error 0.07793811356865593\n",
            "Loss: 0.0\n",
            "training error 0.04485554642576, test error 0.07757545553445808\n",
            "Loss: 0.0\n",
            "training error 0.04464661102767286, test error 0.07703329480280065\n",
            "Loss: 0.0\n",
            "training error 0.04454597791299141, test error 0.07650637159112954\n",
            "Loss: 0.0\n",
            "training error 0.04427603602123975, test error 0.07632501560467131\n",
            "Loss: 0.0\n",
            "training error 0.04413619731501491, test error 0.07590794870113222\n",
            "Loss: 0.0\n",
            "training error 0.04394936013949892, test error 0.0754562255781331\n",
            "Loss: 0.0\n",
            "training error 0.04381721904717037, test error 0.07506178843230996\n",
            "Loss: 0.0\n",
            "training error 0.04369239433754819, test error 0.07463841981509838\n",
            "Loss: 0.0\n",
            "training error 0.043545949495414384, test error 0.07438398072777826\n",
            "Loss: 0.0\n",
            "training error 0.04338990789819722, test error 0.07408137165179335\n",
            "Loss: 0.0\n",
            "training error 0.04323780652052576, test error 0.0736976076501416\n",
            "Loss: 0.0\n",
            "training error 0.04308371863966067, test error 0.07353993243242203\n",
            "Loss: 0.0\n",
            "training error 0.042944384851073214, test error 0.07326514823252667\n",
            "Loss: 0.0\n",
            "training error 0.04281931312185823, test error 0.07310822747489815\n",
            "Loss: 0.0\n",
            "training error 0.04266293568248189, test error 0.07269146383948087\n",
            "Loss: 0.0\n",
            "training error 0.042552341754139265, test error 0.07253530397135807\n",
            "Loss: 0.0\n",
            "training error 0.04242772192745558, test error 0.07208391121655708\n",
            "Loss: 0.0\n",
            "training error 0.04231746220844505, test error 0.07189649629793737\n",
            "Loss: 0.0\n",
            "training error 0.042155593523933096, test error 0.07141542120322779\n",
            "Loss: 0.0\n",
            "training error 0.04206524147333411, test error 0.07107843679673274\n",
            "Loss: 0.0\n",
            "training error 0.04191084519390581, test error 0.07083853293927629\n",
            "Loss: 0.0\n",
            "training error 0.041890997462645414, test error 0.0705129453130368\n",
            "Loss: 0.0\n",
            "training error 0.04169079170314829, test error 0.07026483884388098\n",
            "Loss: 0.0\n",
            "training error 0.04157276562584043, test error 0.06992450207520667\n",
            "Loss: 0.0\n",
            "training error 0.041546051155461436, test error 0.06975346054821127\n",
            "Loss: 0.0\n",
            "training error 0.04136829984166443, test error 0.06939542713568342\n",
            "Loss: 0.0\n",
            "training error 0.04127645348289916, test error 0.06925182672329644\n",
            "Loss: 0.0\n",
            "training error 0.041243449359957024, test error 0.06894193652047627\n",
            "Loss: 0.0\n",
            "training error 0.04106031511582871, test error 0.06873645896426377\n",
            "Loss: 0.0\n",
            "training error 0.04096246579581786, test error 0.06849699271285564\n",
            "Loss: 0.0\n",
            "training error 0.0409220826923403, test error 0.06837670615238893\n",
            "Loss: 0.0\n",
            "training error 0.04075951019058696, test error 0.0679104819143967\n",
            "Loss: 0.0\n",
            "training error 0.04067188328004338, test error 0.06777917361035868\n",
            "Loss: 0.0\n",
            "training error 0.04056354760371251, test error 0.06754585857223723\n",
            "Loss: 0.0\n",
            "training error 0.04051723868441997, test error 0.06746654965776092\n",
            "Loss: 0.0\n",
            "training error 0.04039102227158851, test error 0.06718430428918669\n",
            "Loss: 0.0\n",
            "training error 0.040310488452408644, test error 0.0670219807545016\n",
            "Loss: 0.0\n",
            "training error 0.040309536978210624, test error 0.06666379352571743\n",
            "Loss: 0.0\n",
            "training error 0.04011947714977581, test error 0.06650981475125418\n",
            "Loss: 0.0\n",
            "training error 0.04003989949894131, test error 0.06633868352296578\n",
            "Loss: 0.0\n",
            "training error 0.03997560466029171, test error 0.066221587671613\n",
            "Loss: 0.0\n",
            "training error 0.03989712845648652, test error 0.06617431746637643\n",
            "Loss: 0.0\n",
            "training error 0.03979255192171686, test error 0.06601841376433257\n",
            "Loss: 0.0\n",
            "training error 0.03972676949292972, test error 0.06570169183081764\n",
            "Loss: 0.0\n",
            "training error 0.03967406941124713, test error 0.0655906614242256\n",
            "Loss: 0.0\n",
            "training error 0.0395792969233835, test error 0.06563321366837398\n",
            "Loss: 0.06487546126905386\n",
            "training error 0.0395195369196872, test error 0.06548517743769701\n",
            "Loss: 0.0\n",
            "training error 0.03947087715372015, test error 0.06527031623991236\n",
            "Loss: 0.0\n",
            "training error 0.0393874247070004, test error 0.06495185845906842\n",
            "Loss: 0.0\n",
            "training error 0.03934685217296929, test error 0.06472097636105091\n",
            "Loss: 0.0\n",
            "training error 0.039238794273087366, test error 0.06457502144079785\n",
            "Loss: 0.0\n",
            "training error 0.039164976211104416, test error 0.06434992459474323\n",
            "Loss: 0.0\n",
            "training error 0.03911237606864063, test error 0.0641961484328101\n",
            "Loss: 0.0\n",
            "training error 0.03905930812240866, test error 0.06411116602722144\n",
            "Loss: 0.0\n",
            "training error 0.038993409071272675, test error 0.06399034291457933\n",
            "Loss: 0.0\n",
            "training error 0.038939220817200026, test error 0.06382467532465494\n",
            "Loss: 0.0\n",
            "training error 0.038864560262579624, test error 0.06362761745565984\n",
            "Loss: 0.0\n",
            "training error 0.03885858018279441, test error 0.06347449704130369\n",
            "Loss: 0.0\n",
            "training error 0.03874515079151512, test error 0.0634472346369446\n",
            "Loss: 0.0\n",
            "training error 0.038702474478288465, test error 0.0632248092848277\n",
            "Loss: 0.0\n",
            "training error 0.038622225240872145, test error 0.06313435825074377\n",
            "Loss: 0.0\n",
            "training error 0.038564054724502916, test error 0.06310168864310482\n",
            "Loss: 0.0\n",
            "training error 0.03854163311073152, test error 0.0629397839180838\n",
            "Loss: 0.0\n",
            "training error 0.03846409874458935, test error 0.0627782433046875\n",
            "Loss: 0.0\n",
            "training error 0.03844559170657569, test error 0.06278255441571699\n",
            "Loss: 0.0068672055835827805\n",
            "training error 0.03836751420083697, test error 0.0626919511595296\n",
            "Loss: 0.0\n",
            "training error 0.03832425064674715, test error 0.06244695115686876\n",
            "Loss: 0.0\n",
            "training error 0.03831897229178773, test error 0.06230904793780708\n",
            "Loss: 0.0\n",
            "training error 0.03821144273976436, test error 0.06245835993549664\n",
            "Loss: 0.23963132583664226\n",
            "training error 0.03814893200166957, test error 0.062261707526835366\n",
            "Loss: 0.0\n",
            "training error 0.038165365292679145, test error 0.062191906783565086\n",
            "Loss: 0.0\n",
            "training error 0.03808583058512738, test error 0.062113316329836035\n",
            "Loss: 0.0\n",
            "training error 0.038009651991577326, test error 0.06189821482402254\n",
            "Loss: 0.0\n",
            "training error 0.03797714538135162, test error 0.06178607217611298\n",
            "Loss: 0.0\n",
            "training error 0.03793883314840635, test error 0.06173212310490997\n",
            "Loss: 0.0\n",
            "training error 0.03789093341022399, test error 0.061645657105389065\n",
            "Loss: 0.0\n",
            "training error 0.03786019496395311, test error 0.061305720325992026\n",
            "Loss: 0.0\n",
            "training error 0.03780036251714611, test error 0.0612903648858816\n",
            "Loss: 0.0\n",
            "training error 0.037751761618040676, test error 0.061152897272077546\n",
            "Loss: 0.0\n",
            "training error 0.037742036766361745, test error 0.061174732823161736\n",
            "Loss: 0.03570648662325482\n",
            "training error 0.03767764699663772, test error 0.061037835107634636\n",
            "Loss: 0.0\n",
            "training error 0.03765385984189126, test error 0.06099060826780104\n",
            "Loss: 0.0\n",
            "training error 0.03763087185264989, test error 0.060881128916670635\n",
            "Loss: 0.0\n",
            "training error 0.037609003347841445, test error 0.06068538518443429\n",
            "Loss: 0.0\n",
            "training error 0.03754065507381494, test error 0.06075476126788912\n",
            "Loss: 0.11432090814613716\n",
            "training error 0.037558572937660246, test error 0.060643752393293046\n",
            "Loss: 0.0\n",
            "training error 0.037468113203893656, test error 0.06044369464821539\n",
            "Loss: 0.0\n",
            "training error 0.037458262453777036, test error 0.06030652810984818\n",
            "Loss: 0.0\n",
            "training error 0.0373991439856482, test error 0.060144966844120615\n",
            "Loss: 0.0\n",
            "training error 0.03737498250476192, test error 0.060153454745261126\n",
            "Loss: 0.014112404721267069\n",
            "training error 0.037344714771352185, test error 0.060080749917510126\n",
            "Loss: 0.0\n",
            "training error 0.037318245789407505, test error 0.060160910572489935\n",
            "Loss: 0.13342152867577006\n",
            "training error 0.03727449431586202, test error 0.06010542164778363\n",
            "Loss: 0.041064284828973285\n",
            "training error 0.037218967218186676, test error 0.05998544274122342\n",
            "Loss: 0.0\n",
            "training error 0.03722051434466854, test error 0.060006459509961696\n",
            "Loss: 0.035036448474579096\n",
            "training error 0.03723911882916972, test error 0.060074785263580385\n",
            "Loss: 0.1489403399794531\n",
            "training error 0.037150515250608016, test error 0.05971759425536703\n",
            "Loss: 0.0\n",
            "training error 0.037092822440843824, test error 0.05963751768486812\n",
            "Loss: 0.0\n",
            "training error 0.03715085249482482, test error 0.059694407668874475\n",
            "Loss: 0.09539294426532674\n",
            "training error 0.037045531572582134, test error 0.0594627825614873\n",
            "Loss: 0.0\n",
            "training error 0.03706340254049177, test error 0.059468531006341065\n",
            "Loss: 0.00966729878109085\n",
            "training error 0.03696783653205083, test error 0.0594894820430219\n",
            "Loss: 0.04490116402977584\n",
            "training error 0.036997003566786646, test error 0.059307824588410614\n",
            "Loss: 0.0\n",
            "training error 0.037012942676229475, test error 0.05913734772058908\n",
            "Loss: 0.0\n",
            "training error 0.036909694196927835, test error 0.05926067604622211\n",
            "Loss: 0.2085455814077486\n",
            "training error 0.03687108083716816, test error 0.05921574541725851\n",
            "Loss: 0.13256884133499014\n",
            "training error 0.03689193639141852, test error 0.05933374415523657\n",
            "Loss: 0.3321022031211429\n",
            "training error 0.03681842124564115, test error 0.05918530597817906\n",
            "Loss: 0.08109639582851713\n",
            "training error 0.03684388768587225, test error 0.05912891722466198\n",
            "Loss: 0.0\n",
            "training error 0.03686397087563662, test error 0.0590406425877348\n",
            "Loss: 0.0\n",
            "training error 0.03678149053611433, test error 0.058957883765279634\n",
            "Loss: 0.0\n",
            "training error 0.036722111306983815, test error 0.058840943011489406\n",
            "Loss: 0.0\n",
            "training error 0.03671793845794664, test error 0.05874633523457287\n",
            "Loss: 0.0\n",
            "training error 0.03670855677126073, test error 0.05874012169426429\n",
            "Loss: 0.0\n",
            "training error 0.03666141990926685, test error 0.05869429461917835\n",
            "Loss: 0.0\n",
            "training error 0.036643865816230324, test error 0.05872943825099257\n",
            "Loss: 0.05987572053167689\n",
            "training error 0.03663398999378171, test error 0.05866421140522063\n",
            "Loss: 0.0\n",
            "training error 0.03655426669531157, test error 0.05866250058346067\n",
            "Loss: 0.0\n",
            "training error 0.036556610080705716, test error 0.05866843103206814\n",
            "Loss: 0.01010943711654555\n",
            "training error 0.036529192923200254, test error 0.05861366892760573\n",
            "Loss: 0.0\n",
            "training error 0.03652711757049516, test error 0.05860572697300632\n",
            "Loss: 0.0\n",
            "training error 0.03648751664168421, test error 0.058560777562590684\n",
            "Loss: 0.0\n",
            "training error 0.036478403441844914, test error 0.058559389199658224\n",
            "Loss: 0.0\n",
            "training error 0.03646496347613538, test error 0.05842016407608997\n",
            "Loss: 0.0\n",
            "training error 0.0364324116220656, test error 0.05840663238587111\n",
            "Loss: 0.0\n",
            "training error 0.03641055019419117, test error 0.05842566256140216\n",
            "Loss: 0.03258221669983907\n",
            "training error 0.0364019984603528, test error 0.05827971262408391\n",
            "Loss: 0.0\n",
            "training error 0.03637537360170171, test error 0.0582827321034266\n",
            "Loss: 0.005181012751664937\n",
            "training error 0.036372281989292, test error 0.058216622646564985\n",
            "Loss: 0.0\n",
            "training error 0.036344032049975675, test error 0.05823003109679526\n",
            "Loss: 0.02303199605322437\n",
            "training error 0.03632694536024915, test error 0.058168820541234505\n",
            "Loss: 0.0\n",
            "training error 0.036318447422323645, test error 0.058265978720134504\n",
            "Loss: 0.16702793351486278\n",
            "training error 0.036290489660177214, test error 0.05814534799081263\n",
            "Loss: 0.0\n",
            "training error 0.036286906169919315, test error 0.058210048956281554\n",
            "Loss: 0.1112745347729982\n",
            "training error 0.036251532605965586, test error 0.05823592018439449\n",
            "Loss: 0.155768598368633\n",
            "training error 0.03623440622506858, test error 0.058076956036206814\n",
            "Loss: 0.0\n",
            "training error 0.036192403070398685, test error 0.05801167458119932\n",
            "Loss: 0.0\n",
            "training error 0.0362038691429017, test error 0.058052737324207536\n",
            "Loss: 0.07078358503638249\n",
            "training error 0.03619589782025742, test error 0.058010002738849674\n",
            "Loss: 0.0\n",
            "training error 0.03618254138473554, test error 0.058161353798211614\n",
            "Loss: 0.26090510638878683\n",
            "training error 0.03610929790747769, test error 0.057966491340135776\n",
            "Loss: 0.0\n",
            "training error 0.03614006191435902, test error 0.05790911118297121\n",
            "Loss: 0.0\n",
            "training error 0.036098278072145426, test error 0.05791387127769713\n",
            "Loss: 0.008219940918929858\n",
            "training error 0.03611384043926895, test error 0.057754353982736095\n",
            "Loss: 0.0\n",
            "training error 0.0360875585908772, test error 0.05796130787757128\n",
            "Loss: 0.3583347065003073\n",
            "training error 0.0360532286088429, test error 0.05781539713669869\n",
            "Loss: 0.10569446241377456\n",
            "training error 0.03606811722653119, test error 0.05788306700960698\n",
            "Loss: 0.22286289776414225\n",
            "training error 0.03602119292925476, test error 0.05768224573978645\n",
            "Loss: 0.0\n",
            "training error 0.03600218791356637, test error 0.057696611872807876\n",
            "Loss: 0.024905640959671516\n",
            "training error 0.03599857121569079, test error 0.05762594296337394\n",
            "Loss: 0.0\n",
            "training error 0.036050799577290335, test error 0.05768391330188727\n",
            "Loss: 0.10059763976473235\n",
            "training error 0.03596961099760817, test error 0.05759446610966938\n",
            "Loss: 0.0\n",
            "training error 0.03596383163629502, test error 0.05730937121586186\n",
            "Loss: 0.0\n",
            "training error 0.03595545809211268, test error 0.05740736561437584\n",
            "Loss: 0.1709919275590499\n",
            "training error 0.035936751140094564, test error 0.05732525802792726\n",
            "Loss: 0.02772114181039953\n",
            "training error 0.035932620983953785, test error 0.057182878843310976\n",
            "Loss: 0.0\n",
            "training error 0.03588979430166406, test error 0.05722942254829901\n",
            "Loss: 0.08139447668518152\n",
            "training error 0.0358725347854382, test error 0.05721963367137931\n",
            "Loss: 0.06427593155819977\n",
            "training error 0.035909716545659526, test error 0.05720205662570174\n",
            "Loss: 0.03353763010658373\n",
            "training error 0.03587313468781071, test error 0.057131024271074214\n",
            "Loss: 0.0\n",
            "training error 0.035871445043162774, test error 0.0572863641265356\n",
            "Loss: 0.2719010510372266\n",
            "training error 0.03584219684352892, test error 0.057187952878079915\n",
            "Loss: 0.09964569641809362\n",
            "training error 0.03585850233372767, test error 0.057299474149913286\n",
            "Loss: 0.2948483437646976\n",
            "training error 0.035816221830733574, test error 0.05719121329987814\n",
            "Loss: 0.10535261632689252\n",
            "training error 0.03580827897583404, test error 0.057262198541732846\n",
            "Loss: 0.22960251865298265\n",
            "training error 0.035771798381573953, test error 0.05715844804415841\n",
            "Loss: 0.04800154282911251\n",
            "training error 0.03580919000708516, test error 0.057218547039983136\n",
            "Loss: 0.15319656880934396\n",
            "training error 0.03576862901312482, test error 0.05710441823852656\n",
            "Loss: 0.0\n",
            "training error 0.03575925902032782, test error 0.05710599836860179\n",
            "Loss: 0.0027670889993780534\n",
            "training error 0.03577215880185623, test error 0.05698468382803409\n",
            "Loss: 0.0\n",
            "training error 0.03573138747957332, test error 0.05705607375146007\n",
            "Loss: 0.12527914279811814\n",
            "training error 0.03575158215683037, test error 0.05709512859502343\n",
            "Loss: 0.19381482807316885\n",
            "training error 0.03574887443623415, test error 0.057200682283570795\n",
            "Loss: 0.37904651044224913\n",
            "training error 0.0357239948044818, test error 0.05725005262769822\n",
            "Loss: 0.465684429284452\n",
            "training error 0.03569832686304916, test error 0.05711958455919901\n",
            "Loss: 0.23673156031192022\n",
            "training error 0.03570031970269744, test error 0.05714140028195265\n",
            "Loss: 0.27501504508036234\n",
            "training error 0.035682087925746356, test error 0.05704244954627447\n",
            "Loss: 0.10137060409900478\n",
            "training error 0.03570621198527562, test error 0.05712758252528516\n",
            "Loss: 0.25076685111091734\n",
            "training error 0.03568051929406909, test error 0.05699163623335221\n",
            "Loss: 0.012200480639856792\n",
            "training error 0.03567073671679401, test error 0.057045968713149244\n",
            "Loss: 0.10754624049524875\n",
            "training error 0.03563681699914505, test error 0.05704969991707535\n",
            "Loss: 0.11409397170205615\n",
            "training error 0.03564074858515312, test error 0.057008602283291215\n",
            "Loss: 0.04197348068002427\n",
            "training error 0.03562542268569571, test error 0.056890063188169235\n",
            "Loss: 0.0\n",
            "training error 0.03577971539126222, test error 0.057210516968259476\n",
            "Loss: 0.5632860329761069\n",
            "training error 0.03559950031035296, test error 0.05702021095940723\n",
            "Loss: 0.2287706568500747\n",
            "training error 0.03560246631888637, test error 0.056983110628725055\n",
            "Loss: 0.16355657797049172\n",
            "training error 0.035619989677569595, test error 0.05683653785728655\n",
            "Loss: 0.0\n",
            "training error 0.03557700190073018, test error 0.05689226503232754\n",
            "Loss: 0.09804815201961414\n",
            "training error 0.035577317728519776, test error 0.05671186246441051\n",
            "Loss: 0.0\n",
            "training error 0.03561063466170905, test error 0.05680707060599261\n",
            "Loss: 0.1678804705838255\n",
            "training error 0.035634510238930155, test error 0.05655936239643983\n",
            "Loss: 0.0\n",
            "training error 0.03557043530761101, test error 0.05664600589190171\n",
            "Loss: 0.15319036812080356\n",
            "training error 0.03556471426704455, test error 0.0566880616214125\n",
            "Loss: 0.22754716375794626\n",
            "training error 0.035529174253425964, test error 0.05670704323587001\n",
            "Loss: 0.2611076808027857\n",
            "training error 0.035609301242461054, test error 0.05678939470636672\n",
            "Loss: 0.40670951754111506\n",
            "training error 0.0355293491841295, test error 0.05667885289902786\n",
            "Loss: 0.21126564643796364\n",
            "training error 0.03556982772035519, test error 0.05685507457228892\n",
            "Loss: 0.5228350591655717\n",
            "training error 0.035509994280325426, test error 0.056648317915608766\n",
            "Loss: 0.1572781506011811\n",
            "training error 0.03552215683730044, test error 0.05653819987746459\n",
            "Loss: 0.0\n",
            "training error 0.03548182980893757, test error 0.05652394849906964\n",
            "Loss: 0.0\n",
            "training error 0.035470592738602334, test error 0.05635381083370536\n",
            "Loss: 0.0\n",
            "training error 0.03547433894744103, test error 0.05652009414857453\n",
            "Loss: 0.29507022224255497\n",
            "training error 0.03547304228774257, test error 0.056493617001970295\n",
            "Loss: 0.24808644916223255\n",
            "training error 0.035504835648213225, test error 0.056245111746753555\n",
            "Loss: 0.0\n",
            "training error 0.03541407053405751, test error 0.05633673743117877\n",
            "Loss: 0.1629042623966459\n",
            "training error 0.035481173307391034, test error 0.05614464210542749\n",
            "Loss: 0.0\n",
            "training error 0.03547585922285063, test error 0.056234961393062825\n",
            "Loss: 0.16086893468076902\n",
            "training error 0.035396365091971214, test error 0.056452994506345826\n",
            "Loss: 0.549210733838712\n",
            "training error 0.03538567221840965, test error 0.056387177993587334\n",
            "Loss: 0.43198403100408456\n",
            "training error 0.035411399983515994, test error 0.05625007024639686\n",
            "Loss: 0.1877795227038792\n",
            "training error 0.03541117528564503, test error 0.056395458897242526\n",
            "Loss: 0.4467332632454246\n",
            "training error 0.0353806595900417, test error 0.05634492883897807\n",
            "Loss: 0.3567334763208363\n",
            "training error 0.035410145414943396, test error 0.05649578145540239\n",
            "Loss: 0.6254191616637872\n",
            "training error 0.035429629524086306, test error 0.05636467184571874\n",
            "Loss: 0.39189801918781964\n",
            "training error 0.03535778391592702, test error 0.05620753828700641\n",
            "Loss: 0.1120252605062877\n",
            "training error 0.03533875888625314, test error 0.056140718274103905\n",
            "Loss: 0.0\n",
            "training error 0.035345797042395634, test error 0.05627191597264439\n",
            "Loss: 0.23369437116909086\n",
            "training error 0.03534151826934118, test error 0.056236148629633914\n",
            "Loss: 0.1699842083674019\n",
            "training error 0.03545374365791147, test error 0.05630683223913329\n",
            "Loss: 0.29588856383764295\n",
            "training error 0.03531539286116313, test error 0.05619312399851903\n",
            "Loss: 0.09334708572708017\n",
            "training error 0.03532740189408998, test error 0.056213095629099485\n",
            "Loss: 0.12892131989157285\n",
            "training error 0.0353268380648275, test error 0.056167664208259256\n",
            "Loss: 0.04799713110863468\n",
            "training error 0.035323106386763274, test error 0.05617125921158298\n",
            "Loss: 0.054400688872480174\n",
            "training error 0.03533908596774566, test error 0.05600206624235812\n",
            "Loss: 0.0\n",
            "training error 0.0353251575822026, test error 0.05622978650776965\n",
            "Loss: 0.4066283276513927\n",
            "training error 0.03536709427597552, test error 0.056283864147208915\n",
            "Loss: 0.5031919780089211\n",
            "training error 0.035323081568995135, test error 0.056078905315547475\n",
            "Loss: 0.1372075681222551\n",
            "training error 0.03527671112066092, test error 0.05628639900832351\n",
            "Loss: 0.5077183487032233\n",
            "training error 0.03530486200256352, test error 0.05622979629795951\n",
            "Loss: 0.4066458094882597\n",
            "training error 0.03528076218478887, test error 0.056209424695992126\n",
            "Loss: 0.3702692910233507\n",
            "training error 0.0353028975100375, test error 0.056377856506962316\n",
            "Loss: 0.6710292848444155\n",
            "training error 0.03528749308219609, test error 0.05633126029617317\n",
            "Loss: 0.5878248355880356\n",
            "training error 0.035253887811826375, test error 0.05620693194308338\n",
            "Loss: 0.3658181107794478\n",
            "training error 0.03526379038557742, test error 0.05615482918820641\n",
            "Loss: 0.27278090988140935\n",
            "training error 0.03525712645623718, test error 0.056241395416841616\n",
            "Loss: 0.42735775756514993\n",
            "training error 0.03525320271372319, test error 0.05615737846484195\n",
            "Loss: 0.27733302162760154\n",
            "training error 0.03530154330170549, test error 0.05635153949409077\n",
            "Loss: 0.624036352909263\n",
            "training error 0.03525391089342888, test error 0.056221920407111534\n",
            "Loss: 0.3925822375945076\n",
            "training error 0.03527551884888945, test error 0.056169891528812806\n",
            "Loss: 0.2996769542902067\n",
            "training error 0.035242840220602666, test error 0.056296857960527295\n",
            "Loss: 0.5263943599748799\n",
            "training error 0.03526400349937494, test error 0.05621442037486386\n",
            "Loss: 0.3791898170091512\n",
            "training error 0.03525197410389596, test error 0.05636391869447521\n",
            "Loss: 0.646141252272936\n",
            "training error 0.03529693672025426, test error 0.056483383904345826\n",
            "Loss: 0.8594641131716818\n",
            "training error 0.03522136740830106, test error 0.05626665403855236\n",
            "Loss: 0.4724607750171028\n",
            "training error 0.03519912732606618, test error 0.056192853099745546\n",
            "Loss: 0.3406782466949698\n",
            "training error 0.03528726155550717, test error 0.05624873547907259\n",
            "Loss: 0.440464527946105\n",
            "training error 0.035255176828576526, test error 0.05616393130410271\n",
            "Loss: 0.28903408857110513\n",
            "training error 0.03523111453445007, test error 0.056333339597876876\n",
            "Loss: 0.5915377373490349\n",
            "training error 0.03522449734440245, test error 0.056239285271083725\n",
            "Loss: 0.4235897791681431\n",
            "training error 0.03524121442152353, test error 0.05638622851157322\n",
            "Loss: 0.6859787414853047\n",
            "training error 0.035210713495351764, test error 0.05630451083596849\n",
            "Loss: 0.540059704764273\n",
            "training error 0.03519601630724844, test error 0.056355772982597564\n",
            "Loss: 0.6315958748891903\n",
            "training error 0.03522543591169596, test error 0.0563331415372057\n",
            "Loss: 0.5911840706283922\n",
            "training error 0.035187207327300934, test error 0.056282942591824967\n",
            "Loss: 0.5015464041117745\n",
            "training error 0.03515810867634593, test error 0.05621126179674181\n",
            "Loss: 0.3735497070382454\n",
            "training error 0.03518293713193993, test error 0.05621210649679017\n",
            "Loss: 0.3750580443283358\n",
            "training error 0.035199768789874325, test error 0.0561393253610497\n",
            "Loss: 0.24509652572026752\n",
            "training error 0.035166919681361625, test error 0.056211434216738215\n",
            "Loss: 0.37385758852899187\n",
            "training error 0.03521267799041518, test error 0.056363284643192596\n",
            "Loss: 0.6450090596144209\n",
            "training error 0.03513504028188632, test error 0.05620451261533967\n",
            "Loss: 0.3614980420640679\n",
            "training error 0.03517501024493525, test error 0.056084383102719175\n",
            "Loss: 0.14698897002265454\n",
            "training error 0.03516359298555596, test error 0.056226218266159984\n",
            "Loss: 0.4002567027291626\n",
            "training error 0.03512874051688357, test error 0.056137639627490146\n",
            "Loss: 0.24208639828628975\n",
            "training error 0.035144742996783516, test error 0.056264679477040884\n",
            "Loss: 0.4689349024128209\n",
            "training error 0.03518078974006221, test error 0.055945125103830824\n",
            "Loss: 0.0\n",
            "training error 0.03519146494602519, test error 0.05592084389533323\n",
            "Loss: 0.0\n",
            "training error 0.035151857871423366, test error 0.0561216504735451\n",
            "Loss: 0.3590907508257857\n",
            "training error 0.03511767316788004, test error 0.05618557791088093\n",
            "Loss: 0.47340847724544055\n",
            "training error 0.035121327344698514, test error 0.05613118533546565\n",
            "Loss: 0.3761413910815081\n",
            "training error 0.03510964052967289, test error 0.05616760242450961\n",
            "Loss: 0.4412639581016231\n",
            "training error 0.035113203955531434, test error 0.05623664863833619\n",
            "Loss: 0.564735295472385\n",
            "training error 0.035083774495912684, test error 0.056328884605333744\n",
            "Loss: 0.7296755227160778\n",
            "training error 0.03510285925694417, test error 0.0563500430179726\n",
            "Loss: 0.7675118841959927\n",
            "training error 0.03513366843021644, test error 0.05636986227230526\n",
            "Loss: 0.802953506589521\n",
            "training error 0.03510876625538734, test error 0.05614063664400707\n",
            "Loss: 0.39304261767798554\n",
            "training error 0.03508118196520934, test error 0.05617014886901515\n",
            "Loss: 0.44581761703836875\n",
            "training error 0.03515987842632196, test error 0.056353307820454605\n",
            "Loss: 0.7733501410150723\n",
            "training error 0.03509783573535892, test error 0.056215459323794514\n",
            "Loss: 0.5268436739129356\n",
            "training error 0.035097533764529065, test error 0.05611791264649235\n",
            "Loss: 0.3524066116169067\n",
            "training error 0.03506746197332565, test error 0.05614387537654091\n",
            "Loss: 0.3988342551216384\n",
            "training error 0.03510076721251767, test error 0.05610539817582891\n",
            "Loss: 0.33002770995571584\n",
            "training error 0.03512145394323988, test error 0.05615085909419848\n",
            "Loss: 0.4113228321370954\n",
            "training error 0.035079509261047075, test error 0.056025779942781385\n",
            "Loss: 0.18765104411615496\n",
            "training error 0.03514069370606271, test error 0.05624967892792566\n",
            "Loss: 0.588036606185538\n",
            "training error 0.03512290124455554, test error 0.05613614140423444\n",
            "Loss: 0.3850040412554989\n",
            "training error 0.03507871661027196, test error 0.056063630485921334\n",
            "Loss: 0.2553369739114686\n",
            "training error 0.035056276988426685, test error 0.05614685302072164\n",
            "Loss: 0.40415900341459476\n",
            "training error 0.035066984582736306, test error 0.05619962372265604\n",
            "Loss: 0.4985257873515003\n",
            "training error 0.035059752422501936, test error 0.056152070974315633\n",
            "Loss: 0.41348996702408325\n",
            "training error 0.03505036090298519, test error 0.056154338274687604\n",
            "Loss: 0.41754444870576446\n",
            "training error 0.035093740611268257, test error 0.05620600544780581\n",
            "Loss: 0.5099378560994516\n",
            "training error 0.035075990171884855, test error 0.05614928574677622\n",
            "Loss: 0.4085093062446754\n",
            "training error 0.035077443799024215, test error 0.05611858861374213\n",
            "Loss: 0.3536154046226869\n",
            "training error 0.03505858050184621, test error 0.05615658140165774\n",
            "Loss: 0.42155570249573504\n",
            "training error 0.035113285576137944, test error 0.05615271944097896\n",
            "Loss: 0.414649582326998\n",
            "training error 0.03504023668182686, test error 0.056012946337772616\n",
            "Loss: 0.1647014530248736\n",
            "training error 0.03508248918696134, test error 0.05621662082462852\n",
            "Loss: 0.5289207184514089\n",
            "training error 0.03503710522413496, test error 0.05608187793095242\n",
            "Loss: 0.28796782094455686\n",
            "training error 0.03504060033536981, test error 0.05625779319771493\n",
            "Loss: 0.6025468839711445\n",
            "training error 0.035075100638479476, test error 0.05636831446483959\n",
            "Loss: 0.8001856523193629\n",
            "training error 0.03502208000917166, test error 0.05626955870261655\n",
            "Loss: 0.6235864536236368\n",
            "training error 0.03506677007236597, test error 0.056269248860869335\n",
            "Loss: 0.6230323816075023\n",
            "training error 0.03505678934260602, test error 0.05614630330866976\n",
            "Loss: 0.40317598525250187\n",
            "training error 0.03502689625137065, test error 0.05601481159157742\n",
            "Loss: 0.168036978161612\n",
            "training error 0.03504560988687815, test error 0.05603658173546275\n",
            "Loss: 0.20696726313027547\n",
            "training error 0.035031170758541376, test error 0.05599411361982462\n",
            "Loss: 0.1310239963984161\n",
            "training error 0.03503950518947949, test error 0.05601467814544952\n",
            "Loss: 0.16779834419509143\n",
            "training error 0.035085291610431134, test error 0.05595859051785871\n",
            "Loss: 0.06750009459108863\n",
            "training error 0.03501365819216573, test error 0.05611594739050668\n",
            "Loss: 0.3488922583833487\n",
            "training error 0.035090254405148354, test error 0.05600205348500676\n",
            "Loss: 0.14522239654597424\n",
            "training error 0.03500090802630936, test error 0.05617547378661511\n",
            "Loss: 0.4553398581724988\n",
            "training error 0.035006201092695324, test error 0.05623158894945081\n",
            "Loss: 0.5556873474570567\n",
            "training error 0.0350108936574767, test error 0.056262095466859265\n",
            "Loss: 0.6102403822173308\n",
            "training error 0.03504292771230533, test error 0.05612427882976768\n",
            "Loss: 0.36379088773270674\n",
            "training error 0.035053008489332155, test error 0.056221545330764\n",
            "Loss: 0.5377269269998708\n",
            "training error 0.035043066847838145, test error 0.05614786667329633\n",
            "Loss: 0.40597165949072256\n",
            "training error 0.03503251429590865, test error 0.05636115228594245\n",
            "Loss: 0.7873779434254269\n",
            "training error 0.03502554306881681, test error 0.056303869587306286\n",
            "Loss: 0.6849426176220863\n",
            "training error 0.03499737947640479, test error 0.0562243870894387\n",
            "Loss: 0.5428086791279707\n",
            "training error 0.035008814346635615, test error 0.056045632392446504\n",
            "Loss: 0.22315202779636945\n",
            "training error 0.03504640975753439, test error 0.05611910560864957\n",
            "Loss: 0.35453991661396067\n",
            "training error 0.03500295373499457, test error 0.056021046307343246\n",
            "Loss: 0.17918615855934128\n",
            "training error 0.0350371782980818, test error 0.05607644651767424\n",
            "Loss: 0.2782551397690902\n",
            "training error 0.03499135196788652, test error 0.056047802300552375\n",
            "Loss: 0.22703234854033472\n",
            "training error 0.03497976968083168, test error 0.056045021877236915\n",
            "Loss: 0.22206027887581303\n",
            "training error 0.03498454307159297, test error 0.056223491235848216\n",
            "Loss: 0.5412066761393053\n",
            "training error 0.03496475813353059, test error 0.05623827680084352\n",
            "Loss: 0.56764684400048\n",
            "training error 0.03495851416551789, test error 0.05619754298051394\n",
            "Loss: 0.49480491692615036\n",
            "training error 0.035017850695192115, test error 0.05622897353209065\n",
            "Loss: 0.5510103483669537\n",
            "training error 0.03497075623896279, test error 0.05611038862785101\n",
            "Loss: 0.33895184570631454\n",
            "training error 0.03496196581546215, test error 0.056291969525949115\n",
            "Loss: 0.6636624284685588\n",
            "training error 0.035001571157854496, test error 0.056357374509413624\n",
            "Loss: 0.7806223648867849\n",
            "training error 0.03495213267724486, test error 0.05622386688487771\n",
            "Loss: 0.5418784274995092\n",
            "training error 0.03496345664095453, test error 0.05610351004568469\n",
            "Loss: 0.32665127638873415\n",
            "training error 0.03501052587942753, test error 0.05603044555829298\n",
            "Loss: 0.1959942935855663\n",
            "training error 0.03502582650066636, test error 0.056012431141601406\n",
            "Loss: 0.16378015760920572\n",
            "training error 0.03500759458312934, test error 0.05634568878851639\n",
            "Loss: 0.7597254683394006\n",
            "training error 0.034990922015210195, test error 0.056345215082250606\n",
            "Loss: 0.7588783669139065\n",
            "training error 0.03495813788205267, test error 0.056204126951528946\n",
            "Loss: 0.5065786502183922\n",
            "training error 0.03500654846137692, test error 0.056134982497785424\n",
            "Loss: 0.3829316361051216\n",
            "training error 0.03496385528616797, test error 0.056198345604436484\n",
            "Loss: 0.49624020270984115\n",
            "training error 0.034942277632950706, test error 0.056294730017859305\n",
            "Loss: 0.6685988559576694\n",
            "training error 0.034946921711904576, test error 0.05625571392914249\n",
            "Loss: 0.5988286486449335\n",
            "training error 0.034943070662462355, test error 0.05624423602624419\n",
            "Loss: 0.5783033809651617\n",
            "training error 0.034957895566444575, test error 0.05625648000570405\n",
            "Loss: 0.6001985789038322\n",
            "training error 0.034932734651052094, test error 0.05628823754241271\n",
            "Loss: 0.6569887388808482\n",
            "training error 0.03494697047523569, test error 0.05627763837738841\n",
            "Loss: 0.6380348671471969\n",
            "training error 0.03499808320470082, test error 0.05609482080375546\n",
            "Loss: 0.31111280929141394\n",
            "training error 0.03492962567436449, test error 0.05614182007177336\n",
            "Loss: 0.3951588728770439\n",
            "training error 0.035009882156449235, test error 0.05622954098640917\n",
            "Loss: 0.552025093994879\n",
            "training error 0.034959093313908576, test error 0.056141440968859174\n",
            "Loss: 0.3944809451353004\n",
            "training error 0.03494175960168508, test error 0.05604353043254324\n",
            "Loss: 0.21939321488002594\n",
            "training error 0.034995341690105236, test error 0.05601358221283404\n",
            "Loss: 0.16583855149681082\n",
            "training error 0.03496624570233857, test error 0.05612107160165124\n",
            "Loss: 0.35805558781045743\n",
            "training error 0.0349257285309939, test error 0.05604477130926473\n",
            "Loss: 0.22161220271184057\n",
            "training error 0.034956801239281084, test error 0.056144393036494517\n",
            "Loss: 0.3997599563763776\n",
            "training error 0.0349912311570051, test error 0.05593605487966878\n",
            "Loss: 0.027200920579861787\n",
            "training error 0.03496871935306992, test error 0.05604436444917388\n",
            "Loss: 0.2208846384218477\n",
            "training error 0.03496673423547582, test error 0.056034544562166944\n",
            "Loss: 0.20332430434442106\n",
            "training error 0.034942917283729856, test error 0.056089144692608256\n",
            "Loss: 0.3009625491168011\n",
            "training error 0.03502858713677627, test error 0.05615852435786254\n",
            "Loss: 0.42503017832522527\n",
            "training error 0.03497060320859403, test error 0.056091605007317885\n",
            "Loss: 0.30536218713770324\n",
            "training error 0.03508547490995036, test error 0.05626789209794541\n",
            "Loss: 0.6206061612048508\n",
            "training error 0.034926030936706334, test error 0.05604475949797572\n",
            "Loss: 0.22159108126913463\n",
            "training error 0.03492254751272057, test error 0.056172381164255346\n",
            "Loss: 0.449809501074272\n",
            "training error 0.03490669566260959, test error 0.056093573098231805\n",
            "Loss: 0.30888160990893976\n",
            "training error 0.034948581076801266, test error 0.05618757758852914\n",
            "Loss: 0.47698438474059746\n",
            "training error 0.03490973213115227, test error 0.056126834736446904\n",
            "Loss: 0.36836146732555086\n",
            "training error 0.03492683404354319, test error 0.056090847257772074\n",
            "Loss: 0.3040071475978401\n",
            "training error 0.03492628070870524, test error 0.05603631179384451\n",
            "Loss: 0.20648454219933665\n",
            "training error 0.034917194785301336, test error 0.05613278135912342\n",
            "Loss: 0.3789954675699647\n",
            "training error 0.03492190870965392, test error 0.05610176914542602\n",
            "Loss: 0.3235381254822034\n",
            "training error 0.03493986866760299, test error 0.05596416127691619\n",
            "Loss: 0.07746195973730163\n",
            "training error 0.034915270860718495, test error 0.05609980451852076\n",
            "Loss: 0.32002489719662464\n",
            "training error 0.03494282668159709, test error 0.055993842388100415\n",
            "Loss: 0.1305389684458591\n",
            "training error 0.03491347054015852, test error 0.05588022883373179\n",
            "Loss: 0.0\n",
            "training error 0.034911520084919725, test error 0.055902050612860965\n",
            "Loss: 0.039050983835631214\n",
            "training error 0.03492567678849305, test error 0.055795035795198275\n",
            "Loss: 0.0\n",
            "training error 0.03494499876563868, test error 0.055838303735553624\n",
            "Loss: 0.07754801074806306\n",
            "training error 0.03495041807288474, test error 0.05577161036245059\n",
            "Loss: 0.0\n",
            "training error 0.03491090055876572, test error 0.055876392841424725\n",
            "Loss: 0.18787780789037978\n",
            "training error 0.03489887424588441, test error 0.0558675395394986\n",
            "Loss: 0.17200359900777507\n",
            "training error 0.034904095966938935, test error 0.055922719623773036\n",
            "Loss: 0.27094297679484125\n",
            "training error 0.03489002074665586, test error 0.05594842871134046\n",
            "Loss: 0.31704006346733316\n",
            "training error 0.03490669803510159, test error 0.055942280409800316\n",
            "Loss: 0.3060159931559614\n",
            "training error 0.03490397820821711, test error 0.05592988902476216\n",
            "Loss: 0.28379790592909515\n",
            "training error 0.034928169229260814, test error 0.05574901423589526\n",
            "Loss: 0.0\n",
            "training error 0.03493821917881501, test error 0.055788142422592356\n",
            "Loss: 0.07018632927127122\n",
            "training error 0.03493584402306346, test error 0.05595906790232357\n",
            "Loss: 0.37678453925569233\n",
            "training error 0.03494763321611995, test error 0.05599649476336458\n",
            "Loss: 0.44391910935346335\n",
            "training error 0.03487650766821377, test error 0.05584452606639137\n",
            "Loss: 0.171324698391917\n",
            "training error 0.03489522088851063, test error 0.05559937034059771\n",
            "Loss: 0.0\n",
            "training error 0.034928398277907814, test error 0.055635409981731894\n",
            "Loss: 0.06482023251954683\n",
            "training error 0.034992553065675495, test error 0.05542096393580759\n",
            "Loss: 0.0\n",
            "training error 0.034882688218518636, test error 0.05554588143591444\n",
            "Loss: 0.2253975593992541\n",
            "training error 0.03489379055905498, test error 0.05556796434789898\n",
            "Loss: 0.2652433333019122\n",
            "training error 0.034879688484987334, test error 0.055658179073264244\n",
            "Loss: 0.42802419988836604\n",
            "training error 0.03489145786941972, test error 0.055486407240907504\n",
            "Loss: 0.11808402534412288\n",
            "training error 0.03496422054193568, test error 0.05538078851774482\n",
            "Loss: 0.0\n",
            "training error 0.034859252879506294, test error 0.05548990347273822\n",
            "Loss: 0.1970267269821191\n",
            "training error 0.03489978275630625, test error 0.05568345786103153\n",
            "Loss: 0.5465240770086366\n",
            "training error 0.03489613076383497, test error 0.05559635143212784\n",
            "Loss: 0.3892377124857127\n",
            "training error 0.03486811699447115, test error 0.05569532087654237\n",
            "Loss: 0.5679448906668716\n",
            "training error 0.034850847633194966, test error 0.0557400763366353\n",
            "Loss: 0.648758944223693\n",
            "training error 0.034927064133748305, test error 0.05588409689288343\n",
            "Loss: 0.9088140284917445\n",
            "training error 0.03490184318484607, test error 0.055888407820475945\n",
            "Loss: 0.9165981856117389\n",
            "training error 0.034869570951877704, test error 0.055867832614470876\n",
            "Loss: 0.8794459410234046\n",
            "training error 0.03498740236798072, test error 0.05602286719964299\n",
            "Loss: 1.1593888405768027\n",
            "training error 0.034854788607233, test error 0.05592830293703817\n",
            "Loss: 0.988636012500832\n",
            "training error 0.034885475821932625, test error 0.055988565572542864\n",
            "Loss: 1.0974510675363591\n",
            "training error 0.03482861235404849, test error 0.05596050569097966\n",
            "Loss: 1.0467838915819927\n",
            "training error 0.034876623135376594, test error 0.05593671824163339\n",
            "Loss: 1.0038313624054718\n",
            "training error 0.034896417951036374, test error 0.05601021927869875\n",
            "Loss: 1.1365507386234652\n",
            "training error 0.03483737109908653, test error 0.055868272916912966\n",
            "Loss: 0.8802409864784533\n",
            "training error 0.034853421470340006, test error 0.05586136075192618\n",
            "Loss: 0.8677598261848107\n",
            "training error 0.034866928371685875, test error 0.05588118541447345\n",
            "Loss: 0.903556829221186\n",
            "training error 0.03484697336829923, test error 0.05597854735320614\n",
            "Loss: 1.079361366026399\n",
            "training error 0.034842080043154484, test error 0.05601731432784138\n",
            "Loss: 1.1493621292383738\n",
            "training error 0.03489045415840558, test error 0.055927942755991875\n",
            "Loss: 0.9879856406734611\n",
            "training error 0.03485960018263908, test error 0.05599199797181317\n",
            "Loss: 1.1036488833533253\n",
            "training error 0.03485814261761946, test error 0.0560340636795526\n",
            "Loss: 1.179606104016484\n",
            "training error 0.034827711902402685, test error 0.05607753229177475\n",
            "Loss: 1.258096521696661\n",
            "training error 0.03485223597024931, test error 0.05591537197214537\n",
            "Loss: 0.9652868236595502\n",
            "training error 0.03488472366315224, test error 0.056095657895798706\n",
            "Loss: 1.2908255681928882\n",
            "training error 0.0348197951840891, test error 0.05586751677450834\n",
            "Loss: 0.8788756350183835\n",
            "training error 0.03484217928349254, test error 0.05581866791299486\n",
            "Loss: 0.7906702070696214\n",
            "training error 0.03483399364760877, test error 0.055849639247782884\n",
            "Loss: 0.8465945368181904\n",
            "training error 0.03484584775289422, test error 0.05585890826243499\n",
            "Loss: 0.8633314141725856\n",
            "training error 0.03490611470745472, test error 0.05580889241753539\n",
            "Loss: 0.7730187872883043\n",
            "training error 0.03494044392009025, test error 0.055986450035474776\n",
            "Loss: 1.093631083883717\n",
            "training error 0.034834486837402444, test error 0.05586212398953195\n",
            "Loss: 0.8691379893099649\n",
            "training error 0.03482262327344157, test error 0.056040544584302805\n",
            "Loss: 1.1913085461876083\n",
            "training error 0.03480835218845717, test error 0.05598468179144197\n",
            "Loss: 1.0904382004305546\n",
            "training error 0.03480634351577926, test error 0.05592156774029915\n",
            "Loss: 0.9764744002896508\n",
            "training error 0.03484817534008016, test error 0.05576245909257263\n",
            "Loss: 0.6891750461543555\n",
            "training error 0.03480638652900643, test error 0.055897479182507225\n",
            "Loss: 0.9329781655182723\n",
            "training error 0.03484040496546932, test error 0.055820789153925986\n",
            "Loss: 0.7945004900755226\n",
            "training error 0.03491638406607468, test error 0.05590743682942356\n",
            "Loss: 0.9509584926007175\n",
            "training error 0.03485084738845333, test error 0.056110939127005245\n",
            "Loss: 1.3184185866665254\n",
            "training error 0.03482841931823343, test error 0.05597220197510403\n",
            "Loss: 1.0679036416567422\n",
            "training error 0.03481560258143006, test error 0.05594316981245277\n",
            "Loss: 1.0154808368749713\n",
            "training error 0.034879100471821044, test error 0.05576982114510743\n",
            "Loss: 0.7024685595402147\n",
            "training error 0.03503455413601133, test error 0.05606304556787473\n",
            "Loss: 1.2319381294314757\n",
            "training error 0.03483929707402246, test error 0.05587648565191332\n",
            "Loss: 0.8950705604519715\n",
            "training error 0.03483890845940747, test error 0.055886829823272616\n",
            "Loss: 0.9137488271147465\n",
            "training error 0.03482937213491908, test error 0.05592076910762801\n",
            "Loss: 0.9750323249914983\n",
            "training error 0.0348613286453751, test error 0.0559595703356663\n",
            "Loss: 1.0450949388993047\n",
            "training error 0.03483939364186786, test error 0.05584469047693429\n",
            "Loss: 0.8376586386826812\n",
            "training error 0.034824806402367646, test error 0.055820449821860615\n",
            "Loss: 0.7938877648427267\n",
            "training error 0.034808045327436865, test error 0.05589329263883388\n",
            "Loss: 0.9254186059933778\n",
            "training error 0.03482855696899644, test error 0.055924194668791324\n",
            "Loss: 0.9812177933732125\n",
            "training error 0.034862415190442066, test error 0.055981458782182164\n",
            "Loss: 1.0846184760350264\n",
            "training error 0.03488496720906249, test error 0.05599668401977393\n",
            "Loss: 1.112110387940346\n",
            "training error 0.03484275343442994, test error 0.05594001028010367\n",
            "Loss: 1.0097757314879496\n",
            "training error 0.03486595950488768, test error 0.05554760066334472\n",
            "Loss: 0.30120940864981627\n",
            "training error 0.03481689685879603, test error 0.05552026052450378\n",
            "Loss: 0.2518418579653847\n",
            "training error 0.03485108392337708, test error 0.055538474177819964\n",
            "Loss: 0.2847298933358777\n",
            "training error 0.034829537115793126, test error 0.055405976519568904\n",
            "Loss: 0.0454814792245406\n",
            "training error 0.034800227712678176, test error 0.05550975327651485\n",
            "Loss: 0.23286912704161278\n",
            "training error 0.03482112683006211, test error 0.05564136443289298\n",
            "Loss: 0.47051680216627645\n",
            "training error 0.034810719296940226, test error 0.05556699996677092\n",
            "Loss: 0.33623834909182193\n",
            "training error 0.03482596101409949, test error 0.05555112692194217\n",
            "Loss: 0.30757670440675344\n",
            "training error 0.03482025124616452, test error 0.05570085063154125\n",
            "Loss: 0.5779298604494887\n",
            "training error 0.03487047735344582, test error 0.05555651719420146\n",
            "Loss: 0.31730981295134963\n",
            "training error 0.03486460984338788, test error 0.05585173908670868\n",
            "Loss: 0.8503861746442354\n",
            "training error 0.034810398644243144, test error 0.05586143926977019\n",
            "Loss: 0.8679016043106058\n",
            "training error 0.034800700002056616, test error 0.055846588740488624\n",
            "Loss: 0.8410862958272203\n",
            "training error 0.03485156089943123, test error 0.05593541465428709\n",
            "Loss: 1.0014775003872822\n",
            "training error 0.03487443600283728, test error 0.055735960269056056\n",
            "Loss: 0.6413266419950547\n",
            "training error 0.03481098783537464, test error 0.05593107961087993\n",
            "Loss: 0.9936497978153325\n",
            "training error 0.03484761535650448, test error 0.056013988386078475\n",
            "Loss: 1.1433565416475888\n",
            "training error 0.034815246208400985, test error 0.05594177064284978\n",
            "Loss: 1.0129543838568011\n",
            "training error 0.034805992706645666, test error 0.055926694155140684\n",
            "Loss: 0.9857310666874008\n",
            "training error 0.03483149489133965, test error 0.055917359256674\n",
            "Loss: 0.9688752242255516\n",
            "training error 0.034874910623088715, test error 0.05582772720393713\n",
            "Loss: 0.8070283904482611\n",
            "training error 0.03487362961475524, test error 0.056051401241720124\n",
            "Loss: 1.2109121988402638\n",
            "training error 0.034817570390837195, test error 0.05598597558052096\n",
            "Loss: 1.0927743699102876\n",
            "training error 0.0348252112735618, test error 0.05607969644821647\n",
            "Loss: 1.2620042963955136\n",
            "training error 0.034907651557111556, test error 0.056130198065229134\n",
            "Loss: 1.3531940724249347\n",
            "training error 0.034812842538938724, test error 0.05593150193854228\n",
            "Loss: 0.9944123865643473\n",
            "training error 0.034854606263763195, test error 0.05564363959064016\n",
            "Loss: 0.4746250097380278\n",
            "training error 0.03486936788303651, test error 0.055873880390223236\n",
            "Loss: 0.8903662906865728\n",
            "training error 0.034807271587446564, test error 0.05583927809057521\n",
            "Loss: 0.8278855991432632\n",
            "training error 0.034832183700561466, test error 0.05583153418946424\n",
            "Loss: 0.8139025892977214\n",
            "training error 0.034801590240212114, test error 0.05586803840733947\n",
            "Loss: 0.8798175371564687\n",
            "training error 0.03482346167866876, test error 0.05585044816485928\n",
            "Loss: 0.8480551824645355\n",
            "training error 0.034967549778619225, test error 0.0556630086338266\n",
            "Loss: 0.5095993098605867\n",
            "training error 0.034816867870121525, test error 0.05589470525111372\n",
            "Loss: 0.9279693321886162\n",
            "training error 0.034811569744446325, test error 0.05600565024792081\n",
            "Loss: 1.1283005296607085\n",
            "training error 0.03482077856864513, test error 0.0559366680454376\n",
            "Loss: 1.003740724122526\n",
            "training error 0.034822002651857316, test error 0.05594056134697516\n",
            "Loss: 1.010770782093462\n",
            "training error 0.03480711737881427, test error 0.05590596303394141\n",
            "Loss: 0.9482972891011077\n",
            "training error 0.03482150783470911, test error 0.055920193044414965\n",
            "Loss: 0.9739921389839257\n",
            "training error 0.03485309699512323, test error 0.05583609735195667\n",
            "Loss: 0.8221422020128211\n",
            "training error 0.03483143662736709, test error 0.056109207257645094\n",
            "Loss: 1.3152913842439773\n",
            "training error 0.03484335520785828, test error 0.05617919139819399\n",
            "Loss: 1.4416603696304264\n",
            "training error 0.03484714145587862, test error 0.05596051418536699\n",
            "Loss: 1.0467992297300333\n",
            "training error 0.03482942622204575, test error 0.056075167796531225\n",
            "Loss: 1.253826999165808\n",
            "training error 0.03482443163552279, test error 0.0559776164563182\n",
            "Loss: 1.0776804638347626\n",
            "training error 0.03483062015116609, test error 0.05605368094940943\n",
            "Loss: 1.2150286221529694\n",
            "training error 0.03481276560220543, test error 0.05615748627759835\n",
            "Loss: 1.402467860501222\n",
            "training error 0.03492621892946729, test error 0.05590805970343336\n",
            "Loss: 0.9520832039428218\n",
            "training error 0.03479516802292087, test error 0.05608102548761735\n",
            "Loss: 1.2644041166878006\n",
            "training error 0.03480088161110566, test error 0.05586981361320531\n",
            "Loss: 0.8830229914545296\n",
            "training error 0.03478128116874145, test error 0.05587823844595747\n",
            "Loss: 0.8982355461646385\n",
            "training error 0.034795347603910745, test error 0.056048146028778285\n",
            "Loss: 1.205034324889831\n",
            "training error 0.03485936279792707, test error 0.056064176642676336\n",
            "Loss: 1.2339804889425654\n",
            "training error 0.03480650574009909, test error 0.055649902183512026\n",
            "Loss: 0.48593325044654634\n",
            "training error 0.03478978719453608, test error 0.055681992522950544\n",
            "Loss: 0.5438781448718677\n",
            "training error 0.034792093455548845, test error 0.05565622828690659\n",
            "Loss: 0.4973561708561114\n",
            "training error 0.03477783945432616, test error 0.05564479387605964\n",
            "Loss: 0.47670928020504544\n",
            "training error 0.03478953363086223, test error 0.05561759259341972\n",
            "Loss: 0.42759245943027047\n",
            "training error 0.03480189704217733, test error 0.05566752990622794\n",
            "Loss: 0.5177632824625578\n",
            "training error 0.03480570868532034, test error 0.05585495425056935\n",
            "Loss: 0.856191732756928\n",
            "training error 0.03482077474728315, test error 0.055890063371403995\n",
            "Loss: 0.9195875813432952\n",
            "training error 0.03480316747980185, test error 0.055708434551854306\n",
            "Loss: 0.5916239961164527\n",
            "training error 0.034795776433895115, test error 0.055813972227684173\n",
            "Loss: 0.7821913005094716\n",
            "training error 0.03480563182239022, test error 0.055773518689965294\n",
            "Loss: 0.7091451435268681\n",
            "training error 0.034796641985202964, test error 0.05578222378181354\n",
            "Loss: 0.7248637565716365\n",
            "training error 0.03480602859810217, test error 0.05595790368529405\n",
            "Loss: 1.0420855011197716\n",
            "training error 0.034831772615678776, test error 0.056113863081578605\n",
            "Loss: 1.3236983138997793\n",
            "training error 0.034786980311692264, test error 0.056078690394100225\n",
            "Loss: 1.2601876842757154\n",
            "training error 0.03485285698720791, test error 0.05588046616405575\n",
            "Loss: 0.9022580928959245\n",
            "training error 0.03479155830705624, test error 0.05601532940139038\n",
            "Loss: 1.1457779866067552\n",
            "training error 0.03482361982618779, test error 0.05596349538243618\n",
            "Loss: 1.0521823186115498\n",
            "training error 0.034818714083561866, test error 0.05600760238340334\n",
            "Loss: 1.1318254622858515\n",
            "training error 0.034792812543188764, test error 0.05599822954392549\n",
            "Loss: 1.1149011104867768\n",
            "training error 0.03481688620215081, test error 0.05606140978497242\n",
            "Loss: 1.2289844284349982\n",
            "training error 0.03482099787405702, test error 0.055976250584843\n",
            "Loss: 1.0752141365906853\n",
            "training error 0.03479725754305467, test error 0.05593210626453314\n",
            "Loss: 0.9955036061136457\n",
            "training error 0.03479967467418098, test error 0.055927005307328845\n",
            "Loss: 0.9862929080704852\n",
            "training error 0.03482235765621555, test error 0.05602875827210847\n",
            "Loss: 1.1700262341985956\n",
            "training error 0.03478700844156884, test error 0.05603946457813586\n",
            "Loss: 1.189358400305185\n",
            "training error 0.03480390634013799, test error 0.055881084490308944\n",
            "Loss: 0.9033745924434067\n",
            "training error 0.0347903466296016, test error 0.05583287670046253\n",
            "Loss: 0.8163267349883485\n",
            "training error 0.034862824250538285, test error 0.05592043985420526\n",
            "Loss: 0.9744377985653419\n",
            "training error 0.0347948243460402, test error 0.05558710343230415\n",
            "Loss: 0.3725387811934544\n",
            "training error 0.03480052717469757, test error 0.05573898330780415\n",
            "Loss: 0.6467852835727594\n",
            "training error 0.03478308324041758, test error 0.05569962094381969\n",
            "Loss: 0.5757094375294258\n",
            "training error 0.03479699076675109, test error 0.05585830974699046\n",
            "Loss: 0.8622506866124313\n",
            "training error 0.03476731732206187, test error 0.05576898524461097\n",
            "Loss: 0.7009591904632462\n",
            "training error 0.034782891240860446, test error 0.05587311201554201\n",
            "Loss: 0.8889788516453612\n",
            "training error 0.03476622230836074, test error 0.055897241140983545\n",
            "Loss: 0.9325483386233291\n",
            "training error 0.03481652432622671, test error 0.05578555241048697\n",
            "Loss: 0.7308741958639065\n",
            "training error 0.03479322935118683, test error 0.055885511674466384\n",
            "Loss: 0.9113686717548974\n",
            "training error 0.0347991346995775, test error 0.055883069715293644\n",
            "Loss: 0.9069592741314692\n",
            "training error 0.03482028169641671, test error 0.05599168617103676\n",
            "Loss: 1.103085870827214\n",
            "training error 0.034905786738342826, test error 0.056084556336739554\n",
            "Loss: 1.2707797014649413\n",
            "training error 0.03480702690901769, test error 0.05606965836804399\n",
            "Loss: 1.2438787325652623\n",
            "training error 0.0347800522443293, test error 0.05597646600967427\n",
            "Loss: 1.0756031249692155\n",
            "training error 0.034788810310478056, test error 0.05591895933188965\n",
            "Loss: 0.9717644485549348\n",
            "training error 0.034806799381280985, test error 0.055907727789599584\n",
            "Loss: 0.9514838736648379\n",
            "training error 0.034806635494495836, test error 0.05598735321313762\n",
            "Loss: 1.0952619340160785\n",
            "training error 0.03481507655416612, test error 0.05582924823080737\n",
            "Loss: 0.8097748787359027\n",
            "training error 0.03480397853156627, test error 0.05585835187508231\n",
            "Loss: 0.8623267564788817\n",
            "training error 0.0347752336051828, test error 0.055871093665500506\n",
            "Loss: 0.8853343566940897\n",
            "training error 0.034818271877067236, test error 0.0559127476952199\n",
            "Loss: 0.9605482184578129\n",
            "training error 0.034776275085199336, test error 0.0560021303669061\n",
            "Loss: 1.1219447497794244\n",
            "training error 0.034824149164101204, test error 0.05567189415814113\n",
            "Loss: 0.5256437262590286\n",
            "training error 0.0348106134890369, test error 0.055711173770614277\n",
            "Loss: 0.5965701495268538\n",
            "training error 0.034801692249687564, test error 0.05586545982917607\n",
            "Loss: 0.8751614493100801\n",
            "training error 0.03481129584349752, test error 0.05566999802571297\n",
            "Loss: 0.5222199172470798\n",
            "training error 0.03479902773046398, test error 0.05586329644431688\n",
            "Loss: 0.8712550678426245\n",
            "training error 0.03481855664793272, test error 0.05582577469408482\n",
            "Loss: 0.803502781830967\n",
            "training error 0.03479821512520921, test error 0.05590060025370335\n",
            "Loss: 0.9386138223582252\n",
            "training error 0.03478626694352352, test error 0.055768088164198926\n",
            "Loss: 0.69933935001667\n",
            "training error 0.03477910146437889, test error 0.0558140267533759\n",
            "Loss: 0.7822897564780273\n",
            "training error 0.03477652084100893, test error 0.055773725001437076\n",
            "Loss: 0.709517676091509\n",
            "training error 0.03480120260833503, test error 0.05586527946071019\n",
            "Loss: 0.8748357615206759\n",
            "training error 0.03479755102291967, test error 0.05596748316133205\n",
            "Loss: 1.0593829724891757\n",
            "training error 0.03478242372832875, test error 0.055924489990557325\n",
            "Loss: 0.9817510500745197\n",
            "training error 0.03478590033219143, test error 0.055837668625642586\n",
            "Loss: 0.8249794199867155\n",
            "training error 0.03477526819488665, test error 0.055742912525179635\n",
            "Loss: 0.6538801940654704\n",
            "training error 0.0347652709956493, test error 0.05582653449246997\n",
            "Loss: 0.8048747348231133\n",
            "training error 0.03483663494706825, test error 0.0558075161237216\n",
            "Loss: 0.7705336406325314\n",
            "training error 0.03480147951501871, test error 0.055712419001292895\n",
            "Loss: 0.5988186380585958\n",
            "training error 0.03482008277813393, test error 0.05570806208541532\n",
            "Loss: 0.5909514407972827\n",
            "training error 0.03482447404899621, test error 0.05576589164257544\n",
            "Loss: 0.6953731341460268\n",
            "training error 0.0348233803798597, test error 0.056039218807123835\n",
            "Loss: 1.1889146164252296\n",
            "training error 0.03482826153923839, test error 0.05572287057156339\n",
            "Loss: 0.61769083282186\n",
            "training error 0.03478080337459378, test error 0.05586556643652245\n",
            "Loss: 0.8753539480975459\n",
            "training error 0.03477237899266283, test error 0.05590829812638513\n",
            "Loss: 0.9525137195749434\n",
            "training error 0.03476805909621488, test error 0.055859390530726166\n",
            "Loss: 0.864202236535494\n",
            "training error 0.03478569961019951, test error 0.05566939623888034\n",
            "Loss: 0.5211332826058346\n",
            "training error 0.034774669579338086, test error 0.05569086883265609\n",
            "Loss: 0.5599059226321978\n",
            "training error 0.03480015181868013, test error 0.055667976257586356\n",
            "Loss: 0.5185692503267836\n",
            "training error 0.03477163642396324, test error 0.055737940397509296\n",
            "Loss: 0.6449021209765649\n",
            "training error 0.034850410326564565, test error 0.05585890930718773\n",
            "Loss: 0.8633333006620392\n",
            "training error 0.03486120125414158, test error 0.05566795665637664\n",
            "Loss: 0.5185338568081344\n",
            "training error 0.034790297471841945, test error 0.0558497763173712\n",
            "Loss: 0.8468420406764654\n",
            "training error 0.03478820656896056, test error 0.055723649834100226\n",
            "Loss: 0.6190979318496881\n",
            "training error 0.034770097304425075, test error 0.05559270709663536\n",
            "Loss: 0.382657207603021\n",
            "training error 0.03479288971946665, test error 0.055671451880928785\n",
            "Loss: 0.5248451150001721\n",
            "training error 0.03479873791397285, test error 0.05543886336431591\n",
            "Loss: 0.10486460761114369\n",
            "training error 0.034887567871308665, test error 0.05572623508254339\n",
            "Loss: 0.6237660640889597\n",
            "training error 0.034795068263512044, test error 0.05562295976417016\n",
            "Loss: 0.4372838540349422\n",
            "training error 0.034787747878522145, test error 0.055525060405174735\n",
            "Loss: 0.2605089080371048\n",
            "training error 0.03482294754477453, test error 0.05560159982905811\n",
            "Loss: 0.3987146395406427\n",
            "training error 0.034802191751384354, test error 0.05559198694326091\n",
            "Loss: 0.3813568408265189\n",
            "training error 0.03478765183875764, test error 0.055808376613783205\n",
            "Loss: 0.7720874106033726\n",
            "training error 0.03481507111407291, test error 0.05583532599981837\n",
            "Loss: 0.8207493866359661\n",
            "training error 0.034778642656996935, test error 0.05574340932275785\n",
            "Loss: 0.6547772516760864\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfXxU5Zn/8c81EwgoCAhUFCLgSlWsEEpEBiviE+JqtVW7yg+Lre4Gta3argXdvnZtbWsBt9vWXavS1bUUutWqVUvtYrFSECICCigoQm00UdAI8iAKIZnr98c5CZNk8jyTmWS+b1/zypz7PN1nDs419+Mxd0dERKS+SKYzICIi2UkBQkREklKAEBGRpBQgREQkKQUIERFJSgFCRESSUoAQaSMzO8PMNmc6HyLpYhoHIZ2RmZUC/+juSzKdF5GuSiUIkUaYWTTTeWivrnANkjkKENKlmFnEzG41s7+a2Q4ze8TMjkxY/1sz225mu81smZmdnLDuITO718yeNrN9wFlmVmpmt5jZhnCfh82sR7j9JDMrT9i/0W3D9TPNbJuZvWtm/2hmbmbHN3IdR5rZ/4TbfmhmT4TpXzGz5+ttW3ucJNdwS3i90YTtv2hmG1ryeUluU4CQruYbwBeAM4FjgA+BexLW/xEYAXwKeAlYWG///wf8EOgN1HwR/wMwBRgOjAK+0sT5k25rZlOAbwHnAscDk5q5jl8BhwEnh3n9STPbN3YNPwP2AWfXW//r8H1zn5fkMAUI6WquA77j7uXufgD4LnC5meUBuPuD7r43Yd1oM+uTsP+T7r7C3ePuvj9Mu9vd33X3ncDvgcImzt/Ytv8A/I+7b3T3j8NzJ2VmRwMXANe5+4fuftDd/9KKz6D+NfwvMDU8dm/g78M0aObzktymACFdzVDgd2a2y8x2Aa8B1cBRZhY1s9lhdcoeoDTcZ0DC/mVJjrk94f3HQK8mzt/YtsfUO3ay89QoAHa6+4dNbNOU+sf+NXCpmeUDlwIvuftb4bpGP682nlu6EAUI6WrKgAvcvW/Cq4e7v0NQtXIJQTVPH2BYuI8l7J+ubn3bgCEJywVNbFsGHGlmfZOs20dQ9QSAmQ1Ksk2da3D3TcBbBKWSxOqlmnM19nlJjlOAkM6sm5n1SHjlAfcBPzSzoQBmNtDMLgm37w0cAHYQfMne2YF5fQT4qpmdZGaHAf/a2Ibuvo2greTnZtbPzLqZ2cRw9XrgZDMrDBvAv9vC8/8auAmYCPw2Ib2pz0tynAKEdGZPA58kvL5L0Cj7FPCMme0FXgBOC7efT/BL+h1gU7iuQ7j7H4G7geeArQnnPtDILl8GDgKvA+8DN4fHeQO4A1gCbOFQQ3pz/pegIfrP7v5BQnpTn5fkOA2UE8kAMzsJeBXId/eqTOdHJBmVIEQ6SDj+IN/M+gFzgN8rOEg2U4AQ6TgzCKqL/krQU+j6zGZHpGmqYhIRkaRUghARkaS6zGjJAQMG+LBhwzKdDRGRTmXt2rUfuPvAZOu6TIAYNmwYa9asyXQ2REQ6FTN7q7F1qmISEZGkFCBERCQpBQgREUmqy7RBiEh2OHjwIOXl5ezfv7/5jaXD9OjRgyFDhtCtW7cW76MAISIpVV5eTu/evRk2bBhm1vwOknbuzo4dOygvL2f48OEt3k9VTCKSUvv376d///4KDlnEzOjfv3+rS3VpLUGEj1n8GRAF/tvdZ9db/y3gH4EqoAK4puZBJmZWDbwSbvq2u1+czrwmU1JWwtLSpUwaNolYQaw2fd7aeTzw0gP06NaDkQNGMn309Nr1JWUlzF8/H6BOeuK+j216jMtGXkbx2OImz1ezvLFiI4veWMQnVZ8Q9zjujpkRsQjdo93pm9+XA9UHGHzEYMYPHs+Yo8ew4+MdDfIt0lEUHLJPW+5J2qbaCB+S/gZwHlAOrAamhg8vqdnmLGCVu39sZtcDk9z9inDdR+7e1JO76igqKvK2joOYtWQW97x4D/ur9gdfvESojldTTXXtNhEiRCxC3OPEiTc4RpQoGFR7dZ10w4halIhFOBg/iCc8yyVxXf3zpYphRCPB8+oTA0tNoOnZrSc3nHoDc86dk/JzS2567bXXOOmkkzKdDUki2b0xs7XuXpRs+3RWMY0Dtrr7m+5eCfyG4Gletdz9ufD5vBDMQz+EDnbT0zcxd8Vc9h3cR7VXUxWvojJe2eDLOk6cKq9KGhwAqqluEBwAHKfKg2N6vYeVJa5LR3CoPUe8iqp41aHrq66sXf6o8iPmrpiLfc/I/0E+w382nHlr56UlLyIdYceOHRQWFlJYWMigQYMYPHhw7XJlZWWT+65Zs4Ybb7yx2XNMmDAhJXldunQpffr0qc1fYWEhS5YsScmxUyGdVUyDqfts3HKafhDJtQRP0arRw8zWEFQ/zXb3J+rvYGbFQDHAscce26ZMLtq6qE37dUWV1ZWU7iplxqIZfO3pr3HFyVew4NIFmc6WSKv079+fdevWAfDd736XXr16ccstt9Sur6qqIi8v+VdfUVERRUVJf0zXsXLlytRkFjjjjDNYtKjx7yF3x92JRCJJlxvT1HW2VFY0UpvZVUARcFdC8tCw2PP/gJ+a2d/V38/d57l7kbsXDRyYdCqRZl0+8vI27ZcpUYsStSh5kTwiabx9VfEqFr6ykF539lKJQtKupAR+9KPgbzp85Stf4brrruO0005j5syZvPjii8RiMcaMGcOECRPYvHkzEPyiv+iii4AguFxzzTVMmjSJ4447jrvvvrv2eL169ardftKkSVx++eWceOKJTJs2jZpq+6effpoTTzyRsWPHcuONN9YetyVKS0s54YQTmD59Op/5zGdYvnx5neWysjK+/e1v85nPfIZTTjmFhx9+uDY/Z5xxBhdffDEjR45s9+eWzhLEO9R9MPuQMK0OMzsX+A5wprvXPn6x5qHp7v6mmS0FxhDMo59SNXXvddogwjp6gO6R7rVVMzXrohalZ7eedI92Z1/lPj45+AkYRCz4wq75Aq+pykk8ZsQiHJF/BPsO7KMyXtnk+aKRKPnRfPIieZxy1CnMPmd2g0bnWUtm8fimxzmu33Fs3rGZin0VON7g/OFnWed8VfHmn1Wz7+A+ZiyawQMvPcCqf1qVss9dcsPNN0P4Y75Ru3fDhg0Qj0MkAqNGQZ8+jW9fWAg//Wnr81JeXs7KlSuJRqPs2bOH5cuXk5eXx5IlS/iXf/kXHnvssQb7vP766zz33HPs3buXE044geuvv77BOIKXX36ZjRs3cswxx3D66aezYsUKioqKmDFjBsuWLWP48OFMnTq10XwtX76cwsLC2uXHHnuMaDTKli1b+OUvf8n48eMpLS2ts/zYY4+xbt061q9fzwcffMCpp57KxInBY8tfeuklXn311VZ1Z21MOgPEamCEmQ0nCAxXEpQGapnZGOB+YIq7v5+Q3g/42N0PmNkA4HRgbroyOufcOZ22kba9ea9poP/k4CeNtq8AvPjui3zqrk/x5JVPqmeUpNTu3UFwgODv7t1NB4i2+tKXvkQ0Gg3PuZurr76aLVu2YGYcPHgw6T4XXngh+fn55Ofn86lPfYr33nuPIUPqNpWOGzeuNq2wsJDS0lJ69erFcccdV/slPXXqVObNS14ST1bFVFpaytChQxk/fnxtWuLy888/z9SpU4lGoxx11FGceeaZrF69miOOOIJx48alJDhAGgOEu1eZ2deBxQTdXB90941mdgewxt2fIqhS6gX8NvyVW9Od9STgfjOLE1SDzU7s/SSpkxhgSspKuHXJrax4e0XSRvOKjys4/cHTue+i+xp00RVJpiW/9EtK4JxzoLISuneHhQshlobfIIcffnjt+3/913/lrLPO4ne/+x2lpaVMmjQp6T75+fm176PRKFVVDUvdLdmmvflNttzS/dojrW0Q7v60u3/a3f/O3X8Ypv1bGBxw93Pd/Sh3LwxfF4fpK939FHcfHf59IJ35lECsIMZfvvoXqm6vYtop05Ju4zgzFs1Qu4SkTCwGzz4L3/9+8DcdwaG+3bt3M3jwYAAeeuihlB//hBNO4M0336S0tBSgto0gVc444wwefvhhqqurqaioYNmyZYwbNy6l54AsaaSW7LPg0gWsvGYlAw9L3vg/Y9EMSsrS1KIoOScWg9tu65jgADBz5kxuu+02xowZk7Jf/Il69uzJz3/+c6ZMmcLYsWPp3bs3fRqpN6tpg6h5Pfroo80e/4tf/CKjRo1i9OjRnH322cydO5dBgwal+jK6zjOp2zNQTpp2/q/O55k3n2mQPqT3EMq+VZZkD8llGigX+Oijj+jVqxfuzte+9jVGjBjBN7/5zYzmKZsGykkXsfjLi5NWOZXvLef8X52fgRyJZL9f/OIXFBYWcvLJJ7N7925mzJiR6Sy1mgKEtMiCSxcw+bjJDdKfefMZZi2ZlYEciWS3b37zm6xbt45NmzaxcOFCDjvssExnqdUUIKTFFn95MeOOadgQNnfFXLVHiHRBChDSKqv+aRXH9zu+QfrcFWkbpiIiGaIAIa02/4vzG6S9UP5CBnIiIumkACGtFiuIMXHoxDpp2/dtV1uESBejACFtMvuc2Q3S7lpxl9oiJOPaM903BBPeNTZb60MPPcTAgQPrjFvYtKnrTvKgZ1JLm9SUIpa9taw2zXHmr5+vuZoko5qb7rs5S5cupVevXo0+8+GKK67gv/7rvxrdv/402y2ddjsV03OnmkoQ0mbJShFqi5C2KCkr4UfLf5S2EujatWs588wzGTt2LOeffz7btm0D4O6772bkyJGMGjWKK6+8ktLSUu677z5+8pOfUFhYyPLly1t0/PrTbNdf3r9/P1/96lc55ZRTGDNmDM899xwQlEguvvhizj77bM4555y0XHt7ZFe4kk4lVhDjCyd+gSdeP/Qsp3XvrWPe2nmazE8AuPn/bmbd9qbn+959YDcb3ttQOx3+qKNG0Se/8elcCwcV8tMpLZ/v2935xje+wZNPPsnAgQN5+OGH+c53vsODDz7I7Nmz+dvf/kZ+fj67du2ib9++XHfddU2WOh5++GGef/752uWS8CEWidNsL126tM7yj3/8Y8yMV155hddff53Jkyfzxhtv1O63YcMGjjzyyBZfU0dRCULaZeaEmQ3SfvpCGybrl5y1e//u2uehxD3O7v27U3r8AwcO8Oqrr3LeeedRWFjID37wA8rLywEYNWoU06ZNY8GCBS2u3rniiitYt25d7atnz54ADabZTlx+/vnnueqqqwA48cQTGTp0aG2AOO+887IyOIBKENJOydoiXv/gdUrKStQWIS36pV9SVsI588+hsrqS7tHuLLx0YUr/7bg7J598cu0v/UR/+MMfWLZsGb///e/54Q9/yCuvvNLm82TD9NypphKEtNvsc2ZjWO1yTWO1SEvECmI8O/1Zvn/W93l2+rMp/2GRn59PRUVFbYA4ePAgGzduJB6PU1ZWxllnncWcOXPYvXs3H330Eb1792bv3r0pzcMZZ5zBwoULAXjjjTd4++23OeGEE1J6jnRQCULaLVYQY/Sg0XXqmtVYLa0RK4ilrcQZiUR49NFHufHGG9m9ezdVVVXcfPPNfPrTn+aqq65i9+7duDs33ngjffv25fOf/zyXX345Tz75JP/5n//JGWecUed49dsgfv7znzebhxtuuIHrr7+eU045hby8PB566KE6DxrKVpruW1LitF+cxovvvli7bBgrrlmhaqYcpOm+s5em+5aMuPaz19ZZVjWTSOenACEpUTy2mMJBhXXSNlV03RGmIrlAAUJSZvzg8XWWn3/7eU29kaO6StV1V9KWe6IAISkzffR0Ign/pOLEVc2Ug3r06MGOHTsUJLKIu7Njxw569OjRqv3Ui0lSJlYQ43NDP1dnTISqmXLPkCFDKC8vp6KiItNZkQQ9evRgyJAhrdpHAUJSauSAkXUCRE01k3oz5Y5u3brVGVEsnZeqmCSlpo+eXmfQnKqZRDovBQhJqVhBjM+f8PlMZ0NEUkABQlLuwhEX1lkec/SYDOVERNpDAUJS7uVtL9dZ/uOWP2YoJyLSHgoQknZPbX5K4yFEOiEFCEk5jYcQ6RoUICTlasZDJNr+0fYM5UZE2koBQtJi5ICRmc6CiLSTAoSkxfTR08mLHBqH+Yctf1A7hEgnowAhaREriHHRiItqlw/GD6odQqSTUYCQDqN2CJHORQFC0mZQr0GZzoKItIMChKSN2iFEOjcFCEmbWEGszrQbaocQ6VzSGiDMbIqZbTazrWZ2a5L13zKzTWa2wcyeNbOhCeuuNrMt4evqdOZT0idxZldQO4RIZ5K2AGFmUeAe4AJgJDDVzOp3jn8ZKHL3UcCjwNxw3yOB24HTgHHA7WbWL115lfRRO4RI55XOEsQ4YKu7v+nulcBvgEsSN3D359z943DxBaDmcUfnA39y953u/iHwJ2BKGvMqaTJ99HSiFq1dVjuESOeRzgAxGChLWC4P0xpzLVAz7WeL9jWzYjNbY2Zr9HjD7BQriHHB8RfULqsdQqTzyIpGajO7CigC7mrNfu4+z92L3L1o4MCB6cmctFtiTyZQO4RIZ5HOAPEOUJCwPCRMq8PMzgW+A1zs7gdas690DvXbIdQuIdI5pDNArAZGmNlwM+sOXAk8lbiBmY0B7icIDu8nrFoMTDazfmHj9OQwTTqh+k+UO6LHERnKiYi0RtoChLtXAV8n+GJ/DXjE3Tea2R1mdnG42V1AL+C3ZrbOzJ4K990JfJ8gyKwG7gjTpBPa8fGOOss/KfmJGqpFOoG85jdpO3d/Gni6Xtq/Jbw/t4l9HwQeTF/upKNMGjaJvEgeVfEqAKriVSwtXUqsIJbhnIlIU7KikVq6tlhBjG/FvlW77Dj9D+ufwRyJSEsoQEiH2LN/T53ll7e9nKGciEhLKUBIRqirq0j2U4CQDqGZXUU6HwUI6RCa2VWk81GAkA5zdK+jM50FEWkFBQjpMBowJ9K5KEBIh9GAOZHORQFCOkzNgLkaNQPmRCQ7KUBIh4kVxPjWeA2YE+ksFCCkQ+05oAFzIp2FAoSIiCSlACEdSj2ZRDoPBQjpUDs+3oFhtcvqySSSvRQgpENNGjaJaCRau6yeTCLZSwFCOpSm/hbpPBQgpMP1ze9b+96wBgPoRCQ7KEBIh0ssMTjOrgO7MpgbEWmMAoR0ODVUi3QOChDS4dRQLdI5KEBIh1NDtUjnoAAhGaFnVItkPwUIyQp6RrVI9lGAkIzQM6pFsp8ChGRErCDGRSMuql3WM6pFso8ChGTMoF6DMp0FEWmCAoRkjGZ2FcluChCSMXpGtUh2U4CQjNEzqkWymwKEZIwGzIlkNwUIySgNmBPJXgoQIiKSlAKEZJR6MolkLwUIyShN/S2SvRQgJKM09bdI9lKAkIxSTyaR7NVsgDCziJlNaMvBzWyKmW02s61mdmuS9RPN7CUzqzKzy+utqzazdeHrqbacXzoH9WQSyU55zW3g7nEzuwcY09y2icwsCtwDnAeUA6vN7Cl335Sw2dvAV4BbkhziE3cvbM05pWvQ1N8i2aGlVUzPmtllZmbNb1prHLDV3d9090rgN8AliRu4e6m7bwDirTiudDHTR0+nW6Rb7bKm/hbJDi0NEDOA3wKVZrbHzPaa2Z5m9hkMlCUsl4dpLdXDzNaY2Qtm9oVW7CedTKwgxoUjLqxd1tTfItmh2SomAHfvne6MJDHU3d8xs+OAP5vZK+7+18QNzKwYKAY49thjM5BFSRVN/S2SfVrci8nMLjazfw9fFzW/B+8ABQnLQ8K0FnH3d8K/bwJLSdIG4u7z3L3I3YsGDhzY0kNLFtKAOZHs06IAYWazgZuATeHrJjP7UTO7rQZGmNlwM+sOXAm0qDeSmfUzs/zw/QDg9PC80kVp6m+R7NPSEsTfA+e5+4Pu/iAwBbiwqR3cvQr4OrAYeA14xN03mtkdZnYxgJmdamblwJeA+81sY7j7ScAaM1sPPAfMrtf7SboYTf0tkn1a1AYR6gvsDN/3ackO7v408HS9tH9LeL+aoOqp/n4rgVNakTfp5GoGzM1dMRfQgDmRbNDSAHEn8LKZPQcYMBFoMPBNpD00YE4kuzQbIMwsQjBOYTxwapg8y901mklSqv4AOQ2YE8mslo6knunuj9DCRmaRtlBXV5Hs0tJG6iVmdouZFZjZkTWvtOZMco5GVItkl5YGiCuArwHLgLXha026MiW5SSOqRbJLS9sgbnX3hzsgPyJ1qB1CJHOaLUG4exz4dgfkRUTtECJZRG0QklWmj55eZ8Cc2iFEMkdtEJJV1A4hkj1aOpvr8HRnRKSGUfexI2qHEMmMJksQZjYz4f2X6q27M12ZktymdgiR7NBcFdOVCe9vq7duSorzIgKoHUIkWzQXIKyR98mWRVJC7RAi2aG5AOGNvE+2LJIy9dshNlVotneRjtZcgBhd8wxqYFT4vmZZ03FL2tRvh3j+7edVzSTSwZrsxeTu0Y7KSCbNmgX33AP794MZRCIQj4N7sAyH3qd7XTQKvXpB9+7BNkceCRddBC+8ABs2QHU19OwJ550HJ58Mu3bBunVQWAh79sD27TBoEEyfDrFYx36OqTR99HTmrZ1HnDgAceLMXz+fWEEnviiRTqY1Dwzqkm65BX7840zn4pDqati589Dy9u2wqV7tyt69sHBh3bRnnqm7fN99QeCBQ3+bClaRCJx0Etx7b3YEllhBjItPuJgnNj+R6ayI5KyWDpTrsh5/PNM5SJ94PHhVVQWv6urgb2Vl3eWatPXrYcIEyMsLXvn5MHw4zJuXmfxfMOKCOstjjh6TmYyI5KicDxCXXZbpHGSf6urgVVkJpaUwYwb06BFUxXWk+k+U++OWP3ZsBkRyXM4HiLvugpkz4fDDg/r/vLyg/j8v79By4vt0rotk8d04cADmzg3yeuaZUJKB9uKnNj+lhmqRDmTuXaO3alFRka9Z0/mnhyopCb6IN2+GgQODhuetW4MAcuGF0Lt30GBdWhr8ym9MTQmgJQ3mNVVRrTVzJsyZ06bLbJGSshI+9+DnahuqAa4bex33XnRv+k4qkmPMbK27FyVbl/ON1NkmFoPf/a7jzztvHtx5J1RUBIGlJUFj7tygDWf+/PQ0bMcKYnxu6OdY9tay2jSNhxDpOFlcqSEdqbg4KJXs2wcHDwYlkJUrg+6z1sSY+a1b4fTT01flNHLAyDrLK8pWqJpJpIMoQEijYjF4+eWgJDFz5qGxGfW5w5Qp6QkS00dPJ2qHhuPEPc7S0qWpP5GINKAAIS0yZ07QUD1tWvL1e/YEJYlUd4mNFcT45wn/XLvsOP0P65/ak4hIUgoQ0ioLFgRVT0OGNFznHnSJTXVJYs/+PXWW1d1VpGMoQEirxWJQVgbjxiVff/XV6T2/uruKdAwFCGmzVatg8uSG6Vu2wFVXpe4800dPJ5LwT7VmXiYRSS8FCGmXxYuTt0ssXJi6qqaa7q6J9BhSkfRTgJB2W7AAjj++YfoNN6TuHEf2OLLO8s5PdjaypYikigKEpMT8JDU+69albv4mPR9CpOMpQEhKxGLBWIn65s5NTVWT2iFEOp4ChKTMnDkwenTD9Ftvbf+xk7VDaNoNkfRSgJCUujfJPHrLlqWmFFF/2o3lby9XNZNIGilASErFYjBxYsP0VJQipo+ejnFoYijHuXVJCg4sIkkpQEjKzZ7dMC0VpYhYQYyTBp5UJ02lCJH0UYCQlGuswToVpYibTrupzrLjaqwWSZO0Bggzm2Jmm81sq5k1+Hows4lm9pKZVZnZ5fXWXW1mW8JXmidvkFSbMweGDaublopSRPHYYgoHFdZJU2O1SHqkLUCYWRS4B7gAGAlMNbOR9TZ7G/gK8Ot6+x4J3A6cBowDbjezfunKq6THbbc1TEvF4Lnxg8fXWdaYCJH0SGcJYhyw1d3fdPdK4DfAJYkbuHupu28A6j+77HzgT+6+090/BP4ETEljXiUNiosbliLWrWv/lOAaEyHSMdIZIAYDZQnL5WFayvY1s2IzW2NmayoqKtqcUUmfZKWIBx5o3zFjBTFGDRpVJ+2F8hfad1ARaaBTN1K7+zx3L3L3ooEDB2Y6O5JEcXHDeZo+/LD9x+0eqft4u/XvrVc1k0iKpTNAvAMUJCwPCdPSva9kmXPPrbu8ZUv7q5mu/ey1dZYdZ+6Kue07qIjUkc4AsRoYYWbDzaw7cCXwVAv3XQxMNrN+YeP05DBNOqHp0xum3Xln+46ZrDfTk5ufVClCJIXSFiDcvQr4OsEX+2vAI+6+0czuMLOLAczsVDMrB74E3G9mG8N9dwLfJwgyq4E7wjTphJKNrn7rrfaXIur3ZlIpQiS1zN0znYeUKCoq8jVr1mQ6G9KIkhKYMKFu2tChUFrajmOWlXD6g6fjHPo3bBgrrllBrCDW9gOL5BAzW+vuRcnWdepGauk80lGKiBXEuOTEOj2nVYoQSSEFCOkwyeZoam+X15kTZtaZwA/UFiGSKgoQ0mFiMSis267c7i6vjZUiNHBOpP0UIKRDja/brpySLq8zJzScGfDZN59t30FFRAFCOlayLq8//Wn7jhkriDFyYN1pvrZ8uIVZS1L0QGyRHKUAIR0qWWP1a6+1f5bX+tOAA9y14i61RYi0gwKEdLhkjdXtfVZE8dhiRh9V94HY6tEk0j4KENLhYjEYWW/i91Q8K+LeCxs+EPuJzU+oFCHSRgoQkhE3NawRancpIlYQ4wsnfqFB+g1/SMFDKERykAKEZESyZ0UsX97+UkSyHk3r3lvHvLXt7ColkoMUICRj6j8rwh3mt3P4QqwgxsShExuk3/7c7e07sEgOUoCQjCkuhhEj6qa9kILn/sw+Z3aD0dXb923n/F+d3/6Di+QQBQjJqH71njS+bl37q5liBTHuu+i+BunPvPkMVz1+VfsOLpJDFCAko669tmFaexurIej2Ou2UaQ3SF76yUO0RIi2kACEZlayxOhVdXgEWXLqA4/sd3yD9tiVJHpQtIg0oQEjG1W+shtSUIgDmf7Fhq/fO/Ts57RenpeYEIl2YAoRkXDpLEbGCGDNPb9j19cV3X2TkPSOT7CEiNRQgJCuksxQx59w5TD5ucoP01z54jU/d9SmNtBZphAKEZIXiYl/fAkUAAA9mSURBVBg0qG5aqkoRAIu/vJhxx4xrkF7xcQWnP3i6Gq5FklCAkKxR/1kRAHNTONfeqn9axUkDTmqQ7jgzFs3Q9OAi9ShASNaY2bCpgCefTF0pAmDT1zYlDRIAc1fM1TgJkQQKEJI1YjH4Qr259txTW4qAIEgka5OAYJyERlyLBBQgJKvMnAlWd5YMnngitaUICNokkg2kg2DEda87e6ldQnKeAoRklVgMLrmkYfoNaZixe8GlCxoNEvsO7mPGohn0n9NfgUJylgKEZJ1kpYh162BeGr6nF1y6IOk4iRo79+9kxqIZFPxHgbrDSs5RgJCsE4vBt7/dMP32NM3YPefcOay8ZiUDDxvY6Dble8uZ8OAEjv7x0SpRSM5QgJCsNGdOw3ER27enpxQBwYjr97/9fqON17V5+Gg7MxbNoNv3uzH8Z8MVLKRLU4CQrPW97zVMS1cposbiLy9m5TUrKTyqsMntquJVlO4qZcaiGeTdkadgIV2SuXum85ASRUVFvmbNmkxnQ1KssBDWr6+bNnkyLF6c/nOXlJXwD7/9B8r3lrd4nwgRjuhxBMVji5lz7pw05k4kNcxsrbsXJV2nACHZrKQEJkxomH7//cH0HB1h3tp53LbkNnbu39nqffMieUQsQvdodz579GeZfc5sYgWxNORSpG0UIKRTu+oqWLiwbtqgQbBtW8fmY97aedy5/E62f7SdA9UH2nycqEUBsLCrlrvTPdqdIUcMoV+Pflz72WspHttB0U9yngKEdHr9+8POej/gp02DBQsyk5+SshJu+MMNvPL+K1R7dcqPHyFCJBLB3TEzIhYh7vHa5R55Peib35cD1Qc4aeBJTDtlGjs+3sGkYZNUQpFWUYCQTm/ePJgxo2H6zJlBj6dMqgkWGys2UhWvwsns/1OJJZT6gQWCEks0EiU/mk9ldSVV8ao665LtF7EIUYuSF8mjOl5NZbyywTHr7xeNROnVvRcA+w/up8qrkualsfMlinscgCPyj2Di0InMnDCzyUBYUlbC0tKl9D+sPy9ve5lNFZuo+LiCEwacwAXHX8COj3fQ/7D+df42FlxrjrXrwC6W/m0pxxxxDDMnBGNn5q+fz6aKTby1+y3MjL49+tI90r22FFhSVsLcFXPZvGMzAw8fCA77q/bXKSXOWzuPxzY9xmUjL2u05FiTh3T8AFCAkC4hWYM1dGx7REvMWjKLe168h/1V+wHSUsIQiBIlEgkCCw4WCYJOPB4nTrxNx4wQIS+SR5wgWOFQTdvun2FN/liw8L/EvBpG1KLEPU7EIpgZ8Xi8Th5qSpc1QRWHAYcP4HuTvtemqkkFCOkSSkrg9NODCfzqW7kyGGCXjWp+Rb5Q/gJ7Duxp8Ou7Ol6d8VKHdA33X3R/q4OEAoR0GY1VNQ0ZAmVlHZ+fVJm3dh4PvPQA7+59l/f2vVcbMJJVwcQ9roAiSU0+bjKLv9y6PuBNBYi8lORKpIMUF8Nf/9pwCvDycjjtNFi1KjP5aq/iscWt+uU3a8ksHt/0OMf1O47NOzZTsa+CKq8CaFCXX7/ev34VTE2VBSQPSNCwmixZT6zE/ZJV80Qtipk12wZR/3x5kTzcXVV1LXDZyMtSery0liDMbArwMyAK/Le7z663Ph+YD4wFdgBXuHupmQ0DXgM2h5u+4O7XNXUulSByy/nnwzPPNEwfMgQeeSR7q5uyRWsbPUvKSpi/fj4A00dPb/E+c1fM5d2977a6626y/NUcb9lby9hzYA8YzTbC11Tf9enRh97de7Nt7zaqvbq2sb3Kq4LAhVFZXZn0mDUN5t2j3Tm619Hs+HgHew7sIRKJkB/NJy+Sx9C+Qzmi+xG8tfst9lbuZf/B/VTGK4GgYf3YPseyfW/QPfrwboez4+MdVMYra3uk9czrya79uxotOQJ0j3Sn2qtrOxUkrhtwWCdrgzCzKPAGcB5QDqwGprr7poRtbgBGuft1ZnYl8EV3vyIMEIvc/TMtPZ8CRO457TR48cXk67Kt4VokWzUVINI5F9M4YKu7v+nulcBvgPoz/V8C/DJ8/yhwjln9iZ5Fklu1CsaNS75uxgyYpUdMi7RLOgPEYCCx2bA8TEu6jbtXAbuB/uG64Wb2spn9xczOSHYCMys2szVmtqaioiK1uZdOoakgMXduUBUlIm2TrbO5bgOOdfcxwLeAX5vZEfU3cvd57l7k7kUDBzY+l790bU0FiWeegYKC1D+yVCQXpDNAvAMUJCwPCdOSbmNmeUAfYIe7H3D3HQDuvhb4K/DpNOZVOrlVq4JZXpMpLw8m/CssVKAQaY10BojVwAgzG25m3YErgafqbfMUcHX4/nLgz+7uZjYwbOTGzI4DRgBvpjGv0gUsXhxMvdGY9euDQHHVVR2XJ5HOLG0BImxT+DqwmKDL6iPuvtHM7jCzi8PNHgD6m9lWgqqkW8P0icAGM1tH0Hh9nbu3fq5lyTlz5gQ9mJqycCFEoypRiDRHI6mlSyopgauvhi1bmt+2Z0849VSYPVvjJyT3ZKqbq0jGxGLwxhtBaaJ376a3/eQTWLYsqH467DA480yVLERAAUK6uOJi2LMnaJvo3r357RODRbduQXBRwJBcpQAhOWHOHDhwoOWBAqCqCj766FDAyMsLXvn5MHx4MHGgSFemACE5pSZQ3H8/DB0aNFa3VHV18KqshNLSYLR2JBKUNLp1C4KHSh3SlShASE4qLg6+5KuqglJFnz7Bl31ruQfHqKoKgkeyUkdi8MjPr7tc816lEslG6sUkkmDWrKB08cknwZd9vG0PJmu3vLwg+LgHgSsSCfLiDjWzldW8T+U6CKrgolE4/HAYPz4IoOrd1XXpgUEibZQYMOLxQ69cU1O6qh9YIpHkgSweb3xduoNcrq0DGDAAvve9ts1grAAhkkIlJXDrrbB6ddAekfg/rntuBhDJDm2Z5l7jIERSKBaDv/wFPv44qIY6eDB41bRD3H8/DBp0qNdTXl5QZZOXF1TfJC7n5bWt7UMkmcceS+3x9E9TJMWKi2HbtkOBoyZ4HDwY9KBKXD54MAgqK1fCxInBqO7E4NFUYEn1utb06JLsdFlqnziqZ1KLZIOaUkmmlZTA0qXQv38wZ9VLLwXVaIn14NlYD5/L66B9bRBNURuEiEgOUxuEiIi0mgKEiIgkpQAhIiJJKUCIiEhSChAiIpKUAoSIiCTVZbq5mlkF8FY7DjEA+CBF2ekscu2ac+16QdecK9pzzUPdfWCyFV0mQLSXma1prC9wV5Vr15xr1wu65lyRrmtWFZOIiCSlACEiIkkpQBySi8/yyrVrzrXrBV1zrkjLNasNQkREklIJQkREklKAEBGRpHI+QJjZFDPbbGZbzezWTOcnVcyswMyeM7NNZrbRzG4K0480sz+Z2Zbwb78w3czs7vBz2GBmn83sFbSNmUXN7GUzWxQuDzezVeF1PWxm3cP0/HB5a7h+WCbz3R5m1tfMHjWz183sNTOLdeX7bGbfDP9Nv2pm/2tmPbrifTazB83sfTN7NSGt1ffVzK4Ot99iZle3Jg85HSDMLArcA1wAjASmmtnIzOYqZaqAf3b3kcB44Gvhtd0KPOvuI4Bnw2UIPoMR4asYuLfjs5wSNwGvJSzPAX7i7scDHwLXhunXAh+G6T8Jt+usfgb8n7ufCIwmuP4ueZ/NbDBwI1Dk7p8BosCVdM37/BAwpV5aq+6rmR0J3A6cBowDbq8JKi3i7jn7AmLA4oTl24DbMp2vNF3rk8B5wGbg6DDtaGBz+P5+YGrC9rXbdZYXMCT8n+ZsYBFgBKNL8+rfb2AxEAvf54XbWaavoQ3X3Af4W/28d9X7DAwGyoAjw/u2CDi/q95nYBjwalvvKzAVuD8hvc52zb1yugTBoX9sNcrDtC4lLFaPAVYBR7n7tnDVduCo8H1X+Cx+CswEwgcx0h/Y5e5V4XLiNdVeb7h+d7h9ZzMcqAD+J6xa+28zO5wuep/d/R3g34G3gW0E920tXf8+12jtfW3X/c71ANHlmVkv4DHgZnffk7jOg58UXaKfs5ldBLzv7msznZcOlgd8FrjX3ccA+zhU7QB0ufvcD7iEIDAeAxxOw2qYnNAR9zXXA8Q7QEHC8pAwrUsws24EwWGhuz8eJr9nZkeH648G3g/TO/tncTpwsZmVAr8hqGb6GdDXzPLCbRKvqfZ6w/V9gB0dmeEUKQfK3X1VuPwoQcDoqvf5XOBv7l7h7geBxwnufVe/zzVae1/bdb9zPUCsBkaEPSC6EzR2PZXhPKWEmRnwAPCau/9HwqqngJqeDFcTtE3UpE8Pe0OMB3YnFGWznrvf5u5D3H0YwX38s7tPA54DLg83q3+9NZ/D5eH2ne5XtrtvB8rM7IQw6RxgE130PhNULY03s8PCf+M119ul73OC1t7XxcBkM+sXlr4mh2ktk+lGmEy/gL8H3gD+Cnwn0/lJ4XV9jqD4uQFYF77+nqD+9VlgC7AEODLc3gh6dP0VeIWgl0jGr6ON1z4JWBS+Pw54EdgK/BbID9N7hMtbw/XHZTrf7bjeQmBNeK+fAPp15fsMfA94HXgV+BWQ3xXvM/C/BO0sBwlKite25b4C14TXvxX4amvyoKk2REQkqVyvYhIRkUYoQIiISFIKECIikpQChIiIJKUAISIiSSlAiDTDzKrNbF3CK2Wz/prZsMTZOkWySV7zm4jkvE/cvTDTmRDpaCpBiLSRmZWa2Vwze8XMXjSz48P0YWb253Be/mfN7Ngw/Sgz+52ZrQ9fE8JDRc3sF+EzDp4xs57h9jda8DyPDWb2mwxdpuQwBQiR5vWsV8V0RcK63e5+CvBfBLPJAvwn8Et3HwUsBO4O0+8G/uLuownmS9oYpo8A7nH3k4FdwGVh+q3AmPA416Xr4kQao5HUIs0ws4/cvVeS9FLgbHd/M5wYcbu79zezDwjm7D8Ypm9z9wFmVgEMcfcDCccYBvzJgwfAYGazgG7u/gMz+z/gI4LpM55w94/SfKkidagEIdI+3sj71jiQ8L6aQ22DFxLMr/NZYHXCbKUiHUIBQqR9rkj4WxK+X0kwoyzANGB5+P5Z4HqofXZ2n8YOamYRoMDdnwNmEUxT3aAUI5JO+kUi0ryeZrYuYfn/3L2mq2s/M9tAUAqYGqZ9g+AJb98meNrbV8P0m4B5ZnYtQUnheoLZOpOJAgvCIGLA3e6+K2VXJNICaoMQaaOwDaLI3T/IdF5E0kFVTCIikpRKECIikpRKECIikpQChIiIJKUAISIiSSlAiIhIUgoQIiKS1P8HrYwgzID8cWcAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.05574340932275785\n",
            "training error 0.12501991024728132, test error 0.25000397831628196\n",
            "training error 0.12510841608538098, test error 0.25006573048639114\n",
            "training error 0.12507894802058617, test error 0.25014835682937925\n",
            "training error 0.12500362452953967, test error 0.25013127042930977\n",
            "training error 0.1250588513061386, test error 0.25010860697832726\n",
            "training error 0.1250126010037951, test error 0.2502175682287461\n",
            "training error 0.12507872782684268, test error 0.25029360669775574\n",
            "training error 0.12499060712470773, test error 0.25028247152009314\n",
            "training error 0.12519072092062244, test error 0.25039437682113913\n",
            "training error 0.12496783933081675, test error 0.25027557361570807\n",
            "training error 0.1250423298649822, test error 0.25025441147511474\n",
            "training error 0.1249697030475004, test error 0.2503583679969363\n",
            "training error 0.1250810041697216, test error 0.2505032255262019\n",
            "training error 0.1250281982199141, test error 0.2503486235697473\n",
            "training error 0.12498564565110566, test error 0.25032752930841157\n",
            "training error 0.12502722225150956, test error 0.2502543179165019\n",
            "training error 0.12498190976272193, test error 0.2503200091195453\n",
            "training error 0.12505843822886503, test error 0.2503887683254119\n",
            "training error 0.1249895116522477, test error 0.2502871942952768\n",
            "training error 0.1251286263998892, test error 0.2501924819415737\n",
            "training error 0.1250339099282107, test error 0.25020550968559074\n",
            "training error 0.12498259272944626, test error 0.2502156254661666\n",
            "training error 0.12505795575350714, test error 0.2502601864815254\n",
            "training error 0.12499924608271211, test error 0.2502332039526163\n",
            "training error 0.12497511563014162, test error 0.2502663886638124\n",
            "training error 0.12508908040740158, test error 0.2501942465585182\n",
            "training error 0.12504347737712804, test error 0.2502624542834783\n",
            "training error 0.1251034057984221, test error 0.2503346818315376\n",
            "training error 0.12500119946212082, test error 0.25028255569497376\n",
            "training error 0.12499450753852516, test error 0.25036154466828137\n",
            "training error 0.1250590749560551, test error 0.25036247496481645\n",
            "training error 0.12502032773166516, test error 0.2503311708617428\n",
            "training error 0.12510043720096098, test error 0.2503471077123783\n",
            "training error 0.12505151410579346, test error 0.2503998599167126\n",
            "training error 0.1251148089403409, test error 0.25039639000158664\n",
            "training error 0.12503735708471628, test error 0.25046539444181837\n",
            "training error 0.12512343384826827, test error 0.25045955106772116\n",
            "training error 0.124971733274992, test error 0.2504986873614417\n",
            "training error 0.1249715822577752, test error 0.25054434109145224\n",
            "training error 0.12502630866440206, test error 0.25046575082356015\n",
            "training error 0.12496094560688947, test error 0.2504781560834242\n",
            "training error 0.12506584310953275, test error 0.25056743895398725\n",
            "training error 0.12501318503944037, test error 0.2506102557576279\n",
            "training error 0.125031012175926, test error 0.2505553042007212\n",
            "training error 0.1250087671451194, test error 0.25044660436272614\n",
            "training error 0.12499839409673728, test error 0.25042212393492097\n",
            "training error 0.1250220819169313, test error 0.2505081150542788\n",
            "training error 0.12496281856024234, test error 0.2504259079660799\n",
            "training error 0.12508387387412648, test error 0.2504670867645007\n",
            "training error 0.12500308550697345, test error 0.25036369578182066\n",
            "Loss: 0.14388469654016411\n",
            "training error 0.12513180110388594, test error 0.25031308009988634\n",
            "Loss: 0.12363874594560897\n",
            "training error 0.12513036389957433, test error 0.2503431244162039\n",
            "Loss: 0.13565628123439755\n",
            "training error 0.12498111648075351, test error 0.25040142844470525\n",
            "Loss: 0.15897752151787792\n",
            "training error 0.12496163562183411, test error 0.250384988436092\n",
            "Loss: 0.1524016227165914\n",
            "training error 0.12505688156193562, test error 0.25029836124996185\n",
            "Loss: 0.11775129966431486\n",
            "training error 0.12495762018848194, test error 0.25038687632845386\n",
            "Loss: 0.15315676764451336\n",
            "training error 0.1249569532467453, test error 0.2504296841374587\n",
            "Loss: 0.1702796187659672\n",
            "training error 0.12496424559067155, test error 0.2505148186108713\n",
            "Loss: 0.20433286623267044\n",
            "training error 0.12499336491911131, test error 0.2506058812983782\n",
            "Loss: 0.2407573616027614\n",
            "training error 0.12496554619965898, test error 0.25049635438593476\n",
            "Loss: 0.19694729378660902\n",
            "training error 0.12497478201190057, test error 0.2504475388972594\n",
            "Loss: 0.1774214090370485\n",
            "training error 0.12495322079469334, test error 0.25046667199792233\n",
            "Loss: 0.18507452751612696\n",
            "training error 0.12498708044016711, test error 0.25053953075935986\n",
            "Loss: 0.21421756833019945\n",
            "training error 0.12503228484020062, test error 0.25039340408000405\n",
            "Loss: 0.15576782671411493\n",
            "training error 0.1251492643172951, test error 0.2505113841274157\n",
            "Loss: 0.20295909471161888\n",
            "training error 0.12504576837691458, test error 0.2503449695176961\n",
            "Loss: 0.13639431008682923\n",
            "training error 0.12500279440332066, test error 0.25037571662935226\n",
            "Loss: 0.1486929590376329\n",
            "training error 0.12499856854243004, test error 0.250432620403529\n",
            "Loss: 0.17145410650416526\n",
            "training error 0.1250150897387759, test error 0.25036051004533794\n",
            "Loss: 0.14261042222492026\n",
            "training error 0.12498279166676368, test error 0.25037926650232817\n",
            "Loss: 0.15011288563233638\n",
            "training error 0.12497295088127572, test error 0.250464836318924\n",
            "Loss: 0.18434026760127953\n",
            "training error 0.12511132257755775, test error 0.25031819396961896\n",
            "Loss: 0.1256842612878284\n",
            "training error 0.12502153632244253, test error 0.25049411828477247\n",
            "Loss: 0.19605286755495221\n",
            "training error 0.12497975422462555, test error 0.2503661265490963\n",
            "Loss: 0.14485698797808144\n",
            "training error 0.12498715371999171, test error 0.25036394151704744\n",
            "Loss: 0.14398298906670437\n",
            "training error 0.12504387911082418, test error 0.25042554133977113\n",
            "Loss: 0.16862252606071326\n",
            "training error 0.12494862270283574, test error 0.2504927367962607\n",
            "Loss: 0.19550028094370564\n",
            "training error 0.12501988491298788, test error 0.2503959119556063\n",
            "Loss: 0.15677096099186993\n",
            "training error 0.1249495930381336, test error 0.2503972825740097\n",
            "Loss: 0.15731919962898022\n",
            "training error 0.12511634124830506, test error 0.2505121877642424\n",
            "Loss: 0.20328054432696696\n",
            "training error 0.12497933615486966, test error 0.2505086642522803\n",
            "Loss: 0.2018711619700131\n",
            "training error 0.12503191094367685, test error 0.2504308065742892\n",
            "Loss: 0.17072858635363009\n",
            "training error 0.12497417712051685, test error 0.25043387903249437\n",
            "Loss: 0.1719575500788828\n",
            "training error 0.12502771060343942, test error 0.2504687167361275\n",
            "Loss: 0.18589240978299504\n",
            "training error 0.12511022661019683, test error 0.2503827757170773\n",
            "Loss: 0.1515165491951187\n",
            "training error 0.1249666006001293, test error 0.25045525275557934\n",
            "Loss: 0.18050690326474328\n",
            "training error 0.12501172484526607, test error 0.2505624610384671\n",
            "Loss: 0.223389534017171\n",
            "training error 0.12495648168402164, test error 0.25061531794656144\n",
            "Loss: 0.2445319608098684\n",
            "training error 0.12509032477665602, test error 0.2505831170829002\n",
            "Loss: 0.23165182031046339\n",
            "training error 0.1250232268347592, test error 0.25077157023261365\n",
            "Loss: 0.3070318806529526\n",
            "training error 0.124978137455517, test error 0.25073168007443886\n",
            "Loss: 0.29107607129206237\n",
            "training error 0.12496269548799697, test error 0.25087637756757614\n",
            "Loss: 0.3489541475178104\n",
            "training error 0.1250533172405593, test error 0.2509093647593923\n",
            "Loss: 0.36214881427403256\n",
            "training error 0.12493817075636919, test error 0.2507670813756349\n",
            "Loss: 0.3052363664339586\n",
            "training error 0.12494250284570377, test error 0.25079266618525103\n",
            "Loss: 0.31547012742785263\n",
            "training error 0.1249716292385775, test error 0.2507265202980704\n",
            "Loss: 0.28901219358772323\n",
            "training error 0.12492875165772382, test error 0.2507369398801439\n",
            "Loss: 0.29317996009434744\n",
            "training error 0.1249677704402778, test error 0.2506692897995644\n",
            "Loss: 0.2661203584691352\n",
            "training error 0.1250100395247073, test error 0.25068134626046196\n",
            "Loss: 0.2709428660863322\n",
            "training error 0.12499626976764938, test error 0.2507686084798523\n",
            "Loss: 0.3058471984005662\n",
            "training error 0.125008974478799, test error 0.250604622833943\n",
            "Loss: 0.24025398383908936\n",
            "training error 0.12499919337143721, test error 0.25072331508979717\n",
            "Loss: 0.28773013068021935\n",
            "training error 0.12494246961276673, test error 0.2505891897121476\n",
            "Loss: 0.23408083335589858\n",
            "training error 0.12497064870404097, test error 0.2505723715644519\n",
            "Loss: 0.22735368132855704\n",
            "training error 0.12523398399560112, test error 0.2507251705650985\n",
            "Loss: 0.2884723089902952\n",
            "training error 0.12513407033505036, test error 0.25052073820000437\n",
            "Loss: 0.2067006642064939\n",
            "training error 0.12495965429542948, test error 0.25055908182873793\n",
            "Loss: 0.22203787163486144\n",
            "training error 0.12509285948267676, test error 0.2504475857946277\n",
            "Loss: 0.17744016768588278\n",
            "training error 0.12492980298739612, test error 0.2505468135178431\n",
            "Loss: 0.2171306253672345\n",
            "training error 0.12491618559823835, test error 0.2505405474447014\n",
            "Loss: 0.2146242359954087\n",
            "training error 0.12495572706666642, test error 0.2505051137386969\n",
            "Loss: 0.20045097913639065\n",
            "training error 0.12506355596971164, test error 0.2504882786046127\n",
            "Loss: 0.19371703266179985\n",
            "training error 0.12494016613277806, test error 0.25062766475382303\n",
            "Loss: 0.24947060512454655\n",
            "training error 0.12493280485806162, test error 0.2505797398905132\n",
            "Loss: 0.23030096485217033\n",
            "training error 0.12491198062427351, test error 0.2506135706378318\n",
            "Loss: 0.24383304843997866\n",
            "training error 0.12499759593327416, test error 0.25076313474706413\n",
            "Loss: 0.3036577401267504\n",
            "training error 0.12493398673821868, test error 0.25066482933829826\n",
            "Loss: 0.26433620235444266\n",
            "training error 0.1249427995820863, test error 0.2507238121871633\n",
            "Loss: 0.2879289664625473\n",
            "training error 0.12492264163064122, test error 0.25077952299635\n",
            "Loss: 0.3102129355265104\n",
            "training error 0.12508360364538162, test error 0.2506376736315217\n",
            "Loss: 0.2534740924954537\n",
            "training error 0.12490578951712414, test error 0.25077980198669886\n",
            "Loss: 0.31032452989023795\n",
            "training error 0.12493855138349316, test error 0.2507774553850122\n",
            "Loss: 0.3093859041521929\n",
            "training error 0.12491832106395456, test error 0.25069073514796536\n",
            "Loss: 0.2746983613254983\n",
            "training error 0.12504666556695382, test error 0.25078337828160424\n",
            "Loss: 0.3117550250885337\n",
            "training error 0.12490038074300767, test error 0.25063861830937\n",
            "Loss: 0.2538519576217091\n",
            "training error 0.12489827846471531, test error 0.2506868915856398\n",
            "Loss: 0.27316096086034136\n",
            "training error 0.12492425526734821, test error 0.25076648067619334\n",
            "Loss: 0.30499609048089127\n",
            "training error 0.1249391008058821, test error 0.2507872495983476\n",
            "Loss: 0.31330352714415977\n",
            "training error 0.1249527486868589, test error 0.2508354719567808\n",
            "Loss: 0.33259216357226506\n",
            "training error 0.12493154356940503, test error 0.25080360435083515\n",
            "Loss: 0.3198453240378374\n",
            "training error 0.12495357422603068, test error 0.25072267725563224\n",
            "Loss: 0.2874750010742\n",
            "training error 0.12491147128431035, test error 0.25053192277370046\n",
            "Loss: 0.21117442249283247\n",
            "training error 0.12494662590864991, test error 0.2505095643510217\n",
            "Loss: 0.2022311957372569\n",
            "training error 0.12494840823332179, test error 0.2504884299946079\n",
            "Loss: 0.1937775876962533\n",
            "training error 0.12493138776871629, test error 0.2505696195774062\n",
            "Loss: 0.22625290402724652\n",
            "training error 0.12492073408310277, test error 0.2507431693843642\n",
            "Loss: 0.29567172213038617\n",
            "training error 0.12487964962058631, test error 0.25069520637380177\n",
            "Loss: 0.27648682319980633\n",
            "training error 0.12499091966759389, test error 0.25067244753924584\n",
            "Loss: 0.26738343424206334\n",
            "training error 0.12485725874486876, test error 0.2507152017285268\n",
            "Loss: 0.2844848378152909\n",
            "training error 0.1249225895663867, test error 0.2506746405526881\n",
            "Loss: 0.26826062566001063\n",
            "training error 0.12487072587231404, test error 0.2506763972251909\n",
            "Loss: 0.2689632834795397\n",
            "training error 0.12487472161691295, test error 0.25065667665539637\n",
            "Loss: 0.2610751810872003\n",
            "training error 0.12486412577162678, test error 0.25065182737330904\n",
            "Loss: 0.2591354991189343\n",
            "training error 0.12489187211437817, test error 0.2505652824472207\n",
            "Loss: 0.22451807955976566\n",
            "training error 0.12489523599560803, test error 0.2505549412284811\n",
            "Loss: 0.22038165788791364\n",
            "training error 0.12483946919038563, test error 0.25061549762785496\n",
            "Loss: 0.2446038321835653\n",
            "training error 0.12493542605893425, test error 0.25060775062400886\n",
            "Loss: 0.24150507995639448\n",
            "training error 0.12486578801918445, test error 0.2505181436053856\n",
            "Loss: 0.2056628428741103\n",
            "training error 0.1248635311606285, test error 0.2504316909138903\n",
            "Loss: 0.17108231656506323\n",
            "training error 0.12486706104592336, test error 0.25045600623652103\n",
            "Loss: 0.18080829084536898\n",
            "training error 0.12486555753123368, test error 0.25053469406509044\n",
            "Loss: 0.21228292140897942\n",
            "training error 0.1248263648452352, test error 0.25056900250092556\n",
            "Loss: 0.22600607736280587\n",
            "training error 0.1248627574790206, test error 0.25058531362589476\n",
            "Loss: 0.2325304235268444\n",
            "training error 0.12489310752429708, test error 0.2507763487163849\n",
            "Loss: 0.3089432437454187\n",
            "training error 0.12483037912417098, test error 0.2506425075317403\n",
            "Loss: 0.2554076218141299\n",
            "training error 0.12477688071215293, test error 0.2506673991374807\n",
            "Loss: 0.26536410567012947\n",
            "training error 0.12482577341185279, test error 0.25066023383923663\n",
            "Loss: 0.26249803198108435\n",
            "training error 0.12481149776052075, test error 0.2507630739189234\n",
            "Loss: 0.30363340925763094\n",
            "training error 0.12478031056742257, test error 0.25079174117705216\n",
            "Loss: 0.3151001300361722\n",
            "training error 0.12476323064095254, test error 0.25074786941944993\n",
            "Loss: 0.2975517062479893\n",
            "training error 0.12475352026169448, test error 0.2507067617861149\n",
            "Loss: 0.2811089145724921\n",
            "training error 0.12479060028820041, test error 0.2507086209293012\n",
            "Loss: 0.2818525600131849\n",
            "training error 0.1247269507547576, test error 0.2506253993763246\n",
            "Loss: 0.24856446854477632\n",
            "training error 0.1247184455489617, test error 0.25055283259739103\n",
            "Loss: 0.21953821887374936\n",
            "training error 0.12480147211335973, test error 0.2505708169730245\n",
            "Loss: 0.22673185465291734\n",
            "training error 0.12469648435035832, test error 0.25056266786175874\n",
            "Loss: 0.2234722620173546\n",
            "training error 0.12484841117223312, test error 0.2504675086273159\n",
            "Loss: 0.18540917394824774\n",
            "training error 0.12494502839542933, test error 0.25063415437783876\n",
            "Loss: 0.2520664134230577\n",
            "training error 0.12470579101462738, test error 0.25040607807226806\n",
            "Loss: 0.16083734294716212\n",
            "training error 0.12472406259114865, test error 0.25041749283289116\n",
            "Loss: 0.1654031745391027\n",
            "training error 0.12466153443653259, test error 0.2504263868899086\n",
            "Loss: 0.16896074073358136\n",
            "training error 0.12465640394072916, test error 0.25041116711915645\n",
            "Loss: 0.16287292930969866\n",
            "training error 0.12461987574517228, test error 0.25033568662090316\n",
            "Loss: 0.13268121045719727\n",
            "training error 0.12462352603736636, test error 0.25027121983827005\n",
            "Loss: 0.10689490774822286\n",
            "training error 0.12460711733632052, test error 0.25027467491206973\n",
            "Loss: 0.10827691527584005\n",
            "training error 0.12459744113374947, test error 0.2502559082882173\n",
            "Loss: 0.10077038518827575\n",
            "training error 0.1245874496735811, test error 0.2502830622954225\n",
            "Loss: 0.11163181522955146\n",
            "training error 0.1245801384843829, test error 0.2502471107219782\n",
            "Loss: 0.09725141469094645\n",
            "training error 0.12453097260322145, test error 0.2501878597564025\n",
            "Loss: 0.07355140560521001\n",
            "training error 0.12457383762506642, test error 0.2500547667426563\n",
            "Loss: 0.020315047271002307\n",
            "training error 0.12453224801602811, test error 0.250113113588641\n",
            "Loss: 0.0436534142752798\n",
            "training error 0.12465244442390182, test error 0.25005955687255266\n",
            "Loss: 0.02223106873939784\n",
            "training error 0.12458380710269566, test error 0.25006945749010323\n",
            "Loss: 0.026191252740148663\n",
            "training error 0.1246913962880489, test error 0.24994875805734437\n",
            "Loss: 0.0\n",
            "training error 0.12445806887807145, test error 0.24995983853885034\n",
            "Loss: 0.004433101245271409\n",
            "training error 0.1243950565301217, test error 0.24988827873475902\n",
            "Loss: 0.0\n",
            "training error 0.12439537372147955, test error 0.2499765175690321\n",
            "Loss: 0.03531131380785091\n",
            "training error 0.12449073402250208, test error 0.24982993626881597\n",
            "Loss: 0.0\n",
            "training error 0.12435921921007732, test error 0.24981013674687905\n",
            "Loss: 0.0\n",
            "training error 0.12464095020443704, test error 0.24960206913151914\n",
            "Loss: 0.0\n",
            "training error 0.12467611763853367, test error 0.24970209144387687\n",
            "Loss: 0.04007270961565723\n",
            "training error 0.12428175169262747, test error 0.24950925701253163\n",
            "Loss: 0.0\n",
            "training error 0.12422878707484228, test error 0.2494143961809316\n",
            "Loss: 0.0\n",
            "training error 0.12421564364715083, test error 0.24939773627818265\n",
            "Loss: 0.0\n",
            "training error 0.12426493908518454, test error 0.24940633570014922\n",
            "Loss: 0.003448075389500538\n",
            "training error 0.12412616526122176, test error 0.24918092711520656\n",
            "Loss: 0.0\n",
            "training error 0.12414277611894682, test error 0.249083467070512\n",
            "Loss: 0.0\n",
            "training error 0.12409036965658267, test error 0.24911292650815495\n",
            "Loss: 0.01182713489153997\n",
            "training error 0.12399589144437956, test error 0.24913719360674091\n",
            "Loss: 0.02156969182289803\n",
            "training error 0.12397551072572467, test error 0.24898556751345097\n",
            "Loss: 0.0\n",
            "training error 0.1239289323453448, test error 0.24897964328971992\n",
            "Loss: 0.0\n",
            "training error 0.12389308009035535, test error 0.24886308293600193\n",
            "Loss: 0.0\n",
            "training error 0.12384702996971203, test error 0.24886127568826794\n",
            "Loss: 0.0\n",
            "training error 0.1238751304262453, test error 0.2488548154964409\n",
            "Loss: 0.0\n",
            "training error 0.12372390473789488, test error 0.24867100713610985\n",
            "Loss: 0.0\n",
            "training error 0.12369157436663833, test error 0.24853665147906445\n",
            "Loss: 0.0\n",
            "training error 0.12369351529745697, test error 0.24861035214627114\n",
            "Loss: 0.029653842508969852\n",
            "training error 0.12355043386077984, test error 0.24839494883901905\n",
            "Loss: 0.0\n",
            "training error 0.12362537690135221, test error 0.24822338209608386\n",
            "Loss: 0.0\n",
            "training error 0.12345122445495481, test error 0.24812897577967463\n",
            "Loss: 0.0\n",
            "training error 0.123517426553974, test error 0.2479108819283046\n",
            "Loss: 0.0\n",
            "training error 0.12335073180561636, test error 0.24775027140007977\n",
            "Loss: 0.0\n",
            "training error 0.12320015262020127, test error 0.24773638922592112\n",
            "Loss: 0.0\n",
            "training error 0.12313302160357747, test error 0.24755815827364358\n",
            "Loss: 0.0\n",
            "training error 0.12314395537183805, test error 0.24748745831185684\n",
            "Loss: 0.0\n",
            "training error 0.12307281214903838, test error 0.24708997627514195\n",
            "Loss: 0.0\n",
            "training error 0.12292910281005259, test error 0.24698668749692615\n",
            "Loss: 0.0\n",
            "training error 0.12277248962792708, test error 0.2467664941995625\n",
            "Loss: 0.0\n",
            "training error 0.12270228813462963, test error 0.24662011671927567\n",
            "Loss: 0.0\n",
            "training error 0.12259099911075208, test error 0.2463926071702011\n",
            "Loss: 0.0\n",
            "training error 0.12247362652311251, test error 0.24610364907343937\n",
            "Loss: 0.0\n",
            "training error 0.1224446131646086, test error 0.24595192086840614\n",
            "Loss: 0.0\n",
            "training error 0.12225067449516863, test error 0.24561162829010097\n",
            "Loss: 0.0\n",
            "training error 0.12215489708327552, test error 0.24531695566944467\n",
            "Loss: 0.0\n",
            "training error 0.12204978409674318, test error 0.24516257276695066\n",
            "Loss: 0.0\n",
            "training error 0.12206587924021992, test error 0.2448421019227875\n",
            "Loss: 0.0\n",
            "training error 0.12185091159817403, test error 0.2446841855362065\n",
            "Loss: 0.0\n",
            "training error 0.12167882636786048, test error 0.2442673565634321\n",
            "Loss: 0.0\n",
            "training error 0.12143692929196409, test error 0.24406044987162714\n",
            "Loss: 0.0\n",
            "training error 0.12128069826047898, test error 0.2437106055350316\n",
            "Loss: 0.0\n",
            "training error 0.12112106105298417, test error 0.24333819730957276\n",
            "Loss: 0.0\n",
            "training error 0.12097933347478809, test error 0.24296185538103257\n",
            "Loss: 0.0\n",
            "training error 0.12084079052949352, test error 0.2427081873255028\n",
            "Loss: 0.0\n",
            "training error 0.12064182871422871, test error 0.2423623296754157\n",
            "Loss: 0.0\n",
            "training error 0.12041747833706803, test error 0.24197406387330397\n",
            "Loss: 0.0\n",
            "training error 0.1203176111047659, test error 0.24150401035323218\n",
            "Loss: 0.0\n",
            "training error 0.1200942877219714, test error 0.24118948427246834\n",
            "Loss: 0.0\n",
            "training error 0.11989569777130997, test error 0.2406256833882831\n",
            "Loss: 0.0\n",
            "training error 0.11958533052493875, test error 0.24028443525274704\n",
            "Loss: 0.0\n",
            "training error 0.11940732492299261, test error 0.23983839287501246\n",
            "Loss: 0.0\n",
            "training error 0.11915128917392655, test error 0.2394017029028798\n",
            "Loss: 0.0\n",
            "training error 0.11893187733884804, test error 0.2388178765194512\n",
            "Loss: 0.0\n",
            "training error 0.1186501113470489, test error 0.23827127740600032\n",
            "Loss: 0.0\n",
            "training error 0.11834100733613517, test error 0.23770350027834153\n",
            "Loss: 0.0\n",
            "training error 0.11806602977161133, test error 0.23712284569733108\n",
            "Loss: 0.0\n",
            "training error 0.11780517061278073, test error 0.23642838105698682\n",
            "Loss: 0.0\n",
            "training error 0.1174784716285552, test error 0.2358293722427961\n",
            "Loss: 0.0\n",
            "training error 0.11722339254588784, test error 0.23525603789103489\n",
            "Loss: 0.0\n",
            "training error 0.11699598217423911, test error 0.23466826087457396\n",
            "Loss: 0.0\n",
            "training error 0.11649116768611044, test error 0.2338885270732701\n",
            "Loss: 0.0\n",
            "training error 0.1161756584195383, test error 0.2331217669762613\n",
            "Loss: 0.0\n",
            "training error 0.1158060838250102, test error 0.23250450782709733\n",
            "Loss: 0.0\n",
            "training error 0.11541628526785425, test error 0.23163460610301426\n",
            "Loss: 0.0\n",
            "training error 0.11505561099744162, test error 0.2307893996252107\n",
            "Loss: 0.0\n",
            "training error 0.11464595364165735, test error 0.22996756872433377\n",
            "Loss: 0.0\n",
            "training error 0.11430883482091851, test error 0.2290877618742074\n",
            "Loss: 0.0\n",
            "training error 0.11381595377072214, test error 0.22824440847783958\n",
            "Loss: 0.0\n",
            "training error 0.1133841152702594, test error 0.22728671423625763\n",
            "Loss: 0.0\n",
            "training error 0.11296629578449266, test error 0.2263751520329181\n",
            "Loss: 0.0\n",
            "training error 0.11265445256736768, test error 0.2255255899204465\n",
            "Loss: 0.0\n",
            "training error 0.11206964840241464, test error 0.2243821924379056\n",
            "Loss: 0.0\n",
            "training error 0.11154528417182583, test error 0.223242570708172\n",
            "Loss: 0.0\n",
            "training error 0.11102551957211956, test error 0.2222732156845308\n",
            "Loss: 0.0\n",
            "training error 0.11063848882662966, test error 0.2212462464867419\n",
            "Loss: 0.0\n",
            "training error 0.11019601307075341, test error 0.21993351135698963\n",
            "Loss: 0.0\n",
            "training error 0.10957952264374798, test error 0.2189641346026305\n",
            "Loss: 0.0\n",
            "training error 0.10886244274182552, test error 0.21776011112520394\n",
            "Loss: 0.0\n",
            "training error 0.1082900352636586, test error 0.21666400158917667\n",
            "Loss: 0.0\n",
            "training error 0.10796011376421445, test error 0.21540252643173513\n",
            "Loss: 0.0\n",
            "training error 0.10714111739333473, test error 0.2140123081671294\n",
            "Loss: 0.0\n",
            "training error 0.10650448116686084, test error 0.21262036485999547\n",
            "Loss: 0.0\n",
            "training error 0.1058533606066208, test error 0.21138306721205893\n",
            "Loss: 0.0\n",
            "training error 0.10524663184826254, test error 0.21023100970737002\n",
            "Loss: 0.0\n",
            "training error 0.10460414818439456, test error 0.2088112674803475\n",
            "Loss: 0.0\n",
            "training error 0.10385105063323807, test error 0.2074025851738137\n",
            "Loss: 0.0\n",
            "training error 0.10317464353676152, test error 0.20596716591160616\n",
            "Loss: 0.0\n",
            "training error 0.10248786292283912, test error 0.20455753485828174\n",
            "Loss: 0.0\n",
            "training error 0.10181145779143293, test error 0.20313583444613448\n",
            "Loss: 0.0\n",
            "training error 0.10117186742348491, test error 0.2015635182110084\n",
            "Loss: 0.0\n",
            "training error 0.10040170410226125, test error 0.20007140234929432\n",
            "Loss: 0.0\n",
            "training error 0.09984390405582486, test error 0.19850926729520552\n",
            "Loss: 0.0\n",
            "training error 0.09894583402775392, test error 0.1970527278267316\n",
            "Loss: 0.0\n",
            "training error 0.09823394662352314, test error 0.1954262045453276\n",
            "Loss: 0.0\n",
            "training error 0.0975604290469109, test error 0.19398563693861634\n",
            "Loss: 0.0\n",
            "training error 0.09667705535713889, test error 0.1921880416820449\n",
            "Loss: 0.0\n",
            "training error 0.09597371753267232, test error 0.19064746206946132\n",
            "Loss: 0.0\n",
            "training error 0.09511453289878555, test error 0.18897978533266419\n",
            "Loss: 0.0\n",
            "training error 0.0943651611071057, test error 0.18728550199995575\n",
            "Loss: 0.0\n",
            "training error 0.0935924453845019, test error 0.18567686637102676\n",
            "Loss: 0.0\n",
            "training error 0.0929112004386499, test error 0.1841584754215395\n",
            "Loss: 0.0\n",
            "training error 0.09201658290808824, test error 0.1823291546254191\n",
            "Loss: 0.0\n",
            "training error 0.09119202901012394, test error 0.1807150228240914\n",
            "Loss: 0.0\n",
            "training error 0.09060890955771062, test error 0.1789768390887386\n",
            "Loss: 0.0\n",
            "training error 0.08960849221000004, test error 0.17728092771482037\n",
            "Loss: 0.0\n",
            "training error 0.08881747564334981, test error 0.1755693685471356\n",
            "Loss: 0.0\n",
            "training error 0.08810098579763309, test error 0.17380557914260103\n",
            "Loss: 0.0\n",
            "training error 0.08726298098103528, test error 0.17215902362391858\n",
            "Loss: 0.0\n",
            "training error 0.08645149142313999, test error 0.1705219822449892\n",
            "Loss: 0.0\n",
            "training error 0.085625873399177, test error 0.1688467183097965\n",
            "Loss: 0.0\n",
            "training error 0.0848383963573387, test error 0.16700235469708033\n",
            "Loss: 0.0\n",
            "training error 0.08414324315519468, test error 0.16529960758804346\n",
            "Loss: 0.0\n",
            "training error 0.08340209732803106, test error 0.16373855507282836\n",
            "Loss: 0.0\n",
            "training error 0.08248255407244608, test error 0.16222663688239428\n",
            "Loss: 0.0\n",
            "training error 0.08180235972323341, test error 0.16040587235223078\n",
            "Loss: 0.0\n",
            "training error 0.08094482536986201, test error 0.15886541614491675\n",
            "Loss: 0.0\n",
            "training error 0.08016699942393385, test error 0.15704365597476852\n",
            "Loss: 0.0\n",
            "training error 0.07939295487219768, test error 0.1554551593555678\n",
            "Loss: 0.0\n",
            "training error 0.07869532409674575, test error 0.15384345340490682\n",
            "Loss: 0.0\n",
            "training error 0.07790780323883364, test error 0.1522829307751458\n",
            "Loss: 0.0\n",
            "training error 0.0771202620776255, test error 0.15055874361456104\n",
            "Loss: 0.0\n",
            "training error 0.0763779079769533, test error 0.14889405479942433\n",
            "Loss: 0.0\n",
            "training error 0.0757269696369773, test error 0.14716702460376788\n",
            "Loss: 0.0\n",
            "training error 0.07494385619149663, test error 0.14562973409676536\n",
            "Loss: 0.0\n",
            "training error 0.0742189343937024, test error 0.1441415760542219\n",
            "Loss: 0.0\n",
            "training error 0.0735594782780632, test error 0.14261347051873835\n",
            "Loss: 0.0\n",
            "training error 0.07279822591216996, test error 0.14094431861918796\n",
            "Loss: 0.0\n",
            "training error 0.07208364953254874, test error 0.13948509803664388\n",
            "Loss: 0.0\n",
            "training error 0.0713951304800963, test error 0.1380073767203889\n",
            "Loss: 0.0\n",
            "training error 0.07091661927828062, test error 0.13646354800208801\n",
            "Loss: 0.0\n",
            "training error 0.07009543787650883, test error 0.1351824209192109\n",
            "Loss: 0.0\n",
            "training error 0.06947723425371691, test error 0.13374461131014834\n",
            "Loss: 0.0\n",
            "training error 0.0688039366173643, test error 0.13223667826477473\n",
            "Loss: 0.0\n",
            "training error 0.06817300105594568, test error 0.13080714520994519\n",
            "Loss: 0.0\n",
            "training error 0.06752900899209822, test error 0.1295986114672208\n",
            "Loss: 0.0\n",
            "training error 0.06695940272205471, test error 0.12828855984789392\n",
            "Loss: 0.0\n",
            "training error 0.06630674176418132, test error 0.12689960080503287\n",
            "Loss: 0.0\n",
            "training error 0.06567846917010159, test error 0.1255274863270117\n",
            "Loss: 0.0\n",
            "training error 0.06515230314716612, test error 0.12429266526512384\n",
            "Loss: 0.0\n",
            "training error 0.06452803341223239, test error 0.12298214480486062\n",
            "Loss: 0.0\n",
            "training error 0.06401062519097395, test error 0.12187511259288895\n",
            "Loss: 0.0\n",
            "training error 0.063446978624463, test error 0.12051496715610993\n",
            "Loss: 0.0\n",
            "training error 0.06292281239043486, test error 0.11945251130163934\n",
            "Loss: 0.0\n",
            "training error 0.06231469459573829, test error 0.11825279210248597\n",
            "Loss: 0.0\n",
            "training error 0.061790575761252374, test error 0.11709588312121888\n",
            "Loss: 0.0\n",
            "training error 0.061315559061761844, test error 0.11578983235222494\n",
            "Loss: 0.0\n",
            "training error 0.060728104016908445, test error 0.11469621519471025\n",
            "Loss: 0.0\n",
            "training error 0.06032724977859771, test error 0.11357319108558489\n",
            "Loss: 0.0\n",
            "training error 0.05984185183195641, test error 0.11253292524767687\n",
            "Loss: 0.0\n",
            "training error 0.059286792911175105, test error 0.11120168235502266\n",
            "Loss: 0.0\n",
            "training error 0.05877152958096741, test error 0.10994401410124459\n",
            "Loss: 0.0\n",
            "training error 0.05831886195692767, test error 0.10882518408897052\n",
            "Loss: 0.0\n",
            "training error 0.05795906587103617, test error 0.10769994877849032\n",
            "Loss: 0.0\n",
            "training error 0.05746437251069591, test error 0.10698062462788097\n",
            "Loss: 0.0\n",
            "training error 0.05705120614294697, test error 0.10588910026413877\n",
            "Loss: 0.0\n",
            "training error 0.05660393359433603, test error 0.10484090315193446\n",
            "Loss: 0.0\n",
            "training error 0.056118447143334985, test error 0.10391434897596137\n",
            "Loss: 0.0\n",
            "training error 0.055694520422624134, test error 0.10301826308197012\n",
            "Loss: 0.0\n",
            "training error 0.055313141342560074, test error 0.10214136300795185\n",
            "Loss: 0.0\n",
            "training error 0.054934509273922745, test error 0.10113269840013804\n",
            "Loss: 0.0\n",
            "training error 0.05452143001119202, test error 0.10021956179788503\n",
            "Loss: 0.0\n",
            "training error 0.05414137755082294, test error 0.09932098373322354\n",
            "Loss: 0.0\n",
            "training error 0.05387652544313601, test error 0.09848429279400941\n",
            "Loss: 0.0\n",
            "training error 0.05338466964367062, test error 0.097620363465773\n",
            "Loss: 0.0\n",
            "training error 0.05312017386749694, test error 0.09673778696042291\n",
            "Loss: 0.0\n",
            "training error 0.05268081409510134, test error 0.09589595872395353\n",
            "Loss: 0.0\n",
            "training error 0.05236528422944718, test error 0.095359611113745\n",
            "Loss: 0.0\n",
            "training error 0.05214672308474068, test error 0.09458003971281545\n",
            "Loss: 0.0\n",
            "training error 0.051742028401858715, test error 0.09383316795060159\n",
            "Loss: 0.0\n",
            "training error 0.05137930640829673, test error 0.09306459005204437\n",
            "Loss: 0.0\n",
            "training error 0.051086644799017464, test error 0.09233376867247362\n",
            "Loss: 0.0\n",
            "training error 0.05075963090990537, test error 0.09150187506789904\n",
            "Loss: 0.0\n",
            "training error 0.05041298172031806, test error 0.09089542095483921\n",
            "Loss: 0.0\n",
            "training error 0.05006945325663372, test error 0.09006756967056292\n",
            "Loss: 0.0\n",
            "training error 0.04976441767962412, test error 0.08940094579663822\n",
            "Loss: 0.0\n",
            "training error 0.04949355761200404, test error 0.08884712678268614\n",
            "Loss: 0.0\n",
            "training error 0.04923733139533097, test error 0.08804874684985343\n",
            "Loss: 0.0\n",
            "training error 0.048949981973218384, test error 0.08746565000501662\n",
            "Loss: 0.0\n",
            "training error 0.04874190704425118, test error 0.08707473231475443\n",
            "Loss: 0.0\n",
            "training error 0.04843095769153714, test error 0.08639127516078766\n",
            "Loss: 0.0\n",
            "training error 0.04817912546191379, test error 0.08564595486623269\n",
            "Loss: 0.0\n",
            "training error 0.047924731507483116, test error 0.08508851854585582\n",
            "Loss: 0.0\n",
            "training error 0.04764572330390772, test error 0.08446408853620613\n",
            "Loss: 0.0\n",
            "training error 0.04740719079511983, test error 0.08377004198210242\n",
            "Loss: 0.0\n",
            "training error 0.04720766418300413, test error 0.08298866588997365\n",
            "Loss: 0.0\n",
            "training error 0.04692766550366663, test error 0.08256662683367984\n",
            "Loss: 0.0\n",
            "training error 0.04671543305812449, test error 0.08207090953516061\n",
            "Loss: 0.0\n",
            "training error 0.0465458126203118, test error 0.08168939785871351\n",
            "Loss: 0.0\n",
            "training error 0.04625414758180735, test error 0.08095440848350305\n",
            "Loss: 0.0\n",
            "training error 0.046109920330558225, test error 0.08049539339167087\n",
            "Loss: 0.0\n",
            "training error 0.04593550765623055, test error 0.08007472382803078\n",
            "Loss: 0.0\n",
            "training error 0.04563890928020869, test error 0.0794251246032032\n",
            "Loss: 0.0\n",
            "training error 0.045679081842095313, test error 0.07885202643470461\n",
            "Loss: 0.0\n",
            "training error 0.045270288655406964, test error 0.07866304074778098\n",
            "Loss: 0.0\n",
            "training error 0.045128206045901016, test error 0.07833514157452887\n",
            "Loss: 0.0\n",
            "training error 0.04495798229882977, test error 0.07790118284116496\n",
            "Loss: 0.0\n",
            "training error 0.044745974295185904, test error 0.07753369583302967\n",
            "Loss: 0.0\n",
            "training error 0.04463427723734985, test error 0.0772119235866807\n",
            "Loss: 0.0\n",
            "training error 0.044362830355931374, test error 0.07665072305319348\n",
            "Loss: 0.0\n",
            "training error 0.044212282254415564, test error 0.07618403898853653\n",
            "Loss: 0.0\n",
            "training error 0.043995617163732695, test error 0.07576275448314244\n",
            "Loss: 0.0\n",
            "training error 0.04391665040327281, test error 0.0753341781347648\n",
            "Loss: 0.0\n",
            "training error 0.04367576621248237, test error 0.0751399713318437\n",
            "Loss: 0.0\n",
            "training error 0.04354757326086232, test error 0.07467655114194964\n",
            "Loss: 0.0\n",
            "training error 0.043370245252508106, test error 0.07448521145044089\n",
            "Loss: 0.0\n",
            "training error 0.04321947486500834, test error 0.07396874117934156\n",
            "Loss: 0.0\n",
            "training error 0.04306525096022209, test error 0.07359881401270474\n",
            "Loss: 0.0\n",
            "training error 0.042957534827214955, test error 0.07335765826876553\n",
            "Loss: 0.0\n",
            "training error 0.04276867410307086, test error 0.07286161826078015\n",
            "Loss: 0.0\n",
            "training error 0.043084365381205365, test error 0.0727784317616252\n",
            "Loss: 0.0\n",
            "training error 0.042538830421094714, test error 0.07225022752825752\n",
            "Loss: 0.0\n",
            "training error 0.04242890628347983, test error 0.07170386620394752\n",
            "Loss: 0.0\n",
            "training error 0.042238489145301136, test error 0.07137195733977529\n",
            "Loss: 0.0\n",
            "training error 0.04211929369692305, test error 0.07107410712142272\n",
            "Loss: 0.0\n",
            "training error 0.042038409367867145, test error 0.07092231962484613\n",
            "Loss: 0.0\n",
            "training error 0.04188654814608455, test error 0.07046593086726952\n",
            "Loss: 0.0\n",
            "training error 0.04175387423461578, test error 0.0702659097371304\n",
            "Loss: 0.0\n",
            "training error 0.04169622463443992, test error 0.0698908655420914\n",
            "Loss: 0.0\n",
            "training error 0.04153262435757109, test error 0.06975084126230488\n",
            "Loss: 0.0\n",
            "training error 0.041392474248530875, test error 0.06949066795234107\n",
            "Loss: 0.0\n",
            "training error 0.041337639411470846, test error 0.06942369997795719\n",
            "Loss: 0.0\n",
            "training error 0.04120963310241644, test error 0.06891756297013592\n",
            "Loss: 0.0\n",
            "training error 0.04114273609459063, test error 0.0686802374054063\n",
            "Loss: 0.0\n",
            "training error 0.04102983705158582, test error 0.06831223364202849\n",
            "Loss: 0.0\n",
            "training error 0.04088730865349713, test error 0.06816012133840267\n",
            "Loss: 0.0\n",
            "training error 0.04081240194998737, test error 0.06782752336084487\n",
            "Loss: 0.0\n",
            "training error 0.04069147306700373, test error 0.06762945831407334\n",
            "Loss: 0.0\n",
            "training error 0.040616146193931946, test error 0.06739386308660827\n",
            "Loss: 0.0\n",
            "training error 0.04048950074421023, test error 0.06725734230120149\n",
            "Loss: 0.0\n",
            "training error 0.04040583916874801, test error 0.06704924310933347\n",
            "Loss: 0.0\n",
            "training error 0.040312375172625946, test error 0.0668558865884094\n",
            "Loss: 0.0\n",
            "training error 0.040242210802222095, test error 0.06665752520695767\n",
            "Loss: 0.0\n",
            "training error 0.04024911778740634, test error 0.06661880284402205\n",
            "Loss: 0.0\n",
            "training error 0.040050644025971006, test error 0.06634702843063996\n",
            "Loss: 0.0\n",
            "training error 0.04005725219451071, test error 0.06612194611314676\n",
            "Loss: 0.0\n",
            "training error 0.03991637173121221, test error 0.06592338431900896\n",
            "Loss: 0.0\n",
            "training error 0.03983011982876966, test error 0.06574230794533664\n",
            "Loss: 0.0\n",
            "training error 0.039729412677990246, test error 0.06550015859671496\n",
            "Loss: 0.0\n",
            "training error 0.0396753422531878, test error 0.06545233322550494\n",
            "Loss: 0.0\n",
            "training error 0.039583125109579134, test error 0.06534378746359255\n",
            "Loss: 0.0\n",
            "training error 0.0395195773635733, test error 0.0651535916063049\n",
            "Loss: 0.0\n",
            "training error 0.03947004469978945, test error 0.06495947205645812\n",
            "Loss: 0.0\n",
            "training error 0.039372917527721346, test error 0.0648506234869351\n",
            "Loss: 0.0\n",
            "training error 0.03934881416071823, test error 0.06459902561830268\n",
            "Loss: 0.0\n",
            "training error 0.03925322105441593, test error 0.06457313616995312\n",
            "Loss: 0.0\n",
            "training error 0.039159688886899094, test error 0.06435301555466394\n",
            "Loss: 0.0\n",
            "training error 0.03908494430035924, test error 0.0641681286297335\n",
            "Loss: 0.0\n",
            "training error 0.039096012484223, test error 0.06408320209055397\n",
            "Loss: 0.0\n",
            "training error 0.038997942506609325, test error 0.06395552202977295\n",
            "Loss: 0.0\n",
            "training error 0.03890349454851557, test error 0.06366842045772882\n",
            "Loss: 0.0\n",
            "training error 0.03885364871643592, test error 0.06357974067512454\n",
            "Loss: 0.0\n",
            "training error 0.0388113925046556, test error 0.06359458602480762\n",
            "Loss: 0.02334918249973228\n",
            "training error 0.03879388175735657, test error 0.06320339850214785\n",
            "Loss: 0.0\n",
            "training error 0.038698757246776336, test error 0.06334517917626951\n",
            "Loss: 0.22432444691538045\n",
            "training error 0.03859727567956416, test error 0.06291242610381449\n",
            "Loss: 0.0\n",
            "training error 0.03853742400314023, test error 0.06268730304782767\n",
            "Loss: 0.0\n",
            "training error 0.038494594958927146, test error 0.0625554551462731\n",
            "Loss: 0.0\n",
            "training error 0.03843216750424928, test error 0.062423818914213405\n",
            "Loss: 0.0\n",
            "training error 0.03836970916720621, test error 0.06242260232979377\n",
            "Loss: 0.0\n",
            "training error 0.038341582462053374, test error 0.06230763391438461\n",
            "Loss: 0.0\n",
            "training error 0.038272809938983944, test error 0.06213546929002026\n",
            "Loss: 0.0\n",
            "training error 0.038200188236760586, test error 0.06210818104189798\n",
            "Loss: 0.0\n",
            "training error 0.03821461919049717, test error 0.061979504009285896\n",
            "Loss: 0.0\n",
            "training error 0.03811842115980894, test error 0.06203392850861789\n",
            "Loss: 0.0878104789671097\n",
            "training error 0.0380847652951125, test error 0.06175638693570186\n",
            "Loss: 0.0\n",
            "training error 0.03799078413716981, test error 0.061549280458278004\n",
            "Loss: 0.0\n",
            "training error 0.03793550110110499, test error 0.06147236859815317\n",
            "Loss: 0.0\n",
            "training error 0.03792786153399654, test error 0.061341852440606505\n",
            "Loss: 0.0\n",
            "training error 0.03789575191209526, test error 0.061415142416839125\n",
            "Loss: 0.11947793116222627\n",
            "training error 0.03782755710542053, test error 0.06120383626411092\n",
            "Loss: 0.0\n",
            "training error 0.03778586507275564, test error 0.06121677932536334\n",
            "Loss: 0.021147467287119248\n",
            "training error 0.03771837463769641, test error 0.061043513378634034\n",
            "Loss: 0.0\n",
            "training error 0.037681414861639145, test error 0.0610138568264874\n",
            "Loss: 0.0\n",
            "training error 0.037637844269431196, test error 0.06091315201024736\n",
            "Loss: 0.0\n",
            "training error 0.03761367774297506, test error 0.06085059396791478\n",
            "Loss: 0.0\n",
            "training error 0.03757075721590797, test error 0.06079515790368757\n",
            "Loss: 0.0\n",
            "training error 0.03754880938627481, test error 0.06087480863694008\n",
            "Loss: 0.1310149294762697\n",
            "training error 0.037488172515545246, test error 0.0606309533688694\n",
            "Loss: 0.0\n",
            "training error 0.037450701912564464, test error 0.06047712424953399\n",
            "Loss: 0.0\n",
            "training error 0.037423733000459126, test error 0.060437681975014305\n",
            "Loss: 0.0\n",
            "training error 0.037358292183490745, test error 0.06037407265778683\n",
            "Loss: 0.0\n",
            "training error 0.037339471564700744, test error 0.06019859620585295\n",
            "Loss: 0.0\n",
            "training error 0.037306192837371646, test error 0.060252231902072996\n",
            "Loss: 0.08909791855715721\n",
            "training error 0.03726875428860271, test error 0.06005966112360553\n",
            "Loss: 0.0\n",
            "training error 0.03720992580506234, test error 0.06003969653300823\n",
            "Loss: 0.0\n",
            "training error 0.03724853442541067, test error 0.060012207498586385\n",
            "Loss: 0.0\n",
            "training error 0.037175506922427506, test error 0.06004710101728562\n",
            "Loss: 0.05814403461172901\n",
            "training error 0.03713213776503572, test error 0.05986013815332605\n",
            "Loss: 0.0\n",
            "training error 0.037135182974532194, test error 0.05988130797024747\n",
            "Loss: 0.03536546619253844\n",
            "training error 0.03707325340245967, test error 0.059796466290458546\n",
            "Loss: 0.0\n",
            "training error 0.03702952524847131, test error 0.05963125737098128\n",
            "Loss: 0.0\n",
            "training error 0.03698748521703894, test error 0.0595178874675826\n",
            "Loss: 0.0\n",
            "training error 0.03699127860118384, test error 0.05941664698605913\n",
            "Loss: 0.0\n",
            "training error 0.0369256362252401, test error 0.05941346182198107\n",
            "Loss: 0.0\n",
            "training error 0.03690092371326636, test error 0.05931580916498453\n",
            "Loss: 0.0\n",
            "training error 0.03691485345045269, test error 0.059393270791204864\n",
            "Loss: 0.13059187307868925\n",
            "training error 0.036911179640958046, test error 0.05934485172729639\n",
            "Loss: 0.048962599888136005\n",
            "training error 0.03680376296087967, test error 0.05908233520249371\n",
            "Loss: 0.0\n",
            "training error 0.03679925278172685, test error 0.05901306092487038\n",
            "Loss: 0.0\n",
            "training error 0.03678250445956567, test error 0.058964386517021475\n",
            "Loss: 0.0\n",
            "training error 0.036715472544922634, test error 0.058965960305608196\n",
            "Loss: 0.002669049369763421\n",
            "training error 0.036704234530623076, test error 0.05885429504686448\n",
            "Loss: 0.0\n",
            "training error 0.036703335720108345, test error 0.05884933463902902\n",
            "Loss: 0.0\n",
            "training error 0.03673245490200485, test error 0.05870503794465743\n",
            "Loss: 0.0\n",
            "training error 0.03664133794288089, test error 0.058597613303595766\n",
            "Loss: 0.0\n",
            "training error 0.036631704099112476, test error 0.05860698408759344\n",
            "Loss: 0.015991750293853357\n",
            "training error 0.036545107935390696, test error 0.05865277970299934\n",
            "Loss: 0.09414444768893748\n",
            "training error 0.03654879046324834, test error 0.058497044477727723\n",
            "Loss: 0.0\n",
            "training error 0.0365315767981068, test error 0.05865247729037442\n",
            "Loss: 0.26571053979638215\n",
            "training error 0.036502836399826745, test error 0.0586694670091023\n",
            "Loss: 0.2947542613716658\n",
            "training error 0.036481758732710035, test error 0.05853559979963084\n",
            "Loss: 0.06590986304924051\n",
            "training error 0.036430713727165914, test error 0.05851180492259971\n",
            "Loss: 0.02523280450110743\n",
            "training error 0.03642498779808236, test error 0.05840036574276193\n",
            "Loss: 0.0\n",
            "training error 0.03640846417085097, test error 0.058381730508453446\n",
            "Loss: 0.0\n",
            "training error 0.0363752261757871, test error 0.05823645453584438\n",
            "Loss: 0.0\n",
            "training error 0.03635659880136343, test error 0.05821048231192891\n",
            "Loss: 0.0\n",
            "training error 0.03635447733059091, test error 0.05835735122432188\n",
            "Loss: 0.2523066405908825\n",
            "training error 0.036360809869247425, test error 0.05836919745037633\n",
            "Loss: 0.27265731556205264\n",
            "training error 0.03629254487369297, test error 0.05824633179023047\n",
            "Loss: 0.061585949605191814\n",
            "training error 0.036253328604470905, test error 0.05814141603411142\n",
            "Loss: 0.0\n",
            "training error 0.036231660360434466, test error 0.05814008115426926\n",
            "Loss: 0.0\n",
            "training error 0.03624067418976228, test error 0.05811695139934843\n",
            "Loss: 0.0\n",
            "training error 0.03623316469256944, test error 0.057883415324304376\n",
            "Loss: 0.0\n",
            "training error 0.036262911529639105, test error 0.0579416643257301\n",
            "Loss: 0.10063159041215908\n",
            "training error 0.036156432809802956, test error 0.05798743292295631\n",
            "Loss: 0.17970190264198216\n",
            "training error 0.0361290966492833, test error 0.05797214337437863\n",
            "Loss: 0.1532875169461434\n",
            "training error 0.03610192997116467, test error 0.05789818742136868\n",
            "Loss: 0.02552043099313117\n",
            "training error 0.036094940058274426, test error 0.057726282350849055\n",
            "Loss: 0.0\n",
            "training error 0.036110510710626306, test error 0.057869162619563104\n",
            "Loss: 0.24751337327710843\n",
            "training error 0.036094932146275974, test error 0.05775502028394412\n",
            "Loss: 0.04978310039160938\n",
            "training error 0.036066208978355906, test error 0.05760194229903679\n",
            "Loss: 0.0\n",
            "training error 0.03607980494457791, test error 0.05743062032342665\n",
            "Loss: 0.0\n",
            "training error 0.036001975207150455, test error 0.05752409855322303\n",
            "Loss: 0.16276722986787728\n",
            "training error 0.03602087202141503, test error 0.05729444552525147\n",
            "Loss: 0.0\n",
            "training error 0.03600656399899016, test error 0.05741483649326904\n",
            "Loss: 0.2101267704292864\n",
            "training error 0.03596674081009383, test error 0.057142136889890664\n",
            "Loss: 0.0\n",
            "training error 0.035944082325797384, test error 0.05713677105533829\n",
            "Loss: 0.0\n",
            "training error 0.03591934006045147, test error 0.05708060636262343\n",
            "Loss: 0.0\n",
            "training error 0.03598928052055786, test error 0.05719656141257627\n",
            "Loss: 0.20314263870324645\n",
            "training error 0.03589579842360015, test error 0.057016701805878554\n",
            "Loss: 0.0\n",
            "training error 0.0359277243900471, test error 0.05722927098900321\n",
            "Loss: 0.3728191501647604\n",
            "training error 0.03587932545847146, test error 0.05699295603257223\n",
            "Loss: 0.0\n",
            "training error 0.03590838169057994, test error 0.05685759454592293\n",
            "Loss: 0.0\n",
            "training error 0.035823530727014045, test error 0.056681068069452334\n",
            "Loss: 0.0\n",
            "training error 0.03583330881097796, test error 0.05681396976634982\n",
            "Loss: 0.23447281680479826\n",
            "training error 0.03578227208434016, test error 0.056751026569693525\n",
            "Loss: 0.12342480941867073\n",
            "training error 0.035778442264376535, test error 0.05670318390692631\n",
            "Loss: 0.03901803234702683\n",
            "training error 0.03578170221328489, test error 0.05688145226992115\n",
            "Loss: 0.35352933050463164\n",
            "training error 0.03574632845130922, test error 0.056794777241857275\n",
            "Loss: 0.2006122613384953\n",
            "training error 0.03572805096214586, test error 0.056805369640802635\n",
            "Loss: 0.21929998072369727\n",
            "training error 0.035714173923718555, test error 0.056837449692144694\n",
            "Loss: 0.2758974522158564\n",
            "training error 0.03574704437940001, test error 0.05677124743581342\n",
            "Loss: 0.1590996243235665\n",
            "training error 0.03568537830561203, test error 0.056818734811562306\n",
            "Loss: 0.24287958360502682\n",
            "training error 0.035697938556586206, test error 0.05684989355626288\n",
            "Loss: 0.297851632936208\n",
            "training error 0.03566981998637571, test error 0.056696906898664134\n",
            "Loss: 0.027943773381955417\n",
            "training error 0.03568657178307186, test error 0.05677453815127135\n",
            "Loss: 0.1649052937825468\n",
            "training error 0.03577132813584499, test error 0.056774347553939515\n",
            "Loss: 0.16456903100852127\n",
            "training error 0.035752280662648336, test error 0.05651638081482455\n",
            "Loss: 0.0\n",
            "training error 0.035640201807059776, test error 0.056724301801915476\n",
            "Loss: 0.36789508474042165\n",
            "training error 0.03571327891242675, test error 0.05686902979368606\n",
            "Loss: 0.623976577723484\n",
            "training error 0.03562741036993079, test error 0.05677115413164221\n",
            "Loss: 0.4507955271453312\n",
            "training error 0.03561780358983904, test error 0.05640312762977314\n",
            "Loss: 0.0\n",
            "training error 0.035565343130886384, test error 0.05650379675284737\n",
            "Loss: 0.17848145538137405\n",
            "training error 0.03557853327982646, test error 0.056558274488070166\n",
            "Loss: 0.27506782835766685\n",
            "training error 0.0355789115525673, test error 0.056484751964680495\n",
            "Loss: 0.1447159729210945\n",
            "training error 0.03555006764543821, test error 0.0564785032177248\n",
            "Loss: 0.1336372487114934\n",
            "training error 0.03553271389870448, test error 0.05656382742137521\n",
            "Loss: 0.284912908831747\n",
            "training error 0.03552066324701509, test error 0.056526409234613925\n",
            "Loss: 0.21857228494490677\n",
            "training error 0.03551463131774167, test error 0.05656869607613009\n",
            "Loss: 0.29354479674199574\n",
            "training error 0.03552619674641039, test error 0.0564629633182218\n",
            "Loss: 0.10608576326018859\n",
            "training error 0.03550181543483356, test error 0.05617112144401658\n",
            "Loss: 0.0\n",
            "training error 0.03551954748407896, test error 0.05622174003213523\n",
            "Loss: 0.09011496800737095\n",
            "training error 0.035546270191293076, test error 0.05624499260403425\n",
            "Loss: 0.1315109225499267\n",
            "training error 0.03551782559573948, test error 0.055967721488005384\n",
            "Loss: 0.0\n",
            "training error 0.03546439294501695, test error 0.05599137676935926\n",
            "Loss: 0.042265936016261385\n",
            "training error 0.035497287792156836, test error 0.055970912259315275\n",
            "Loss: 0.005701092031373811\n",
            "training error 0.03546520350791855, test error 0.056079558232054495\n",
            "Loss: 0.19982365026791626\n",
            "training error 0.03541937320658944, test error 0.0560481140617238\n",
            "Loss: 0.14364096229224632\n",
            "training error 0.03540544929954602, test error 0.05610552238103032\n",
            "Loss: 0.24621494204382088\n",
            "training error 0.035392859992078106, test error 0.05609812745769981\n",
            "Loss: 0.23300210590559978\n",
            "training error 0.0354044703630511, test error 0.05606547455308077\n",
            "Loss: 0.17465971898880106\n",
            "training error 0.035424550689464984, test error 0.05606360923685156\n",
            "Loss: 0.17132687609362485\n",
            "training error 0.03538173361515345, test error 0.05605799343839759\n",
            "Loss: 0.16129288095381167\n",
            "training error 0.03537235758890914, test error 0.056095724468609456\n",
            "Loss: 0.22870857916112985\n",
            "training error 0.03540800570526902, test error 0.05611657063159644\n",
            "Loss: 0.2659553393163483\n",
            "training error 0.03535659642277413, test error 0.05606251985590159\n",
            "Loss: 0.1693804310338498\n",
            "training error 0.03539234847667514, test error 0.05622630988299258\n",
            "Loss: 0.4620313068178339\n",
            "training error 0.03536615282540289, test error 0.05621592591371396\n",
            "Loss: 0.4434778102620607\n",
            "training error 0.03537131195475975, test error 0.05613135221795677\n",
            "Loss: 0.29236625254871473\n",
            "training error 0.035321154257305754, test error 0.056224631140920175\n",
            "Loss: 0.45903182420934385\n",
            "training error 0.035319015333586286, test error 0.05618625830321332\n",
            "Loss: 0.3904693802029513\n",
            "training error 0.035358993135534325, test error 0.05630260920419112\n",
            "Loss: 0.5983586740394786\n",
            "training error 0.0353459968880419, test error 0.05614717006215158\n",
            "Loss: 0.320628693424041\n",
            "training error 0.03529100285623032, test error 0.056222212734487756\n",
            "Loss: 0.45471075061884747\n",
            "training error 0.03530016967070431, test error 0.05626231149766424\n",
            "Loss: 0.526356981893561\n",
            "training error 0.035336122099801144, test error 0.05590746915365764\n",
            "Loss: 0.0\n",
            "training error 0.03528605300589665, test error 0.05583060188401182\n",
            "Loss: 0.0\n",
            "training error 0.035244455403834786, test error 0.055898115321706035\n",
            "Loss: 0.12092550575484307\n",
            "training error 0.03523718715155094, test error 0.055916503603902254\n",
            "Loss: 0.1538613537946265\n",
            "training error 0.035223882459747266, test error 0.05592057896460419\n",
            "Loss: 0.16116086439348365\n",
            "training error 0.035291700856157196, test error 0.05600685562209954\n",
            "Loss: 0.3156937810806504\n",
            "training error 0.035239553340493716, test error 0.055931579805017465\n",
            "Loss: 0.18086482609560584\n",
            "training error 0.03523874012618504, test error 0.05606707041308365\n",
            "Loss: 0.42354644422979426\n",
            "training error 0.035207384759060434, test error 0.055767698592152276\n",
            "Loss: 0.0\n",
            "training error 0.035254416564400294, test error 0.05580683863870664\n",
            "Loss: 0.07018408064605186\n",
            "training error 0.03521096923426061, test error 0.05571949405981744\n",
            "Loss: 0.0\n",
            "training error 0.035228673334262696, test error 0.05584546100084964\n",
            "Loss: 0.22607337549938045\n",
            "training error 0.03528961257071703, test error 0.05608681243112039\n",
            "Loss: 0.6592277577190719\n",
            "training error 0.03521231353407599, test error 0.05584436360482252\n",
            "Loss: 0.22410387443760893\n",
            "training error 0.0351704643145604, test error 0.0558449055613274\n",
            "Loss: 0.22507652595575856\n",
            "training error 0.03518666994226674, test error 0.056063185849291805\n",
            "Loss: 0.6168250363246308\n",
            "training error 0.03521111371722429, test error 0.05596575022010019\n",
            "Loss: 0.4419569208908847\n",
            "training error 0.03516001679881949, test error 0.05584958769196416\n",
            "Loss: 0.23347956463326547\n",
            "training error 0.03516216739461746, test error 0.05591092689935984\n",
            "Loss: 0.3435652867502581\n",
            "training error 0.0351497390256621, test error 0.05597366060297237\n",
            "Loss: 0.45615371683396244\n",
            "training error 0.03519205941190082, test error 0.05601677599094708\n",
            "Loss: 0.5335330769703273\n",
            "training error 0.03513430512087563, test error 0.055899501022622096\n",
            "Loss: 0.3230592198332216\n",
            "training error 0.03509638800490005, test error 0.0559521808209938\n",
            "Loss: 0.4176038657611736\n",
            "training error 0.03514461023496152, test error 0.05566584906422627\n",
            "Loss: 0.0\n",
            "training error 0.03511407173598433, test error 0.05566308979106498\n",
            "Loss: 0.0\n",
            "training error 0.03509694178738389, test error 0.05564736279748106\n",
            "Loss: 0.0\n",
            "training error 0.0350921890034309, test error 0.05568745288179912\n",
            "Loss: 0.07204309836561507\n",
            "training error 0.035113585020449485, test error 0.05565651274055774\n",
            "Loss: 0.01644272543512848\n",
            "training error 0.035071773218726954, test error 0.05557383358893932\n",
            "Loss: 0.0\n",
            "training error 0.03511630911928462, test error 0.055656694094094225\n",
            "Loss: 0.1490998547406308\n",
            "training error 0.035104653683657165, test error 0.0557323672509488\n",
            "Loss: 0.2852667375479401\n",
            "training error 0.035074997828691085, test error 0.055540578952119786\n",
            "Loss: 0.0\n",
            "training error 0.03513524389072884, test error 0.05574075490939269\n",
            "Loss: 0.3604138830556103\n",
            "training error 0.035066313414916915, test error 0.05573227793129872\n",
            "Loss: 0.3451512079919006\n",
            "training error 0.03508793379946228, test error 0.05572166842986262\n",
            "Loss: 0.32604895584351645\n",
            "training error 0.03515527582121573, test error 0.0557001227227033\n",
            "Loss: 0.28725622525658245\n",
            "training error 0.03508325284103849, test error 0.055485032269895256\n",
            "Loss: 0.0\n",
            "training error 0.0350344704497253, test error 0.05551130665396993\n",
            "Loss: 0.04735400341278062\n",
            "training error 0.03507399162194623, test error 0.05559192816580955\n",
            "Loss: 0.19265717535195126\n",
            "training error 0.03504774715105759, test error 0.05543652402255999\n",
            "Loss: 0.0\n",
            "training error 0.03503410955022518, test error 0.05552654942716518\n",
            "Loss: 0.16239366769921215\n",
            "training error 0.03506467225279974, test error 0.05545645679018552\n",
            "Loss: 0.03595601992907316\n",
            "training error 0.03508279707350814, test error 0.055721405790790476\n",
            "Loss: 0.513888223068526\n",
            "training error 0.0350144698475347, test error 0.05569790725896561\n",
            "Loss: 0.47150004624973985\n",
            "training error 0.03500584707301093, test error 0.05562590735461217\n",
            "Loss: 0.3416219458043823\n",
            "training error 0.03506964696036869, test error 0.05563388269491296\n",
            "Loss: 0.35600838225833265\n",
            "training error 0.034993127185022094, test error 0.055547493551788625\n",
            "Loss: 0.2001740390206841\n",
            "training error 0.03506925789071011, test error 0.05548717642090301\n",
            "Loss: 0.09137008359760834\n",
            "training error 0.035010395775136885, test error 0.055667826314954\n",
            "Loss: 0.4172380871136028\n",
            "training error 0.03499358751792415, test error 0.05567420267143402\n",
            "Loss: 0.42874017277365617\n",
            "training error 0.03502486865592965, test error 0.05553110378682153\n",
            "Loss: 0.17060911723658645\n",
            "training error 0.03500264128631391, test error 0.055627162345218105\n",
            "Loss: 0.3438857793113703\n",
            "training error 0.03505826697577473, test error 0.055636018258657986\n",
            "Loss: 0.35986065074500306\n",
            "training error 0.03499828293138549, test error 0.055724369707336595\n",
            "Loss: 0.5192347280999599\n",
            "training error 0.03500284016198181, test error 0.05575447946901082\n",
            "Loss: 0.5735486704062431\n",
            "training error 0.03496408177137639, test error 0.05588325275980435\n",
            "Loss: 0.805838290046057\n",
            "training error 0.03498339855399184, test error 0.056049624899757254\n",
            "Loss: 1.1059511540582267\n",
            "training error 0.0349808041848631, test error 0.05600742683419996\n",
            "Loss: 1.0298315446467088\n",
            "training error 0.03500057401953199, test error 0.05589125553055231\n",
            "Loss: 0.8202742073209146\n",
            "training error 0.03498617785777233, test error 0.05596299537249342\n",
            "Loss: 0.9496831903082148\n",
            "training error 0.034964107768536035, test error 0.05592805268701675\n",
            "Loss: 0.8866513063783277\n",
            "training error 0.03505992544497204, test error 0.05596820905555842\n",
            "Loss: 0.9590879702018329\n",
            "training error 0.03503303930877675, test error 0.05575577173222087\n",
            "Loss: 0.5758797386556269\n",
            "training error 0.034950076495732285, test error 0.05586688085945253\n",
            "Loss: 0.7763055936144347\n",
            "training error 0.03499907057456788, test error 0.05594971472269351\n",
            "Loss: 0.9257266922519714\n",
            "training error 0.03496040760309916, test error 0.05584908928463356\n",
            "Loss: 0.7442119962385751\n",
            "training error 0.03498358546156146, test error 0.05588835338312976\n",
            "Loss: 0.8150391254435396\n",
            "training error 0.034941003151830616, test error 0.055803573741028165\n",
            "Loss: 0.6621081046113364\n",
            "training error 0.034985053214866405, test error 0.055771269829170855\n",
            "Loss: 0.6038362117990026\n",
            "training error 0.034986880297800255, test error 0.05568681988692517\n",
            "Loss: 0.4514999249652041\n",
            "training error 0.035034755556607636, test error 0.05602659010586159\n",
            "Loss: 1.0643994978138949\n",
            "training error 0.034938448218650525, test error 0.055855195341171636\n",
            "Loss: 0.7552264973201828\n",
            "training error 0.03494282016250969, test error 0.0557530356164822\n",
            "Loss: 0.5709441555054928\n",
            "training error 0.03491611697333779, test error 0.05585552632469604\n",
            "Loss: 0.755823546883172\n",
            "training error 0.03495475976456038, test error 0.055922106296855414\n",
            "Loss: 0.8759248218698223\n",
            "training error 0.03492851676223655, test error 0.05551240991413569\n",
            "Loss: 0.13688789640711185\n",
            "training error 0.03492665722339925, test error 0.05552626855063709\n",
            "Loss: 0.16188700438826142\n",
            "training error 0.03488940505766965, test error 0.05545916341652026\n",
            "Loss: 0.040838408178434094\n",
            "training error 0.034915973986041585, test error 0.05555931735258397\n",
            "Loss: 0.2215025782893898\n",
            "training error 0.03491238627028035, test error 0.05553285974021201\n",
            "Loss: 0.1737766199280788\n",
            "training error 0.03494585427884662, test error 0.055580581564871744\n",
            "Loss: 0.2598603445142622\n",
            "training error 0.034880084433125604, test error 0.05537965682441536\n",
            "Loss: 0.0\n",
            "training error 0.034860418323044214, test error 0.05539797678222892\n",
            "Loss: 0.03308066330502957\n",
            "training error 0.03490125784031515, test error 0.055377159419836224\n",
            "Loss: 0.0\n",
            "training error 0.03489885797788261, test error 0.055282201159268435\n",
            "Loss: 0.0\n",
            "training error 0.03488369639254397, test error 0.055299793277876916\n",
            "Loss: 0.031822391727498456\n",
            "training error 0.03485088661386399, test error 0.05541165831532697\n",
            "Loss: 0.23417511123620116\n",
            "training error 0.034865701160111624, test error 0.055433406732954604\n",
            "Loss: 0.27351583423849135\n",
            "training error 0.03485435815687036, test error 0.05534698255319741\n",
            "Loss: 0.11718309432422469\n",
            "training error 0.03485004912794681, test error 0.05539911096245352\n",
            "Loss: 0.2114781986489067\n",
            "training error 0.0349057933562331, test error 0.055353539880565926\n",
            "Loss: 0.12904464692344852\n",
            "training error 0.034837225640502976, test error 0.05551277698177594\n",
            "Loss: 0.41708871512409207\n",
            "training error 0.0348957695648677, test error 0.05549696581396532\n",
            "Loss: 0.388487886142852\n",
            "training error 0.03484418398000173, test error 0.0555098000579591\n",
            "Loss: 0.4117037562143322\n",
            "training error 0.03484901226257896, test error 0.05556161338196639\n",
            "Loss: 0.5054289026823922\n",
            "training error 0.03493248106113828, test error 0.05570918079526086\n",
            "Loss: 0.7723636668559797\n",
            "training error 0.03486094037424247, test error 0.055619324370653846\n",
            "Loss: 0.6098223375986089\n",
            "training error 0.03482481065605398, test error 0.055550958197596895\n",
            "Loss: 0.4861547346028505\n",
            "training error 0.03488071292606381, test error 0.0555931404164104\n",
            "Loss: 0.562458170299962\n",
            "training error 0.034856833615481676, test error 0.05557883180633725\n",
            "Loss: 0.5365753187254851\n",
            "training error 0.034841198136637296, test error 0.05548479134207239\n",
            "Loss: 0.3664654781387888\n",
            "training error 0.034834388581171424, test error 0.05551377740775177\n",
            "Loss: 0.41889838614812547\n",
            "training error 0.03486887653070676, test error 0.05559730981373502\n",
            "Loss: 0.5700001951057576\n",
            "training error 0.034846958339426845, test error 0.05560408403217428\n",
            "Loss: 0.5822540820661359\n",
            "training error 0.0348310688689863, test error 0.05540578112810047\n",
            "Loss: 0.22354386446372665\n",
            "training error 0.034830915433135184, test error 0.055372764289016066\n",
            "Loss: 0.1638196885227483\n",
            "training error 0.034822743454785046, test error 0.055520623512608606\n",
            "Loss: 0.4312823084834827\n",
            "training error 0.0348494475564528, test error 0.05546491315063413\n",
            "Loss: 0.3305078081809798\n",
            "training error 0.03481651509093032, test error 0.05544610297015961\n",
            "Loss: 0.2964820637640253\n",
            "training error 0.03482418990878984, test error 0.055609964460902016\n",
            "Loss: 0.5928911923917202\n",
            "training error 0.034812874656617386, test error 0.05552668916201702\n",
            "Loss: 0.4422544645865667\n",
            "training error 0.03479700357412319, test error 0.0554694975597721\n",
            "Loss: 0.33880054805355275\n",
            "training error 0.034810359308434805, test error 0.05546461152891925\n",
            "Loss: 0.32996220451730895\n",
            "training error 0.03490228008792278, test error 0.055416764986401676\n",
            "Loss: 0.24341257097479918\n",
            "training error 0.03484871658547926, test error 0.05557160486981915\n",
            "Loss: 0.5235025098167423\n",
            "training error 0.03486177504533883, test error 0.055355069336030197\n",
            "Loss: 0.13181127964103645\n",
            "training error 0.034801466829301624, test error 0.05559284134158838\n",
            "Loss: 0.5619171737120121\n",
            "training error 0.034889344925564586, test error 0.055435868035395186\n",
            "Loss: 0.27796808539521933\n",
            "training error 0.0347924647014294, test error 0.05557209482901216\n",
            "Loss: 0.5243887972342876\n",
            "training error 0.034783380933026346, test error 0.055617284721510216\n",
            "Loss: 0.6061328152914935\n",
            "training error 0.034792463892716126, test error 0.055459653756358494\n",
            "Loss: 0.3209940873714734\n",
            "training error 0.03484726595925872, test error 0.05537771973091213\n",
            "Loss: 0.17278358972809826\n",
            "training error 0.03482202429044408, test error 0.0553459794096124\n",
            "Loss: 0.11536850741564741\n",
            "training error 0.03480271434526975, test error 0.0552424987323763\n",
            "Loss: 0.0\n",
            "training error 0.03478990286139861, test error 0.05517710846298869\n",
            "Loss: 0.0\n",
            "training error 0.03477164853279214, test error 0.05526475277625418\n",
            "Loss: 0.15884180180314456\n",
            "training error 0.0347797653289026, test error 0.05530591246604175\n",
            "Loss: 0.23343739213783632\n",
            "training error 0.03476993127038239, test error 0.055303958967477564\n",
            "Loss: 0.2298969772472903\n",
            "training error 0.03477648500155255, test error 0.05542724098810127\n",
            "Loss: 0.45332662779957733\n",
            "training error 0.03475845343082711, test error 0.05530869778391855\n",
            "Loss: 0.2384853512541918\n",
            "training error 0.0348033288907206, test error 0.055489346617287494\n",
            "Loss: 0.565883503134712\n",
            "training error 0.034767001545405386, test error 0.055401253391846546\n",
            "Loss: 0.4062281172421356\n",
            "training error 0.03474078540812819, test error 0.05539259441804556\n",
            "Loss: 0.39053506256387926\n",
            "training error 0.03483239441226217, test error 0.05529964575762859\n",
            "Loss: 0.22207994955389943\n",
            "training error 0.03477831353957773, test error 0.055405097455106925\n",
            "Loss: 0.4131948890927495\n",
            "training error 0.034738325250278236, test error 0.055410079304166915\n",
            "Loss: 0.4222237222425207\n",
            "training error 0.0347773185526307, test error 0.05551512990497185\n",
            "Loss: 0.6126117359156247\n",
            "training error 0.03478514519637629, test error 0.05551321068333084\n",
            "Loss: 0.6091334426623574\n",
            "training error 0.03475444654627621, test error 0.05542173832720148\n",
            "Loss: 0.44335390350669357\n",
            "training error 0.03476084410635354, test error 0.05536092996742864\n",
            "Loss: 0.33314812892606405\n",
            "training error 0.0347872118087209, test error 0.055480144813823344\n",
            "Loss: 0.5492066534039663\n",
            "training error 0.03473551386826791, test error 0.05541426026484678\n",
            "Loss: 0.4298010687116216\n",
            "training error 0.03479715497568409, test error 0.055537036202095456\n",
            "Loss: 0.6523135211918385\n",
            "training error 0.03474609026985519, test error 0.05550260389176015\n",
            "Loss: 0.589910268657512\n",
            "training error 0.034771070890223826, test error 0.05553807938118314\n",
            "Loss: 0.654204122415325\n",
            "training error 0.03479287984330743, test error 0.05521685029338848\n",
            "Loss: 0.07202593884825248\n",
            "training error 0.034740032407305034, test error 0.05529477885462761\n",
            "Loss: 0.21325943840977413\n",
            "training error 0.03475541046023965, test error 0.05540421511533427\n",
            "Loss: 0.4115957843240592\n",
            "training error 0.03472149133085099, test error 0.055322788718208234\n",
            "Loss: 0.2640229966332175\n",
            "training error 0.03475179475501307, test error 0.05541713576445904\n",
            "Loss: 0.4350124683161871\n",
            "training error 0.034727098012264666, test error 0.05539449862452686\n",
            "Loss: 0.39398614315571123\n",
            "training error 0.03473804275464159, test error 0.05525369145012462\n",
            "Loss: 0.1387948540059858\n",
            "training error 0.034730865783843046, test error 0.05523904880534678\n",
            "Loss: 0.11225731844870523\n",
            "training error 0.03470825263622712, test error 0.05539949025794957\n",
            "Loss: 0.4030327089540098\n",
            "training error 0.03474881125254612, test error 0.055493404485111406\n",
            "Loss: 0.5732377627850394\n",
            "training error 0.03474385915603667, test error 0.05544619082997502\n",
            "Loss: 0.4876702938625632\n",
            "training error 0.03472940361311739, test error 0.05535293084724077\n",
            "Loss: 0.3186509571628138\n",
            "training error 0.03473885695427121, test error 0.05554724281126669\n",
            "Loss: 0.6708114263114684\n",
            "training error 0.03470758098601132, test error 0.05556633974086255\n",
            "Loss: 0.7054216661877888\n",
            "training error 0.034721362531774325, test error 0.05542062860440714\n",
            "Loss: 0.44134270207689674\n",
            "training error 0.03472502285257531, test error 0.055589011579392986\n",
            "Loss: 0.7465108772066076\n",
            "training error 0.03470031018978866, test error 0.05545004288920336\n",
            "Loss: 0.49465155717203224\n",
            "training error 0.03469890159809986, test error 0.055460104331622864\n",
            "Loss: 0.5128863699408903\n",
            "training error 0.0347862695983314, test error 0.05529367738906804\n",
            "Loss: 0.21126320194460035\n",
            "training error 0.034732656708440196, test error 0.055497724157536725\n",
            "Loss: 0.5810665029014617\n",
            "training error 0.034708408773052035, test error 0.055500370845800145\n",
            "Loss: 0.5858632172222045\n",
            "training error 0.034773327137593575, test error 0.05551791771209099\n",
            "Loss: 0.6176642063998372\n",
            "training error 0.03469401671470239, test error 0.05540912984975856\n",
            "Loss: 0.4205029825466555\n",
            "training error 0.034666954406463966, test error 0.05545914070413022\n",
            "Loss: 0.5111399437154551\n",
            "training error 0.03470300791509742, test error 0.0554653168578637\n",
            "Loss: 0.5223332699072714\n",
            "training error 0.03470365190459537, test error 0.055255032223258616\n",
            "Loss: 0.141224798545192\n",
            "training error 0.03469384352244626, test error 0.055137425586453105\n",
            "Loss: 0.0\n",
            "training error 0.03473214242673467, test error 0.05512929810516185\n",
            "Loss: 0.0\n",
            "training error 0.0347085346529672, test error 0.05527902116046324\n",
            "Loss: 0.271585273978614\n",
            "training error 0.034701891603141494, test error 0.05517162290806286\n",
            "Loss: 0.07677370174434817\n",
            "training error 0.03471526029344908, test error 0.055373870523298896\n",
            "Loss: 0.4436341955061929\n",
            "training error 0.03472816890295242, test error 0.055343901411616936\n",
            "Loss: 0.3892726986034045\n",
            "training error 0.034679370868959544, test error 0.055241952209568096\n",
            "Loss: 0.20434525429899875\n",
            "training error 0.03470500616540415, test error 0.05530397787783055\n",
            "Loss: 0.3168547009894551\n",
            "training error 0.03475039230287091, test error 0.05536588121772167\n",
            "Loss: 0.42914225410330964\n",
            "training error 0.0347014890259015, test error 0.055420734876209896\n",
            "Loss: 0.528642266571433\n",
            "training error 0.03471117505646985, test error 0.055200896825248945\n",
            "Loss: 0.12987417316743244\n",
            "training error 0.03467117035465091, test error 0.055117947217855026\n",
            "Loss: 0.0\n",
            "training error 0.03470047904915107, test error 0.055153000018354555\n",
            "Loss: 0.06359598328469218\n",
            "training error 0.03469097253241896, test error 0.05524597908733449\n",
            "Loss: 0.23228707878653054\n",
            "training error 0.03470602981154598, test error 0.05527114472578762\n",
            "Loss: 0.2779448721612843\n",
            "training error 0.03468697878367308, test error 0.05538037135426399\n",
            "Loss: 0.4761137699336393\n",
            "training error 0.0346604809363213, test error 0.055336187545801496\n",
            "Loss: 0.3959514803479003\n",
            "training error 0.034701232995279045, test error 0.05532320712773985\n",
            "Loss: 0.3724012236405061\n",
            "training error 0.034704931401821175, test error 0.05534745869058981\n",
            "Loss: 0.41640061780172655\n",
            "training error 0.03468754997722561, test error 0.05515050340689557\n",
            "Loss: 0.05906640338375624\n",
            "training error 0.03471390143240274, test error 0.055160678143021676\n",
            "Loss: 0.07752633638142825\n",
            "training error 0.03474483507974119, test error 0.05498193823341117\n",
            "Loss: 0.0\n",
            "training error 0.03469396889646866, test error 0.055085369709210776\n",
            "Loss: 0.18811900620985256\n",
            "training error 0.03470706397696358, test error 0.055134039508036935\n",
            "Loss: 0.2766386189953174\n",
            "training error 0.034656497620367405, test error 0.055188479308512006\n",
            "Loss: 0.37565259017247055\n",
            "training error 0.03465714152398592, test error 0.05531469707730717\n",
            "Loss: 0.6052148297925664\n",
            "training error 0.03467447208670111, test error 0.055352423612132946\n",
            "Loss: 0.6738310627555144\n",
            "training error 0.0346705504458426, test error 0.05528277174761668\n",
            "Loss: 0.5471497074701182\n",
            "training error 0.03466689701279921, test error 0.05524562497385556\n",
            "Loss: 0.4795879318131302\n",
            "training error 0.034663195769724875, test error 0.05518833487364862\n",
            "Loss: 0.37538989506198384\n",
            "training error 0.03468613873807968, test error 0.05528837445795156\n",
            "Loss: 0.5573397999166341\n",
            "training error 0.034676008229839454, test error 0.055352637644938314\n",
            "Loss: 0.6742203411481018\n",
            "training error 0.034654367815182927, test error 0.05528940412611617\n",
            "Loss: 0.5592125388518276\n",
            "training error 0.03468989593186233, test error 0.05517718907139358\n",
            "Loss: 0.35511814289543864\n",
            "training error 0.03467195387575186, test error 0.05523626598748716\n",
            "Loss: 0.46256600303231643\n",
            "training error 0.03467584860537123, test error 0.0551772780767954\n",
            "Loss: 0.35528002405982306\n",
            "training error 0.034654711928030074, test error 0.05516817539823489\n",
            "Loss: 0.33872426256253974\n",
            "training error 0.03464019725529817, test error 0.05517251916304377\n",
            "Loss: 0.3466246112014737\n",
            "training error 0.03467153436584556, test error 0.05524454226442731\n",
            "Loss: 0.47761872253633264\n",
            "training error 0.0346476540284018, test error 0.05523008146533477\n",
            "Loss: 0.4513177234134025\n",
            "training error 0.03465382848961202, test error 0.055116151507708065\n",
            "Loss: 0.24410429790073795\n",
            "training error 0.03465977401095339, test error 0.05503550602852476\n",
            "Loss: 0.09742798605276715\n",
            "training error 0.03469394894095034, test error 0.05495494415575565\n",
            "Loss: 0.0\n",
            "training error 0.03466640792656081, test error 0.05492144365889677\n",
            "Loss: 0.0\n",
            "training error 0.034704231603214744, test error 0.054965093649768454\n",
            "Loss: 0.07947713673148726\n",
            "training error 0.0346862869544105, test error 0.054746729438741176\n",
            "Loss: 0.0\n",
            "training error 0.0346378039702877, test error 0.054807829778898545\n",
            "Loss: 0.11160546170294605\n",
            "training error 0.03470638238114852, test error 0.05473938004039448\n",
            "Loss: 0.0\n",
            "training error 0.034680807249222505, test error 0.054843839970187\n",
            "Loss: 0.19083140823925593\n",
            "training error 0.034625483925876804, test error 0.055034878079025094\n",
            "Loss: 0.5398271562676804\n",
            "training error 0.03467037932904319, test error 0.055003995746009324\n",
            "Loss: 0.4834101252509271\n",
            "training error 0.03466445819995634, test error 0.05518037613284997\n",
            "Loss: 0.8056285842661381\n",
            "training error 0.03468644518445588, test error 0.05516000542905884\n",
            "Loss: 0.7684146008850812\n",
            "training error 0.034684002471662694, test error 0.05530954577073688\n",
            "Loss: 1.0416006354504903\n",
            "training error 0.03472968806609424, test error 0.055283929212689376\n",
            "Loss: 0.9948033242120236\n",
            "training error 0.034644600724664194, test error 0.05530215031293929\n",
            "Loss: 1.0280903293561572\n",
            "training error 0.03470585698442011, test error 0.05520800040134564\n",
            "Loss: 0.8560936579941947\n",
            "training error 0.03465929826766928, test error 0.05523722326769276\n",
            "Loss: 0.9094791116211143\n",
            "training error 0.034626802440094216, test error 0.05533329958802411\n",
            "Loss: 1.0849950203881686\n",
            "training error 0.03462895512012351, test error 0.05531960295819693\n",
            "Loss: 1.059973491432098\n",
            "training error 0.034658723093519736, test error 0.05535748449422648\n",
            "Loss: 1.1291769350984193\n",
            "training error 0.034677900464104364, test error 0.05514789891172651\n",
            "Loss: 0.7462979504528011\n",
            "training error 0.0346439673661747, test error 0.055239045181813634\n",
            "Loss: 0.912807454250375\n",
            "training error 0.03464463179386141, test error 0.05526509316707286\n",
            "Loss: 0.9603929132745659\n",
            "training error 0.03464020989432673, test error 0.05537798154252612\n",
            "Loss: 1.1666217294759251\n",
            "training error 0.03465583631317728, test error 0.055301724347257976\n",
            "Loss: 1.0273121589037482\n",
            "training error 0.03464832433030076, test error 0.05526945427628685\n",
            "Loss: 0.9683599549377586\n",
            "training error 0.03467160405577064, test error 0.05538293293090738\n",
            "Loss: 1.1756671157729492\n",
            "training error 0.034629229236647914, test error 0.05544897867507434\n",
            "Loss: 1.2963220156242627\n",
            "training error 0.03463393920699394, test error 0.055416242877360695\n",
            "Loss: 1.2365190041734753\n",
            "training error 0.034627139062120456, test error 0.055420759336801906\n",
            "Loss: 1.2447698455930745\n",
            "training error 0.03461757970527268, test error 0.055250048804945814\n",
            "Loss: 0.9329092952358842\n",
            "training error 0.03468295154106858, test error 0.05530715916352249\n",
            "Loss: 1.0372406898087272\n",
            "training error 0.034679680044984874, test error 0.0550055269602976\n",
            "Loss: 0.48620740627081105\n",
            "training error 0.03461317420949373, test error 0.05519325044789981\n",
            "Loss: 0.829147877031855\n",
            "training error 0.034613633550003374, test error 0.055257101007445804\n",
            "Loss: 0.9457925293806291\n",
            "training error 0.03461248381579018, test error 0.055174738729744585\n",
            "Loss: 0.7953299599462671\n",
            "training error 0.0346391371032683, test error 0.055187735957029964\n",
            "Loss: 0.8190737935004577\n",
            "training error 0.03466903002537333, test error 0.0553731076186405\n",
            "Loss: 1.1577178582920844\n",
            "training error 0.03466458129568894, test error 0.055070530279010224\n",
            "Loss: 0.6049579633006008\n",
            "training error 0.034632742576175865, test error 0.05513940236794462\n",
            "Loss: 0.7307761382298272\n",
            "training error 0.034636146278939066, test error 0.05536797575693536\n",
            "Loss: 1.1483427764015897\n",
            "training error 0.034661806518443534, test error 0.055182308745601635\n",
            "Loss: 0.8091591554020283\n",
            "training error 0.034604805931050156, test error 0.055048411020178725\n",
            "Loss: 0.5645496524735893\n",
            "training error 0.034631873124682994, test error 0.055189012934838334\n",
            "Loss: 0.8214066255628927\n",
            "training error 0.03462454701991274, test error 0.05518994991951288\n",
            "Loss: 0.8231183451217428\n",
            "training error 0.03465737206708446, test error 0.055248848757304236\n",
            "Loss: 0.9307170021542133\n",
            "training error 0.03461604918636547, test error 0.055083688195546356\n",
            "Loss: 0.628995350144268\n",
            "training error 0.03465173402421897, test error 0.05513972707218543\n",
            "Loss: 0.7313693203969907\n",
            "training error 0.03464745700226486, test error 0.05510738258890577\n",
            "Loss: 0.6722811771703086\n",
            "training error 0.03462082009420151, test error 0.05504838095477347\n",
            "Loss: 0.5644947278375501\n",
            "training error 0.034624944430044415, test error 0.05523749765286758\n",
            "Loss: 0.9099803689875952\n",
            "training error 0.034617861477988426, test error 0.05509610922136588\n",
            "Loss: 0.6516865567497421\n",
            "training error 0.034616438152433524, test error 0.05515816465914484\n",
            "Loss: 0.7650518117693794\n",
            "training error 0.03465441989812904, test error 0.05509877351177631\n",
            "Loss: 0.6565537847097014\n",
            "training error 0.03460415408653731, test error 0.05515774278762224\n",
            "Loss: 0.7642811206832079\n",
            "training error 0.034659087077092414, test error 0.05529889015673518\n",
            "Loss: 1.0221345508988566\n",
            "training error 0.034628052342864764, test error 0.05531907138263936\n",
            "Loss: 1.0590023888051192\n",
            "training error 0.03462328502969933, test error 0.055191055153979304\n",
            "Loss: 0.8251374298567482\n",
            "training error 0.03468755645165457, test error 0.0551078590587373\n",
            "Loss: 0.6731516105423774\n",
            "training error 0.03461715950432862, test error 0.05513522170296584\n",
            "Loss: 0.723138739019813\n",
            "training error 0.03459422560132826, test error 0.055214522638843785\n",
            "Loss: 0.86800873173698\n",
            "training error 0.0346106217680912, test error 0.055141336932934604\n",
            "Loss: 0.7343102757895847\n",
            "training error 0.03461033937690645, test error 0.05508815276934152\n",
            "Loss: 0.6371514048746585\n",
            "training error 0.03462631802477725, test error 0.05514639441732464\n",
            "Loss: 0.7435494823467392\n",
            "training error 0.034613374821179446, test error 0.0553309855334253\n",
            "Loss: 1.080767616648659\n",
            "training error 0.03463242880244884, test error 0.05523947048726325\n",
            "Loss: 0.9135844185661934\n",
            "training error 0.034603929238911424, test error 0.05522683336904209\n",
            "Loss: 0.8904984460691878\n",
            "training error 0.03462110149521506, test error 0.05520203777186856\n",
            "Loss: 0.8452008976584491\n",
            "training error 0.03459255702527721, test error 0.0553528297352138\n",
            "Loss: 1.1206734427146037\n",
            "training error 0.03462930351386211, test error 0.05550736145967133\n",
            "Loss: 1.4029779268784681\n",
            "training error 0.034604078895506825, test error 0.05522869560292658\n",
            "Loss: 0.893900446389817\n",
            "training error 0.03458310657974909, test error 0.05514984216980378\n",
            "Loss: 0.7498479688779813\n",
            "training error 0.03461447119549993, test error 0.05494478759170227\n",
            "Loss: 0.37524639693802264\n",
            "training error 0.03455894255846939, test error 0.055141546633715045\n",
            "Loss: 0.7346933652222498\n",
            "training error 0.03458175817160497, test error 0.055114750163560204\n",
            "Loss: 0.6857405452687315\n",
            "training error 0.0345621533682624, test error 0.05510917346289587\n",
            "Loss: 0.6755528144975598\n",
            "training error 0.03461873841514897, test error 0.05520556185518952\n",
            "Loss: 0.851638828300616\n",
            "training error 0.034578011221862305, test error 0.055054446912898986\n",
            "Loss: 0.5755762529133568\n",
            "training error 0.03459996535120254, test error 0.054938615420752686\n",
            "Loss: 0.36397083819945664\n",
            "training error 0.03457954409978299, test error 0.05503945314193755\n",
            "Loss: 0.5481850567573776\n",
            "training error 0.03459076316176759, test error 0.05516849909375186\n",
            "Loss: 0.7839311534780347\n",
            "training error 0.03457594940171087, test error 0.055019937738585004\n",
            "Loss: 0.5125335690383981\n",
            "training error 0.03461800911220398, test error 0.055095979445145576\n",
            "Loss: 0.6514494765705203\n",
            "training error 0.034610682047945866, test error 0.05517354047878019\n",
            "Loss: 0.7931409491034103\n",
            "training error 0.0345913746478499, test error 0.05507936699226627\n",
            "Loss: 0.6211012101724478\n",
            "training error 0.03457737844391127, test error 0.05515544310758943\n",
            "Loss: 0.7600799769524658\n",
            "training error 0.0345840867440371, test error 0.0552695248757289\n",
            "Loss: 0.9684889287076448\n",
            "training error 0.03459574597621359, test error 0.05536414367645631\n",
            "Loss: 1.1413421847320748\n",
            "training error 0.034582101529325796, test error 0.05523915149364589\n",
            "Loss: 0.9130016687850961\n",
            "training error 0.034597205708408976, test error 0.055153639539219375\n",
            "Loss: 0.756785149044803\n",
            "training error 0.03462002117998167, test error 0.055180652987681114\n",
            "Loss: 0.8061343532955734\n",
            "training error 0.034563545547069925, test error 0.05514568542180802\n",
            "Loss: 0.7422542621303263\n",
            "training error 0.034620652849252934, test error 0.05516696116053447\n",
            "Loss: 0.7811215980605857\n",
            "training error 0.03463273298869203, test error 0.055307447235365466\n",
            "Loss: 1.037766950505814\n",
            "training error 0.03461718041009428, test error 0.05530878762079927\n",
            "Loss: 1.0402156180515876\n",
            "training error 0.03465294165603347, test error 0.055226444373626046\n",
            "Loss: 0.8897878143160343\n",
            "training error 0.03467992875898088, test error 0.055215893995132825\n",
            "Loss: 0.8705139780295523\n",
            "training error 0.03462511660241073, test error 0.05518192840855366\n",
            "Loss: 0.8084643410879222\n",
            "training error 0.034574864943720966, test error 0.0550381441681079\n",
            "Loss: 0.5457937731354612\n",
            "training error 0.034612863401395945, test error 0.05516275276699791\n",
            "Loss: 0.7734335432571848\n",
            "training error 0.0345774676611336, test error 0.0551400426476376\n",
            "Loss: 0.7319458257427414\n",
            "training error 0.0345832523382494, test error 0.05509408341353623\n",
            "Loss: 0.6479857332691852\n",
            "training error 0.03458214649251484, test error 0.055203588362012036\n",
            "Loss: 0.8480335752341306\n",
            "training error 0.03459849863003559, test error 0.05522933152079872\n",
            "Loss: 0.8950621655610558\n",
            "training error 0.034654696677666554, test error 0.05505332444149658\n",
            "Loss: 0.5735256790822874\n",
            "training error 0.03463882445538694, test error 0.0550561845594594\n",
            "Loss: 0.5787506523295294\n",
            "training error 0.03457177360056974, test error 0.0553190749266927\n",
            "Loss: 1.0590088632177475\n",
            "training error 0.034573608610689796, test error 0.05529446153877146\n",
            "Loss: 1.0140441816611112\n",
            "training error 0.03458221493477402, test error 0.055402149786402725\n",
            "Loss: 1.2107732048100672\n",
            "training error 0.03458737267556208, test error 0.05537490534549208\n",
            "Loss: 1.1610020146896494\n",
            "training error 0.03461148232598753, test error 0.05529099406393372\n",
            "Loss: 1.007709665568357\n",
            "training error 0.03457783253614398, test error 0.05541231221742759\n",
            "Loss: 1.2293383237744537\n",
            "training error 0.03458060197009901, test error 0.055429907175653284\n",
            "Loss: 1.2614814686414677\n",
            "training error 0.034582568691312765, test error 0.05546169834137608\n",
            "Loss: 1.3195587901225325\n",
            "training error 0.03462597437396844, test error 0.05553692724154245\n",
            "Loss: 1.4569898317434937\n",
            "training error 0.034617702503442906, test error 0.05554883836570195\n",
            "Loss: 1.4787495304297238\n",
            "training error 0.03460726382622368, test error 0.05549555695155564\n",
            "Loss: 1.3814129984722978\n",
            "training error 0.0345671237312996, test error 0.055320596495269395\n",
            "Loss: 1.061788523081586\n",
            "training error 0.034577781864745465, test error 0.05529374426498961\n",
            "Loss: 1.0127338383921103\n",
            "training error 0.03455779015771076, test error 0.055237711293940485\n",
            "Loss: 0.9103706566977454\n",
            "training error 0.03461993752362554, test error 0.05552889286486381\n",
            "Loss: 1.44231232411971\n",
            "training error 0.034579997515675365, test error 0.055515310818590964\n",
            "Loss: 1.4175001207976656\n",
            "training error 0.03454702683918413, test error 0.05538986997242472\n",
            "Loss: 1.188339969415475\n",
            "training error 0.03460703395171088, test error 0.05551276661241215\n",
            "Loss: 1.412852267320086\n",
            "training error 0.03456996296412204, test error 0.05525140794728828\n",
            "Loss: 0.9353922286221561\n",
            "training error 0.03455022116999642, test error 0.05534323072494689\n",
            "Loss: 1.103137602411297\n",
            "training error 0.03456638826336773, test error 0.0552939783168286\n",
            "Loss: 1.0131614132729672\n",
            "training error 0.034563597691891074, test error 0.055174452186228067\n",
            "Loss: 0.7948064912546071\n",
            "training error 0.034611376486300784, test error 0.05516656214683996\n",
            "Loss: 0.7803926645319148\n",
            "training error 0.03459062844222711, test error 0.05528165951734673\n",
            "Loss: 0.9906569576639157\n",
            "training error 0.03454870352829725, test error 0.05526561297184363\n",
            "Loss: 0.9613425125765573\n",
            "training error 0.03459077051463657, test error 0.05529568959748919\n",
            "Loss: 1.0162876464515858\n",
            "training error 0.03454543832649474, test error 0.055286880877192024\n",
            "Loss: 1.0001955381911953\n",
            "training error 0.034549959916812624, test error 0.05523847436923096\n",
            "Loss: 0.9117646719202455\n",
            "training error 0.03454198886990697, test error 0.055200361618580565\n",
            "Loss: 0.8421388365120519\n",
            "training error 0.03454444960534204, test error 0.05518932549020582\n",
            "Loss: 0.8219776137020629\n",
            "training error 0.034550139288494854, test error 0.055144795623737905\n",
            "Loss: 0.7406287448711524\n",
            "training error 0.034559788108400484, test error 0.05521745459583472\n",
            "Loss: 0.8733649432774993\n",
            "training error 0.03454767824484567, test error 0.05521947054978195\n",
            "Loss: 0.8770477653075259\n",
            "training error 0.03459196400698779, test error 0.05515653745669718\n",
            "Loss: 0.762079175896524\n",
            "training error 0.034553144290890236, test error 0.05525153292526272\n",
            "Loss: 0.9356205431817255\n",
            "training error 0.0345610297799659, test error 0.05527155839237621\n",
            "Loss: 0.9722038349521345\n",
            "training error 0.03462764013725043, test error 0.054999467709138716\n",
            "Loss: 0.47513813373902813\n",
            "training error 0.03458529229013932, test error 0.055145472107976504\n",
            "Loss: 0.7418645722373052\n",
            "training error 0.034549523880524485, test error 0.05518488412325856\n",
            "Loss: 0.8138639541319659\n",
            "training error 0.03455707639837082, test error 0.05515965499820302\n",
            "Loss: 0.7677744203504089\n",
            "training error 0.03455357401081271, test error 0.05510676600015911\n",
            "Loss: 0.6711547691872433\n",
            "training error 0.03456438124394989, test error 0.055080313396164794\n",
            "Loss: 0.6228301371311362\n",
            "training error 0.03463052799761272, test error 0.055122016123026085\n",
            "Loss: 0.6990142788413811\n",
            "training error 0.034604181663549345, test error 0.05503726497390311\n",
            "Loss: 0.5441876274243862\n",
            "training error 0.03457350849814529, test error 0.05507986984485022\n",
            "Loss: 0.6220198405690436\n",
            "training error 0.03461266597220512, test error 0.055153567203912436\n",
            "Loss: 0.7566530041303254\n",
            "training error 0.034598316869252535, test error 0.055166670757844496\n",
            "Loss: 0.7805910792827886\n",
            "training error 0.03456086109432471, test error 0.0549849303249967\n",
            "Loss: 0.4485806825379024\n",
            "training error 0.03462204204097935, test error 0.05507836080589663\n",
            "Loss: 0.6192630703014901\n",
            "training error 0.03455853494776894, test error 0.05496052298896902\n",
            "Loss: 0.40399242448736494\n",
            "training error 0.03455890944658198, test error 0.05500350413531405\n",
            "Loss: 0.4825120319679632\n",
            "training error 0.034576733233857876, test error 0.054798745560416307\n",
            "Loss: 0.10845121003932867\n",
            "training error 0.03459756259212995, test error 0.05473561055052049\n",
            "Loss: 0.0\n",
            "training error 0.03456489063164561, test error 0.0549034006015224\n",
            "Loss: 0.3065464134122342\n",
            "training error 0.03460357480523776, test error 0.05504906679904595\n",
            "Loss: 0.5726733389337824\n",
            "training error 0.034571856101629356, test error 0.054996700833142974\n",
            "Loss: 0.4770025948308332\n",
            "training error 0.03462094016017361, test error 0.055071981702955866\n",
            "Loss: 0.6145380476297202\n",
            "training error 0.03458781750648292, test error 0.055099239757135786\n",
            "Loss: 0.6643375363095227\n",
            "training error 0.034611472366343404, test error 0.05490756044163829\n",
            "Loss: 0.3141462923101912\n",
            "training error 0.03458662825542842, test error 0.055091006211147465\n",
            "Loss: 0.6492951426913418\n",
            "training error 0.03455728608124146, test error 0.05508574821681356\n",
            "Loss: 0.6396889753698787\n",
            "training error 0.03457047970839654, test error 0.05514654777826162\n",
            "Loss: 0.7507675964659954\n",
            "training error 0.03454697582252393, test error 0.05501543697786887\n",
            "Loss: 0.5112328601689864\n",
            "training error 0.03461051780772497, test error 0.055025550893228775\n",
            "Loss: 0.5297106212794889\n",
            "training error 0.03458838246917734, test error 0.055089374292094394\n",
            "Loss: 0.6463136850321405\n",
            "training error 0.03454748682184012, test error 0.05520027055702308\n",
            "Loss: 0.8489171890641911\n",
            "training error 0.03464109845260317, test error 0.055328535189655095\n",
            "Loss: 1.0832520788040645\n",
            "training error 0.03460008490941644, test error 0.055232886509382684\n",
            "Loss: 0.9085053658133946\n",
            "training error 0.03461223203460178, test error 0.055451763077774836\n",
            "Loss: 1.3083850167220579\n",
            "training error 0.034576000455639064, test error 0.055294175660528196\n",
            "Loss: 1.0204784497510166\n",
            "training error 0.03459060818292053, test error 0.05507445212754521\n",
            "Loss: 0.6190514248707979\n",
            "training error 0.03456178602018176, test error 0.055101769007657994\n",
            "Loss: 0.6689583864229354\n",
            "training error 0.034654342377340504, test error 0.055347787993280655\n",
            "Loss: 1.1184262614466256\n",
            "training error 0.03453780552277645, test error 0.055127756118918514\n",
            "Loss: 0.7164359079105953\n",
            "training error 0.034551549051899395, test error 0.05508095381447215\n",
            "Loss: 0.6309297740141551\n",
            "training error 0.03462068750735146, test error 0.05520328502888425\n",
            "Loss: 0.8544245211846047\n",
            "training error 0.03455195668644668, test error 0.0550892318073343\n",
            "Loss: 0.6460533704788496\n",
            "training error 0.034609587457757336, test error 0.054985493312757636\n",
            "Loss: 0.45652685650872105\n",
            "training error 0.03455567287447495, test error 0.05513637915887217\n",
            "Loss: 0.7321898930528592\n",
            "training error 0.034627672632676465, test error 0.05520753247507621\n",
            "Loss: 0.8621844532457512\n",
            "training error 0.03456530635700393, test error 0.05528933203052165\n",
            "Loss: 1.0116293112142793\n",
            "training error 0.034763766243987144, test error 0.05504431764159242\n",
            "Loss: 0.5639967983676719\n",
            "training error 0.03468700045397751, test error 0.05538967890924786\n",
            "Loss: 1.1949594645037864\n",
            "training error 0.03456462563469211, test error 0.05527530968462337\n",
            "Loss: 0.986010987499153\n",
            "training error 0.034588966984814576, test error 0.0554050767168965\n",
            "Loss: 1.223090707571628\n",
            "training error 0.034547130296616074, test error 0.05526978249511022\n",
            "Loss: 0.9759130102270364\n",
            "training error 0.03455532722110738, test error 0.055300027265775535\n",
            "Loss: 1.0311691229498487\n",
            "training error 0.03457922376769009, test error 0.05524967700671969\n",
            "Loss: 0.9391810030596526\n",
            "training error 0.034564867068004226, test error 0.05537097544423973\n",
            "Loss: 1.1607889038395047\n",
            "training error 0.034553341109384, test error 0.055265152834054115\n",
            "Loss: 0.9674547852989823\n",
            "training error 0.034562334803383804, test error 0.0552271728863584\n",
            "Loss: 0.8980667811939513\n",
            "training error 0.03455543807910954, test error 0.05524985055508907\n",
            "Loss: 0.939498069714495\n",
            "training error 0.03465216495242003, test error 0.05519620339036937\n",
            "Loss: 0.8414866212623329\n",
            "training error 0.0345566992690226, test error 0.05527135725421965\n",
            "Loss: 0.978790038716526\n",
            "training error 0.03455855262600988, test error 0.05521713756816463\n",
            "Loss: 0.8797326142908402\n",
            "training error 0.03456509845770704, test error 0.05517797629187224\n",
            "Loss: 0.808186365151542\n",
            "training error 0.034548441668171656, test error 0.055224287893225664\n",
            "Loss: 0.8927960020727932\n",
            "training error 0.034596564712125216, test error 0.05511783593877049\n",
            "Loss: 0.6983120940931231\n",
            "training error 0.03458475644002566, test error 0.055162376864465866\n",
            "Loss: 0.7796867700077481\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.055162376864465866\n",
            "training error 0.12504280428301404, test error 0.2500412701881496\n",
            "training error 0.12503766662869648, test error 0.2502097005127674\n",
            "training error 0.12498304580536292, test error 0.2502011510074309\n",
            "training error 0.12504861923213423, test error 0.25031997850623394\n",
            "training error 0.12498460263683248, test error 0.2502860334327375\n",
            "training error 0.1251148968095411, test error 0.2503942456792911\n",
            "training error 0.12502946292995212, test error 0.25030553345493006\n",
            "training error 0.12500441143917596, test error 0.25040601688272807\n",
            "training error 0.12507411964763449, test error 0.25035812465518076\n",
            "training error 0.1251408690104304, test error 0.25049044161798906\n",
            "training error 0.12496648844471854, test error 0.2503486978159542\n",
            "training error 0.1250327490977499, test error 0.25023876053024025\n",
            "training error 0.12496990909568001, test error 0.2503275187317574\n",
            "training error 0.12497363629967749, test error 0.25033286065621607\n",
            "training error 0.12498481901639212, test error 0.2503710292633707\n",
            "training error 0.12498337550411803, test error 0.25040449305295687\n",
            "training error 0.12527109618963544, test error 0.250223621604713\n",
            "training error 0.12497272816137628, test error 0.2503772637017127\n",
            "training error 0.1250227580875637, test error 0.2503004716036959\n",
            "training error 0.12497301975275739, test error 0.2503821118294015\n",
            "training error 0.12497935606714312, test error 0.25032122298171733\n",
            "training error 0.12499174731884115, test error 0.25038772919325497\n",
            "training error 0.12500970151251517, test error 0.2504128493312774\n",
            "training error 0.12547583617599917, test error 0.250323624237175\n",
            "training error 0.1250837942262248, test error 0.2505966627350828\n",
            "training error 0.12499386145565299, test error 0.25061820484200475\n",
            "training error 0.12502606448745324, test error 0.2505196578762241\n",
            "training error 0.12515992197134687, test error 0.2504187390747529\n",
            "training error 0.12501310165975044, test error 0.2504941499680809\n",
            "training error 0.12520286766207672, test error 0.2503871446732044\n",
            "training error 0.1250650217182115, test error 0.25048331055110923\n",
            "training error 0.12500526597799586, test error 0.2503760124330509\n",
            "training error 0.12512638670966336, test error 0.2503951915499201\n",
            "training error 0.12498584916799255, test error 0.25041351171395654\n",
            "training error 0.12499156417567163, test error 0.25040792168662646\n",
            "training error 0.12496375900076727, test error 0.25045684448588096\n",
            "training error 0.12504965342073512, test error 0.2505507193941513\n",
            "training error 0.12500495598296604, test error 0.25054845943394294\n",
            "training error 0.12508967779976385, test error 0.2504384073625565\n",
            "training error 0.12502212968084123, test error 0.2504291598533534\n",
            "training error 0.1252350107930396, test error 0.2505941333768687\n",
            "training error 0.12504084039341998, test error 0.25045754197229736\n",
            "training error 0.12503599848510183, test error 0.2506104476680437\n",
            "training error 0.12495449413833397, test error 0.25059678521616907\n",
            "training error 0.12496499823045369, test error 0.25053036687402286\n",
            "training error 0.12498323003507875, test error 0.25064337543331194\n",
            "training error 0.12495345576142451, test error 0.25061858233410095\n",
            "training error 0.1251273138844379, test error 0.25070435677296976\n",
            "training error 0.12503226747331606, test error 0.25051710215170603\n",
            "training error 0.1250852871454136, test error 0.25045277108565567\n",
            "Loss: 0.16457319113618585\n",
            "training error 0.12502893586127523, test error 0.2506278555265072\n",
            "Loss: 0.2345954081565127\n",
            "training error 0.12508346344746535, test error 0.25047390165674815\n",
            "Loss: 0.17302402450325438\n",
            "training error 0.12503905411296448, test error 0.250588896915174\n",
            "Loss: 0.21901453572537033\n",
            "training error 0.12503550463726523, test error 0.25046003867750605\n",
            "Loss: 0.16747974805972898\n",
            "training error 0.12497906336159152, test error 0.2505501188408349\n",
            "Loss: 0.20350586617257083\n",
            "training error 0.12495242382928319, test error 0.2506076090624643\n",
            "Loss: 0.22649815923929673\n",
            "training error 0.12504311733782406, test error 0.25056247511035745\n",
            "Loss: 0.2084475582033507\n",
            "training error 0.12496526120568978, test error 0.25067955356467686\n",
            "Loss: 0.25527121024739863\n",
            "training error 0.12496135049111666, test error 0.2507455368621072\n",
            "Loss: 0.2816601729097368\n",
            "training error 0.12495297377495493, test error 0.2508248354629985\n",
            "Loss: 0.31337437786143063\n",
            "training error 0.1249956520852966, test error 0.25084299286813466\n",
            "Loss: 0.32063614113853056\n",
            "training error 0.12495117224813809, test error 0.25080514147933475\n",
            "Loss: 0.3054980846203259\n",
            "training error 0.1250084548499088, test error 0.25077360063278736\n",
            "Loss: 0.2928838283722923\n",
            "training error 0.12506820359183415, test error 0.25076432298373913\n",
            "Loss: 0.28917338127640857\n",
            "training error 0.12500740621234527, test error 0.25084379594955153\n",
            "Loss: 0.3209573206847338\n",
            "training error 0.12509416233788737, test error 0.25086551064233586\n",
            "Loss: 0.3296417641639904\n",
            "training error 0.12498390407270855, test error 0.2508097266742414\n",
            "Loss: 0.3073318598620034\n",
            "training error 0.12501370163151773, test error 0.2507324140136238\n",
            "Loss: 0.27641189990521564\n",
            "training error 0.12503041791472388, test error 0.25058008858800995\n",
            "Loss: 0.21549178639785893\n",
            "training error 0.12500571478373304, test error 0.25061842962468345\n",
            "Loss: 0.23082566973826601\n",
            "training error 0.12516184730962754, test error 0.2505804708520939\n",
            "Loss: 0.21564466679382388\n",
            "training error 0.12496893333576974, test error 0.2505555843790404\n",
            "Loss: 0.2056917206122888\n",
            "training error 0.12500505225571176, test error 0.25055065272943383\n",
            "Loss: 0.20371938636407627\n",
            "training error 0.12496320238596527, test error 0.2505841544808405\n",
            "Loss: 0.2171178750941305\n",
            "training error 0.12498524004477428, test error 0.25060755953760816\n",
            "Loss: 0.22647835256655036\n",
            "training error 0.12493847353187786, test error 0.25057862890597243\n",
            "Loss: 0.21490800995311865\n",
            "training error 0.12497483103584799, test error 0.25053643853915053\n",
            "Loss: 0.1980346486915252\n",
            "training error 0.12494120120951388, test error 0.250608707888585\n",
            "Loss: 0.2269376171415427\n",
            "training error 0.12496888621251263, test error 0.25065532338878604\n",
            "Loss: 0.2455807396012588\n",
            "training error 0.12503484699065032, test error 0.2507165961083049\n",
            "Loss: 0.27008578209795076\n",
            "training error 0.12503350623769666, test error 0.2508144301542414\n",
            "Loss: 0.309212941331638\n",
            "training error 0.12493539182700929, test error 0.250718554200727\n",
            "Loss: 0.2708688897907763\n",
            "training error 0.1249639549402111, test error 0.2506273296021234\n",
            "Loss: 0.23438507312525392\n",
            "training error 0.12494099703765034, test error 0.25058734944145566\n",
            "Loss: 0.2183956484044236\n",
            "training error 0.1249517946939057, test error 0.25058935347264544\n",
            "Loss: 0.21919712857139118\n",
            "training error 0.1249318210181975, test error 0.2506119670642667\n",
            "Loss: 0.22824107223886614\n",
            "training error 0.12504237590029704, test error 0.2505804174147452\n",
            "Loss: 0.21562329538236114\n",
            "training error 0.12492642365833259, test error 0.25065216798216355\n",
            "Loss: 0.24431878527664264\n",
            "training error 0.12503110374636928, test error 0.25073303302455585\n",
            "Loss: 0.2766594634100672\n",
            "training error 0.12494875454078323, test error 0.2505947854772429\n",
            "Loss: 0.22136957178180428\n",
            "training error 0.12498800410850931, test error 0.2506127412400604\n",
            "Loss: 0.22855069144416262\n",
            "training error 0.12494609666076018, test error 0.2505689655048864\n",
            "Loss: 0.21104328750998214\n",
            "training error 0.12494922384780427, test error 0.2505937874470783\n",
            "Loss: 0.22097042560731417\n",
            "training error 0.12494029571550586, test error 0.2506761537422344\n",
            "Loss: 0.25391150573146337\n",
            "training error 0.12503061554366618, test error 0.25074248262516025\n",
            "Loss: 0.2804386797759406\n",
            "training error 0.12510522329974016, test error 0.25076033992445174\n",
            "Loss: 0.2875804205286103\n",
            "training error 0.12496002992771815, test error 0.2506535796864092\n",
            "Loss: 0.24488337377222003\n",
            "training error 0.12497683419560496, test error 0.2507048345433871\n",
            "Loss: 0.2653819326458473\n",
            "training error 0.12494553422880116, test error 0.2506347451189113\n",
            "Loss: 0.23735079025759553\n",
            "training error 0.12496052569309926, test error 0.25071688161575706\n",
            "Loss: 0.27019996622921205\n",
            "training error 0.1249362712292021, test error 0.2506411722739207\n",
            "Loss: 0.2399212279315721\n",
            "training error 0.12494336357575873, test error 0.25069632833892147\n",
            "Loss: 0.261980012451124\n",
            "training error 0.1250348596700426, test error 0.25072698417963285\n",
            "Loss: 0.2742403247940839\n",
            "training error 0.12502716976936737, test error 0.25055207374466887\n",
            "Loss: 0.20428769864067053\n",
            "training error 0.12498540872510616, test error 0.25072910747102173\n",
            "Loss: 0.2750895011669696\n",
            "training error 0.1249749491796945, test error 0.25071960808283933\n",
            "Loss: 0.2712903730569316\n",
            "training error 0.12511453729986313, test error 0.25060197864038347\n",
            "Loss: 0.22424636213531635\n",
            "training error 0.12500409034551313, test error 0.250675512551683\n",
            "Loss: 0.25365507184320624\n",
            "training error 0.12501874020558004, test error 0.25069195797951516\n",
            "Loss: 0.26023215722585924\n",
            "training error 0.12497561336976279, test error 0.25067920462697185\n",
            "Loss: 0.2551316582027674\n",
            "training error 0.12491026112078459, test error 0.25076991289854295\n",
            "Loss: 0.2914089781439255\n",
            "training error 0.1249461039227023, test error 0.25077975932857804\n",
            "Loss: 0.2953469000828379\n",
            "training error 0.12494347633083978, test error 0.250795296810649\n",
            "Loss: 0.3015608671048664\n",
            "training error 0.12502606800112873, test error 0.25083023529114584\n",
            "Loss: 0.3155339526161427\n",
            "training error 0.12492104759844454, test error 0.25076240133567623\n",
            "Loss: 0.28840484892114215\n",
            "training error 0.12493376590076447, test error 0.2507774408270854\n",
            "Loss: 0.29441965255649905\n",
            "training error 0.12494637722069096, test error 0.2507253607455598\n",
            "Loss: 0.2735910583462564\n",
            "training error 0.12508927998551098, test error 0.25077334336170637\n",
            "Loss: 0.2927809369253076\n",
            "training error 0.12502409357414104, test error 0.250640486124362\n",
            "Loss: 0.2396468134086449\n",
            "training error 0.12491597318505594, test error 0.2506583323523182\n",
            "Loss: 0.24678412635814606\n",
            "training error 0.12511081445404723, test error 0.2506954586041736\n",
            "Loss: 0.2616321759730811\n",
            "training error 0.12495045865558031, test error 0.2506470955178424\n",
            "Loss: 0.2422901344393935\n",
            "training error 0.1249823109165215, test error 0.2506038565997099\n",
            "Loss: 0.22499742188037963\n",
            "training error 0.12497731468106761, test error 0.2505390748006478\n",
            "Loss: 0.19908897924074687\n",
            "training error 0.12490744410354786, test error 0.2506080958912296\n",
            "Loss: 0.22669285860430488\n",
            "training error 0.1249563369434906, test error 0.2506738203411406\n",
            "Loss: 0.2529782993483609\n",
            "training error 0.12490471233397414, test error 0.2506408000319685\n",
            "Loss: 0.2397723557266307\n",
            "training error 0.12491133247301249, test error 0.2506700218316981\n",
            "Loss: 0.25145914635429367\n",
            "training error 0.12497641774400296, test error 0.25066340635143325\n",
            "Loss: 0.24881339101161526\n",
            "training error 0.1249712706861044, test error 0.25072691978656125\n",
            "Loss: 0.27421457181677766\n",
            "training error 0.12491089952810439, test error 0.25075038412597483\n",
            "Loss: 0.28359875843362303\n",
            "training error 0.12492752402233959, test error 0.25074742538806\n",
            "Loss: 0.282415458607721\n",
            "training error 0.12518862090946872, test error 0.25056550076916206\n",
            "Loss: 0.20965762196696325\n",
            "training error 0.12492484610790605, test error 0.2506666973718259\n",
            "Loss: 0.25012958189090106\n",
            "training error 0.1249496895124075, test error 0.25082043863435133\n",
            "Loss: 0.3116159366873372\n",
            "training error 0.1249004531939714, test error 0.2507314213681264\n",
            "Loss: 0.27601490724209476\n",
            "training error 0.12490789335563728, test error 0.250716873277605\n",
            "Loss: 0.27019663151888196\n",
            "training error 0.12488664621597033, test error 0.25072823307864084\n",
            "Loss: 0.27473980194323566\n",
            "training error 0.12495322951537706, test error 0.2507107518923331\n",
            "Loss: 0.2677484815525544\n",
            "training error 0.12491567493687465, test error 0.2508143406434363\n",
            "Loss: 0.3091771429192258\n",
            "training error 0.12535824013209257, test error 0.25088529180828484\n",
            "Loss: 0.337552924563278\n",
            "training error 0.12520175741703032, test error 0.25083145615181646\n",
            "Loss: 0.3160222162814419\n",
            "training error 0.12489640962146387, test error 0.25084812992009825\n",
            "Loss: 0.3226906227685955\n",
            "training error 0.12489093938098429, test error 0.2508477410236639\n",
            "Loss: 0.32253508987034607\n",
            "training error 0.12496451963467772, test error 0.25089111272826287\n",
            "Loss: 0.3398809082491905\n",
            "training error 0.12494028686012586, test error 0.2509240869343444\n",
            "Loss: 0.3530684136784812\n",
            "training error 0.12500908011546422, test error 0.25086625342737123\n",
            "Loss: 0.329938829138432\n",
            "training error 0.12500746907987761, test error 0.25073059783711016\n",
            "Loss: 0.27568554920629484\n",
            "training error 0.12487972979823633, test error 0.25084425690417966\n",
            "Loss: 0.32114167210310907\n",
            "training error 0.12493021980349463, test error 0.2508379031560227\n",
            "Loss: 0.3186005923236923\n",
            "training error 0.12487032503971092, test error 0.25082847897849636\n",
            "Loss: 0.3148315435105742\n",
            "training error 0.12503521551486108, test error 0.25070107913857687\n",
            "Loss: 0.26388001865882416\n",
            "training error 0.12490491720515298, test error 0.2507341468802473\n",
            "Loss: 0.2771049321483421\n",
            "training error 0.12486798816177416, test error 0.2507738075541628\n",
            "Loss: 0.2929665832612338\n",
            "training error 0.1249397849185662, test error 0.250731820743597\n",
            "Loss: 0.27617463106301976\n",
            "training error 0.12493629487208598, test error 0.25076112491005403\n",
            "Loss: 0.28789436294367476\n",
            "training error 0.12501285096563067, test error 0.2507529801732997\n",
            "Loss: 0.28463700596887964\n",
            "training error 0.1249194936941661, test error 0.25076655733735476\n",
            "Loss: 0.2900669752075036\n",
            "training error 0.12487284290109989, test error 0.250758907478778\n",
            "Loss: 0.2870075368311742\n",
            "training error 0.12486439743729094, test error 0.25075298537295293\n",
            "Loss: 0.2846390854868819\n",
            "training error 0.12487007488651715, test error 0.2507771104645388\n",
            "Loss: 0.2942875293488578\n",
            "training error 0.1248777198917971, test error 0.25080415968342124\n",
            "Loss: 0.3051054310744705\n",
            "training error 0.12498041816016604, test error 0.25076370134223686\n",
            "Loss: 0.28892476571713566\n",
            "training error 0.12486284077273975, test error 0.25063394767721775\n",
            "Loss: 0.23703186622838768\n",
            "training error 0.12484960071652407, test error 0.2506753845610253\n",
            "Loss: 0.25360388403024015\n",
            "training error 0.12486125956685197, test error 0.2507396126723299\n",
            "Loss: 0.2792908881221301\n",
            "training error 0.12485290679823985, test error 0.250684841466543\n",
            "Loss: 0.25738602187916904\n",
            "training error 0.12496611621883151, test error 0.25063078101951036\n",
            "Loss: 0.23576541221261849\n",
            "training error 0.12489671101495611, test error 0.2506898019717099\n",
            "Loss: 0.2593698964464375\n",
            "training error 0.12483476426206568, test error 0.25076151956376425\n",
            "Loss: 0.28805219837215734\n",
            "training error 0.12495137616589382, test error 0.2506613539330698\n",
            "Loss: 0.24799255916978513\n",
            "training error 0.12490989254392637, test error 0.25066432300165964\n",
            "Loss: 0.24917999058364781\n",
            "training error 0.12486898191244195, test error 0.25069276479805286\n",
            "Loss: 0.26055483137366053\n",
            "training error 0.12483626331987416, test error 0.2506417439556633\n",
            "Loss: 0.24014986288538598\n",
            "training error 0.1248571830784124, test error 0.25067526063625084\n",
            "Loss: 0.2535543223021408\n",
            "training error 0.12486218870425612, test error 0.25069006452732134\n",
            "Loss: 0.25947490135671547\n",
            "training error 0.12485933994840863, test error 0.2507051616643496\n",
            "Loss: 0.2655127594338502\n",
            "training error 0.12489929615544516, test error 0.2505712400444591\n",
            "Loss: 0.21195295317077978\n",
            "training error 0.12488608531830919, test error 0.25056529101753927\n",
            "Loss: 0.20957373516594657\n",
            "training error 0.12487949284196549, test error 0.25058573208401086\n",
            "Loss: 0.2177488122067084\n",
            "training error 0.12491035179203508, test error 0.2506556424403542\n",
            "Loss: 0.2457083391643078\n",
            "training error 0.1248144084064729, test error 0.25073261253991785\n",
            "Loss: 0.27649129731586175\n",
            "training error 0.12492357008263408, test error 0.2508354910152456\n",
            "Loss: 0.3176358952657665\n",
            "training error 0.12480386167126478, test error 0.250782482830333\n",
            "Loss: 0.2964361209754074\n",
            "training error 0.12484503957657998, test error 0.2507226287784599\n",
            "Loss: 0.2724984518746032\n",
            "training error 0.12488197638459014, test error 0.2507577258354896\n",
            "Loss: 0.2865349575295717\n",
            "training error 0.12478872235041581, test error 0.2507570848917873\n",
            "Loss: 0.286278622364633\n",
            "training error 0.12481051907214966, test error 0.2506988326101348\n",
            "Loss: 0.2629815556009829\n",
            "training error 0.12497541914046596, test error 0.25056371968716457\n",
            "Loss: 0.208945306757502\n",
            "training error 0.12505240461518127, test error 0.2507321651658887\n",
            "Loss: 0.27631237724046187\n",
            "training error 0.12491703964244816, test error 0.2505633750430922\n",
            "Loss: 0.20880747188243287\n",
            "training error 0.12479986640173621, test error 0.25069961266360674\n",
            "Loss: 0.26329352548952745\n",
            "training error 0.1247744622453967, test error 0.2506906529385851\n",
            "Loss: 0.25971022701447577\n",
            "training error 0.1247775551022948, test error 0.2507588941300969\n",
            "Loss: 0.28700219824002726\n",
            "training error 0.12480649840652175, test error 0.25081388025972384\n",
            "Loss: 0.30899301982945104\n",
            "training error 0.12479806999732103, test error 0.25073117640659975\n",
            "Loss: 0.2759169388041549\n",
            "training error 0.12474720978550032, test error 0.25070010664878695\n",
            "Loss: 0.26349108694800893\n",
            "training error 0.12483688188184436, test error 0.2507471941481992\n",
            "Loss: 0.2823229779301606\n",
            "training error 0.12479207854496734, test error 0.2506534241302659\n",
            "Loss: 0.2448211615849294\n",
            "training error 0.12480274496394547, test error 0.2506172939950055\n",
            "Loss: 0.23037149284295122\n",
            "training error 0.12491458947603074, test error 0.25074878253043653\n",
            "Loss: 0.28295822595787445\n",
            "training error 0.12471479440669248, test error 0.2507171691326097\n",
            "Loss: 0.27031495398801564\n",
            "training error 0.12484428350351524, test error 0.25064518350742593\n",
            "Loss: 0.24152545650639734\n",
            "training error 0.12470956085597631, test error 0.25061758743580836\n",
            "Loss: 0.23048884979071094\n",
            "training error 0.12480221764489036, test error 0.25068407936955023\n",
            "Loss: 0.2570812333967609\n",
            "training error 0.12474854444294044, test error 0.2506451468572103\n",
            "Loss: 0.24151079883985016\n",
            "training error 0.1247497524325425, test error 0.2505318668911663\n",
            "Loss: 0.19620629132444112\n",
            "training error 0.12476068898489799, test error 0.25057047596690984\n",
            "Loss: 0.2116473725965573\n",
            "training error 0.12467832206984034, test error 0.2507308821465982\n",
            "Loss: 0.2757992542310017\n",
            "training error 0.1246996731446264, test error 0.25059647348934205\n",
            "Loss: 0.2220446651765462\n",
            "training error 0.12470180783566154, test error 0.25065399641896163\n",
            "Loss: 0.24505003927990376\n",
            "training error 0.12466263664674579, test error 0.25063156229361017\n",
            "Loss: 0.23607787027173366\n",
            "training error 0.12463651038905829, test error 0.25057873813652654\n",
            "Loss: 0.21495169496319466\n",
            "training error 0.12467765701538108, test error 0.25055355584827144\n",
            "Loss: 0.20488044223114787\n",
            "training error 0.12465623057502011, test error 0.25044931740371085\n",
            "Loss: 0.1631919463751741\n",
            "training error 0.12461105329272949, test error 0.2504703924278122\n",
            "Loss: 0.17162056461308417\n",
            "training error 0.12464527873566074, test error 0.250485267094325\n",
            "Loss: 0.177569449171866\n",
            "training error 0.12458153072592745, test error 0.25038836657257496\n",
            "Loss: 0.13881563798014973\n",
            "training error 0.12455649576197499, test error 0.2503817291691603\n",
            "Loss: 0.13616111482497306\n",
            "training error 0.12459833069390584, test error 0.2504614810730862\n",
            "Loss: 0.16805661106280567\n",
            "training error 0.12456411166521426, test error 0.25044169724892584\n",
            "Loss: 0.1601443875544728\n",
            "training error 0.12461126010109809, test error 0.2503930920776665\n",
            "Loss: 0.1407055280323144\n",
            "training error 0.1245463512128594, test error 0.2502792373836737\n",
            "Loss: 0.09517116728172681\n",
            "training error 0.12455250085515805, test error 0.25035034072527806\n",
            "Loss: 0.12360780958116102\n",
            "training error 0.12449278509987143, test error 0.2503625912862425\n",
            "Loss: 0.12850722516772883\n",
            "training error 0.12445622113489993, test error 0.2503524886317125\n",
            "Loss: 0.12446683034714745\n",
            "training error 0.12443911144024304, test error 0.2504015942069359\n",
            "Loss: 0.14410581841757253\n",
            "training error 0.12450012324345235, test error 0.2504492750943388\n",
            "Loss: 0.16317502541967066\n",
            "training error 0.12441355979027101, test error 0.2503228411847897\n",
            "Loss: 0.11260980894403083\n",
            "training error 0.12437429072949358, test error 0.2502916892242197\n",
            "Loss: 0.10015108141214757\n",
            "training error 0.12438284088163161, test error 0.25029522223862744\n",
            "Loss: 0.10156405392067747\n",
            "training error 0.12435003556046226, test error 0.2501586142397022\n",
            "Loss: 0.046929873402223876\n",
            "training error 0.12448300228321026, test error 0.2502240167234494\n",
            "Loss: 0.07308654893742883\n",
            "training error 0.12426925750727698, test error 0.2500480518158064\n",
            "Loss: 0.0027122033301685278\n",
            "training error 0.12431367123610786, test error 0.2499805807756178\n",
            "Loss: 0.0\n",
            "training error 0.12423743274577254, test error 0.2500170596925996\n",
            "Loss: 0.014592700308391215\n",
            "training error 0.12423431100550399, test error 0.24993922226493567\n",
            "Loss: 0.0\n",
            "training error 0.12420457697129506, test error 0.2499101752334945\n",
            "Loss: 0.0\n",
            "training error 0.12422399688097642, test error 0.249717773333095\n",
            "Loss: 0.0\n",
            "training error 0.12412202151476612, test error 0.24970326697017178\n",
            "Loss: 0.0\n",
            "training error 0.12412486017811192, test error 0.24963108497986522\n",
            "Loss: 0.0\n",
            "training error 0.12404045182468558, test error 0.2495817489202434\n",
            "Loss: 0.0\n",
            "training error 0.12405409292844377, test error 0.24944553838839453\n",
            "Loss: 0.0\n",
            "training error 0.12400782562703667, test error 0.24943849880290092\n",
            "Loss: 0.0\n",
            "training error 0.12399600962145918, test error 0.24932627887814787\n",
            "Loss: 0.0\n",
            "training error 0.1240266818370058, test error 0.24925380047883056\n",
            "Loss: 0.0\n",
            "training error 0.12393250045803252, test error 0.24919436282638033\n",
            "Loss: 0.0\n",
            "training error 0.12381111432956056, test error 0.2491458930111915\n",
            "Loss: 0.0\n",
            "training error 0.12379987340781293, test error 0.24901131068350746\n",
            "Loss: 0.0\n",
            "training error 0.12385465311229774, test error 0.248869246084986\n",
            "Loss: 0.0\n",
            "training error 0.12374817192801084, test error 0.24877964300389493\n",
            "Loss: 0.0\n",
            "training error 0.12360653107201529, test error 0.24869574434623792\n",
            "Loss: 0.0\n",
            "training error 0.12358012844902505, test error 0.24852804217908897\n",
            "Loss: 0.0\n",
            "training error 0.12350716648727397, test error 0.24837304571998367\n",
            "Loss: 0.0\n",
            "training error 0.1234695376202882, test error 0.24837729971563863\n",
            "Loss: 0.001712744489901219\n",
            "training error 0.12372454627337763, test error 0.24841727463022464\n",
            "Loss: 0.017807451735651902\n",
            "training error 0.12333399295465476, test error 0.24800369354679452\n",
            "Loss: 0.0\n",
            "training error 0.12327083586915877, test error 0.2478982325619646\n",
            "Loss: 0.0\n",
            "training error 0.12319038240345308, test error 0.2476600844345753\n",
            "Loss: 0.0\n",
            "training error 0.12309342580528165, test error 0.24752487766086922\n",
            "Loss: 0.0\n",
            "training error 0.12300783237640989, test error 0.24744689384800492\n",
            "Loss: 0.0\n",
            "training error 0.12291487032889348, test error 0.247313068040477\n",
            "Loss: 0.0\n",
            "training error 0.12285257790679315, test error 0.2471501152683195\n",
            "Loss: 0.0\n",
            "training error 0.12272063299748975, test error 0.24692159790389484\n",
            "Loss: 0.0\n",
            "training error 0.12264697714243954, test error 0.2467956243620747\n",
            "Loss: 0.0\n",
            "training error 0.12253518206743828, test error 0.24663934483232508\n",
            "Loss: 0.0\n",
            "training error 0.12247715772897533, test error 0.24638461269702994\n",
            "Loss: 0.0\n",
            "training error 0.12248393554723258, test error 0.24625850814374012\n",
            "Loss: 0.0\n",
            "training error 0.12234427575659328, test error 0.24598851884469522\n",
            "Loss: 0.0\n",
            "training error 0.12212047969311168, test error 0.24570776772613434\n",
            "Loss: 0.0\n",
            "training error 0.12204013253325798, test error 0.2453309386385916\n",
            "Loss: 0.0\n",
            "training error 0.12187945715610969, test error 0.24519470672768146\n",
            "Loss: 0.0\n",
            "training error 0.12170103457433076, test error 0.24500798667602097\n",
            "Loss: 0.0\n",
            "training error 0.1215703366358009, test error 0.24474153573283233\n",
            "Loss: 0.0\n",
            "training error 0.12143644180021784, test error 0.2443479447617782\n",
            "Loss: 0.0\n",
            "training error 0.12137216653164962, test error 0.24410990335966073\n",
            "Loss: 0.0\n",
            "training error 0.12114818157580097, test error 0.24375786942189415\n",
            "Loss: 0.0\n",
            "training error 0.1209770159565514, test error 0.24345205894019667\n",
            "Loss: 0.0\n",
            "training error 0.12083486222334011, test error 0.24310265921223928\n",
            "Loss: 0.0\n",
            "training error 0.12062156400838536, test error 0.24274411704446436\n",
            "Loss: 0.0\n",
            "training error 0.1204435157159131, test error 0.24222694856078578\n",
            "Loss: 0.0\n",
            "training error 0.12023093788662471, test error 0.2418383171967789\n",
            "Loss: 0.0\n",
            "training error 0.12004806908390851, test error 0.24136512510105154\n",
            "Loss: 0.0\n",
            "training error 0.11981131287793023, test error 0.24087796261766556\n",
            "Loss: 0.0\n",
            "training error 0.11958452949048348, test error 0.24044301119491673\n",
            "Loss: 0.0\n",
            "training error 0.11936422305133673, test error 0.23985864238032611\n",
            "Loss: 0.0\n",
            "training error 0.11915259589379902, test error 0.23936301412851957\n",
            "Loss: 0.0\n",
            "training error 0.11887838207660789, test error 0.2388569066969106\n",
            "Loss: 0.0\n",
            "training error 0.11868774841991338, test error 0.23827287278793877\n",
            "Loss: 0.0\n",
            "training error 0.11839799357301636, test error 0.2378390572512389\n",
            "Loss: 0.0\n",
            "training error 0.11806221110901899, test error 0.23726111646651044\n",
            "Loss: 0.0\n",
            "training error 0.1177951447565773, test error 0.23672737161849491\n",
            "Loss: 0.0\n",
            "training error 0.11782809663780511, test error 0.23606485934885954\n",
            "Loss: 0.0\n",
            "training error 0.11727340421363455, test error 0.2356323970584482\n",
            "Loss: 0.0\n",
            "training error 0.11689097319829111, test error 0.23483293744223288\n",
            "Loss: 0.0\n",
            "training error 0.11649993878269475, test error 0.2341714843585511\n",
            "Loss: 0.0\n",
            "training error 0.11615582505177073, test error 0.23346376282392214\n",
            "Loss: 0.0\n",
            "training error 0.1158996067073847, test error 0.23276022431104998\n",
            "Loss: 0.0\n",
            "training error 0.11550832924883189, test error 0.2319209143879215\n",
            "Loss: 0.0\n",
            "training error 0.11509272105224515, test error 0.23095643263525054\n",
            "Loss: 0.0\n",
            "training error 0.11487893953228559, test error 0.23022213764547464\n",
            "Loss: 0.0\n",
            "training error 0.11419326001522791, test error 0.22928897551611596\n",
            "Loss: 0.0\n",
            "training error 0.1140142855051994, test error 0.22843070037876764\n",
            "Loss: 0.0\n",
            "training error 0.11333843869272071, test error 0.22744025833509512\n",
            "Loss: 0.0\n",
            "training error 0.11289898878202367, test error 0.22652654448583856\n",
            "Loss: 0.0\n",
            "training error 0.11256318930887321, test error 0.22528700340972285\n",
            "Loss: 0.0\n",
            "training error 0.11186860431880724, test error 0.22442438842460297\n",
            "Loss: 0.0\n",
            "training error 0.11142418703958619, test error 0.22343703867533588\n",
            "Loss: 0.0\n",
            "training error 0.11087487043150503, test error 0.22228737518988786\n",
            "Loss: 0.0\n",
            "training error 0.1105018224281501, test error 0.2212604581734697\n",
            "Loss: 0.0\n",
            "training error 0.10984734164862824, test error 0.21994989962424158\n",
            "Loss: 0.0\n",
            "training error 0.10922875797082782, test error 0.21876164613541343\n",
            "Loss: 0.0\n",
            "training error 0.1086710876091048, test error 0.21767474351231633\n",
            "Loss: 0.0\n",
            "training error 0.10807907288643462, test error 0.21642745968022553\n",
            "Loss: 0.0\n",
            "training error 0.10753496053082048, test error 0.2152845483546507\n",
            "Loss: 0.0\n",
            "training error 0.10687310099005798, test error 0.21394652707984513\n",
            "Loss: 0.0\n",
            "training error 0.10632896533861065, test error 0.21255776439784893\n",
            "Loss: 0.0\n",
            "training error 0.10561837141792302, test error 0.21120542427233305\n",
            "Loss: 0.0\n",
            "training error 0.104907604773915, test error 0.20981938459764704\n",
            "Loss: 0.0\n",
            "training error 0.10424634674045793, test error 0.20848431630300512\n",
            "Loss: 0.0\n",
            "training error 0.10361996833940733, test error 0.20699941517198794\n",
            "Loss: 0.0\n",
            "training error 0.10286210602160281, test error 0.20553883342383958\n",
            "Loss: 0.0\n",
            "training error 0.10214285045291309, test error 0.2040553211648535\n",
            "Loss: 0.0\n",
            "training error 0.10144570099263135, test error 0.20255348517285837\n",
            "Loss: 0.0\n",
            "training error 0.10086834164148302, test error 0.2011204753873669\n",
            "Loss: 0.0\n",
            "training error 0.09997849594587375, test error 0.19956366947358883\n",
            "Loss: 0.0\n",
            "training error 0.0992004305164132, test error 0.1979968302840816\n",
            "Loss: 0.0\n",
            "training error 0.09845886375575269, test error 0.19645363876758823\n",
            "Loss: 0.0\n",
            "training error 0.09767462075669987, test error 0.19484415369127522\n",
            "Loss: 0.0\n",
            "training error 0.09695536772651382, test error 0.19308468980559312\n",
            "Loss: 0.0\n",
            "training error 0.09614030471769766, test error 0.1914375686184324\n",
            "Loss: 0.0\n",
            "training error 0.09536397839230223, test error 0.1899044443222138\n",
            "Loss: 0.0\n",
            "training error 0.09453265288843504, test error 0.1882367249876319\n",
            "Loss: 0.0\n",
            "training error 0.09375715650810056, test error 0.1864839452956071\n",
            "Loss: 0.0\n",
            "training error 0.09291533997394265, test error 0.18472024351977936\n",
            "Loss: 0.0\n",
            "training error 0.09217045516559548, test error 0.18290286199111103\n",
            "Loss: 0.0\n",
            "training error 0.09136301184060827, test error 0.18138194213117556\n",
            "Loss: 0.0\n",
            "training error 0.09051392857396777, test error 0.17948566294387566\n",
            "Loss: 0.0\n",
            "training error 0.08979081267293743, test error 0.17773512903061933\n",
            "Loss: 0.0\n",
            "training error 0.08893765046489766, test error 0.17582956226545252\n",
            "Loss: 0.0\n",
            "training error 0.08799327655512686, test error 0.17420388313110655\n",
            "Loss: 0.0\n",
            "training error 0.08723639148349466, test error 0.17254009099190823\n",
            "Loss: 0.0\n",
            "training error 0.0864189754611162, test error 0.17066061588360382\n",
            "Loss: 0.0\n",
            "training error 0.08557353852944052, test error 0.16882277750128702\n",
            "Loss: 0.0\n",
            "training error 0.08505259284467888, test error 0.16735949369070524\n",
            "Loss: 0.0\n",
            "training error 0.08396810502918026, test error 0.16533593326395007\n",
            "Loss: 0.0\n",
            "training error 0.08323803535978277, test error 0.16367129109028683\n",
            "Loss: 0.0\n",
            "training error 0.08233439570614445, test error 0.16190343487919445\n",
            "Loss: 0.0\n",
            "training error 0.08155396282384865, test error 0.16019734458506205\n",
            "Loss: 0.0\n",
            "training error 0.08081425108242386, test error 0.1584452115794904\n",
            "Loss: 0.0\n",
            "training error 0.07997740170976746, test error 0.15661127378053044\n",
            "Loss: 0.0\n",
            "training error 0.07914900469926775, test error 0.15488276737016624\n",
            "Loss: 0.0\n",
            "training error 0.07838125466267419, test error 0.15322600573694728\n",
            "Loss: 0.0\n",
            "training error 0.07760909461447921, test error 0.15154484560402934\n",
            "Loss: 0.0\n",
            "training error 0.07681122501015736, test error 0.14998495892732255\n",
            "Loss: 0.0\n",
            "training error 0.07612067623428442, test error 0.14835985058993834\n",
            "Loss: 0.0\n",
            "training error 0.07544493396642314, test error 0.14670366832699178\n",
            "Loss: 0.0\n",
            "training error 0.07476182640215229, test error 0.145293128859624\n",
            "Loss: 0.0\n",
            "training error 0.073893327312639, test error 0.1437555875951502\n",
            "Loss: 0.0\n",
            "training error 0.0731456587132452, test error 0.14205466870160574\n",
            "Loss: 0.0\n",
            "training error 0.07238910119556419, test error 0.1404237595459792\n",
            "Loss: 0.0\n",
            "training error 0.07171226019832196, test error 0.1388627953746634\n",
            "Loss: 0.0\n",
            "training error 0.07107337858574814, test error 0.1374127838946242\n",
            "Loss: 0.0\n",
            "training error 0.07025843868408294, test error 0.1359806593269348\n",
            "Loss: 0.0\n",
            "training error 0.06965048785807314, test error 0.13443627190998245\n",
            "Loss: 0.0\n",
            "training error 0.06892384104555627, test error 0.1328238109558413\n",
            "Loss: 0.0\n",
            "training error 0.06829786657191655, test error 0.13158444605871938\n",
            "Loss: 0.0\n",
            "training error 0.06776626115253014, test error 0.13019833183414412\n",
            "Loss: 0.0\n",
            "training error 0.06704537923715631, test error 0.12875215936783757\n",
            "Loss: 0.0\n",
            "training error 0.066428308353599, test error 0.12709782009172213\n",
            "Loss: 0.0\n",
            "training error 0.06578050577480384, test error 0.12586239939906024\n",
            "Loss: 0.0\n",
            "training error 0.06535517146025988, test error 0.12425748215491923\n",
            "Loss: 0.0\n",
            "training error 0.06460221017069875, test error 0.12317663952541753\n",
            "Loss: 0.0\n",
            "training error 0.06399780783831849, test error 0.1217736704855418\n",
            "Loss: 0.0\n",
            "training error 0.06341698564639014, test error 0.12060632140626121\n",
            "Loss: 0.0\n",
            "training error 0.06290231602803567, test error 0.11925720728932612\n",
            "Loss: 0.0\n",
            "training error 0.06230092966567497, test error 0.11805862972240769\n",
            "Loss: 0.0\n",
            "training error 0.06182627297530602, test error 0.11674570132325285\n",
            "Loss: 0.0\n",
            "training error 0.06127060102795944, test error 0.11562803996380808\n",
            "Loss: 0.0\n",
            "training error 0.06072202176249222, test error 0.11453750053408107\n",
            "Loss: 0.0\n",
            "training error 0.06024519556408122, test error 0.11331555986815331\n",
            "Loss: 0.0\n",
            "training error 0.059810246905198865, test error 0.11211705695630685\n",
            "Loss: 0.0\n",
            "training error 0.059433813661421774, test error 0.11115129226498298\n",
            "Loss: 0.0\n",
            "training error 0.05875957923341947, test error 0.10992916892349926\n",
            "Loss: 0.0\n",
            "training error 0.05835847650085593, test error 0.10897337783760885\n",
            "Loss: 0.0\n",
            "training error 0.057828410765099064, test error 0.10778496622199059\n",
            "Loss: 0.0\n",
            "training error 0.057335180236102457, test error 0.10671668919004947\n",
            "Loss: 0.0\n",
            "training error 0.05691385841195198, test error 0.10581731876904087\n",
            "Loss: 0.0\n",
            "training error 0.05647067103830266, test error 0.10470263260070775\n",
            "Loss: 0.0\n",
            "training error 0.05603244274587835, test error 0.10370777553022811\n",
            "Loss: 0.0\n",
            "training error 0.055638371696716676, test error 0.102767509552372\n",
            "Loss: 0.0\n",
            "training error 0.055252994300572865, test error 0.10187484041721784\n",
            "Loss: 0.0\n",
            "training error 0.054897464182729325, test error 0.10094205696069834\n",
            "Loss: 0.0\n",
            "training error 0.05457072887082728, test error 0.1002569938041064\n",
            "Loss: 0.0\n",
            "training error 0.05399771644615012, test error 0.09906690496642137\n",
            "Loss: 0.0\n",
            "training error 0.053670465769263764, test error 0.0981969919956252\n",
            "Loss: 0.0\n",
            "training error 0.05323460864228929, test error 0.09742395070254543\n",
            "Loss: 0.0\n",
            "training error 0.05291884250790695, test error 0.09657186683945959\n",
            "Loss: 0.0\n",
            "training error 0.05256874032913681, test error 0.09582124794654341\n",
            "Loss: 0.0\n",
            "training error 0.05221541398396491, test error 0.09501772350663654\n",
            "Loss: 0.0\n",
            "training error 0.05182061744013493, test error 0.09434321063097599\n",
            "Loss: 0.0\n",
            "training error 0.05151840649112339, test error 0.09354891878041337\n",
            "Loss: 0.0\n",
            "training error 0.05117408394647272, test error 0.0927480531610296\n",
            "Loss: 0.0\n",
            "training error 0.05094134938544162, test error 0.09200235018932083\n",
            "Loss: 0.0\n",
            "training error 0.05062873550995596, test error 0.09131535215431977\n",
            "Loss: 0.0\n",
            "training error 0.05024704748917222, test error 0.09069445976011785\n",
            "Loss: 0.0\n",
            "training error 0.04995654858576227, test error 0.08998322849085436\n",
            "Loss: 0.0\n",
            "training error 0.04967047258451884, test error 0.08902974432069982\n",
            "Loss: 0.0\n",
            "training error 0.04936505941365888, test error 0.08855090795931424\n",
            "Loss: 0.0\n",
            "training error 0.04917492488565215, test error 0.08790053647074568\n",
            "Loss: 0.0\n",
            "training error 0.04883777465623149, test error 0.08718876853220167\n",
            "Loss: 0.0\n",
            "training error 0.04859485608658195, test error 0.08641532764861004\n",
            "Loss: 0.0\n",
            "training error 0.04828478791384924, test error 0.08585597824744759\n",
            "Loss: 0.0\n",
            "training error 0.04805386561270444, test error 0.0853036315404015\n",
            "Loss: 0.0\n",
            "training error 0.04776149274876108, test error 0.08461058379080462\n",
            "Loss: 0.0\n",
            "training error 0.04754161507414036, test error 0.08411915634698552\n",
            "Loss: 0.0\n",
            "training error 0.04727083640274406, test error 0.08340664684939111\n",
            "Loss: 0.0\n",
            "training error 0.047033372113694724, test error 0.08278208342749184\n",
            "Loss: 0.0\n",
            "training error 0.046776631836569174, test error 0.082240143382447\n",
            "Loss: 0.0\n",
            "training error 0.046568634347672125, test error 0.08169513201803187\n",
            "Loss: 0.0\n",
            "training error 0.04636181562992519, test error 0.08117910953562692\n",
            "Loss: 0.0\n",
            "training error 0.046133858883696185, test error 0.0807167605512924\n",
            "Loss: 0.0\n",
            "training error 0.045920520088775195, test error 0.08012846568037349\n",
            "Loss: 0.0\n",
            "training error 0.04570791668534607, test error 0.07972402267389427\n",
            "Loss: 0.0\n",
            "training error 0.04552834686172164, test error 0.07925836090007997\n",
            "Loss: 0.0\n",
            "training error 0.045315915342336824, test error 0.07890124226001291\n",
            "Loss: 0.0\n",
            "training error 0.04513510644794913, test error 0.07841023132246774\n",
            "Loss: 0.0\n",
            "training error 0.044924293339678144, test error 0.07799109605322467\n",
            "Loss: 0.0\n",
            "training error 0.04477641605710446, test error 0.07754657051101728\n",
            "Loss: 0.0\n",
            "training error 0.04456502369840794, test error 0.0772212599210255\n",
            "Loss: 0.0\n",
            "training error 0.044376711847830215, test error 0.0767382247645719\n",
            "Loss: 0.0\n",
            "training error 0.0441961406691466, test error 0.07636037801341321\n",
            "Loss: 0.0\n",
            "training error 0.04411548110196588, test error 0.07604320699367277\n",
            "Loss: 0.0\n",
            "training error 0.043883875216158656, test error 0.07544442811174193\n",
            "Loss: 0.0\n",
            "training error 0.04370524257085035, test error 0.07504007267596768\n",
            "Loss: 0.0\n",
            "training error 0.04356573079258901, test error 0.07472811880289028\n",
            "Loss: 0.0\n",
            "training error 0.04337714702385393, test error 0.07439132865330948\n",
            "Loss: 0.0\n",
            "training error 0.04324062948019247, test error 0.07398450446224059\n",
            "Loss: 0.0\n",
            "training error 0.04311575456798117, test error 0.07365842025446553\n",
            "Loss: 0.0\n",
            "training error 0.0429618545511878, test error 0.07321435784710341\n",
            "Loss: 0.0\n",
            "training error 0.0428080748255912, test error 0.07300761434845295\n",
            "Loss: 0.0\n",
            "training error 0.04266645677232421, test error 0.07258942238445507\n",
            "Loss: 0.0\n",
            "training error 0.04257985919663149, test error 0.07233394743816088\n",
            "Loss: 0.0\n",
            "training error 0.0424260296162069, test error 0.07191859667372347\n",
            "Loss: 0.0\n",
            "training error 0.04225900438428568, test error 0.07166484115351644\n",
            "Loss: 0.0\n",
            "training error 0.042112097405044374, test error 0.07134552590233147\n",
            "Loss: 0.0\n",
            "training error 0.042120332326185825, test error 0.07114740826213407\n",
            "Loss: 0.0\n",
            "training error 0.04187648686010593, test error 0.07075179451630162\n",
            "Loss: 0.0\n",
            "training error 0.04173015227253209, test error 0.07034979927157166\n",
            "Loss: 0.0\n",
            "training error 0.041623649077709914, test error 0.07005754638894585\n",
            "Loss: 0.0\n",
            "training error 0.041569877144712455, test error 0.06989027321755574\n",
            "Loss: 0.0\n",
            "training error 0.041378905334480395, test error 0.06962376082261336\n",
            "Loss: 0.0\n",
            "training error 0.041327413163892525, test error 0.06919006701552059\n",
            "Loss: 0.0\n",
            "training error 0.0411886525569887, test error 0.06896671620853635\n",
            "Loss: 0.0\n",
            "training error 0.04105968921103422, test error 0.06874152137160995\n",
            "Loss: 0.0\n",
            "training error 0.04095801915773009, test error 0.06847590327492754\n",
            "Loss: 0.0\n",
            "training error 0.040859562005892795, test error 0.06820555177432724\n",
            "Loss: 0.0\n",
            "training error 0.04077121900286267, test error 0.06788323283413636\n",
            "Loss: 0.0\n",
            "training error 0.04068329260419791, test error 0.06771010025958775\n",
            "Loss: 0.0\n",
            "training error 0.04051231799823552, test error 0.06748267097452318\n",
            "Loss: 0.0\n",
            "training error 0.04043893522774529, test error 0.06728622697252123\n",
            "Loss: 0.0\n",
            "training error 0.040412468023575926, test error 0.06699960622639631\n",
            "Loss: 0.0\n",
            "training error 0.040320800168587995, test error 0.06694785507736285\n",
            "Loss: 0.0\n",
            "training error 0.04018351192891465, test error 0.06662311845546845\n",
            "Loss: 0.0\n",
            "training error 0.04020559257727926, test error 0.066257323057133\n",
            "Loss: 0.0\n",
            "training error 0.040017607507998904, test error 0.0661562347198292\n",
            "Loss: 0.0\n",
            "training error 0.03992634111808809, test error 0.0659388114317387\n",
            "Loss: 0.0\n",
            "training error 0.03986828450445908, test error 0.06596711823867192\n",
            "Loss: 0.04292890077723932\n",
            "training error 0.039746002084322114, test error 0.06561599693751054\n",
            "Loss: 0.0\n",
            "training error 0.03966267905995826, test error 0.06549469045145924\n",
            "Loss: 0.0\n",
            "training error 0.039607745425718, test error 0.06543686775868661\n",
            "Loss: 0.0\n",
            "training error 0.03955668532575419, test error 0.06524455096643828\n",
            "Loss: 0.0\n",
            "training error 0.03943972769474266, test error 0.06483590728729263\n",
            "Loss: 0.0\n",
            "training error 0.03936675011666552, test error 0.06470322045821732\n",
            "Loss: 0.0\n",
            "training error 0.039309283636001564, test error 0.06461758494928824\n",
            "Loss: 0.0\n",
            "training error 0.03920924651282854, test error 0.06432335512566716\n",
            "Loss: 0.0\n",
            "training error 0.03915434806652959, test error 0.06410357502325295\n",
            "Loss: 0.0\n",
            "training error 0.03906156490215837, test error 0.06388293714612567\n",
            "Loss: 0.0\n",
            "training error 0.039009619122018446, test error 0.06362977328474026\n",
            "Loss: 0.0\n",
            "training error 0.03893790605002738, test error 0.06339467204482227\n",
            "Loss: 0.0\n",
            "training error 0.038877088463564656, test error 0.06323666074197491\n",
            "Loss: 0.0\n",
            "training error 0.03881337931266744, test error 0.06306934156356485\n",
            "Loss: 0.0\n",
            "training error 0.038743781426132766, test error 0.06298838148429994\n",
            "Loss: 0.0\n",
            "training error 0.038678461846944776, test error 0.06290986184543292\n",
            "Loss: 0.0\n",
            "training error 0.03864592569627486, test error 0.06267868353615826\n",
            "Loss: 0.0\n",
            "training error 0.03858382245144793, test error 0.06269654780239954\n",
            "Loss: 0.028501342455578005\n",
            "training error 0.03853950300804283, test error 0.06251801765244566\n",
            "Loss: 0.0\n",
            "training error 0.03847041508027402, test error 0.06242678541997762\n",
            "Loss: 0.0\n",
            "training error 0.03840270360622114, test error 0.06233660591482456\n",
            "Loss: 0.0\n",
            "training error 0.038375437699385985, test error 0.062360916190152334\n",
            "Loss: 0.038998394235623124\n",
            "training error 0.03829888136817259, test error 0.06220281096344751\n",
            "Loss: 0.0\n",
            "training error 0.038257745333608886, test error 0.06209158724500416\n",
            "Loss: 0.0\n",
            "training error 0.03821796871726661, test error 0.061897183724711015\n",
            "Loss: 0.0\n",
            "training error 0.0381573081789172, test error 0.0617004595900882\n",
            "Loss: 0.0\n",
            "training error 0.03810733061106689, test error 0.061523068279462974\n",
            "Loss: 0.0\n",
            "training error 0.03803869084357887, test error 0.061441505448017034\n",
            "Loss: 0.0\n",
            "training error 0.03803361629262342, test error 0.06152782806468827\n",
            "Loss: 0.140495608045077\n",
            "training error 0.037937342862962686, test error 0.06133470712280027\n",
            "Loss: 0.0\n",
            "training error 0.03792186303135765, test error 0.061251872589131826\n",
            "Loss: 0.0\n",
            "training error 0.037879049626126635, test error 0.06114939769548785\n",
            "Loss: 0.0\n",
            "training error 0.03779676518173605, test error 0.06090965810797731\n",
            "Loss: 0.0\n",
            "training error 0.03779581890794075, test error 0.060674660837957584\n",
            "Loss: 0.0\n",
            "training error 0.03775482827327824, test error 0.060804983388635644\n",
            "Loss: 0.21478908802821106\n",
            "training error 0.03765750210189738, test error 0.060716943361938426\n",
            "Loss: 0.06968728526355328\n",
            "training error 0.0376330768229609, test error 0.060703367290268956\n",
            "Loss: 0.04731209357402566\n",
            "training error 0.037608099842676754, test error 0.06059024105738422\n",
            "Loss: 0.0\n",
            "training error 0.03753232012154595, test error 0.06041768176025935\n",
            "Loss: 0.0\n",
            "training error 0.037485976284155106, test error 0.06029472410579405\n",
            "Loss: 0.0\n",
            "training error 0.037479253545375894, test error 0.06025515015059492\n",
            "Loss: 0.0\n",
            "training error 0.03744944906397075, test error 0.06008330129381469\n",
            "Loss: 0.0\n",
            "training error 0.03739248038932718, test error 0.06007312765758458\n",
            "Loss: 0.0\n",
            "training error 0.03732000271260067, test error 0.05992178262307483\n",
            "Loss: 0.0\n",
            "training error 0.03730650448317352, test error 0.05990858981598731\n",
            "Loss: 0.0\n",
            "training error 0.037302656968800606, test error 0.059715329763531146\n",
            "Loss: 0.0\n",
            "training error 0.037223428927478826, test error 0.059684800733936096\n",
            "Loss: 0.0\n",
            "training error 0.03724405862766255, test error 0.05974587997325841\n",
            "Loss: 0.10233633784688667\n",
            "training error 0.03716621493177409, test error 0.05956847625565177\n",
            "Loss: 0.0\n",
            "training error 0.03713811403853893, test error 0.05934930610601419\n",
            "Loss: 0.0\n",
            "training error 0.03708876801525054, test error 0.05937295037947236\n",
            "Loss: 0.03983917422039074\n",
            "training error 0.03708096710417683, test error 0.05929900207216823\n",
            "Loss: 0.0\n",
            "training error 0.03707803199320826, test error 0.059313255066439946\n",
            "Loss: 0.024035807979316814\n",
            "training error 0.03699622419892481, test error 0.05918673047600091\n",
            "Loss: 0.0\n",
            "training error 0.036973610371602045, test error 0.058987932218929805\n",
            "Loss: 0.0\n",
            "training error 0.03700748011079035, test error 0.058984760639911586\n",
            "Loss: 0.0\n",
            "training error 0.03690857308322567, test error 0.058972943722231254\n",
            "Loss: 0.0\n",
            "training error 0.03686060790469894, test error 0.05883688736856464\n",
            "Loss: 0.0\n",
            "training error 0.036894497340452956, test error 0.05874990037036336\n",
            "Loss: 0.0\n",
            "training error 0.03679290194762259, test error 0.05876479855756577\n",
            "Loss: 0.025358659518559534\n",
            "training error 0.03677363267086892, test error 0.058725820322280416\n",
            "Loss: 0.0\n",
            "training error 0.03678679737412653, test error 0.05880608061844626\n",
            "Loss: 0.1366695190043954\n",
            "training error 0.036725696552254126, test error 0.058659274148560814\n",
            "Loss: 0.0\n",
            "training error 0.03671027988074967, test error 0.05866164635308074\n",
            "Loss: 0.00404404001645986\n",
            "training error 0.03666579810226317, test error 0.05860286564833614\n",
            "Loss: 0.0\n",
            "training error 0.03663781588264648, test error 0.05851582488583354\n",
            "Loss: 0.0\n",
            "training error 0.036613820552066974, test error 0.05851548094716727\n",
            "Loss: 0.0\n",
            "training error 0.036603064832148216, test error 0.058485747818467056\n",
            "Loss: 0.0\n",
            "training error 0.03656133381808571, test error 0.05832738716600707\n",
            "Loss: 0.0\n",
            "training error 0.03663138925721935, test error 0.05813880768990827\n",
            "Loss: 0.0\n",
            "training error 0.036624093408840534, test error 0.0583675160815758\n",
            "Loss: 0.39338335400234126\n",
            "training error 0.036523392077732635, test error 0.058062209853256816\n",
            "Loss: 0.0\n",
            "training error 0.03647234169325779, test error 0.05818391646019419\n",
            "Loss: 0.209614148763837\n",
            "training error 0.03643299332395272, test error 0.05807924601281259\n",
            "Loss: 0.029341217977818523\n",
            "training error 0.03645832152875401, test error 0.05809029052148612\n",
            "Loss: 0.048363071781576394\n",
            "training error 0.03639202561157139, test error 0.057901033020702174\n",
            "Loss: 0.0\n",
            "training error 0.036481097969265484, test error 0.05800082106505731\n",
            "Loss: 0.17234242490882234\n",
            "training error 0.03632862198530607, test error 0.0578115951709914\n",
            "Loss: 0.0\n",
            "training error 0.03636074939163241, test error 0.057827549199251045\n",
            "Loss: 0.027596588906519948\n",
            "training error 0.03631274574516307, test error 0.05777735133563992\n",
            "Loss: 0.0\n",
            "training error 0.03626982657399465, test error 0.05780814321789827\n",
            "Loss: 0.05329403571905367\n",
            "training error 0.03627392854528346, test error 0.05775316822706244\n",
            "Loss: 0.0\n",
            "training error 0.03628669169103126, test error 0.05764466128946342\n",
            "Loss: 0.0\n",
            "training error 0.03625860838517561, test error 0.0576664314233073\n",
            "Loss: 0.03776608857941621\n",
            "training error 0.03621027332670373, test error 0.057542935682500095\n",
            "Loss: 0.0\n",
            "training error 0.036191899484939026, test error 0.0574876996598281\n",
            "Loss: 0.0\n",
            "training error 0.03621149474938387, test error 0.05739153401429759\n",
            "Loss: 0.0\n",
            "training error 0.03612992939453998, test error 0.05733190017827849\n",
            "Loss: 0.0\n",
            "training error 0.03615155864490338, test error 0.05737628074555517\n",
            "Loss: 0.07740990118707547\n",
            "training error 0.036104418634406936, test error 0.057397265434623944\n",
            "Loss: 0.11401201799032812\n",
            "training error 0.03614647520352903, test error 0.05738467032686411\n",
            "Loss: 0.09204325763061405\n",
            "training error 0.03609292354738742, test error 0.0573366850288735\n",
            "Loss: 0.008345878263460449\n",
            "training error 0.036034175201963024, test error 0.057299701853550024\n",
            "Loss: 0.0\n",
            "training error 0.036015558045774596, test error 0.05732423728337642\n",
            "Loss: 0.04281947206130621\n",
            "training error 0.035980275404830564, test error 0.057297840621564816\n",
            "Loss: 0.0\n",
            "training error 0.03601638147328289, test error 0.05710628985491032\n",
            "Loss: 0.0\n",
            "training error 0.03596985151614652, test error 0.056955999456445305\n",
            "Loss: 0.0\n",
            "training error 0.03595530381427574, test error 0.05676461902245941\n",
            "Loss: 0.0\n",
            "training error 0.03591635857655003, test error 0.05660924840369998\n",
            "Loss: 0.0\n",
            "training error 0.03593794150086055, test error 0.05671043966767429\n",
            "Loss: 0.1787539436183483\n",
            "training error 0.03587881822479607, test error 0.05656439158518063\n",
            "Loss: 0.0\n",
            "training error 0.035892519315280244, test error 0.056482279368135234\n",
            "Loss: 0.0\n",
            "training error 0.03584310890018102, test error 0.05659418576813306\n",
            "Loss: 0.19812656509212712\n",
            "training error 0.035876400981032015, test error 0.0566937194357374\n",
            "Loss: 0.37434761834602703\n",
            "training error 0.03588847845855316, test error 0.05661588257512211\n",
            "Loss: 0.2365400413748997\n",
            "training error 0.0358495573864718, test error 0.0562350733517907\n",
            "Loss: 0.0\n",
            "training error 0.03585317804188718, test error 0.056201723946425065\n",
            "Loss: 0.0\n",
            "training error 0.03582610344681679, test error 0.056389353592752066\n",
            "Loss: 0.333850339725994\n",
            "training error 0.03580484647368733, test error 0.05639299346313522\n",
            "Loss: 0.3403267787523534\n",
            "training error 0.0357552099763062, test error 0.056272164246878184\n",
            "Loss: 0.1253347682364181\n",
            "training error 0.03574279040236696, test error 0.05635183196840169\n",
            "Loss: 0.26708793153698007\n",
            "training error 0.03571298235580061, test error 0.05634460547339113\n",
            "Loss: 0.25422979391569456\n",
            "training error 0.03570277532441554, test error 0.05638361299398691\n",
            "Loss: 0.323636064500854\n",
            "training error 0.03571035450653412, test error 0.05627422857307188\n",
            "Loss: 0.12900783384497405\n",
            "training error 0.035718469674027364, test error 0.056467392285419964\n",
            "Loss: 0.47270496408287066\n",
            "training error 0.035665473626659866, test error 0.05637206800681255\n",
            "Loss: 0.3030940128275539\n",
            "training error 0.03567711720643256, test error 0.05636786359617486\n",
            "Loss: 0.295613084588231\n",
            "training error 0.03563451195026907, test error 0.05625738546187089\n",
            "Loss: 0.09903880439483537\n",
            "training error 0.0356696738240869, test error 0.05627025833362269\n",
            "Loss: 0.12194356753709723\n",
            "training error 0.03563069259529595, test error 0.05627037411392655\n",
            "Loss: 0.12214957599330223\n",
            "training error 0.03565549265139975, test error 0.055979894290940364\n",
            "Loss: 0.0\n",
            "training error 0.03559801161439208, test error 0.05613641630110659\n",
            "Loss: 0.27960397594313235\n",
            "training error 0.03557565593036978, test error 0.0560595586543502\n",
            "Loss: 0.14230888503612427\n",
            "training error 0.03559820205270857, test error 0.056059034741562645\n",
            "Loss: 0.14137299047221052\n",
            "training error 0.03556075027157122, test error 0.05621271034398303\n",
            "Loss: 0.4158922698793699\n",
            "training error 0.03555070200333036, test error 0.05620338109293871\n",
            "Loss: 0.39922690964158125\n",
            "training error 0.03554680891099673, test error 0.056129522531946856\n",
            "Loss: 0.2672892525106274\n",
            "training error 0.03553237899679399, test error 0.056018197455415544\n",
            "Loss: 0.06842307396315306\n",
            "training error 0.03554797637305073, test error 0.05611084346748138\n",
            "Loss: 0.2339218003171606\n",
            "training error 0.03550784269346564, test error 0.05610994720481204\n",
            "Loss: 0.23232075644117334\n",
            "training error 0.03556127473659089, test error 0.056244259884737385\n",
            "Loss: 0.47225097000549443\n",
            "training error 0.035578893298744065, test error 0.056252940205795625\n",
            "Loss: 0.4877571105014633\n",
            "training error 0.03548588482385894, test error 0.05601118568169425\n",
            "Loss: 0.055897552416328544\n",
            "training error 0.03547086334992087, test error 0.055896269797435214\n",
            "Loss: 0.0\n",
            "training error 0.035449011579977055, test error 0.05580504855099523\n",
            "Loss: 0.0\n",
            "training error 0.03547528347657291, test error 0.05571330344588672\n",
            "Loss: 0.0\n",
            "training error 0.03543867158444367, test error 0.055843735697504296\n",
            "Loss: 0.23411329709475126\n",
            "training error 0.03547337499875028, test error 0.056003274256789075\n",
            "Loss: 0.5204696059424974\n",
            "training error 0.035419012113060706, test error 0.055924591098745494\n",
            "Loss: 0.37924093491241706\n",
            "training error 0.03547660160849553, test error 0.055970530055043176\n",
            "Loss: 0.46169692559387965\n",
            "training error 0.035458592519129105, test error 0.05598282835085458\n",
            "Loss: 0.48377117904998546\n",
            "training error 0.03540625852157428, test error 0.055666243897755006\n",
            "Loss: 0.0\n",
            "training error 0.035400794039974925, test error 0.05557314173796074\n",
            "Loss: 0.0\n",
            "training error 0.03548050089312703, test error 0.055543619431823725\n",
            "Loss: 0.0\n",
            "training error 0.03538432913530548, test error 0.05560777314840965\n",
            "Loss: 0.11550150537933401\n",
            "training error 0.035417693868209796, test error 0.05579030651270073\n",
            "Loss: 0.44413216747567485\n",
            "training error 0.0353528781763828, test error 0.05569576924509444\n",
            "Loss: 0.27392851749148406\n",
            "training error 0.035352078521988475, test error 0.05564574354326189\n",
            "Loss: 0.18386290357530477\n",
            "training error 0.035347052343922505, test error 0.05578761896062683\n",
            "Loss: 0.4392935341612114\n",
            "training error 0.03539005900615792, test error 0.05566019629006154\n",
            "Loss: 0.20988343833248102\n",
            "training error 0.035334808713729224, test error 0.05580438467924956\n",
            "Loss: 0.4694783128886826\n",
            "training error 0.035368107554256074, test error 0.05571794404856221\n",
            "Loss: 0.3138517412471842\n",
            "training error 0.035312130076037035, test error 0.05583140710397461\n",
            "Loss: 0.518129130032885\n",
            "training error 0.03533517831605442, test error 0.056026709985778814\n",
            "Loss: 0.8697498630748202\n",
            "training error 0.03532985238223488, test error 0.05579494478814108\n",
            "Loss: 0.45248285741594785\n",
            "training error 0.03528545624252558, test error 0.05589142728530587\n",
            "Loss: 0.6261886730465127\n",
            "training error 0.03528319093825065, test error 0.05574436237231199\n",
            "Loss: 0.361414942961491\n",
            "training error 0.035307372541252276, test error 0.055793808920741934\n",
            "Loss: 0.4504378567286249\n",
            "training error 0.03527265670175429, test error 0.05575572879060768\n",
            "Loss: 0.38187889257794705\n",
            "training error 0.03527389110371197, test error 0.05576812769221327\n",
            "Loss: 0.40420171153072726\n",
            "training error 0.035260561435452864, test error 0.055610278624068414\n",
            "Loss: 0.1200123307169676\n",
            "training error 0.035260672178692096, test error 0.055563054768506905\n",
            "Loss: 0.03499112388063441\n",
            "training error 0.035275802508281676, test error 0.055599350713194315\n",
            "Loss: 0.10033786407994683\n",
            "training error 0.03523682108247073, test error 0.055607658641594226\n",
            "Loss: 0.11529534881877801\n",
            "training error 0.03521893813821993, test error 0.0556238334581028\n",
            "Loss: 0.14441627517187694\n",
            "training error 0.03522815041574048, test error 0.05563119918882056\n",
            "Loss: 0.15767743962802339\n",
            "training error 0.035229125355153004, test error 0.055754365338955794\n",
            "Loss: 0.37942415220302284\n",
            "training error 0.03524016463274929, test error 0.05584660896918212\n",
            "Loss: 0.5454983676933312\n",
            "training error 0.0352032172816092, test error 0.05575239999210706\n",
            "Loss: 0.37588576765257287\n",
            "training error 0.03518407509741002, test error 0.055741413580824586\n",
            "Loss: 0.3561059776517439\n",
            "training error 0.03524395177108508, test error 0.05582862936058325\n",
            "Loss: 0.513128117459738\n",
            "training error 0.03521538700954142, test error 0.05551970949139152\n",
            "Loss: 0.0\n",
            "training error 0.03519817484606677, test error 0.05551905905211681\n",
            "Loss: 0.0\n",
            "training error 0.03514713406935559, test error 0.05568236919455026\n",
            "Loss: 0.2941514953993529\n",
            "training error 0.035175530907658174, test error 0.055591325054615566\n",
            "Loss: 0.13016431425991737\n",
            "training error 0.03517323853494427, test error 0.055596406159775576\n",
            "Loss: 0.13931631583696813\n",
            "training error 0.035125658470733026, test error 0.05565662482240461\n",
            "Loss: 0.24778116314734255\n",
            "training error 0.03513748450577338, test error 0.05545614417924861\n",
            "Loss: 0.0\n",
            "training error 0.03522007587234046, test error 0.05554948710430178\n",
            "Loss: 0.168318454942451\n",
            "training error 0.03520306477473005, test error 0.055490180678574724\n",
            "Loss: 0.061375524443429974\n",
            "training error 0.035097502187020985, test error 0.055389175478145265\n",
            "Loss: 0.0\n",
            "training error 0.035106304245982295, test error 0.05544381859426423\n",
            "Loss: 0.09865305927965196\n",
            "training error 0.03523247147252567, test error 0.05558908677248555\n",
            "Loss: 0.36092123165683\n",
            "training error 0.03509870233421483, test error 0.055422284929032686\n",
            "Loss: 0.05977603133031639\n",
            "training error 0.03509342537773588, test error 0.05530689984275044\n",
            "Loss: 0.0\n",
            "training error 0.03509900535779506, test error 0.05527115984401241\n",
            "Loss: 0.0\n",
            "training error 0.03509099225993475, test error 0.05519600806906973\n",
            "Loss: 0.0\n",
            "training error 0.035078775587011404, test error 0.05537013774430999\n",
            "Loss: 0.31547512461835847\n",
            "training error 0.035073253328140624, test error 0.05527358886275783\n",
            "Loss: 0.14055508070622835\n",
            "training error 0.035070319164643676, test error 0.05512173186867381\n",
            "Loss: 0.0\n",
            "training error 0.035077098009205214, test error 0.05487891326698697\n",
            "Loss: 0.0\n",
            "training error 0.03505007079528984, test error 0.05505433163467218\n",
            "Loss: 0.31964621243827374\n",
            "training error 0.03505502453395703, test error 0.05514213416145803\n",
            "Loss: 0.47963940756350354\n",
            "training error 0.03508542764232653, test error 0.054969820974881727\n",
            "Loss: 0.16565143601239285\n",
            "training error 0.03506538955102691, test error 0.05510000307381262\n",
            "Loss: 0.40286841277275\n",
            "training error 0.03511936608228192, test error 0.05499752687833373\n",
            "Loss: 0.2161369536778146\n",
            "training error 0.035066352411852776, test error 0.05518131331557118\n",
            "Loss: 0.5510314082078027\n",
            "training error 0.035023242148178715, test error 0.05516162302646181\n",
            "Loss: 0.5151518910359254\n",
            "training error 0.03502050814690738, test error 0.05524221818254319\n",
            "Loss: 0.6620118619855564\n",
            "training error 0.03501838320204702, test error 0.05524611977173919\n",
            "Loss: 0.6691213125262774\n",
            "training error 0.03498748509772507, test error 0.05512527236236246\n",
            "Loss: 0.4489139465589487\n",
            "training error 0.03501913993085431, test error 0.055177791710946805\n",
            "Loss: 0.5446143630902123\n",
            "training error 0.034981738881823085, test error 0.05512357548758943\n",
            "Loss: 0.44582191234758817\n",
            "training error 0.03502965461640893, test error 0.05495737827092825\n",
            "Loss: 0.14297842152877216\n",
            "training error 0.03500250412231547, test error 0.05508280598703746\n",
            "Loss: 0.371531992731966\n",
            "training error 0.034995768329132704, test error 0.05506997158422662\n",
            "Loss: 0.34814522712967033\n",
            "training error 0.0349657913721349, test error 0.0551744490239153\n",
            "Loss: 0.5385233404505474\n",
            "training error 0.03497088169057846, test error 0.055144119145383155\n",
            "Loss: 0.4832564324041\n",
            "training error 0.035012604036494734, test error 0.05518823826867154\n",
            "Loss: 0.563650012855943\n",
            "training error 0.034962827110804555, test error 0.055213835087229816\n",
            "Loss: 0.6102923697002716\n",
            "training error 0.03500752684312286, test error 0.05530279361220149\n",
            "Loss: 0.7723920172258758\n",
            "training error 0.03494841471885171, test error 0.055235063044645726\n",
            "Loss: 0.6489738160923242\n",
            "training error 0.03497221826089181, test error 0.05517885635074299\n",
            "Loss: 0.5465543428252451\n",
            "training error 0.03495748515448109, test error 0.055008020761843454\n",
            "Loss: 0.23525884018216203\n",
            "training error 0.0350366857645035, test error 0.05490412153529331\n",
            "Loss: 0.04593434309403399\n",
            "training error 0.03494438250812691, test error 0.055165267969658885\n",
            "Loss: 0.5217936829011061\n",
            "training error 0.03500252208035449, test error 0.055052350540450316\n",
            "Loss: 0.31603627538974166\n",
            "training error 0.03496711972160893, test error 0.055136582863526594\n",
            "Loss: 0.4695238684593761\n",
            "training error 0.03494804008289157, test error 0.05533746898449465\n",
            "Loss: 0.8355772558337637\n",
            "training error 0.0349635489057993, test error 0.05504560842864736\n",
            "Loss: 0.303750842968431\n",
            "training error 0.034957680989319125, test error 0.055080297707997286\n",
            "Loss: 0.3669614229250051\n",
            "training error 0.034935515984631765, test error 0.05481205261983193\n",
            "Loss: 0.0\n",
            "training error 0.034899293083168796, test error 0.05481921994925869\n",
            "Loss: 0.013076192341254789\n",
            "training error 0.034893187330222, test error 0.05485747243369599\n",
            "Loss: 0.08286464690363537\n",
            "training error 0.03493778120992842, test error 0.05478047431865601\n",
            "Loss: 0.0\n",
            "training error 0.034908562782672785, test error 0.05475610713752311\n",
            "Loss: 0.0\n",
            "training error 0.034924883920205825, test error 0.054783737361877895\n",
            "Loss: 0.05046053453980903\n",
            "training error 0.03489786717949407, test error 0.054800236552245134\n",
            "Loss: 0.08059268094275929\n",
            "training error 0.0349105276823132, test error 0.05473891613029986\n",
            "Loss: 0.0\n",
            "training error 0.03492141133411353, test error 0.054837445311364835\n",
            "Loss: 0.17999841434646058\n",
            "training error 0.034923554373141596, test error 0.05469758968458307\n",
            "Loss: 0.0\n",
            "training error 0.0349096849984596, test error 0.054586656266197256\n",
            "Loss: 0.0\n",
            "training error 0.0348980921431474, test error 0.05467414282300754\n",
            "Loss: 0.16027095776602884\n",
            "training error 0.03491101393109469, test error 0.05483355386471431\n",
            "Loss: 0.4523039427676734\n",
            "training error 0.03488652064035178, test error 0.05487746056954135\n",
            "Loss: 0.5327388106096009\n",
            "training error 0.034899950331539906, test error 0.05485951136113341\n",
            "Loss: 0.49985676646970845\n",
            "training error 0.034882398281655874, test error 0.05502585708954922\n",
            "Loss: 0.8045937476187426\n",
            "training error 0.034860615982919785, test error 0.054922683019752304\n",
            "Loss: 0.6155840576062666\n",
            "training error 0.03496441637843188, test error 0.05479056473148516\n",
            "Loss: 0.3735500197951769\n",
            "training error 0.03492513657933464, test error 0.05511683332400325\n",
            "Loss: 0.9712576187494104\n",
            "training error 0.03489653856111713, test error 0.05510347732594066\n",
            "Loss: 0.9467901042025328\n",
            "training error 0.03489668296342794, test error 0.05485349956004194\n",
            "Loss: 0.4888434502076766\n",
            "training error 0.034874456801805724, test error 0.05499475396551417\n",
            "Loss: 0.747614393757301\n",
            "training error 0.0348616324342727, test error 0.05494850577193914\n",
            "Loss: 0.6628900366735913\n",
            "training error 0.03483699271330372, test error 0.05483131615287962\n",
            "Loss: 0.4482045675947788\n",
            "training error 0.03484823053201016, test error 0.05497230449244502\n",
            "Loss: 0.7064880918280059\n",
            "training error 0.03484352956643614, test error 0.0549390145506425\n",
            "Loss: 0.645502598156833\n",
            "training error 0.03484541779256968, test error 0.05504045073621878\n",
            "Loss: 0.8313285719655505\n",
            "training error 0.034859877380882615, test error 0.054931257645049275\n",
            "Loss: 0.6312923385003444\n",
            "training error 0.034836957688571146, test error 0.05500710787601863\n",
            "Loss: 0.7702461344600398\n",
            "training error 0.03484561217172084, test error 0.05495091799818196\n",
            "Loss: 0.6673091134367093\n",
            "training error 0.034834925244896835, test error 0.055043677330551544\n",
            "Loss: 0.8372395299788726\n",
            "training error 0.03483497376244478, test error 0.05501551487147895\n",
            "Loss: 0.7856473259514551\n",
            "training error 0.03484347769913937, test error 0.054980939536604566\n",
            "Loss: 0.7223070570297363\n",
            "training error 0.03483099226854958, test error 0.0550648775589922\n",
            "Loss: 0.8760772787819171\n",
            "training error 0.03488544598563179, test error 0.05493301261202631\n",
            "Loss: 0.6345073494518738\n",
            "training error 0.034868466024216636, test error 0.05493293124033502\n",
            "Loss: 0.6343582806191961\n",
            "training error 0.034844280183616606, test error 0.055143577846892576\n",
            "Loss: 1.0202522352338983\n",
            "training error 0.03483224595366196, test error 0.05514206379883895\n",
            "Loss: 1.0174785755939997\n",
            "training error 0.0348317966916743, test error 0.05512148699234491\n",
            "Loss: 0.979782904341131\n",
            "training error 0.034835288421492305, test error 0.055033439049334616\n",
            "Loss: 0.8184835153825398\n",
            "training error 0.03482049432376612, test error 0.055108368627510756\n",
            "Loss: 0.9557507218784789\n",
            "training error 0.03490138646161657, test error 0.05505096967542642\n",
            "Loss: 0.8505987378397029\n",
            "training error 0.03486313453729591, test error 0.0549738816770924\n",
            "Loss: 0.7093774145219722\n",
            "training error 0.03482867034480369, test error 0.05475669525041616\n",
            "Loss: 0.31150283942964485\n",
            "training error 0.03480974394115757, test error 0.05492173212830592\n",
            "Loss: 0.6138420724556504\n",
            "training error 0.034809249264253855, test error 0.05487947250489947\n",
            "Loss: 0.5364245746694252\n",
            "training error 0.034799029855939606, test error 0.05495781371537326\n",
            "Loss: 0.6799417193938728\n",
            "training error 0.0348633621532664, test error 0.054878492394083635\n",
            "Loss: 0.5346290611082827\n",
            "training error 0.03485416014720542, test error 0.05465942272391841\n",
            "Loss: 0.13330447896697795\n",
            "training error 0.03484218830356973, test error 0.05482176374289704\n",
            "Loss: 0.4307050344927843\n",
            "training error 0.034794032075211014, test error 0.05473318319068305\n",
            "Loss: 0.26842993234690304\n",
            "training error 0.03485087916592869, test error 0.0548227676762153\n",
            "Loss: 0.4325441896763538\n",
            "training error 0.034796608478238, test error 0.05492512087950464\n",
            "Loss: 0.6200500936654363\n",
            "training error 0.03480295146297503, test error 0.05489418809763516\n",
            "Loss: 0.5633827980563444\n",
            "training error 0.034777984534308234, test error 0.054895641951855034\n",
            "Loss: 0.5660461856300092\n",
            "training error 0.03482824454584735, test error 0.055078083665164215\n",
            "Loss: 0.900270198948383\n",
            "training error 0.03476403945016334, test error 0.05495684094362787\n",
            "Loss: 0.6781596506394649\n",
            "training error 0.03476924764520996, test error 0.05495033084765728\n",
            "Loss: 0.6662334833013706\n",
            "training error 0.034807312560188354, test error 0.05483323423864192\n",
            "Loss: 0.4517184039304345\n",
            "training error 0.03476591476760163, test error 0.05474383487344496\n",
            "Loss: 0.2879432777146196\n",
            "training error 0.034782226636174536, test error 0.05470124466508831\n",
            "Loss: 0.209920164979982\n",
            "training error 0.034819660367379254, test error 0.0548709036605483\n",
            "Loss: 0.5207268841763923\n",
            "training error 0.03478613542156303, test error 0.054804923526628975\n",
            "Loss: 0.3998546079967191\n",
            "training error 0.03479954082548055, test error 0.05473201935094564\n",
            "Loss: 0.26629783667184714\n",
            "training error 0.034804766808033225, test error 0.05485603979489317\n",
            "Loss: 0.4934970322824528\n",
            "training error 0.034776501401166074, test error 0.05480594810179995\n",
            "Loss: 0.4017315780129316\n",
            "training error 0.03479083213860183, test error 0.05488846431694045\n",
            "Loss: 0.5528971206285149\n",
            "training error 0.03481329515287007, test error 0.054915937642984546\n",
            "Loss: 0.6032268677193198\n",
            "training error 0.034771737693135386, test error 0.054897031457675834\n",
            "Loss: 0.5685916901834176\n",
            "training error 0.03477352568378162, test error 0.05497874481146908\n",
            "Loss: 0.7182864313208137\n",
            "training error 0.03474185815279572, test error 0.05490102994907336\n",
            "Loss: 0.5759167246717434\n",
            "training error 0.0347731347126031, test error 0.05489955491944471\n",
            "Loss: 0.5732145448176462\n",
            "training error 0.034782853873697374, test error 0.054916971878158805\n",
            "Loss: 0.6051215343741312\n",
            "training error 0.034845282949350936, test error 0.05498257387297988\n",
            "Loss: 0.7253010788055958\n",
            "training error 0.03472977796569407, test error 0.05472792598972461\n",
            "Loss: 0.2587990054537137\n",
            "training error 0.03474223536694346, test error 0.05468842431513575\n",
            "Loss: 0.18643393074346815\n",
            "training error 0.0347890979756974, test error 0.05480214334985844\n",
            "Loss: 0.3947614644325137\n",
            "training error 0.034864249174616536, test error 0.05456242386790985\n",
            "Loss: 0.0\n",
            "training error 0.03477212117359367, test error 0.05490926967590024\n",
            "Loss: 0.635686216635234\n",
            "training error 0.034764167759743364, test error 0.05487281099447789\n",
            "Loss: 0.5688660887197106\n",
            "training error 0.034780853033113554, test error 0.0547938558297932\n",
            "Loss: 0.4241599721515721\n",
            "training error 0.03477884471466851, test error 0.054945396062441344\n",
            "Loss: 0.7018973267364892\n",
            "training error 0.03474339156473031, test error 0.05491810137802866\n",
            "Loss: 0.6518726348739179\n",
            "training error 0.03477140294886119, test error 0.05488924041464613\n",
            "Loss: 0.5989773246281471\n",
            "training error 0.034793009655222966, test error 0.054787544301240526\n",
            "Loss: 0.41259243518152466\n",
            "training error 0.03475436830562053, test error 0.054973219455985434\n",
            "Loss: 0.7528910172137637\n",
            "training error 0.03474926011681389, test error 0.05508234906967432\n",
            "Loss: 0.95289975207693\n",
            "training error 0.03473911257732061, test error 0.05510025739511075\n",
            "Loss: 0.9857214710676132\n",
            "training error 0.034764597854697654, test error 0.055093774734817365\n",
            "Loss: 0.9738402901488863\n",
            "training error 0.03473876450092973, test error 0.055083328415667654\n",
            "Loss: 0.9546946613274709\n",
            "training error 0.034749953236501416, test error 0.05510693120350333\n",
            "Loss: 0.9979529811792842\n",
            "training error 0.0347406927159477, test error 0.05503632344556463\n",
            "Loss: 0.868545684117783\n",
            "training error 0.03474904848841092, test error 0.054967913932616574\n",
            "Loss: 0.7431672494762731\n",
            "training error 0.034792287731346495, test error 0.055138283616481494\n",
            "Loss: 1.055414528441312\n",
            "training error 0.03480010767995758, test error 0.055047710673776465\n",
            "Loss: 0.8894157763985167\n",
            "training error 0.03471244111135342, test error 0.05503196051586173\n",
            "Loss: 0.8605494673194602\n",
            "training error 0.03474067523437557, test error 0.054857967498842516\n",
            "Loss: 0.5416614768584127\n",
            "training error 0.034719699803587155, test error 0.05474972300149929\n",
            "Loss: 0.343274950619632\n",
            "training error 0.03478493636995888, test error 0.05495690919346613\n",
            "Loss: 0.7229981690536569\n",
            "training error 0.03470916828697754, test error 0.054881477666414956\n",
            "Loss: 0.5847500457045518\n",
            "training error 0.03470706444975706, test error 0.054931689377201\n",
            "Loss: 0.6767762190790894\n",
            "training error 0.03471534535582711, test error 0.05488499884266399\n",
            "Loss: 0.591203527788764\n",
            "training error 0.03472398088591338, test error 0.05493562676846415\n",
            "Loss: 0.6839925247049017\n",
            "training error 0.03475038678921536, test error 0.05481830536251918\n",
            "Loss: 0.468970174838268\n",
            "training error 0.03470363993019446, test error 0.054899643069666576\n",
            "Loss: 0.6180429274423416\n",
            "training error 0.034775272158749204, test error 0.05486553557631808\n",
            "Loss: 0.5555319703941963\n",
            "training error 0.034733126309804116, test error 0.05506899329318809\n",
            "Loss: 0.9284217770541003\n",
            "training error 0.03475053334885475, test error 0.05504083061352694\n",
            "Loss: 0.876806255483209\n",
            "training error 0.034733159825079754, test error 0.05501696712229855\n",
            "Loss: 0.8330701280593988\n",
            "training error 0.034723524745895275, test error 0.055151981160460974\n",
            "Loss: 1.0805188823326262\n",
            "training error 0.03474170978038246, test error 0.0550180555721113\n",
            "Loss: 0.8350649987700187\n",
            "training error 0.03471886397497355, test error 0.05508172418023431\n",
            "Loss: 0.9517544777366105\n",
            "training error 0.03480070871426797, test error 0.05518263302675186\n",
            "Loss: 1.136696493439282\n",
            "training error 0.03472155716785523, test error 0.0551449287854748\n",
            "Loss: 1.0675935493172695\n",
            "training error 0.03471760676790591, test error 0.055127853049355226\n",
            "Loss: 1.0362977693480602\n",
            "training error 0.034727422735844754, test error 0.055176883414986125\n",
            "Loss: 1.1261588168513637\n",
            "training error 0.034756476414248026, test error 0.05516633459308129\n",
            "Loss: 1.106825324757299\n",
            "training error 0.03475059611048048, test error 0.054886725673445165\n",
            "Loss: 0.5943683996158633\n",
            "training error 0.03470525532325502, test error 0.05507051599697591\n",
            "Loss: 0.9312125324492548\n",
            "training error 0.03474011255357048, test error 0.055103464160049596\n",
            "Loss: 0.9915987116876579\n",
            "training error 0.03479619202172378, test error 0.05511723299645711\n",
            "Loss: 1.0168337277141504\n",
            "training error 0.03472695157364745, test error 0.05497176099496\n",
            "Loss: 0.7502180035863448\n",
            "training error 0.03471627486499684, test error 0.05506397105933486\n",
            "Loss: 0.9192172119024677\n",
            "training error 0.034724768473995464, test error 0.055006359836300006\n",
            "Loss: 0.8136294851285975\n",
            "training error 0.03470831027279273, test error 0.05487291011402085\n",
            "Loss: 0.5690477513657699\n",
            "training error 0.03478208666808402, test error 0.05504846668016128\n",
            "Loss: 0.8908013570439799\n",
            "training error 0.03470017025359727, test error 0.054929755261202536\n",
            "Loss: 0.6732314425436092\n",
            "training error 0.034735263441680965, test error 0.05490468321033594\n",
            "Loss: 0.627280311546774\n",
            "training error 0.034715284129300875, test error 0.05493424106611579\n",
            "Loss: 0.681452860499876\n",
            "training error 0.03470841813613773, test error 0.05499725764938097\n",
            "Loss: 0.7969473323322651\n",
            "training error 0.03474251674377028, test error 0.055102697793598124\n",
            "Loss: 0.99019414349375\n",
            "training error 0.03477982678622371, test error 0.054862505875062605\n",
            "Loss: 0.5499792455687658\n",
            "training error 0.034697881265686376, test error 0.054976197868531904\n",
            "Loss: 0.7583497419831664\n",
            "training error 0.03471621248188624, test error 0.05502582973817873\n",
            "Loss: 0.8493132038832174\n",
            "training error 0.03475111122373216, test error 0.05484516191456714\n",
            "Loss: 0.5181918738466962\n",
            "training error 0.034749351821746495, test error 0.0548239887060072\n",
            "Loss: 0.47938639736859745\n",
            "training error 0.03473958529450631, test error 0.05487862886994925\n",
            "Loss: 0.579528876511981\n",
            "training error 0.034774318270576476, test error 0.0550331226626067\n",
            "Loss: 0.8626794070519583\n",
            "training error 0.03467797726981049, test error 0.05507698405863812\n",
            "Loss: 0.9430669575346773\n",
            "training error 0.03480489499122006, test error 0.054845561446778074\n",
            "Loss: 0.5189241217612883\n",
            "training error 0.03466738201372974, test error 0.0549841581902401\n",
            "Loss: 0.7729391262954799\n",
            "training error 0.0346723795299066, test error 0.05484203868134513\n",
            "Loss: 0.5124677270793576\n",
            "training error 0.03467691655882989, test error 0.05489244310390107\n",
            "Loss: 0.6048470954849172\n",
            "training error 0.03470359093006475, test error 0.05501052542046449\n",
            "Loss: 0.8212640142957195\n",
            "training error 0.03468188438184241, test error 0.05499966249407472\n",
            "Loss: 0.801354843075508\n",
            "training error 0.03475051122903548, test error 0.0551078092615913\n",
            "Loss: 0.9995622536890458\n",
            "training error 0.03473962448720039, test error 0.05481582784105102\n",
            "Loss: 0.46442946478080227\n",
            "training error 0.03471743632907798, test error 0.054969474534605545\n",
            "Loss: 0.7460274632980601\n",
            "training error 0.03470281551046801, test error 0.05507147838941396\n",
            "Loss: 0.9329763698483928\n",
            "training error 0.034736989513097005, test error 0.054964693474344635\n",
            "Loss: 0.7372649122198816\n",
            "training error 0.03470564779553792, test error 0.05513795963481302\n",
            "Loss: 1.0548207467770965\n",
            "training error 0.03470184355217847, test error 0.05498179474197584\n",
            "Loss: 0.7686074854028613\n",
            "training error 0.03471915898110698, test error 0.05509428434149054\n",
            "Loss: 0.9747742784819602\n",
            "training error 0.034786997732397705, test error 0.05501604444921291\n",
            "Loss: 0.8313790868258231\n",
            "training error 0.03469828627742709, test error 0.05525954024179028\n",
            "Loss: 1.2776492033566456\n",
            "training error 0.03471381034379655, test error 0.05527446638097144\n",
            "Loss: 1.3050052812634938\n",
            "training error 0.034687599521165666, test error 0.055087269022740575\n",
            "Loss: 0.9619168607709394\n",
            "training error 0.034710894679110954, test error 0.055163288811238914\n",
            "Loss: 1.1012431280247093\n",
            "training error 0.0346992805526672, test error 0.05497233515495504\n",
            "Loss: 0.7512703028691448\n",
            "training error 0.034712124201108495, test error 0.05503183779860221\n",
            "Loss: 0.8603245556479955\n",
            "training error 0.034733229344750434, test error 0.05477853523956119\n",
            "Loss: 0.3960809588931191\n",
            "training error 0.03471407888285516, test error 0.05488288096132173\n",
            "Loss: 0.5873219529023865\n",
            "training error 0.034674680252407324, test error 0.054977709266002844\n",
            "Loss: 0.7611197755773569\n",
            "training error 0.03477551450137605, test error 0.05473009015498431\n",
            "Loss: 0.3072925929397341\n",
            "training error 0.0347074300767054, test error 0.05505464915219782\n",
            "Loss: 0.9021323639866186\n",
            "training error 0.03473764407599826, test error 0.05503545745263841\n",
            "Loss: 0.8669585241918965\n",
            "training error 0.03466464369949505, test error 0.054985646352499085\n",
            "Loss: 0.775666575249323\n",
            "training error 0.03469725331399405, test error 0.054874828994757555\n",
            "Loss: 0.5725646052748834\n",
            "training error 0.03471453560834299, test error 0.0547202043073086\n",
            "Loss: 0.28917417558413927\n",
            "training error 0.03468607070101256, test error 0.054734365342615526\n",
            "Loss: 0.31512799930211166\n",
            "training error 0.03465821457262059, test error 0.054829543706110186\n",
            "Loss: 0.48956739687189277\n",
            "training error 0.03467442062615763, test error 0.05486621372849173\n",
            "Loss: 0.5567748627101521\n",
            "training error 0.034683298289122644, test error 0.05462480176159269\n",
            "Loss: 0.11432390509968915\n",
            "training error 0.034672728622389425, test error 0.054649018336475376\n",
            "Loss: 0.15870715123500467\n",
            "training error 0.03468428921015908, test error 0.054789383655972024\n",
            "Loss: 0.4159635367585901\n",
            "training error 0.03467544061121463, test error 0.05451461524650647\n",
            "Loss: 0.0\n",
            "training error 0.034672570628086735, test error 0.05452001416800589\n",
            "Loss: 0.009903622129603384\n",
            "training error 0.03466585502930277, test error 0.054578968126890046\n",
            "Loss: 0.11804702297280034\n",
            "training error 0.0346786308033559, test error 0.05442936893866669\n",
            "Loss: 0.0\n",
            "training error 0.034656604466635585, test error 0.05458201108252537\n",
            "Loss: 0.28044077459483674\n",
            "training error 0.03463705380313447, test error 0.05470468915758966\n",
            "Loss: 0.5058302609262544\n",
            "training error 0.03467795847741983, test error 0.054723129684420065\n",
            "Loss: 0.5397099975279707\n",
            "training error 0.03466585438981454, test error 0.05464646445901885\n",
            "Loss: 0.3988573165284892\n",
            "training error 0.034649461174157445, test error 0.05478275930536696\n",
            "Loss: 0.6492641263184273\n",
            "training error 0.03467851372862567, test error 0.054896642762585685\n",
            "Loss: 0.8584957588715358\n",
            "training error 0.034672068895940794, test error 0.05467918377492199\n",
            "Loss: 0.458970664416114\n",
            "training error 0.03465124710642732, test error 0.05459551780950219\n",
            "Loss: 0.3052559198742921\n",
            "training error 0.03467957609110655, test error 0.054709223885480184\n",
            "Loss: 0.5141616599098286\n",
            "training error 0.03464789763865845, test error 0.05459533758741886\n",
            "Loss: 0.30492480803734345\n",
            "training error 0.034654672319654176, test error 0.05476097265301545\n",
            "Loss: 0.6092367426167034\n",
            "training error 0.03466372262540249, test error 0.05474912940278657\n",
            "Loss: 0.5874778090486421\n",
            "training error 0.03464978976377824, test error 0.05463139501962271\n",
            "Loss: 0.37117108813748967\n",
            "training error 0.03467895463587968, test error 0.054646308158239204\n",
            "Loss: 0.39857015394937356\n",
            "training error 0.03464046189217033, test error 0.05481996665415731\n",
            "Loss: 0.7176230831019925\n",
            "training error 0.034684813064930264, test error 0.05455987628515656\n",
            "Loss: 0.23977376375046155\n",
            "training error 0.034649210080246615, test error 0.05455905381213667\n",
            "Loss: 0.2382626806055832\n",
            "training error 0.03465526859241012, test error 0.054745248409221604\n",
            "Loss: 0.5803474791538754\n",
            "training error 0.03467090737646226, test error 0.05477344210581695\n",
            "Loss: 0.6321461627416092\n",
            "training error 0.03467196325489273, test error 0.0547659334380813\n",
            "Loss: 0.61835091234268\n",
            "training error 0.03466280268651779, test error 0.054671152015516526\n",
            "Loss: 0.4442143672881649\n",
            "training error 0.034664474342706864, test error 0.05467741131113963\n",
            "Loss: 0.45571421699275305\n",
            "training error 0.03469371410005155, test error 0.054874484253534195\n",
            "Loss: 0.8177851838941486\n",
            "training error 0.03471695685304407, test error 0.05468405324215976\n",
            "Loss: 0.4679170610632166\n",
            "training error 0.0346800628871627, test error 0.05463484332423424\n",
            "Loss: 0.37750646309915314\n",
            "training error 0.034652444958374634, test error 0.05481845153592498\n",
            "Loss: 0.7148394420973814\n",
            "training error 0.03464787619487059, test error 0.05468913830525155\n",
            "Loss: 0.4772595597747564\n",
            "training error 0.03464531033109242, test error 0.054665166681729355\n",
            "Loss: 0.43321785216428044\n",
            "training error 0.03466350657230836, test error 0.054748736823813496\n",
            "Loss: 0.5867565459130697\n",
            "training error 0.03470779655218884, test error 0.05462155693075518\n",
            "Loss: 0.3530961240889141\n",
            "training error 0.03466372456515134, test error 0.05469662044474161\n",
            "Loss: 0.4910060713290143\n",
            "training error 0.034656918479255595, test error 0.054794826685004024\n",
            "Loss: 0.6714348401671622\n",
            "training error 0.03466683502785658, test error 0.05485109087854161\n",
            "Loss: 0.7748058595905105\n",
            "training error 0.034681886942944586, test error 0.054748327492159854\n",
            "Loss: 0.5860045040253503\n",
            "training error 0.03466048108768604, test error 0.05476747767756896\n",
            "Loss: 0.621188056182076\n",
            "training error 0.03469528524273284, test error 0.054809974769109523\n",
            "Loss: 0.6992655580330354\n",
            "training error 0.034670482353863406, test error 0.05476818963017539\n",
            "Loss: 0.6224960864243956\n",
            "training error 0.034662981966597894, test error 0.054668948757076306\n",
            "Loss: 0.44016644521376325\n",
            "training error 0.03470298519112514, test error 0.05447454806251073\n",
            "Loss: 0.08300504805585085\n",
            "training error 0.03464178255496006, test error 0.05459203142114694\n",
            "Loss: 0.29885057580503194\n",
            "training error 0.03464299636580695, test error 0.05465424320926683\n",
            "Loss: 0.41314877424638574\n",
            "training error 0.034648382294861356, test error 0.05466521862236522\n",
            "Loss: 0.43331327975582035\n",
            "training error 0.03464251044050453, test error 0.054636677297520954\n",
            "Loss: 0.3808759184547261\n",
            "training error 0.03467444603031563, test error 0.054850149655895124\n",
            "Loss: 0.7730766044754089\n",
            "training error 0.03464288014713249, test error 0.05469659359799424\n",
            "Loss: 0.49095674731902683\n",
            "training error 0.03471853079392651, test error 0.05469474808003355\n",
            "Loss: 0.4875660815871319\n",
            "training error 0.03470810474238483, test error 0.054777116767982734\n",
            "Loss: 0.6388974116306567\n",
            "training error 0.03466569987555105, test error 0.05472989490532405\n",
            "Loss: 0.5521393551264797\n",
            "training error 0.034699292260047025, test error 0.05462314702317021\n",
            "Loss: 0.35601751091745015\n",
            "training error 0.03464538330370005, test error 0.05477120121469396\n",
            "Loss: 0.6280291002683791\n",
            "training error 0.03464547366334785, test error 0.05493225160386917\n",
            "Loss: 0.9239178682544535\n",
            "training error 0.034667646638663396, test error 0.05491545529455031\n",
            "Loss: 0.8930589594587479\n",
            "training error 0.03468103947623221, test error 0.05478789729714516\n",
            "Loss: 0.6587038679108481\n",
            "training error 0.034650299611920954, test error 0.054715437483656454\n",
            "Loss: 0.5255775522808648\n",
            "training error 0.034662924520492, test error 0.05469261535516302\n",
            "Loss: 0.48364774684961365\n",
            "training error 0.03465369270678422, test error 0.05472553420256898\n",
            "Loss: 0.5441276826781083\n",
            "training error 0.034645097219691307, test error 0.05480065480394861\n",
            "Loss: 0.6821425133557879\n",
            "training error 0.03470314115802369, test error 0.05505427121611209\n",
            "Loss: 1.148097598099218\n",
            "training error 0.03467477749862837, test error 0.05478871667567018\n",
            "Loss: 0.6602092657153946\n",
            "training error 0.034662599988208746, test error 0.05478495462338921\n",
            "Loss: 0.6532974599121388\n",
            "training error 0.034647415420196455, test error 0.0548346948287728\n",
            "Loss: 0.7446823250198742\n",
            "training error 0.034626996632126913, test error 0.05482942781543022\n",
            "Loss: 0.7350055394071076\n",
            "training error 0.03464968441580011, test error 0.05488168804842303\n",
            "Loss: 0.8310203086609791\n",
            "training error 0.03467892064614816, test error 0.05486831240431569\n",
            "Loss: 0.80644599452111\n",
            "training error 0.0346746185999809, test error 0.055008635936464934\n",
            "Loss: 1.0642544807950793\n",
            "training error 0.034760522622958465, test error 0.0549102287104441\n",
            "Loss: 0.8834564521945198\n",
            "training error 0.03469484380425737, test error 0.05509824208804034\n",
            "Loss: 1.2288827932716906\n",
            "training error 0.03465125145944131, test error 0.054999325287556355\n",
            "Loss: 1.0471485523411372\n",
            "training error 0.03472610055381346, test error 0.05507143240940363\n",
            "Loss: 1.1796268875732263\n",
            "training error 0.03465294260645731, test error 0.055096500665572284\n",
            "Loss: 1.225683376298825\n",
            "training error 0.03468215265582714, test error 0.05507448278804848\n",
            "Loss: 1.185231175670487\n",
            "training error 0.03463048778415237, test error 0.05495298302726584\n",
            "Loss: 0.9620065395010924\n",
            "training error 0.03467663259397083, test error 0.0548862555767462\n",
            "Loss: 0.8394119700236624\n",
            "training error 0.03467869627730005, test error 0.05503939671635893\n",
            "Loss: 1.1207695212848146\n",
            "training error 0.03464426713173947, test error 0.05503648245270938\n",
            "Loss: 1.1154153095671804\n",
            "training error 0.0346526205223119, test error 0.05505497766985793\n",
            "Loss: 1.1493955255961197\n",
            "training error 0.03463917242986431, test error 0.055100053969021144\n",
            "Loss: 1.2322116596835908\n",
            "training error 0.03466248397540836, test error 0.055095811383964265\n",
            "Loss: 1.2244169981991737\n",
            "training error 0.03464972443490757, test error 0.05508703751776194\n",
            "Loss: 1.2082972702408767\n",
            "training error 0.03464579862863981, test error 0.055195185906879086\n",
            "Loss: 1.4069921866544988\n",
            "training error 0.03461605716745724, test error 0.05504986894947855\n",
            "Loss: 1.1400095626886042\n",
            "training error 0.034690700737057135, test error 0.05518330939462617\n",
            "Loss: 1.385172142651614\n",
            "training error 0.03464981586331946, test error 0.05514174901824221\n",
            "Loss: 1.3088156145595997\n",
            "training error 0.03467290558870401, test error 0.05506539501607869\n",
            "Loss: 1.1685347264060786\n",
            "training error 0.0346625724866815, test error 0.05498597274789265\n",
            "Loss: 1.0226166866883268\n",
            "training error 0.03464899032882135, test error 0.05500546741549615\n",
            "Loss: 1.0584331364903887\n",
            "training error 0.034623735802592655, test error 0.0549910067132802\n",
            "Loss: 1.0318653064789185\n",
            "training error 0.03463701985430095, test error 0.05489713513569073\n",
            "Loss: 0.8594003681195339\n",
            "training error 0.03464848069020103, test error 0.05498231801260462\n",
            "Loss: 1.0159020483243353\n",
            "training error 0.03463874591435598, test error 0.055081732308760255\n",
            "Loss: 1.1985503099047001\n",
            "training error 0.03470843983176139, test error 0.055027221605634336\n",
            "Loss: 1.0984008792042532\n",
            "training error 0.03463319982291373, test error 0.05517747858379904\n",
            "Loss: 1.3744595238194712\n",
            "training error 0.034628238185605724, test error 0.05490766610520993\n",
            "Loss: 0.8787483225870307\n",
            "training error 0.034617321268732876, test error 0.05488487256411178\n",
            "Loss: 0.8368710391597034\n",
            "training error 0.0346170211882325, test error 0.0549903786774888\n",
            "Loss: 1.030711451852917\n",
            "training error 0.034654408410823796, test error 0.05513012971748821\n",
            "Loss: 1.2874681306909164\n",
            "training error 0.034636321788036085, test error 0.05508825261830209\n",
            "Loss: 1.2105297057143138\n",
            "training error 0.034634224747398454, test error 0.054912786895452224\n",
            "Loss: 0.8881564607707837\n",
            "training error 0.03462568646157027, test error 0.05499882398596279\n",
            "Loss: 1.0462275392863596\n",
            "training error 0.03460299783954995, test error 0.05499208841570502\n",
            "Loss: 1.0338526571425533\n",
            "training error 0.03459888338079195, test error 0.054957922299457576\n",
            "Loss: 0.9710811848406387\n",
            "training error 0.034631009173530465, test error 0.054871162644290365\n",
            "Loss: 0.8116825791632865\n",
            "training error 0.03462591778961964, test error 0.054980046023198864\n",
            "Loss: 1.0117278507356886\n",
            "training error 0.0346174365110746, test error 0.05494633303914199\n",
            "Loss: 0.9497888925698028\n",
            "training error 0.03461630271061784, test error 0.05492971763819398\n",
            "Loss: 0.9192623564882707\n",
            "training error 0.034617271244370106, test error 0.055037646713006046\n",
            "Loss: 1.1175543391377385\n",
            "training error 0.03462547819131619, test error 0.05502699165121026\n",
            "Loss: 1.097978396951449\n",
            "training error 0.03462774513617903, test error 0.05488025834129463\n",
            "Loss: 0.828393588645171\n",
            "training error 0.03460400190194599, test error 0.05487797711125601\n",
            "Loss: 0.8242024137645743\n",
            "training error 0.03464419755445998, test error 0.05480284422610495\n",
            "Loss: 0.6861650148086484\n",
            "training error 0.034611421415260246, test error 0.05486397743064757\n",
            "Loss: 0.7984815926684874\n",
            "training error 0.03462523807577357, test error 0.05484329120200439\n",
            "Loss: 0.7604759551853801\n",
            "training error 0.0346385891277614, test error 0.054856259844362515\n",
            "Loss: 0.7843025080391142\n",
            "training error 0.034622075177963674, test error 0.054907695261906656\n",
            "Loss: 0.8788018905362627\n",
            "training error 0.03462795358383144, test error 0.05508327581425602\n",
            "Loss: 1.201386105222313\n",
            "training error 0.03465504507965998, test error 0.0549310865731557\n",
            "Loss: 0.9217774232406883\n",
            "training error 0.03461474769603832, test error 0.055017854860135364\n",
            "Loss: 1.0811918876586724\n",
            "training error 0.03463848139313491, test error 0.05478339674749914\n",
            "Loss: 0.6504352626821364\n",
            "training error 0.03461010423557458, test error 0.05491687558397375\n",
            "Loss: 0.8956683768580831\n",
            "training error 0.03465423189758638, test error 0.054956549772295964\n",
            "Loss: 0.9685595183426043\n",
            "training error 0.03461925359402619, test error 0.05483508668161308\n",
            "Loss: 0.7454022540727356\n",
            "training error 0.03463926758837417, test error 0.0547518916005175\n",
            "Loss: 0.5925526386577173\n",
            "training error 0.03461330759564453, test error 0.05473981833350291\n",
            "Loss: 0.5703711082633456\n",
            "training error 0.03460477172633111, test error 0.05477654904537211\n",
            "Loss: 0.6378543669992576\n",
            "training error 0.03465112556103324, test error 0.05493741728151372\n",
            "Loss: 0.933408475522679\n",
            "training error 0.03461917771877447, test error 0.05493681099624973\n",
            "Loss: 0.932294581910087\n",
            "training error 0.03468830889340234, test error 0.05495492882192003\n",
            "Loss: 0.96558143792842\n",
            "training error 0.034605388916089795, test error 0.054793227703988784\n",
            "Loss: 0.6684971228163583\n",
            "training error 0.03463672660244115, test error 0.05480338083488515\n",
            "Loss: 0.6871508957598138\n",
            "training error 0.03463495859914398, test error 0.05480630384958268\n",
            "Loss: 0.6925211852827795\n",
            "training error 0.03462164283615907, test error 0.054855673995548235\n",
            "Loss: 0.7832261611592894\n",
            "training error 0.034614952179184866, test error 0.05489847302625544\n",
            "Loss: 0.8618583987577644\n",
            "training error 0.03460169247459638, test error 0.05490876881396394\n",
            "Loss: 0.8807742669907803\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU1bn/8c/TPTMsQkQBiTIomCgqshgn6ojRUWJ+4AIoSZTodUkUzS8uaIyiuUm8ibnBXH9ZNC4hiXpRAq5EoyhRoMVIu4AaBVxAHGRwCSAOIsIs/fz+qJqhZ5ilB7qnp6e/79erX1N16lT1U13QT9c5VafM3RERkfwVyXYAIiKSXUoEIiJ5TolARCTPKRGIiOQ5JQIRkTynRCAikueUCKTDMrOvmdlb2Y5DpLNTIpAmmVm5mX09mzG4+7PuPjibMXREFlhlZsuzHYt0DkoEkjVmFs12DLsqS/twLLAXsL+ZfbU939jMCtrz/aR9KBFIm5hZxMymmNk7ZrbBzO43sz2Tlj9gZh+aWaWZLTSzIUnL7jaz281sjpl9BhwfnnlcZWavhevcZ2Zdw/plZlaRtH6zdcPlV5vZB2b2vpldYGZuZl9uZj/2NLO7wrobzexvYfl5ZvbPRnXrt9PEPlwV7m80qf5pZvZaKp/XTjoXeASYE04nxzrEzJ4ys4/N7CMzuy4sj5rZdWEcn5rZEjMbYGYDw/0rSNpGzMwuSPo8njOz35rZBuB6M/uSmc0P92e9mc0ws15J6w8ws4fNbF1Y5w9mVhTGNDSp3l5mtsXM+u7i5yG7SIlA2upSYDxwHLAPsBG4NWn5E8ABBL9YXwZmNFr/O8AvgZ5A3Rfut4HRwCBgGHBeC+/fZF0zGw1cCXwd+DJQ1sp+3AN0B4aEsf62lfrN7cPvgc+AExot/2s43drn1SZm1h34JsHnOgM408yKwmU9gaeBJ8P3+jIwL1z1SmAicBLwBeC7wJYU3/ZIYBXQj2C/DfhV+B4HAwOA68MYosBjwGpgINAfmOXuVcAs4Oyk7U4E5rn7utQ/AckId9dLrx1eQDnw9SbK3wBGJc3vDVQDBU3U7QU4sHs4fzcwvYn3OTtp/tfAHeF0GVCRYt07gV8lLfty+N5fbiKuvYEEsEcTy84D/tmorH47zezDDcCd4XRPgsSwX1s/rxSPy9nAOqAA6ApUAqeFyyYCrzSz3lvAuCbKB4b7V5BUFgMuSPo83mslpvF17wuU1sXXRL0jgfcAC+cXA9/O9r91vVxnBNJm+wGzzewTM/uE4IuuFugXNj9MDZsfNhF8cQP0SVp/TRPb/DBpegvQo4X3b67uPo223dT71BkAfOzuG1uo05LG2/4rcLqZdQFOB15299XhsmY/r8YbNbMnzGxz+Dqrmfc+F7jf3WvcfSvwENubhwYA7zSzXkvLWtNgf82sn5nNMrO14XG+l+3HeACw2t1rGm/E3V8gOGZlZnYQQbJ+dCdjkjRSx4+01Rrgu+7+XOMFZvYfwDiC5plyYHeCphBLqpap4W4/AIqT5ge0UHcNsKeZ9XL3Txot+4ygyQgAM/tiE+s32Ad3X25mq4ExNGwWqnuvJj+vHTbqPqal5WZWTNAEdYSZTQiLuwNdzaxP+F5nNrP6GuBLwNJG5Z8lbWdTON14nxsfs/8Oy4a6+8dmNh74Q9L77GtmBU0lA+B/Cc5qPgQeDJOZZJnOCKQlhWbWNelVANwB/NLM9gMws75mNi6s3xPYBmwg+GL573aM9X7gfDM7OGxH/0lzFd39A4K+jNvMbA8zKzSzY8PF/wKGmNmIsCP6+hTf/6/A5QRX9DyQVN7S59VW/wG8DQwGRoSvA4EKgmahx4C9zWyymXUxs55mdmS47p+BX5jZARYYZma9PWifXwucHZ7RfZcgYbSkJ7AZqDSz/sCPkpa9SJCUp5rZbuG/m5FJy+8FTiNIBtN38nOQNFMikJbMAT5Pel1P0Dn6KPAPM/sUeJ6g7ReC/9irCb5YlofL2oW7PwHcDCwAVia997ZmVvkPgrb6N4F/A5PD7bwN/Jyg03UF2zu0WzOToEN4vruvTypv6fNqq3OB29z9w+QXQbI5190/BU4ETiX4xb0COD5c9zcEyfIfBL/8/wJ0C5ddSPBlvoGg83xRK3H8F/AVgv6Jx4GH6xa4e234/l8m6A+oAM5IWr6G4CICB55t+0cgmVDXaSPSqZjZwQTNIF2aaaKQLDGzO4H33f0/sx2LBJQIpNMws9MIzmK6E7RFJ9x9fHajkmRmNhB4FTjM3d/NbjRSR01D0plcRNDM8w7BlTnfz244kszMfkFwlvY/SgIdS8bOCMLTv1OAf7v7oU0sN4L205MILik7z91fzkgwIiLSrEyeEdxNcAdoc8YQ3IF6ADAJuD2DsYiISDMydh+Buy8M2wObM47gDk0HnjezXma2d3hpX7P69OnjAwe2tFkREWlsyZIl6929yXGdsnlDWX8a3rFYEZbtkAjMbBLBWQP77rsvixcvbpcARUQ6i/CmxyblRGexu09z9xJ3L+nbVwMVioikUzYTwVoaDgNQHJaJiEg7ymYieBQ4J7zd/SigsrX+ARERSb+M9RGY2UyCYYT7WPBwkZ8BhQDufgfBjT8nEQwHsAU4P1OxiIhI8zJ51dDEVpY78INMvb+IiKQmJzqLRUQkc/Q8AulQpi2Zxs8W/Iz1W9bXP8XA3TEzIhYh4Yn6eS3TsnxaVhQt4qv9v8rUUVMpHVDapv9Xrcm5QedKSkpc9xF0HvE1caY8PYWXP3iZz2s+p9Zrsx2SSIdWEClg4XkL25wMzGyJu5c0uc20RCbSRnW//D/87MPWK4tIvZpEDbHyWFrPCpQIpN1NWzKNix67KNthiOSkgkgBZQPL0rvNtG5NJAUPLX8opXpRi2JmHaJ9Vsu0LNvLMtlHoEQg7Sq+Js7i95vv4+lW0C1j/9hFpGlKBNJu4mviHHPnMSRINCgvihRx0oEncfXRV+vLXyQLlAik3UyZN2WHJABwy0m3MOnwSVmISERAN5RJO5m2ZBoLVy/cofwb+39DSUAky5QIpF385eW/7FA2ot8I5v7H3CxEIyLJlAikXRRFixrMG8ZtJ9+WpWhEJJkSgWRcfE2cF9a+UD9vGHeccoc6hkU6CHUWS8bFymPUJoKhIwzjosMvYuFvJ3HZg1AbjijhDmYQiUAisX1ey7RMy4L5oiL46ldh6lQoTfNvKCUCybiygWVEIhESiQRF0SJW/e0c/jEj21GJ5JbPP4eFC+HYY4O/6UwGahqSdlE3uKHj/POfWQ5GJIfV1EAslt5tKhFIxk1fuL1pqKq6hi19Y9kNSCSHFRRAWVmat5nezYk0tGgR3PHbPeBUwAFLwJbe9cvNIBrtGG2wWqZlHXmZ+ggkZz3wADA07BAwgmSw9yv1y7/0JVixIhuRiUgdNQ1JRu05PA77Ptfs8tNPb8dgRKRJOiOQjHrk3ekQCZ+C54BHiC49h569YNIkuPHGrIYnIuiMQDJseUVFw4LVx/CLi0rZuFFJQKSjUCKQjJk2DT7/5AsNyuzjQ9J+xYOI7JqMJgIzG21mb5nZSjOb0sTy/cxsnpm9ZmYxMyvOZDzSvh54AFgxJphJGNQWccf3z0n7FQ8ismsylgjMLArcCowBDgEmmtkhjardBEx392HAz4FfZSoeaX8nnwxs/FIw869zubpfjEljlAVEOppMnhEcAax091XuXgXMAsY1qnMIMD+cXtDEcslhZ5wBFH0GwLnDvseNlygJiHREmUwE/YE1SfMVYVmyfwF1FxCeBvQ0s95Ip7BlC1C0GYDxJ/XIbjAi0qxsdxZfBRxnZq8AxwFrgdrGlcxskpktNrPF69ata+8YZSdt2QIc+CgAM5c8lt1gRKRZmUwEa4EBSfPFYVk9d3/f3U9398OAH4dlnzTekLtPc/cSdy/p27dvBkOWdPqfedPgsLsAuH/dT7jm/mlZjkhEmpLJRPAScICZDTKzIuBM4NHkCmbWx8zqYrgWuDOD8Ug7e/K9hxrMP/zmQ83UFJFsylgicPca4BJgLvAGcL+7LzOzn5vZ2LBaGfCWmb0N9AN+mal4pH3F47Dh2QnBTHhj8ekHTcheQCLSrIwOMeHuc4A5jcp+mjT9IPBgJmOQ7IjFgJcnwcgboHArx9bewI3fnpTtsESkCdnuLJZOqqwMbN84FG7D1h/K1G8pCYh0VBp0TjKjOI6fWwZWhfXYAMVxQPcRiHREOiOQjIiVx0hYFQBOglh5LLsBiUizlAgkI8oGltVPRyzaYF5EOhYlAsmMiu3NQLUfF/P60izGIiItUiKQjJg+P759plc5//fF44mviTe/gohkjRKBZMSyz2L19w9gUOtV6icQ6aCUCCQj1i8pA7fw8ZQQ8SL1E4h0UEoEkhFfH1wK75fA5n6w+GKu6ruA0gG6fFSkI1IikIwoKwOqe9Cz+kD+OPZ2PYtApANTIpCM2LIFiNQw5OACJummYpEOTYlAMqIuERRGo9kORURaoUQgGVGXCIoKNYqJSEenRCAZ8eabgNWyZbMSgUhHp0QgaRePw1/+AkRqeCFeQFz3kYl0aEoEknaxGNTWApEaEjUFwbMJRKTDUiKQtCsrg2gUiNQQpSC4lFREOiwlAkm70lIYNw4sWsPXRxVQqlsIRDo0JQLJiL59wQpq+OJeunxUpKNTIpCMqOsjKIjoqiGRjk6JQDKipga84HNe/+h1DT8t0sEpEUhGfBCN40Ubeen9lxg1fZSSgUgHpkQgGfF+l/kAOM622m16FoFIB6ZEIBnhiQhYMJ3wBL27985uQCLSrIwmAjMbbWZvmdlKM5vSxPJ9zWyBmb1iZq+Z2UmZjEfaz6cF5fXTESJs2LIhe8GISIsylgjMLArcCowBDgEmmtkhjar9J3C/ux8GnAnclql4pH31+PxgACIWoUtBFz2dTKQDy+QZwRHASndf5e5VwCxgXKM6DnwhnN4deD+D8Ug76vr5/gB8v+T7zDtnnp5OJtKBZfIi7/7AmqT5CuDIRnWuB/5hZpcCuwFfb2pDZjYJmASw7777pj1QSb9q3woEiWDIXkOyHI2ItCTbncUTgbvdvRg4CbjHzHaIyd2nuXuJu5f07du33YOUtqsmSARdC7pmORIRaU0mE8FaYEDSfHFYlux7wP0A7h4HugJ9MhiTtIN4HD5ar0QgkisymQheAg4ws0FmVkTQGfxoozrvAaMAzOxggkSwLoMxSYbF43DCCbChMkgEr7+iRCDS0WUsEbh7DXAJMBd4g+DqoGVm9nMzGxtW+yFwoZn9C5gJnOfunqmYJPNiMdi6FSgIEsELzykRiHR0GR0RzN3nAHMalf00aXo5MDKTMUj7qn/2wJ4rANjc82Xga9kKR0RSkO3OYulkSkuh+4FxGHE3ONz00f9h2hMaZ0ikI1MikLSrKY5BpDYYYiJSxUNLYlmOSERaokQgadflgzLwSHC7YKKICYeXZTkiEWmJEoGkXfePS+G9kUS29uOPI+cxaYzuKhbpyJQIJO3MgKLNFNKVoYdmOxoRaY0SgaRdVb84fPEVtnVdrYfSiOQAJQJJu6p9YmAOBlW1VXoojUgHp0QgaVf0fhlg4EZRtEhDUIt0cEoEknZFH5XClt70+PQrGoJaJAcoEUhmeIQvbC5REhDJAUoEknZmQLSaiBdmOxQRSYESgWRGpBpTIhDJCUoEkhk6IxDJGUoEknZmBGcEmR3cVkTSRIlA0q42kYBIQk1DIjlCiUDSrtarAdQ0JJIjlAgk7WpRIhDJJa0mAjM71cyUMCRlOiMQyS2pfMGfAawws1+b2UGZDkhyX02iBgBDiUAkF7SaCNz9bOAw4B3gbjOLm9kkM+uZ8egkJ1V/MRhtdGvRmixHIiKpSKnJx903AQ8Cs4C9gdOAl83s0gzGJjkoviZO9fhvA1C+901MWzItyxGJSGtS6SMYa2azgRhQCBzh7mOA4cAPMxue5JpYeQyi24IZS3DJnEv0PAKRDi6VM4IJwG/dfai7/4+7/xvA3bcA38todJJzygaWBc8rBjCo9Vo9j0Ckg0slEVwPvFg3Y2bdzGwggLvPa2lFMxttZm+Z2Uozm9LE8t+a2avh620z+6RN0UuHUzqgFN46OZhJRCi0LnoegUgHl0oieABIJM3XhmUtMrMocCswBjgEmGhmhyTXcfcr3H2Eu48AbgEeTjVw6bgKPj4smFjwC/x/50GFhqIW6chSSQQF7l5VNxNOF6Ww3hHASndfFa4zCxjXQv2JwMwUtisdXNfuNVBbAM9eR215KbFYtiMSkZakkgjWmdnYuhkzGwesT2G9/kDy9YMVYdkOzGw/YBAwv5nlk8xssZktXrduXQpvLdkSj8PmLTXgUQAKCqCsLLsxiUjLUkkEFwPXmdl7ZrYGuAa4KM1xnAk86O61TS1092nuXuLuJX379k3zW0s6xWKA1UCiADM4/3woVcuQSIfW6jjB7v4OcJSZ9QjnN6e47bXAgKT54rCsKWcCP0hxu9KBlZUBC2shUUDXrnDOOdmOSERak9KA8WZ2MjAE6GpmALj7z1tZ7SXgADMbRJAAzgS+08S2DwL2AHSxeSdQWgq99qzhUytg3jydDYjkglRuKLuDYLyhSwEDvgXs19p67l4DXALMBd4A7nf3ZWb28+Q+B4IEMcvdfSfilw4oWlhDQaRASUAkR6RyRnC0uw8zs9fc/b/M7P8BT6SycXefA8xpVPbTRvPXpxqsdHyLFsGmzTVYQk8nE8kVqXQWbw3/bjGzfYBqgvGGRBqIx4M+guraGqo+LyCuxj6RnJBKIvi7mfUC/gd4GSgH/prJoCQ3xWJQXQ1EgquGdP+ASG5o8fw9fCDNPHf/BHjIzB4Durp7ZbtEJzmld+9wIlIDiej2eRHp0Fo8I3D3BMEwEXXz25QEpDkbNoQTFlw+Wj8vIh1aKk1D88xsgtVdNyrSjPo7iMOmId1RLJIbUkkEFxEMMrfNzDaZ2admtinDcUkOKi2FoiIgUkNRoS4fFckVqdxZrEdSSsqiUSBSQ9R0+ahIrmj1f6uZHdtUubsvTH84kuvMgG7rqe7yb+Jr4sHzCUSkQ0vlZ9uPkqa7EgwvvQQ4ISMRSU5L9I/DPi9TYwlGTR/FvHPmKRmIdHCpNA2dmjxvZgOA32UsIslZ8ThU7xMDS4BBVW0VsfKYEoFIB5dKZ3FjFcDB6Q5Ecls8DiecALXvlAXPLHYoihbpMZUiOSCVPoJbgLoB4SLACII7jEXqxWKwdSvBYyk37k/ECpl31V90NiCSA1LpI1icNF0DzHT35zIUj+SoBvcMJAqJfHxokBQGNLeGiHQUqSSCB4GtdU8PM7OomXV39y2ZDU1ySYN7BqLbqNlWxKhR6JkEIjkgpTuLgW5J892ApzMTjnQK0Sqo6UJVFRp4TiQHpJIIuiY/njKc7p65kCTnFWzDEkUUFenB9SK5IJVE8JmZfaVuxswOBz7PXEiS6wq6VFF6RBc1C4nkiFT6CCYDD5jZ+wSPqvwiwaMrRZoWrWLkUUVKAiI5IpUbyl4KHzA/OCx6y92rMxuW5LJattEl2iXbYYhIilJ5eP0PgN3cfam7LwV6mNn/zXxokpOsFrcEz1c8T3yNnlUpkgtS6SO4MHxCGQDuvhG4MHMhSU7bLxiLcN678xg1fZSSgUgOSCURRJMfSmNmUaAocyFJLqp/UP2g+QA4zraaYKwhEenYUuksfhK4z8z+GM5fBDyRuZAkF02fHk78+9DgbyJCNKKxhkRyQSpnBNcA84GLw9frNLzBrFlmNtrM3jKzlWY2pZk63zaz5Wa2zMz+mmrg0nHE43DnneFM16AV0d4ZzR+O0BDUIrmg1UQQPsD+BaCc4FkEJwBvtLZe2IR0KzAGOASYaGaHNKpzAHAtMNLdhxBcqio5JhaDqiqgOA5jLgOgcPA8hh6a1bBEJEXNJgIzO9DMfmZmbwK3AO8BuPvx7v6HFLZ9BLDS3Ve5exUwCxjXqM6FwK1hBzTu/u+d2QnJrvq7hwfGIBJcWVzrNeofEMkRLZ0RvEnw6/8Udz/G3W8Batuw7f7AmqT5irAs2YHAgWb2nJk9b2aj27B96SDqbxwrLwMPup0KTP0DIrmipURwOvABsMDM/mRmowjuLE6nAuAAoAyYCPzJzHo1rmRmk8xssZktXrduXZpDkLSpKIXyr0FtlDH2O/UPiOSIZhOBu//N3c8EDgIWELTf72Vmt5vZN1LY9loajkZfHJYlqwAedfdqd38XeJsgMTSOZZq7l7h7Sd++fVN4a8mK4jgMegYitTzhk3UPgUiOSKWz+DN3/2v47OJi4BWCK4la8xJwgJkNMrMi4Ezg0UZ1/kZwNoCZ9SFoKlqVevjSoQyMgdWCQY3rHgKRXNGmZxa7+8bw1/moFOrWAJcAcwmuMrrf3ZeZ2c/NbGxYbS6wwcyWE5x1/MjdN7RtF6TDKC8DD1oP9bxikdyRyg1lO83d5wBzGpX9NGnagSvDl+S6ilLY2oue3btx0+ifqY9AJEe06YxApEXFcei2kU/5gMlPqo9AJFcoEUj6DIyFE05VrfoIRHKFEoGkz+pjwwuMTX0EIjlEiUDS5/0SAA6InMi8czTOkEiuUCKQ9InUANBn0yglAZEcokQg6TPgOQBeWL52+/MJRKTDUyKQ9CiOw8RgTMHE4bczfb4ygUiuUCKQXRaPE1wxVLA1KLDapCuIRKSjUyKQXRaLEdxVHA5JGLEo5xxblr2ARKRNlAhklx13HMFdxaFz9r9GncUiOUSJQHbZkUc2nB81YnB2AhGRnaJEILusttHjimoTbXl+kYhkmxKB7LIdEoErEYjkEiUC2WU6IxDJbUoEsst0RiCS25QIZJfpjEAktykRyC5rnAgWrl6YnUBEZKcoEcguq60FvjKtfv7+5fczbcm05lcQkQ5FiUB22YsvAkf9tkHZX17+S3aCEZE2UyKQXTbzn3Ho82aDsn167pOlaESkrZQIZJdV7xPb/i/JwYhw9cirsxmSiLSBEoHssn379t4+Y/CjkVdprCGRHFKQ7QDa0zXXwK23wtatYAaRMA1Go1BQAFVVUFMTLANw314vkdg+r2UNl9WUboBRwbKIRejVpVd6D5yIZFTeJIJrroFf/zrbUXRS75aBAxgR76KH1ovkmIw2DZnZaDN7y8xWmtmUJpafZ2brzOzV8HVBpmK5775MbVlYe2TwLIJ3j6ffXD20XiTXZCwRmFkUuBUYAxwCTDSzQ5qoep+7jwhff85UPCeckKktCwWfB39riygry2okIrITMtk0dASw0t1XAZjZLGAcsDyD79msH/wA7roLunTZ3g9QU9OwTjQa/O0obe+5sqx2/1jQMvTluTxc+AzxNTorEMklmUwE/YE1SfMVwJFN1JtgZscCbwNXuPuaxhXMbBIwCWDffffdqWCqq4O/s2fDmDF1221YZ9Ys+OY3d2rzee2Hc+fzm+cBc6pqq4iVx5QIRHJIti8f/Tsw0N2HAU8B/9tUJXef5u4l7l7St2/fnXqjukRQWNh8nUi2P40cdVCfg4DgiqGiaJE6i0VyTCa/+tYCA5Lmi8Oyeu6+wd23hbN/Bg7PVDBKBJnz6bZPARg3eBzzzlGzkEiuyeRX30vAAWY2yMyKgDOBR5MrmNneSbNjgTcyFUwqiaBxU5G0Lr4mzrXzrwXgiZVPZDkaEdkZGUsE7l4DXALMJfiCv9/dl5nZz81sbFjtMjNbZmb/Ai4DzstUPDojyIxYeYzq2uDDresfEJHcktEbytx9DjCnUdlPk6avBa7NZAx1lAgyo3f33nhwzRAJT9C7e+9W1hCRjiZvvvrUNJQZG7ZsqJ+OEGkwLyK5IW+GmFi+KQ7nTuHwh1+i5qEqzAx+DJiDG3iEsS8lYLEHywD3YDpiERKeqJ/Xsu3Lkh9LWRgt1BVDIjkoLxJBfE2c/1o7EgY62xJhobPD3tfWlXtSoTeso2XNLIP6vgIRyS150TQUdGB6MB5OMmv0kl2SIKHOYpEclBeJoGxgGUa06V+4yS/ZJVGLqmlIJAflRdNQ6YBSvvXps9y/fgpFg16i1oI+gpptNOgjiBYlwDpG23suLYtYhIP7HMztJ9+um8lEclBeJIJ4HGbfXArVzxDtBrF5UFq641VCTy9Ao2eKSN7Ji6ahWGz7SKNVVcF8U3QfgYjko7z46isrg65dg2Gmi4qa/9Wv+whEJB/lRdNQaSnMmxecCZSVBfPJ6sbV1xmBiOSjvEgEEHz5N04AdaJRJQIRyV95kwhaUpcA1DQknVF1dTUVFRVs3bo126FIO+jatSvFxcUUtjSeTiNKBGxPBDojkM6ooqKCnj17MnDgwPpLgKVzcnc2bNhARUUFgwYNSnk9ffWhRCCd29atW+ndu7eSQB4wM3r37t3msz999bHjQ+tFOhslgfyxM8daiQCdEYhIftNXH0oEIpm0YcMGRowYwYgRI/jiF79I//796+erqqpaXHfx4sVcdtllrb7H0Ucfna5wAZg8eTL9+/cnkUi0XrkTUGcx25uEdPYsEojHm7/vpq169+7Nq6++CsD1119Pjx49uOqqq+qX19TUUFDQ9FdRSUkJJSUlrb7HokWLdi3IJIlEgtmzZzNgwACeeeYZjj/++LRtO1lL+93eOkYUWVaXAHRGIJ3d5MkQfic3q7ISXntt+701w4bB7rs3X3/ECPjd79oWx3nnnUfXrl155ZVXGDlyJGeeeSaXX345W7dupVu3btx1110MHjyYWCzGTTfdxGOPPcb111/Pe++9x6pVq3jvvfeYPHly/dlCjx492Lx5M7FYjOuvv54+ffqwdOlSDj/8cO69917MjDlz5nDllVey2267MXLkSFatWsVjjz22Q2yxWIwhQ4ZwxhlnMHPmzPpE8NFHH3HxxRezatUqAG6//XaOPvpopk+fzk033YSZMWzYMO655x7OO+88TjnlFL75zW/uEN9PfvIT9thjD958803efvttxo8fz5o1a9i6dSuXX345kyZNAuDJJ5/kuuuuo7a2lj59+vDUU08xePBgFi1aRN++fUkkEhx44IHE43H69u3btgPQiBIB4OEQ1DojEAkSQV2LSCIRzLeUCHZWRUUFixYtIhqNsmnTJsfG9RcAABE/SURBVJ599lkKCgp4+umnue6663jooYd2WOfNN99kwYIFfPrppwwePJjvf//7O1wv/8orr7Bs2TL22WcfRo4cyXPPPUdJSQkXXXQRCxcuZNCgQUycOLHZuGbOnMnEiRMZN24c1113HdXV1RQWFnLZZZdx3HHHMXv2bGpra9m8eTPLli3jhhtuYNGiRfTp04ePP/641f1++eWXWbp0af3lnXfeeSd77rknn3/+OV/96leZMGECiUSCCy+8sD7ejz/+mEgkwtlnn82MGTOYPHkyTz/9NMOHD9/lJABKBA3ojEA6u1R+ucfjMGpUMEBjURHMmLHrzUNN+da3vkU0vGSvsrKSc889lxUrVmBmVFc3/bS7k08+mS5dutClSxf22msvPvroI4qLixvUOeKII+rLRowYQXl5OT169GD//fev//KdOHEi06ZN22H7VVVVzJkzh9/85jf07NmTI488krlz53LKKacwf/58pk+fDkA0GmX33Xdn+vTpfOtb36JPnz4A7Lnnnq3u9xFHHNHgGv+bb76Z2bNnA7BmzRpWrFjBunXrOPbYY+vr1W33u9/9LuPGjWPy5MnceeednH/++a2+XyqUCFDTkEiy1sbmSpfddtutfvonP/kJxx9/PLNnz6a8vJyyZkaG7NKlS/10NBqlpm5Y4TbWac7cuXP55JNPGDp0KABbtmyhW7dunHLKKSlvA6CgoKC+ozmRSDToFE/e71gsxtNPP008Hqd79+6UlZW1eA/AgAED6NevH/Pnz+fFF19kxowZbYqrOfrqS6KmIZFAaSlce23mkkBjlZWV9O/fH4C777477dsfPHgwq1atory8HID77ruvyXozZ87kz3/+M+Xl5ZSXl/Puu+/y1FNPsWXLFkaNGsXtt98OQG1tLZWVlZxwwgk88MADbNiwAaC+aWjgwIEsWbIEgEcffbTZM5zKykr22GMPunfvzptvvsnzzz8PwFFHHcXChQt59913G2wX4IILLuDss89ucEa1qzKaCMxstJm9ZWYrzWxKC/UmmJmbWeuXB2SQzghEsuPqq6/m2muv5bDDDmvTL/hUdevWjdtuu43Ro0dz+OGH07NnT3Zv1PGxZcsWnnzySU4++eT6st12241jjjmGv//97/z+979nwYIFDB06lMMPP5zly5czZMgQfvzjH3PccccxfPhwrrzySgAuvPBCnnnmGYYPH048Hm9wFpBs9OjR1NTUcPDBBzNlyhSOOuooAPr27cu0adM4/fTTGT58OGeccUb9OmPHjmXz5s1paxYCMPfMPKzXzKLA28CJQAXwEjDR3Zc3qtcTeBwoAi5x98UtbbekpMQXL26xShtiDP726QPr18M778D++6dl0yIdxhtvvMHBBx+c7TCybvPmzfTo0QN35wc/+AEHHHAAV1xxRbbDarPFixdzxRVX8OyzzzZbp6ljbmZL3L3JH9uZ/A18BLDS3Ve5exUwCxjXRL1fADcCWR8aUU1DIp3Xn/70J0aMGMGQIUOorKzkoosuynZIbTZ16lQmTJjAr371q7RuN5OJoD+wJmm+IiyrZ2ZfAQa4++MtbcjMJpnZYjNbvG7durQHWtd8p6Yhkc7riiuu4NVXX2X58uXMmDGD7t27ZzukNpsyZQqrV6/mmGOOSet2s/bVZ2YR4DfAD1ur6+7T3L3E3UvScc0sBJfI1amsDP4qEYhIPsrkV99aYEDSfHFYVqcncCgQM7Ny4Cjg0fbqMG7qAfZqGhKRfJTJRPAScICZDTKzIuBM4NG6he5e6e593H2guw8EngfGttZZnC5lZdCt2/YhqEFnBCKSnzL21efuNcAlwFzgDeB+d19mZj83s7GZet9U1d0084tfbL99XolARPJRRu8sdvc5wJxGZT9tpm5ZJmNpSt0D7X/zm2BeTUMi6bdhwwZGjRoFwIcffkg0Gq0fH+fFF1+kqKioxfVjsRhFRUUtDjU9fvx4Pvzww/obsqRtNMREEiUCkUB8TZxYeYyygWWUDti124tbG4a6NbFYjB49ejSbCD755BOWLFlCjx49WLVqFftn6GagjjRsdLp1zr3aSUoE0tlNfnIyr37Y8jjUldsqee2j10h4gohFGNZvGLt3aX740RFfHMHvRrdtHOolS5Zw5ZVXsnnzZvr06cPdd9/N3nvvzc0338wdd9xBQUEBhxxyCFOnTuWOO+4gGo1y7733csstt/C1r32twbYefvhhTj31VPr168esWbO47rrrAFi5ciUXX3wx69atIxqN8sADD/ClL32JG2+8kXvvvZdIJMKYMWOYOnUqZWVl3HTTTZSUlLB+/XpKSkooLy/n7rvv5uGHH2bz5s3U1tby+OOPM27cODZu3Eh1dTU33HAD48YFt0c1Ho76tttuY9iwYbz99tsUFhayadMmhg8fXj/fkSgRiEgDlVsrSXg4YJonqNxa2WIiaCt359JLL+WRRx6hb9++3Hffffz4xz/mzjvvZOrUqbz77rt06dKFTz75hF69enHxxRe3eBYxc+ZMfvrTn9KvXz8mTJhQnwjOOusspkyZwmmnncbWrVtJJBI88cQTPPLII7zwwgt079495WGjX3vtNfbcc09qamqYPXs2X/jCF1i/fj1HHXUUY8eOZfny5TsMR92zZ0/Kysp4/PHHGT9+PLNmzeL000/vcEkAlAhE8koqv9zja+KMmj6KqtoqiqJFzDh9xi43DyXbtm0bS5cu5cQTTwSCAdz23ntvAIYNG8ZZZ53F+PHjGT9+fKvb+uijj1ixYgXHHHMMZkZhYSFLly5lv/32Y+3atZx22mkAdO3aFYCnn36a888/v/5mslSGjT7xxBPr67k71113HQsXLiQSibB27Vo++ugj5s+f3+Rw1BdccAG//vWvGT9+PHfddRd/+tOf2vJRtRslAhFpoHRAKfPOmZe2PoLG3J0hQ4YQT76rM/T444+zcOFC/v73v/PLX/6S119/vcVt3X///WzcuLF+3P5NmzYxc+ZMpkxpdozLJiUPG914GOjkAeNmzJjBunXrWLJkCYWFhQwcOLDFYaNHjhxJeXk5sViM2tpaDj300DbF1V50wSTbh5h46aXsxiHSUZQOKOXar12b9iQAwfMC1q1bV58IqqurWbZsGYlEgjVr1nD88cdz4403UllZyebNm+nZsyeffvppk9uaOXMmTz75ZP2w0UuWLGHWrFn07NmT4uJi/va3vwHBWciWLVs48cQTueuuu9iyZQvQ9LDRDz74YLOxV1ZWstdee1FYWMiCBQtYvXo1QLPDUQOcc845fOc730nraKHplveJIB6HTZuC6QkTGg49ISLpF4lEePDBB7nmmmsYPnw4I0aMYNGiRdTW1nL22WczdOhQDjvsMC677DJ69erFqaeeyuzZsxkxYkSDETfLy8tZvXp1/dDNAIMGDWL33XfnhRde4J577uHmm29m2LBhHH300Xz44YeMHj2asWPHUlJSwogRI7jpppsAuOqqq7j99ts57LDDWL9+fbOxn3XWWSxevJihQ4cyffp0DjroIIBmh6OuW2fjxo0tPh4z2zI2DHWmpHMYaoBf/Qr+8z+DZ7NGo8ENZtdem7bNi2SdhqHOrgcffJBHHnmEe+65p93es63DUOd9H0FZGXTpsv35rM08IU9EpM0uvfRSnnjiCebMmdN65SzK+0TQXs9nFZH8c8stt2Q7hJTkfSKA7UNNiHRW7o7pjsm8sDPN/XnfWSzS2XXt2pUNGzbs1BeE5BZ3Z8OGDfX3TaRKZwQinVxxcTEVFRVk4ul+0vF07dqV4uLiNq2jRCDSyRUWFtbfcCXSFDUNiYjkOSUCEZE8p0QgIpLncu7OYjNbB6zeydX7AM3fP945aZ/zg/Y5P+zKPu/n7n2bWpBziWBXmNni5m6x7qy0z/lB+5wfMrXPahoSEclzSgQiInku3xLBtGwHkAXa5/ygfc4PGdnnvOojEBGRHeXbGYGIiDSiRCAikufyJhGY2Wgze8vMVppZ255s3UGZ2QAzW2Bmy81smZldHpbvaWZPmdmK8O8eYbmZ2c3hZ/CamX0lu3uw88wsamavmNlj4fwgM3sh3Lf7zKwoLO8Szq8Mlw/MZtw7y8x6mdmDZvammb1hZqWd/Tib2RXhv+ulZjbTzLp2tuNsZnea2b/NbGlSWZuPq5mdG9ZfYWbntjWOvEgEZhYFbgXGAIcAE83skOxGlRY1wA/d/RDgKOAH4X5NAea5+wHAvHAegv0/IHxNAm5v/5DT5nLgjaT5G4HfuvuXgY3A98Ly7wEbw/LfhvVy0e+BJ939IGA4wb532uNsZv2By4ASdz8UiAJn0vmO893A6EZlbTquZrYn8DPgSOAI4Gd1ySNl7t7pX0ApMDdp/lrg2mzHlYH9fAQ4EXgL2Dss2xt4K5z+IzAxqX59vVx6AcXhf5ATgMcAI7jbsqDx8QbmAqXhdEFYz7K9D23c392BdxvH3ZmPM9AfWAPsGR63x4D/0xmPMzAQWLqzxxWYCPwxqbxBvVReeXFGwPZ/VHUqwrJOIzwVPgx4Aejn7h+Eiz4E+oXTneVz+B1wNZAI53sDn7h7TTifvF/1+xwurwzr55JBwDrgrrA57M9mthud+Di7+1rgJuA94AOC47aEzn2c67T1uO7y8c6XRNCpmVkP4CFgsrtvSl7mwU+ETnONsJmdAvzb3ZdkO5Z2VAB8Bbjd3Q8DPmN7cwHQKY/zHsA4giS4D7AbOzahdHrtdVzzJRGsBQYkzReHZTnPzAoJksAMd384LP7IzPYOl+8N/Dss7wyfw0hgrJmVA7MImod+D/Qys7oHLSXvV/0+h8t3Bza0Z8BpUAFUuPsL4fyDBImhMx/nrwPvuvs6d68GHiY49p35ONdp63Hd5eOdL4ngJeCA8IqDIoJOp0ezHNMus+Bp5H8B3nD33yQtehSou3LgXIK+g7ryc8KrD44CKpNOQXOCu1/r7sXuPpDgOM5397OABcA3w2qN97nus/hmWD+nfjm7+4fAGjMbHBaNApbTiY8zQZPQUWbWPfx3XrfPnfY4J2nrcZ0LfMPM9gjPpL4RlqUu2x0l7dghcxLwNvAO8ONsx5OmfTqG4LTxNeDV8HUSQdvoPGAF8DSwZ1jfCK6eegd4neCKjKzvxy7sfxnwWDi9P/AisBJ4AOgSlncN51eGy/fPdtw7ua8jgMXhsf4bsEdnP87AfwFvAkuBe4Aune04AzMJ+kCqCc78vrczxxX4brjvK4Hz2xqHhpgQEclz+dI0JCIizVAiEBHJc0oEIiJ5TolARCTPKRGIiOQ5JQKRkJnVmtmrSa+0jVJrZgOTR5gU6UgKWq8ikjc+d/cR2Q5CpL3pjECkFWZWbma/NrPXzexFM/tyWD7QzOaHY8PPM7N9w/J+ZjbbzP4Vvo4ONxU1sz+FY+z/w8y6hfUvs+CZEq+Z2aws7abkMSUCke26NWoaOiNpWaW7DwX+QDD6KcAtwP+6+zBgBnBzWH4z8Iy7DycYE2hZWH4AcKu7DwE+ASaE5VOAw8LtXJypnRNpju4sFgmZ2WZ379FEeTlwgruvCgf5+9Dde5vZeoJx46vD8g/cvY+ZrQOK3X1b0jYGAk958LARzOwaoNDdbzCzJ4HNBENH/M3dN2d4V0Ua0BmBSGq8mem22JY0Xcv2PrqTCcaQ+QrwUtLomiLtQolAJDVnJP2Nh9OLCEZABTgLeDacngd8H+qfrbx7cxs1swgwwN0XANcQDJ+8w1mJSCbpl4fIdt3M7NWk+Sfdve4S0j3M7DWCX/UTw7JLCZ4a9iOCJ4idH5ZfDkwzs+8R/PL/PsEIk02JAveGycKAm939k7TtkUgK1Ecg0oqwj6DE3ddnOxaRTFDTkIhIntMZgYhIntMZgYhInlMiEBHJc0oEIiJ5TolARCTPKRGIiOS5/w8qp6+ttKWU/gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9722222222222222 0.05490876881396394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test error, mean test error and std test error')\n",
        "print(error_vals)\n",
        "print(np.mean(error_vals), np.std(error_vals)) \n",
        "\n",
        "print('Test accuracy, mean test accuracy and std test accuracy')\n",
        "print(acc_vals)\n",
        "print(np.mean(acc_vals), np.std(acc_vals)) \n",
        "\n",
        "print('Train error, mean train error and std train error')\n",
        "\n",
        "print(error_trains)\n",
        "print(np.mean(error_trains), np.std(error_trains)) \n",
        "\n",
        "print('Train accuracy, mean train accuracy and std train accuracy')\n",
        "\n",
        "print(acc_trains)\n",
        "print(np.mean(acc_trains), np.std(acc_trains)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSocgJedpZH7",
        "outputId": "1701534e-2963-4dde-91e8-5db194b6ee09"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test error, mean test error and std test error\n",
            "[0.05569979682810079, 0.05438955805294634, 0.055769960453988604, 0.05404407414707001, 0.05468976923285538, 0.053269746948341384, 0.054916627620149296, 0.05574340932275785, 0.055162376864465866, 0.05490876881396394]\n",
            "0.05485940882846394 0.0007652219308914056\n",
            "Test accuracy, mean test accuracy and std test accuracy\n",
            "[0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222]\n",
            "0.9722222222222221 1.1102230246251565e-16\n",
            "Train error, mean train error and std train error\n",
            "[0.03475771288730631, 0.03452435798531528, 0.03483671767254945, 0.03447597542152125, 0.034568680537784696, 0.034402489706690326, 0.03457347987986513, 0.034778642656996935, 0.03458475644002566, 0.03460169247459638]\n",
            "0.03461045056626515 0.0001318192889368122\n",
            "Train accuracy, mean train accuracy and std train accuracy\n",
            "[0.9344262295081968, 0.9344262295081968, 0.9344262295081968, 0.9344262295081968, 0.9344262295081968, 0.9344262295081968, 0.9344262295081968, 0.9344262295081968, 0.9344262295081968, 0.9344262295081968]\n",
            "0.9344262295081969 1.1102230246251565e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6WvlbC3EkWo6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "88-OAVxXT_W8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nhWIKA1Hm4lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPuZxtJ1m4nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBnisThGm4ql"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}